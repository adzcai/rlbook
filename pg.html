<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Policy Gradient Methods – An Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./imitation_learning.html" rel="next">
<link href="./fitted_dp.html" rel="prev">
<link href="./shared/184.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e39e07555a3cad66df478e90649eca5f.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="7&nbsp; Policy Gradient Methods – An Introduction to Reinforcement Learning">
<meta property="og:description" content="">
<meta property="og:image" content="https://rlbook.adzc.ai/pg_files/figure-html/fig-exm-grad-descent-output-1.png">
<meta property="og:site_name" content="An Introduction to Reinforcement Learning">
<meta property="og:image:height" content="308">
<meta property="og:image:width" content="414">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./pg.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Introduction to Reinforcement Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/adzcai/rlbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proofs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Proofs</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">7.1</span> Introduction</a></li>
  <li><a href="#sec-parameterizations" id="toc-sec-parameterizations" class="nav-link" data-scroll-target="#sec-parameterizations"><span class="header-section-number">7.2</span> Parameterized policies</a></li>
  <li><a href="#sec-gd" id="toc-sec-gd" class="nav-link" data-scroll-target="#sec-gd"><span class="header-section-number">7.3</span> Gradient descent</a>
  <ul class="collapse">
  <li><a href="#sec-computing-derivatives" id="toc-sec-computing-derivatives" class="nav-link" data-scroll-target="#sec-computing-derivatives"><span class="header-section-number">7.3.1</span> Computing derivatives</a></li>
  <li><a href="#sec-sgd" id="toc-sec-sgd" class="nav-link" data-scroll-target="#sec-sgd"><span class="header-section-number">7.3.2</span> Stochastic gradient descent</a></li>
  </ul></li>
  <li><a href="#sec-pg-intro" id="toc-sec-pg-intro" class="nav-link" data-scroll-target="#sec-pg-intro"><span class="header-section-number">7.4</span> Policy (stochastic) gradient descent</a>
  <ul class="collapse">
  <li><a href="#sec-pg-intuition" id="toc-sec-pg-intuition" class="nav-link" data-scroll-target="#sec-pg-intuition"><span class="header-section-number">7.4.1</span> Introduction to policy gradient methods</a></li>
  <li><a href="#sec-reinforce" id="toc-sec-reinforce" class="nav-link" data-scroll-target="#sec-reinforce"><span class="header-section-number">7.4.2</span> The REINFORCE policy gradient</a></li>
  <li><a href="#sec-baselines" id="toc-sec-baselines" class="nav-link" data-scroll-target="#sec-baselines"><span class="header-section-number">7.4.3</span> Baselines and advantages</a></li>
  </ul></li>
  <li><a href="#comparing-policy-gradient-algorithms-to-policy-iteration" id="toc-comparing-policy-gradient-algorithms-to-policy-iteration" class="nav-link" data-scroll-target="#comparing-policy-gradient-algorithms-to-policy-iteration"><span class="header-section-number">7.5</span> Comparing policy gradient algorithms to policy iteration</a></li>
  <li><a href="#sec-trpo" id="toc-sec-trpo" class="nav-link" data-scroll-target="#sec-trpo"><span class="header-section-number">7.6</span> Trust region policy optimization</a></li>
  <li><a href="#sec-npg" id="toc-sec-npg" class="nav-link" data-scroll-target="#sec-npg"><span class="header-section-number">7.7</span> Natural policy gradient</a></li>
  <li><a href="#sec-ppo-kl" id="toc-sec-ppo-kl" class="nav-link" data-scroll-target="#sec-ppo-kl"><span class="header-section-number">7.8</span> Penalty-based proximal policy optimization</a></li>
  <li><a href="#sec-ppo-clip" id="toc-sec-ppo-clip" class="nav-link" data-scroll-target="#sec-ppo-clip"><span class="header-section-number">7.9</span> Advantage clipping</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">7.10</span> Key takeaways</a></li>
  <li><a href="#sec-pg-bib" id="toc-sec-pg-bib" class="nav-link" data-scroll-target="#sec-pg-bib"><span class="header-section-number">7.11</span> Bibliographic notes and further reading</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/pg.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/pg.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-pg" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">7.1</span> Introduction</h2>
<p>The core task of RL is finding the <strong>optimal policy</strong> in a given environment. This is essentially an <em>optimization problem</em> (i.e.&nbsp;computing a minimum or maximum): out of some class of policies <span class="math inline">\(\Pi\)</span>, we want to find the one that achieves the maximum expected total reward:</p>
<p><span id="eq-policy-optimization-goal"><span class="math display">\[
\hat \pi = \arg\max_{\pi \in \Pi} \mathop{\mathbb{E}}_{\tau \sim \rho^\pi} [R(\tau)] \quad \text{where} \quad R(\tau) := \sum_{h=0}^{H-1} r(s_h, a_h).
\tag{7.1}\]</span></span></p>
<p>In a known environment with a finite set of states and actions, we can compute the optimal policy using dynamic programming in <span class="math inline">\(O( H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span> steps (<a href="mdps.html#sec-finite-opt-dp" class="quarto-xref"><span>Section 2.3.2</span></a>). In more general settings, though, such as when the environment is <em>unknown</em>, it is typically intractable to compute the optimal policy exactly. Instead, we start from some random policy, and then iteratively <em>improve</em> it by interacting with the environment. Policy iteration (<a href="mdps.html#sec-pi" class="quarto-xref"><span>Section 2.4.4.2</span></a>) and iterative LQR (<a href="control.html#sec-iterative-lqr" class="quarto-xref"><span>Section 3.6.4</span></a>) are two algorithms we’ve seen that do this.</p>
<p>In this chapter, we will explore <em>policy gradient algorithms,</em> which also iteratively improve a policy. The <em>gradient</em> of a function at a specific input point is the direction that increases the output value most rapidly. If we think of the expected total reward as a function of the policy, we can repeatedly shift the policy by the gradient of that function to obtain policies that achieve higher expected total rewards.</p>
<p>Policy gradient methods are responsible for groundbreaking applications including AlphaGo, OpenAI Five, and large language models. This chapter will explore:</p>
<ol type="1">
<li>Examples of <em>parameterized policies</em> that can be expressed in terms of a finite set of <em>parameters.</em></li>
<li><em>Gradient descent,</em> a general optimization algorithm for differentiable functions.</li>
<li>Different estimators of the <em>policy gradient,</em> enabling us to apply (stochastic) gradient descent to RL.</li>
<li><em>Proximal (policy) optimization</em> techniques that ensure the steps taken are “not too large”. These are some of the most popular and widely used RL algorithms at the time of writing.</li>
</ol>
<div id="rem-conventions" class="proof remark">
<p><span class="proof-title"><em>Remark 7.1</em> (Conventions). </span>Policy gradient algorithms typically optimize over <em>stochastic policies</em> (<a href="mdps.html#def-policy" class="quarto-xref">def.&nbsp;<span>2.6</span></a>). In this chapter, we will only discuss the case of stochastic policies, though there exist policy gradient algorithms for deterministic policies as well <span class="citation" data-cites="silver_deterministic_2014">(<a href="references.html#ref-silver_deterministic_2014" role="doc-biblioref">Silver et al., 2014</a>)</span>.</p>
<p>To ease notation, we will treat the horizon <span class="math inline">\(H\)</span> as finite and avoid adding a discount factor <span class="math inline">\(\gamma\)</span>. We make the policy’s dependence on the timestep <span class="math inline">\(h\)</span> implicit by assuming that the state space conveys information about the timestep. We also use</p>
<p><span id="eq-total-reward"><span class="math display">\[
R(\tau) := \sum_{h=0}^{H-1} r_h
\tag{7.2}\]</span></span></p>
<p>to denote the total reward in a trajectory.</p>
</div>
<div id="2c453b7d" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="83aa8a6e" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> Array, Float, Callable, jax, jnp, rng, latex, gym</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jaxtyping <span class="im">import</span> UInt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.random <span class="im">as</span> jr</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> functools <span class="im">as</span> ft</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> rng(<span class="dv">184</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-parameterizations" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="sec-parameterizations"><span class="header-section-number">7.2</span> Parameterized policies</h2>
<p>Optimizing over the entire space of policies is usually intractable: there’s just too many! There are there are <span class="math inline">\(|\mathcal{A}|^{|\mathcal{S}|}\)</span> deterministic mappings from state to actions. In continuous state spaces, <span class="math inline">\(|\mathcal{S}|\)</span> is infinite, so the space of policies becomes <em>infinite-dimensional</em>. Infinite-dimensional spaces are usually hard to optimize over. Instead, we choose a <em>parameterized policy class</em> with a finite number <span class="math inline">\(D\)</span> of adjustable parameters. Each parameter vector corresponds to a mapping from states to actions. (We also discussed the notion of a parameterized function class in <a href="supervised_learning.html#sec-sl-parameterized" class="quarto-xref"><span>Section 5.3.2</span></a>). The following examples seek to make this more concrete.</p>
<div id="exm-tabular-repr" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1 (Tabular representation)</strong></span> If both the state and action spaces are finite, perhaps we could simply learn a preference value <span class="math inline">\(\theta_{s,a}\)</span> for each state-action pair, resulting in a table of <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> values. These are the <span class="math inline">\(D\)</span> <em>parameters</em> of the policy. Then, for each state, to compute the distribution over actions, we perform a <strong>softmax</strong> operation: we exponentiate each of the values, and then normalize to form a valid distribution.</p>
<p><span id="eq-parameterized-tabular"><span class="math display">\[
\pi^\text{softmax}_\theta(a \mid s) = \frac{\exp(\theta_{s,a})}{\sum_{s,a'} \exp (\theta_{s,a'})}.
\tag{7.3}\]</span></span></p>
<p>In this way, we turn a vector of length <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> into a mapping from <span class="math inline">\(\mathcal{S} \to \triangle(\mathcal{A})\)</span>. However, this doesn’t make use of any structure in the states or actions, so while this is flexible, it is also prone to overfitting. For small state and action spaces, it makes more sense to use a dynamic programming algorithm from <a href="mdps.html" class="quarto-xref"><span>Chapter 2</span></a>.</p>
</div>
<p>For a given <em>parameterization,</em> such as the one above, we call the set of resulting policies the <strong>parameterized policy class,</strong> that is,</p>
<p><span id="eq-parameterized-class"><span class="math display">\[
\{ \pi_\theta \mid \theta \in \mathbb{R}^D \},
\tag{7.4}\]</span></span></p>
<p>where <span class="math inline">\(D\)</span> is the fixed number of parameters. Let us explore some more examples of parameterized policy classes.</p>
<div id="exm-linear-in-features" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2 (Linear in features)</strong></span> Another approach is to map each state-action pair into some <strong>feature space</strong> <span class="math inline">\(\phi(s, a) \in \mathbb{R}^D\)</span>. Then, to map a feature vector to a probability, we take a linear combination of the features and take a softmax. The parameters <span class="math inline">\(\theta\)</span> are the coefficients of the linear combination:</p>
<p><span id="eq-parameterized-linear"><span class="math display">\[
\pi^\text{linear}_{\theta}(a \mid s) = \frac{\exp(\theta^\top \phi(s, a))}{\sum_{a'} \exp(\theta^\top \phi(s, a'))}.
\tag{7.5}\]</span></span></p>
<p>Another interpretation is that <span class="math inline">\(\theta\)</span> represents the feature vector of the “desired” state-action pair, as state-action pairs whose features align closely with <span class="math inline">\(\theta\)</span> are given higher probability.</p>
</div>
<div id="exm-neural-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.3 (Neural policies)</strong></span> More generally, we could map states and actions to unnormalized scores via some parameterized function <span class="math inline">\(f_\theta : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>, such as a neural network, and choose actions according to a softmax:</p>
<p><span id="eq-parameterized-neural"><span class="math display">\[
\pi^\text{neural}_\theta(a \mid s) = \frac{\exp(f_{\theta}(s, a))}{\sum_{a'} \exp(f_{\theta}(s, a'))}.
\tag{7.6}\]</span></span></p>
</div>
<div id="rem-why-softmax" class="proof remark">
<p><span class="proof-title"><em>Remark 7.2</em> (Why softmax). </span>The three parameterized policy classes above all use the <em>softmax</em> operation. Of course, this isn’t the only way to turn a list of values into a probability distribution: for example, you could also subtract by the minimum value and divide by the range. Why is softmax preferred in practice? One reason is that it is a <em>smooth, differentiable</em> function of the input values. It is also nice to work with analytically, and has other interpretations from physics, where it is known as the <em>Gibbs</em> or <em>Boltzmann</em> distribution, and economics, where it is known as the <em>Bradley-Terry model</em> for ranking choices.</p>
</div>
<div id="exm-gaussian-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.4 (Diagonal Gaussian policies for continuous action spaces)</strong></span> Consider an <span class="math inline">\(N\)</span>-dimensional action space <span class="math inline">\(\mathcal{A} = \mathbb{R}^{N}\)</span>. Then for a stochastic policy, we could predict the <em>mean</em> action and then add some random noise to it. For example, we could use a linear model to predict the mean action and then add some noise <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> to it.</p>
<p><span class="math display">\[
\pi_\theta(\cdot \mid s) = \mathcal{N}(\theta^\top \phi(s), \sigma^2 I).
\]</span></p>
</div>
<p>Now we’ve seen some examples of <em>parameterized policies.</em> Optimizing over a parameterized policy class makes the policy optimization problem finite-dimensional:</p>
<p><span id="eq-policy-optimization-parameters"><span class="math display">\[
\begin{aligned}
\hat \theta &amp;= \arg\max_{\theta \in \mathbb{R}^D} J(\theta) \\
\text{where}\quad J(\theta) &amp;:= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \sum_{h=0}^{H-1} r(s_h, a_h).
\end{aligned}
\tag{7.7}\]</span></span></p>
<p>This enables us to apply one of the most popular and general optimization algorithms: <em>gradient descent</em>.</p>
</section>
<section id="sec-gd" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="sec-gd"><span class="header-section-number">7.3</span> Gradient descent</h2>
<p><strong>Gradient descent</strong> is an optimization algorithm that can be applied to any differentiable function. That is, it is a tool for solving</p>
<p><span id="eq-opt-min"><span class="math display">\[
\arg\min_{\theta \in \mathbb{R}^D} J(\theta),
\tag{7.8}\]</span></span></p>
<p>where <span class="math inline">\(J(\theta) \in \mathbb{R}\)</span> is the function to be minimized. For two-dimensional inputs, a suitable analogy for this algorithm is making your way down a hill (with no cliffs), where you keep taking steps in the steepest direction downwards from your current position. Your horizontal position <span class="math inline">\((x, z)\)</span> is the input and your vertical position <span class="math inline">\(y\)</span> is the function to be minimized. The <em>slope</em> of the mountain at your current position can be expressed using the <em>gradient</em>, written <span class="math inline">\(\nabla y(x, z) \in \mathbb{R}^2\)</span>. This can be computed as the vector of partial derivatives,</p>
<p><span id="eq-grad-partial"><span class="math display">\[
\nabla y(x, z) = \begin{pmatrix}
\frac{\partial y}{\partial x} \\
\frac{\partial y}{\partial z}
\end{pmatrix}.
\tag{7.9}\]</span></span></p>
<div id="cell-fig-exm-grad-descent" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">100.0</span> <span class="op">*</span> (y <span class="op">-</span> x<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> x)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of points</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> jnp.mgrid[<span class="dv">0</span>:<span class="dv">1</span>:<span class="ot">40j</span>, <span class="dv">0</span>:<span class="dv">1</span>:<span class="ot">40j</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X, Y)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>contourf <span class="op">=</span> ax.contourf(X, Y, Z, levels<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>fig.colorbar(contourf, ax<span class="op">=</span>ax)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>tx, ty <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">0.8</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>gx, gy <span class="op">=</span> <span class="op">-</span><span class="fl">0.001</span> <span class="op">*</span> jnp.asarray(jax.grad(f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))(tx, ty))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>ax.arrow(tx, ty, gx, gy, fc<span class="op">=</span><span class="st">'blue'</span>, ec<span class="op">=</span><span class="st">'blue'</span>, width<span class="op">=</span><span class="fl">0.01</span>, head_width<span class="op">=</span><span class="fl">0.05</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>ax.scatter(tx, ty, color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="vs">r"$x$"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$y$"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-exm-grad-descent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exm-grad-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-exm-grad-descent-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exm-grad-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Example of gradient descent on the Rosenbrock function.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To calculate the <em>slope</em> (aka “directional derivative”) of the mountain in a given direction <span class="math inline">\((\Delta x, \Delta z)\)</span>, you take the dot product of the difference vector with the gradient:</p>
<p><span id="eq-grad-dot"><span class="math display">\[
\Delta y = \begin{pmatrix}
\Delta x \\ \Delta z
\end{pmatrix}
\cdot
\nabla y(x, z),
\tag{7.10}\]</span></span></p>
<p>where <span class="math inline">\(x, z\)</span> is your current position. What direction should we walk to go down the hill as quickly as possible? That is, we want to find the direction <span class="math inline">\((\Delta x, \Delta z)\)</span> that minimizes the slope:</p>
<p><span id="eq-grad-argmin"><span class="math display">\[
\arg\min_{\Delta x, \Delta z} \Delta y.
\tag{7.11}\]</span></span></p>
<p>We use the useful fact that, for a given vector <span class="math inline">\(v\)</span>, the direction <span class="math inline">\(u\)</span> that minimizes the dot product <span class="math inline">\(u \cdot v\)</span> points in the opposite direction to <span class="math inline">\(v\)</span>. This should make intuitive sense, since <span class="math inline">\(u \cdot v = \|u\| \|v\| \cos \theta\)</span>, where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, and the cosine function is minimized at <span class="math inline">\(\theta = \pi\)</span>. Applying this to <a href="#eq-grad-dot" class="quarto-xref">eq.&nbsp;<span>7.10</span></a>, we see that the direction of steepest decrease is opposite to the gradient. “Walking” in that direction corresponds to subtracting a multiple of the gradient from your current position:</p>
<p><span class="math display">\[
\begin{pmatrix}
x^{t+1} \\ z^{t+1}
\end{pmatrix}
=
\begin{pmatrix}
x^{t} \\ z^{t}
\end{pmatrix}
-
\eta \nabla y(x^{t}, z^{t})
\]</span></p>
<p>where <span class="math inline">\(t\)</span> denotes the iteration of the algorithm and <span class="math inline">\(\eta &gt; 0\)</span> is a “step size” hyperparameter that controls the size of the steps we take. (Note that we could also vary the step size across iterations, that is, <span class="math inline">\(\eta^0, \dots, \eta^T\)</span>.)</p>
<div id="rem-ascent-vs-descent" class="proof remark">
<p><span class="proof-title"><em>Remark 7.3</em> (Minimization vs maximization). </span>Optimization problems are usually posed as <em>minimization</em> problems by convention. To solve a <em>maximization</em> problem with gradient descent, you could simply take steps in the direction of the <em>positive</em> gradient. This also corresponds to flipping the sign of the objective function.</p>
</div>
<p>The case of a two-dimensional input is easy to visualize. The analogy to climbing a hill (if we were maximizing the function) is why gradient descent is sometimes called <strong>hill climbing</strong> and the graph of the objective function is called the <strong>loss landscape</strong>. (The term “loss” comes from supervised learning, which you can read about in <a href="supervised_learning.html" class="quarto-xref"><span>Chapter 5</span></a>.) But this idea can be straightforwardly extended to higher-dimensional inputs. From now on, we’ll use <span class="math inline">\(J\)</span> to denote the function we’re trying to maximize, and <span class="math inline">\(\theta\)</span> to denote the parameters being optimized over. (In the above example, <span class="math inline">\(\theta = \begin{pmatrix} x &amp; z \end{pmatrix}^\top\)</span>). Let’s summarize the algorithm in general:</p>
<div id="def-grad-descent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.1 (Gradient descent)</strong></span> Suppose we are trying to solve the optimization problem</p>
<p><span id="eq-opt"><span class="math display">\[
\theta^\star = \arg\min_{\theta \in \mathbb{R}^D} J(\theta),
\tag{7.12}\]</span></span></p>
<p>where <span class="math inline">\(J(\theta) \in \mathbb{R}\)</span> is the differentiable function to be minimized. Gradient descent starts with an initial guess <span class="math inline">\(\theta^0\)</span> and then takes steps in the direction of <span class="math inline">\(- \nabla J(\theta^t)\)</span>, where <span class="math inline">\(\theta^t\)</span> is the current iterate:</p>
<p><span id="eq-grad-descent"><span class="math display">\[
\theta^{t+1} = \theta^t - \eta \nabla J(\theta^t).
\tag{7.13}\]</span></span></p>
<p>Note that we scale <span class="math inline">\(\nabla J(\theta^t)\)</span> by the <strong>step size</strong> <span class="math inline">\(\eta &gt; 0\)</span>, also known as the learning rate.</p>
</div>
<div id="cell-fig-grad-descent" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    theta_init: Float[Array, <span class="st">" D"</span>],</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    objective: Callable[[Float[Array, <span class="st">" D"</span>]], <span class="bu">float</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    eta: <span class="bu">float</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    n_steps: UInt,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> [theta]</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> jax.grad(objective)(theta)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        history.append(theta)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, jnp.array(history)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>latex(gradient_descent, id_to_latex<span class="op">=</span>{<span class="st">"objective"</span>: <span class="st">"J"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-grad-descent" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grad-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{gradient\_descent}(\theta_{\mathrm{init}}: \mathbb{R}^{D}, J: (\mathbb{R}^{D}) \rightarrow \mathbb{R}, \eta: \mathbb{R}, n_{\mathrm{steps}}: \mathbb{N}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathrm{history} \gets \mathopen{}\left[ \theta \mathclose{}\right] \\ \hspace{1em} \mathbf{for} \ \mathrm{step} \in \mathrm{range} \mathopen{}\left( n_{\mathrm{steps}} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \theta \gets \theta + \eta \cdot \nabla \mathopen{}\left(J\mathclose{}\right) \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{2em} \mathrm{history}.\mathrm{append} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \mathopen{}\left( \theta, \mathrm{jnp}.\mathrm{array} \mathopen{}\left( \mathrm{history} \mathclose{}\right) \mathclose{}\right) \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grad-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Pseudocode for gradient descent.
</figcaption>
</figure>
</div>
</div>
<p>Notice that the parameters will stop changing once <span class="math inline">\(\nabla J(\theta) = 0\)</span>. Once we reach this <strong>stationary point,</strong> our current parameters are ‘locally optimal’ (assuming we’re at a local minimum): it’s impossible to increase the function by moving in any direction. If <span class="math inline">\(J\)</span> is <em>convex</em> (i.e.&nbsp;the function looks like an upward-curved bowl), then the only point where this happens is at the <em>global optimum.</em> Otherwise, if <span class="math inline">\(J\)</span> is nonconvex, the best we can hope for is a <em>local optimum.</em> We won’t go deeper into the theory of convex functions here. For more details, refer to the textbook of <span class="citation" data-cites="boyd_convex_2004">Boyd &amp; Vandenberghe (<a href="references.html#ref-boyd_convex_2004" role="doc-biblioref">2004</a>)</span>.</p>
<div id="rem-sgd-limitations" class="proof remark">
<p><span class="proof-title"><em>Remark 7.4</em> (Limitations of gradient descent). </span>Gradient descent and its variants are responsible for most of the major achievements in modern machine learning. It is important to note, however, that for many problems, gradient descent is <em>not</em> a good optimization algorithm to reach for! If you have more information about the problem, such as access to <em>second</em> derivatives, you can apply more powerful optimization algorithms that converge much more rapidly. Read <span class="citation" data-cites="nocedal_numerical_2006">Nocedal &amp; Wright (<a href="references.html#ref-nocedal_numerical_2006" role="doc-biblioref">2006</a>)</span> if you’re curious about the field of numerical optimization.</p>
</div>
<section id="sec-computing-derivatives" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="sec-computing-derivatives"><span class="header-section-number">7.3.1</span> Computing derivatives</h3>
<p>How does a computer compute the gradient of a function?</p>
<p>One way is <strong>symbolic differentiation,</strong> which is similar to the way you might compute it by hand: the computer applies a list of rules to transform the <em>symbols</em> involved. Python’s <code>sympy</code> package <span class="citation" data-cites="meurer_sympy_2017">(<a href="references.html#ref-meurer_sympy_2017" role="doc-biblioref">Meurer et al., 2017</a>)</span> supports symbolic differentiation. However, functions implemented as algorithms in code may not always have a straightforward symbolic representation.</p>
<p>Another way is <strong>numerical differentiation,</strong> which is based on the limit definition of a (directional) derivative:</p>
<p><span id="eq-numerical-differentiation"><span class="math display">\[
\nabla_{\boldsymbol{u}} J(\boldsymbol{x}) = \lim_{\varepsilon \to 0}
\frac{J(\boldsymbol{x} + \varepsilon \boldsymbol{u}) - J(\boldsymbol{x})}{\varepsilon}
\tag{7.14}\]</span></span></p>
<p>Then, we can substitute a small value of <span class="math inline">\(\varepsilon\)</span> on the r.h.s. to approximate the directional derivative. How small, though? Depending on how smooth the function is and how accurate our estimate needs to be, we may need such a small value of <span class="math inline">\(\varepsilon\)</span> that typical computers will run into <em>rounding errors</em>. Also, to compute the full gradient, we would need to compute the r.h.s. once for each input dimension. This is an issue if computing <span class="math inline">\(J\)</span> is expensive.</p>
<p><strong>Automatic differentiation</strong> achieves the best of both worlds. Like symbolic differentiation, we manually implement the derivative rules for a few basic operations. However, instead of executing these on the <em>symbols</em>, we execute them on the <em>values</em> when the function gets called, like in numerical differentiation. This allows us to differentiate through programming constructs such as branches or loops, and doesn’t involve any arbitrarily small values. <span class="citation" data-cites="baydin_automatic_2018">Baydin et al. (<a href="references.html#ref-baydin_automatic_2018" role="doc-biblioref">2018</a>)</span> provides an accessible survey of automatic differentiation. At the time of writing, all of the popular Python libraries for machine learning, such as PyTorch <span class="citation" data-cites="ansel_pytorch_2024">(<a href="references.html#ref-ansel_pytorch_2024" role="doc-biblioref">Ansel et al., 2024</a>)</span> and Jax <span class="citation" data-cites="bradbury_jax_2018">(<a href="references.html#ref-bradbury_jax_2018" role="doc-biblioref">Bradbury et al., 2018</a>)</span>, use automatic differentiation.</p>
</section>
<section id="sec-sgd" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="sec-sgd"><span class="header-section-number">7.3.2</span> Stochastic gradient descent</h3>
<p>In real applications, computing the gradient of the target function is not so simple. As an example from supervised learning, <span class="math inline">\(J(\theta)\)</span> might be the sum of squared prediction errors across an entire training dataset. If our dataset is very large, it might not fit into our computer’s memory, making it impossible to evaluate <span class="math inline">\(\nabla J(\theta)\)</span> at once. We will see that computing the exact gradient in RL faces a similar challenge where computing the gradient would require computing a complicated integral.</p>
<p>In these cases, we can compute some <em>gradient estimate</em></p>
<p><span id="eq-grad-estimator-notation"><span class="math display">\[
g_x(\theta) \approx \nabla J(\theta)
\tag{7.15}\]</span></span></p>
<p>of the gradient at each step, using some observed data <span class="math inline">\(x\)</span>, and walk in that direction instead. This is called <strong>stochastic</strong> gradient descent. In the SL example above, we might randomly choose a <em>minibatch</em> of samples and use them to estimate the true prediction error.</p>
<div id="cell-fig-sgd" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    rng: jr.PRNGKey,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    theta_init: Float[Array, <span class="st">" D"</span>],</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    estimate_gradient: Callable[[jr.PRNGKey, Float[Array, <span class="st">" D"</span>]], Float[Array, <span class="st">" D"</span>]],</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    eta: <span class="bu">float</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    n_steps: <span class="bu">int</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform `n_steps` steps of SGD.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    rngs <span class="op">=</span> jr.split(rng, n_steps)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> [theta]</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> estimate_gradient(rngs[step], theta)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        history.append(theta)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, jnp.array(history)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>latex(sgd, id_to_latex<span class="op">=</span>{<span class="st">"estimate_gradient"</span>: <span class="st">"g"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-sgd" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="5">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sgd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{sgd}(\mathrm{rng}: \mathrm{jr}.\mathrm{PRNGKey}, \theta_{\mathrm{init}}: \mathbb{R}^{D}, g: (\mathrm{jr}.\mathrm{PRNGKey} \times \mathbb{R}^{D}) \rightarrow \mathbb{R}^{D}, \eta: \mathbb{R}, n_{\mathrm{steps}}: \mathbb{Z}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathrm{rngs} \gets \mathrm{jr}.\mathrm{split} \mathopen{}\left( \mathrm{rng}, n_{\mathrm{steps}} \mathclose{}\right) \\ \hspace{1em} \mathrm{history} \gets \mathopen{}\left[ \theta \mathclose{}\right] \\ \hspace{1em} \mathbf{for} \ \mathrm{step} \in \mathrm{range} \mathopen{}\left( n_{\mathrm{steps}} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \theta \gets \theta + \eta \cdot g \mathopen{}\left( \mathrm{rngs}_{\mathrm{step}}, \theta \mathclose{}\right) \\ \hspace{2em} \mathrm{history}.\mathrm{append} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \mathopen{}\left( \theta, \mathrm{jnp}.\mathrm{array} \mathopen{}\left( \mathrm{history} \mathclose{}\right) \mathclose{}\right) \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sgd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: Pseudocode for stochastic gradient descent.
</figcaption>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> jr.normal(<span class="bu">next</span>(rng), (<span class="dv">100</span>, <span class="dv">2</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>w_true <span class="op">=</span> jnp.ones(<span class="dv">2</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> w_true</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>w_init <span class="op">=</span> jnp.zeros(<span class="dv">2</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_gradient(rng, theta, batch_size):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    batch_idx <span class="op">=</span> jr.randint(rng, batch_size, <span class="dv">0</span>, X.shape[<span class="dv">0</span>])</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    X_batch <span class="op">=</span> X[batch_idx]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    y_batch <span class="op">=</span> y[batch_idx]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> (<span class="dv">1</span> <span class="op">/</span> batch_size) <span class="op">*</span> X_batch.T <span class="op">@</span> (X_batch <span class="op">@</span> theta <span class="op">-</span> y_batch)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(theta):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> jnp.mean((X <span class="op">@</span> theta <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># loss contour</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>w_lim <span class="op">=</span> <span class="bu">slice</span>(<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">1.5</span>, <span class="ot">20j</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>w0, w1 <span class="op">=</span> jnp.mgrid[w_lim, w_lim]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> jnp.stack([w0, w1], axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> jax.vmap(jax.vmap(loss))(thetas)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_history(history):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    plt.contourf(w0, w1, z, levels<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(history) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        plt.arrow(history[i, <span class="dv">0</span>], history[i, <span class="dv">1</span>], history[i<span class="op">+</span><span class="dv">1</span>, <span class="dv">0</span>] <span class="op">-</span> history[i, <span class="dv">0</span>], history[i<span class="op">+</span><span class="dv">1</span>, <span class="dv">1</span>] <span class="op">-</span> history[i, <span class="dv">1</span>],</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>                width<span class="op">=</span><span class="fl">0.03</span>, fc<span class="op">=</span><span class="st">'black'</span>, ec<span class="op">=</span><span class="st">'black'</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="vs">r"$w_0$"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="vs">r"$w_1$"</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    plt.xlim(w_lim.start, w_lim.stop)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    plt.ylim(w_lim.start, w_lim.stop)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    plt.gcf().set_size_inches((<span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># also plot full gradient</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>_, history_gd <span class="op">=</span> gradient_descent(w_init, loss, lr, n_steps)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>_, history_sgd_1 <span class="op">=</span> sgd(<span class="bu">next</span>(rng), w_init, ft.partial(estimate_gradient, batch_size<span class="op">=</span><span class="dv">1</span>), lr, n_steps)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>_, history_sgd_5 <span class="op">=</span> sgd(<span class="bu">next</span>(rng), w_init, ft.partial(estimate_gradient, batch_size<span class="op">=</span><span class="dv">5</span>), lr, n_steps)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>_, history_sgd_10 <span class="op">=</span> sgd(<span class="bu">next</span>(rng), w_init, ft.partial(estimate_gradient, batch_size<span class="op">=</span><span class="dv">10</span>), lr, n_steps)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>plot_history(history_gd)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>plot_history(history_sgd_1)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>plot_history(history_sgd_5)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>plot_history(history_sgd_10)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-sgd-demo" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sgd-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sgd-demo" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-sgd-demo-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sgd-demo-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-sgd-demo-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-sgd-demo">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sgd-demo-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Full-batch GA.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sgd-demo" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-sgd-demo-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sgd-demo-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-sgd-demo-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-sgd-demo">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sgd-demo-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) SGD with a batch size of 1.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sgd-demo" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-sgd-demo-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sgd-demo-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-sgd-demo-output-3.png" class="img-fluid figure-img" data-ref-parent="fig-sgd-demo">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sgd-demo-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) SGD with a batch size of 5.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sgd-demo" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-sgd-demo-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sgd-demo-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-sgd-demo-output-4.png" class="img-fluid figure-img" data-ref-parent="fig-sgd-demo">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sgd-demo-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) SGD with a batch size of 10.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sgd-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: GA and SGD on a two-dimensional convex optimization task. Note that a larger batch size reduces the variance in the step direction.
</figcaption>
</figure>
</div>
<p>What makes one gradient estimator better than another? Ideally, we want this estimator to be <em>unbiased;</em> that is, on average, it matches a single true gradient step.</p>
<div id="def-unbiased-estimator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.2 (Unbiased gradient estimator)</strong></span> We call the gradient estimator <span class="math inline">\(g\)</span> <strong>unbiased</strong> if, for all <span class="math inline">\(\theta \in \mathbb{R}^D\)</span>,</p>
<p><span id="eq-gd-unbiased"><span class="math display">\[
\mathop{\mathbb{E}}_{x \sim p_\theta} [g_x(\theta)] = \nabla J(\theta),
\tag{7.16}\]</span></span></p>
<p>where <span class="math inline">\(p_\theta\)</span> denotes the distribution of the observed data <span class="math inline">\(x\)</span>.</p>
</div>
<p>We also want the <em>variance</em> of the estimator to be low so that its performance doesn’t change drastically at each step.</p>
<p>We can actually show that, for many “nice” functions, in a finite number of steps, SGD will find a <span class="math inline">\(\theta\)</span> that is “close” to a stationary point. In another perspective, for such functions, the local “landscape” of <span class="math inline">\(J\)</span> around <span class="math inline">\(\theta\)</span> becomes flatter and flatter the longer we run SGD.</p>
<div id="thm-sgd-convergence" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.1 (SGD convergence)</strong></span> More formally, suppose we run SGD for <span class="math inline">\(K\)</span> steps, using an unbiased gradient estimator. Let the step size <span class="math inline">\(\eta^k\)</span> scale as <span class="math inline">\(O(1/\sqrt{k}).\)</span> Then if <span class="math inline">\(J\)</span> is bounded and <span class="math inline">\(\beta\)</span>-smooth (see below), and the <em>norm</em> of the gradient estimator has a bounded second moment <span class="math inline">\(\sigma^2,\)</span></p>
<p><span class="math display">\[
\|\nabla J({\theta^i})\|^2 \le O \left( M \beta \sigma^2 / K\right).
\]</span></p>
<p>We call a function <span class="math inline">\(\beta\)</span>-smooth if its gradient is Lipschitz continuous with constant <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\|\nabla J(\theta) - \nabla J(\theta')\| \le \beta \|\theta - \theta'\|.
\]</span></p>
</div>
<p>We’ll now see a concrete application of stochastic gradient descent in the context of policy optimization.</p>
</section>
</section>
<section id="sec-pg-intro" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="sec-pg-intro"><span class="header-section-number">7.4</span> Policy (stochastic) gradient descent</h2>
<p>Policy gradient methods boil down to applying gradient descent to the policy optimization problem for a chosen parameterized policy class (<a href="#eq-parameterized-class" class="quarto-xref">eq.&nbsp;<span>7.4</span></a>):</p>
<p><span id="eq-pg-optimization"><span class="math display">\[
\theta^{t+1} = \theta^t + \eta \nabla J(\theta^t)
\quad \text{where} \quad
J(\theta) = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{h=0}^{H-1} r(s_h, a_h) \right].
\tag{7.17}\]</span></span></p>
<p>The challenges lie mostly in computing unbiased gradient estimators (<a href="#def-unbiased-estimator" class="quarto-xref">def.&nbsp;<span>7.2</span></a>) and in <em>constraining</em> the size of each update to improve stability of the learning algorithm.</p>
<p>To provide some intuition for the gradient estimators we will later derive, the following section constructs a general policy gradient algorithm in a “bottom-up” way. The remainder of the chapter focuses on methods for improving the stability of the policy gradient algorithm.</p>
<div id="rem-pg-on-policy" class="proof remark">
<p><span class="proof-title"><em>Remark 7.5</em> (On-policy). </span>Note that since the gradient estimator is computed using data collected by the current policy, policy gradient algorithms are generally <em>on-policy</em> (<a href="fitted_dp.html#def-on-off-policy" class="quarto-xref">def.&nbsp;<span>6.3</span></a>). Note that on-policy algorithms generally suffer from <em>worse sample efficiency</em> than off-policy algorithms: while off-policy algorithms (such as Q-learning (<a href="fitted_dp.html#sec-q-learning" class="quarto-xref"><span>Section 6.2.2</span></a>)) can use data collected by any method, on-policy algorithms can only make use of data from the <em>current</em> policy.</p>
</div>
<section id="sec-pg-intuition" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="sec-pg-intuition"><span class="header-section-number">7.4.1</span> Introduction to policy gradient methods</h3>
<p>In this section, we’ll intuitively construct an iterative algorithm for improving the parameters. We’ll build up to true policy gradient estimators by considering a series of update rules.</p>
<p>Remember that in RL, the primary goal is to find the <em>optimal policy</em> that achieves the highest total reward. Put simply, we want policies that <em>take better actions more often.</em> Breaking that down, we need</p>
<ol type="1">
<li>a way to shift the policy to make certain actions more likely, and</li>
<li>a way to measure how good an action is in a given state.</li>
</ol>
<p>Let’s tackle the first task: getting the policy to take action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>. Back in the tabular setting, for a deterministic policy <span class="math inline">\(\pi\)</span>, we simply assigned <span class="math inline">\(\pi(s) \gets a\)</span>. Now that we’re using a parameterized policy class, we can’t directly assign a value to <span class="math inline">\(\pi(s)\)</span>. Instead, we adjust the parameters <span class="math inline">\(\theta\)</span> of the policy to maximize the probability of taking <span class="math inline">\(a\)</span>:</p>
<p><span id="eq-max-action-likelihood"><span class="math display">\[
\hat \theta = \arg\max_{\theta} \pi_{\theta}(a \mid s).
\tag{7.18}\]</span></span></p>
<p>Assuming the output of the policy is differentiable with respect to its parameters, we can apply gradient descent:</p>
<p><span id="eq-gd-max-action-likelihood"><span class="math display">\[
\begin{aligned}
\theta^{t+1} &amp;= \theta^t + \eta g_{s, a}(\theta^t) \\
\text{where} \quad g_{s, a}(\theta) &amp;:= \nabla \pi_{\theta}(a \mid s).
\end{aligned}
\tag{7.19}\]</span></span></p>
<p>The notation <span class="math inline">\(g\)</span> is chosen to remind you of a gradient estimator (<a href="#eq-grad-estimator-notation" class="quarto-xref">eq.&nbsp;<span>7.15</span></a>). We will draw the relationship between our intuitive approach and SGD more concretely later on.</p>
<p>Now let’s approach the second task: how do we choose which actions to take more often? Suppose we have some random variable <span class="math inline">\(\psi\)</span> (that’s the Greek letter “psi”) that is correlated with how “good” action <span class="math inline">\(a\)</span> is in state <span class="math inline">\(s\)</span>. Later in the chapter, we’ll explore concrete choices for <span class="math inline">\(\psi\)</span>, but for now, we’ll leave its identity a mystery. Then, to update the policy, we could sample an action <span class="math inline">\(a \sim \pi_{\theta^i}(\cdot \mid s)\)</span>, and weight the corresponding gradient by <span class="math inline">\(\psi\)</span>. We will therefore call <span class="math inline">\(\psi\)</span> the <strong>gradient coefficient</strong>.</p>
<p><span id="eq-gd-weighted"><span class="math display">\[
g_{s, a}(\theta) := \psi \nabla \pi_{\theta}(a \mid s).
\tag{7.20}\]</span></span></p>
<p>To illustrate this, suppose the policy takes one action <span class="math inline">\(a_0\)</span> and obtains a coefficient of <span class="math inline">\(\psi_0 = 2\)</span> and then takes a second action <span class="math inline">\(a_1\)</span> with a coefficient of <span class="math inline">\(\psi_1 = -1\)</span>. Then we would update <span class="math inline">\(\theta \gets \theta + 2 \eta \nabla \pi_{\theta}(a_0 \mid s)\)</span> and then <span class="math inline">\(\theta \gets \theta - \eta \nabla \pi_{\theta}(a_1 \mid s)\)</span>.</p>
<div id="exr-solve-directly" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.1 (An alternative approach)</strong></span> Compare this with the policy iteration update (<a href="mdps.html#sec-pi" class="quarto-xref"><span>Section 2.4.4.2</span></a>), where we updated the deterministic policy according to <span class="math inline">\(\pi(s) = \arg\max_{a \in \mathcal{A}} Q^\pi(s, a)\)</span>. In our current setting, why not just solve for <span class="math inline">\(\theta^{t+1} = \arg\max_{\theta} \pi_{\theta}(a^\star \mid s)\)</span>, where <span class="math inline">\(a^\star = \arg\max_{a \in \mathcal{A}} Q^{\pi_{\theta^i}}(a \mid s)\)</span>, instead of sampling an action from our policy? What type of action space does this approach assume? What does the added stochasticity grant you?</p>
</div>
<p>But the gradient estimator <a href="#eq-gd-weighted" class="quarto-xref">eq.&nbsp;<span>7.20</span></a> has an issue: the amount that we encourage action <span class="math inline">\(a\)</span> depends on how <em>often</em> the policy takes it. This could lead to a positive feedback loop where the most common action becomes more and more likely, regardless of its quality. To cancel out this factor, we divide by the action’s likelihood:</p>
<p><span id="eq-gd-log-step"><span class="math display">\[
\begin{aligned}
g_{s, a}(\theta) &amp;:= \psi \frac{\nabla \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
&amp;= \psi \nabla \log \pi_{\theta}(a \mid s).
\end{aligned}
\tag{7.21}\]</span></span></p>
<p>Now we can extend this across the entire time horizon. Suppose we use <span class="math inline">\(\pi_{\theta^i}\)</span> to roll out a trajectory <span class="math inline">\(\tau = (s_0, a_0, \dots, s_{H-1}, a_{H-1})\)</span> and compute <a href="#eq-gd-log-step" class="quarto-xref">eq.&nbsp;<span>7.21</span></a> at each step of the trajectory. We compute a gradient coefficient at each timestep, so we denote each instance by <span class="math inline">\(\psi_h(\tau)\)</span>.</p>
<p><span id="eq-general-pg-single"><span class="math display">\[
g_\tau(\theta)
:=
\sum_{h=0}^{H-1}
\psi_h(\tau) \nabla \log \pi_{\theta}(a_h\mid s_h).
\tag{7.22}\]</span></span></p>
<p>To reduce the variance, we could roll out multiple trajectories, and average the gradient steps across them. This gives us the general form of the <strong>policy gradient algorithm</strong>:</p>
<div id="def-policy-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.3 (General policy gradient algorithm)</strong></span> Suppose we are given an expression for the gradient coefficients <span class="math inline">\(\psi_h(\tau)\)</span>. Then we can perform policy gradient optimization as follows.</p>
<p>At each iteration <span class="math inline">\(t = 0, \dots, T-1\)</span> of the algorithm, we sample <span class="math inline">\(N\)</span> trajectories <span class="math inline">\(\tau^n = (s^n_0, a^n_0, r^n_0, \dots, s^n_{H-1}, a^n_{H-1}, r^n_{H-1})\)</span>, and compute the update rule</p>
<p><span id="eq-general-pg"><span class="math display">\[
\begin{aligned}
\theta^{t+1} &amp;= \theta^t + \eta \frac{1}{N} \sum_{n=1}^N g_{\tau^n}(\theta^t) \\
\text{where} \quad g_{\tau}(\theta) &amp;= \sum_{h=0}^{H-1}
\psi_h(\tau) \nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.23}\]</span></span></p>
</div>
<p>This algorithm allows us to optimize a policy by sampling trajectories from it and computing the gradient-log-likelihoods (sometimes called the <strong>scores</strong>) of the chosen actions. Then we can update the parameters <span class="math inline">\(\theta\)</span> in the direction given by <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> to obtain a new policy that chooses better actions more often.</p>
<div id="cell-fig-policy-grad-general" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_gradient(env: gym.Env, pi, theta: Float[Array, <span class="st">" D"</span>], get_psi: Callable[[<span class="bu">list</span>[<span class="st">"Transition"</span>]], Float[Array, <span class="st">" H"</span>]]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Estimate the policy gradient using REINFORCE."""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> jnp.zeros_like(theta)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    tau <span class="op">=</span> sample_trajectory(env, pi(theta))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    psis <span class="op">=</span> get_psi(pi(theta), tau)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (s, a, r), psi <span class="kw">in</span> <span class="bu">zip</span>(tau, psis):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> policy_log_likelihood(theta: Float[Array, <span class="st">" D"</span>]) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> log(pi(theta)(s, a))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        g <span class="op">+=</span> psi <span class="op">*</span> jax.grad(policy_log_likelihood)(theta)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> g</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>latex(policy_gradient, id_to_latex<span class="op">=</span>{<span class="st">"jax.grad"</span>: <span class="vs">r"\nabla"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-policy-grad-general" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="7">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-policy-grad-general-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{policy\_gradient}(\mathrm{env}: \mathrm{gym}.\mathrm{Env}, \pi, \theta: \mathbb{R}^{D}, \mathrm{get\_psi}: (\mathrm{list}_{\textrm{"Transition"}}) \rightarrow \mathbb{R}^{H}) \\ \hspace{1em} \textrm{"Estimate the policy gradient using REINFORCE."} \\ \hspace{1em} g \gets \mathrm{jnp}.\mathrm{zeros\_like} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \tau \gets \mathrm{sample\_trajectory} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right) \mathclose{}\right) \\ \hspace{1em} \mathrm{psis} \gets \mathrm{get\_psi} \mathopen{}\left( \pi \mathopen{}\left( \theta \mathclose{}\right), \tau \mathclose{}\right) \\ \hspace{1em} \mathbf{for} \ \mathopen{}\left( \mathopen{}\left( s, a, r \mathclose{}\right), \psi \mathclose{}\right) \in \mathrm{zip} \mathopen{}\left( \tau, \mathrm{psis} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathbf{function} \ \mathrm{policy\_log\_likelihood}(\theta: \mathbb{R}^{D}) \\ \hspace{3em} \mathbf{return} \ \log \pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} g \gets g + \psi \cdot \nabla \mathopen{}\left(\mathrm{policy\_log\_likelihood}\mathclose{}\right) \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ g \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-policy-grad-general-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.5: Pseudocode for the general policy gradient algorithm.
</figcaption>
</figure>
</div>
</div>
<div id="rem-policy-grad-summary" class="proof remark">
<p><span class="proof-title"><em>Remark 7.6</em> (Summary of intuitive derivation). </span>Let us review how we arrived at this expression:</p>
<ol type="1">
<li>We take the gradient of the policy to encourage certain actions (<a href="#eq-gd-max-action-likelihood" class="quarto-xref">eq.&nbsp;<span>7.19</span></a>).</li>
<li>We encourage actions proportionally to their advantage (<a href="#eq-gd-weighted" class="quarto-xref">eq.&nbsp;<span>7.20</span></a>).</li>
<li>We correct for the policy’s sampling distribution (<a href="#eq-gd-log-step" class="quarto-xref">eq.&nbsp;<span>7.21</span></a>).</li>
<li>We extend this to each step of the trajectory (<a href="#eq-general-pg-single" class="quarto-xref">eq.&nbsp;<span>7.22</span></a>).</li>
<li>We sample multiple trajectories to reduce variance (<a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a>).</li>
</ol>
<p>The last piece is to figure out what <span class="math inline">\(\psi\)</span> stands for.</p>
</div>
<div id="exr-grad-coefficient" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.2 (Brainstorming)</strong></span> Can you think of any possibilities? <span class="math inline">\(\psi_h(\tau)\)</span> should correlate with the quality of the action taken at time <span class="math inline">\(h\)</span>. It may depend on the current policy <span class="math inline">\(\pi_{\theta^i}\)</span> or any component of the trajectory <span class="math inline">\((s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1})\)</span>.</p>
</div>
<p>We won’t keep you waiting: it turns out that if we set <span class="math inline">\(\psi_h(\tau)\)</span> to</p>
<ol type="1">
<li><span class="math inline">\(R(\tau)\)</span> (the total reward of the trajectory) (<a href="#eq-estimator-reinforce" class="quarto-xref">eq.&nbsp;<span>7.25</span></a>),</li>
<li><span class="math inline">\(\sum_{h'=h}^{H-1} r(s_{h'}, a_{h'})\)</span> (the remaining reward in the trajectory),</li>
<li><span class="math inline">\(Q^{\pi_{\theta^i}}(s_h, a_h)\)</span> (the current policy’s Q-function), or</li>
<li><span class="math inline">\(A^{\pi_{\theta^i}}(s_h, a_h)\)</span> (the current policy’s advantage function),</li>
</ol>
<p>among other possibilities, the gradient term of <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> is actually an <em>unbiased</em> estimator (<a href="#def-unbiased-estimator" class="quarto-xref">def.&nbsp;<span>7.2</span></a>) of the true “policy gradient” <span class="math inline">\(\nabla J(\theta)\)</span> (see <a href="#eq-policy-optimization-parameters" class="quarto-xref">eq.&nbsp;<span>7.7</span></a>). That is, for any of the <span class="math inline">\(\psi\)</span> above, updating the parameters according to the general policy gradient algorithm <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> <em>is</em> (minibatch) stochastic gradient descent on the expected total reward <span class="math inline">\(J(\theta)\)</span>, with the gradient estimator</p>
<p><span id="eq-sgd-pg-estimator"><span class="math display">\[
\begin{aligned}
g_{\tau}(\theta)
&amp;=
\sum_{h=0}^{H-1}
\psi_h(\tau) \nabla \log \pi_{\theta}(a_h\mid s_h) \\
\text{where} \quad
\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}}[ g_{\tau}(\theta) ]
&amp;=
\nabla J(\theta).
\end{aligned}
\tag{7.24}\]</span></span></p>
</section>
<section id="sec-reinforce" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="sec-reinforce"><span class="header-section-number">7.4.2</span> The REINFORCE policy gradient</h3>
<p>We begin by showing that setting <span class="math inline">\(\psi_h= \sum_{h'=0}^{H-1} r_{h'}\)</span>, i.e.&nbsp;the total reward of the trajectory, provides an unbiased gradient estimator.</p>
<div id="thm-unbiased-reinforce" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.2 (Using the total reward is unbiased)</strong></span> Substituting</p>
<p><span id="eq-estimator-reinforce"><span class="math display">\[
\psi_h(\tau) := R(\tau) := \sum_{h'=0}^{H-1} r_{h'}
\tag{7.25}\]</span></span></p>
<p>into the general policy gradient estimator <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> gives an unbiased estimator. That is,</p>
<p><span id="eq-estimator-reinforce-unbiased"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ g^\text{R}_\tau(\theta) ] \\
\text{where} \quad
g^\text{R}_{\tau} (\theta)
&amp;:=
\sum_{h=0}^{H-1}
R(\tau)
\nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.26}\]</span></span></p>
</div>
<p>The “R” stands for REINFORCE, which stands for “REward Increment <span class="math inline">\(=\)</span> Nonnegative Factor <span class="math inline">\(\times\)</span> Offset Reinforcement <span class="math inline">\(\times\)</span> Characteristic Eligibility” <span class="citation" data-cites="williams_simple_1992">(<a href="references.html#ref-williams_simple_1992" role="doc-biblioref">Williams, 1992, p. 234</a>)</span>. (We will not elaborate further on this etymology.)</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (Proof via calculus). </span>As our first step towards constructing an unbiased policy gradient estimator, let us simplify the expression</p>
<p><span id="eq-objective-def"><span class="math display">\[
\nabla J(\theta) = \nabla \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [R(\tau)].
\tag{7.27}\]</span></span></p>
<p>In supervised learning, we were able to swap the gradient and expectation. That was because the <em>function</em> being averaged depended on the parameters, not the distribution itself:</p>
<p><span id="eq-sl-example-swap"><span class="math display">\[
\nabla \mathop{\mathbb{E}}_{(x, y) \sim p}[
    L(f_\theta(x), y)
] = \mathop{\mathbb{E}}_{(x, y) \sim p} [\nabla L(f_\theta(x), y) ].
\tag{7.28}\]</span></span></p>
<p>Here, though, the distribution depends on the parameters, and the function being averaged does not. One way to compute this type of derivative is to use the identity</p>
<p><span id="eq-grad-log-prob"><span class="math display">\[
\nabla \rho^{\pi_\theta}(\tau)
=
\rho^{\pi_\theta}(\tau) \nabla \log \rho^{\pi_\theta}(\tau).
\tag{7.29}\]</span></span></p>
<p>By expanding the definition of expected value, we can compute the correct value to be</p>
<p><span id="eq-reinforce-grad-log-prob"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp; = \nabla \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [R(\tau)] \\
&amp; = \int \nabla \rho^{\pi_\theta}(\tau) R(\tau) \\
&amp; = \int \rho^{\pi_\theta}(\tau) \nabla \log \rho^{\pi_\theta}(\tau) R(\tau) \\
&amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ \nabla \log \rho^{\pi_\theta}(\tau) R(\tau) ].
\end{aligned}
\tag{7.30}\]</span></span></p>
<p>Now we deal with the <span class="math inline">\(\nabla \log \rho^{\pi_\theta}(\tau)\)</span> term, that is, the gradient-log-likelihood (aka <strong>score</strong>) of the trajectory. Recall <a href="mdps.html#thm-autoregressive-trajectories" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a>, in which we showed that when the state transitions are Markov (i.e.&nbsp;<span class="math inline">\(s_{h}\)</span> only depends on <span class="math inline">\(s_{h-1}, a_{h-1}\)</span>) and the policy is history-independent (i.e.&nbsp;<span class="math inline">\(a_h\sim \pi_\theta (\cdot \mid s_h)\)</span>), we can autoregressively write out the likelihood of a trajectory under the policy <span class="math inline">\(\pi_\theta\)</span>. Taking the log of the trajectory likelihood turns the products into sums:</p>
<p><span id="eq-log-autoregressive"><span class="math display">\[
\log \rho^{\pi_\theta}(\tau) = \log P_0(s_0) + \sum_{h=0}^{H-1} \Big(
    \log \pi_\theta(a_h\mid s_h) + \log P(s_{h+1} \mid s_h, a_h)
\Big)
\tag{7.31}\]</span></span></p>
<p>When we take the gradient with respect to the parameters <span class="math inline">\(\theta\)</span>, only the <span class="math inline">\(\log \pi_\theta(a_h\mid s_h)\)</span> terms depend on <span class="math inline">\(\theta\)</span>:</p>
<p><span id="eq-grad-log-autoregressive"><span class="math display">\[
\nabla \log \rho^{\pi_\theta}(\tau)
=
\sum_{h=0}^{H-1} \nabla \log \pi_\theta(a_h\mid s_h).
\tag{7.32}\]</span></span></p>
<p>Substituting this into <a href="#eq-reinforce-grad-log-prob" class="quarto-xref">eq.&nbsp;<span>7.30</span></a> gives</p>
<p><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}}
\left[
    \sum_{h=0}^{H-1} R(\tau) \nabla \log \pi_\theta(a_h\mid s_h)
\right] \\
&amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [g^\text{R}_\theta(\tau)],
\end{aligned}
\]</span></p>
<p>showing that the REINFORCE policy gradient (<a href="#eq-estimator-reinforce" class="quarto-xref">eq.&nbsp;<span>7.25</span></a>) is unbiased.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em> (Proof via importance sampling.). </span>Another way of deriving <a href="#thm-unbiased-reinforce" class="quarto-xref">Theorem&nbsp;<span>7.2</span></a> involves a technique known as <strong>importance sampling</strong>. We’ll demonstrate this approach here since it will come in handy later on. Importance sampling is useful when we want to estimate an expectation over a distribution that is hard to sample from. Instead, we can sample from a different distribution that supports the same values, and then reweight the samples according to the likelihood ratio between the two distributions.</p>
<div id="thm-importance-sampling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.3 (Importance sampling)</strong></span> Consider some random variable <span class="math inline">\(x \in \mathcal{X}\)</span> with density function <span class="math inline">\(p\)</span>. Let <span class="math inline">\(q\)</span> be the density function of another distribution on <span class="math inline">\(\mathcal{X}\)</span> that supports all of <span class="math inline">\(p\)</span>, that is, <span class="math inline">\(q(x) = 0\)</span> only if <span class="math inline">\(p(x) = 0\)</span>. Then</p>
<p><span id="eq-importance-sampling"><span class="math display">\[
\mathop{\mathbb{E}}_{x \sim p}[f(x)] = \mathop{\mathbb{E}}_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right].
\tag{7.33}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We expand the definition of expected value:</p>
<p><span id="eq-importance-sampling-derivation"><span class="math display">\[
\begin{aligned}
\mathop{\mathbb{E}}_{x \sim p}[f(x)] &amp;= \sum_{x \in \mathcal{X}} f(x) p(x) \\
&amp;= \sum_{x \in \mathcal{X}} f(x) \frac{p(x)}{q(x)} q(x) \\
&amp;= \mathop{\mathbb{E}}_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right].
\end{aligned}
\tag{7.34}\]</span></span></p>
</div>
<div id="exr-importance-sampling" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.3 (Importance sampling for a biased coin)</strong></span> Suppose you are a student and you determine your study routine by flipping a biased coin. Let <span class="math inline">\(x \in \{ \text{heads}, \text{tails} \}\)</span> be the result of the coin flip. The coin shows heads twice as often as it shows tails:</p>
<p><span id="eq-importance-sampling-biased-coin"><span class="math display">\[
p(x) = \begin{cases}
2/3 &amp; x = \text{heads} \\
1/3 &amp; x = \text{tails}.
\end{cases}
\tag{7.35}\]</span></span></p>
<p>Suppose you study for <span class="math inline">\(f(x)\)</span> hours, where</p>
<p><span id="eq-importance-sampling-f"><span class="math display">\[
f(x) = \begin{cases}
1 &amp; x = \text{heads} \\
2 &amp; x = \text{tails}.
\end{cases}
\tag{7.36}\]</span></span></p>
<p>One day, you lose your coin, and have to replace it with a fair one, i.e.</p>
<p><span id="eq-importance-sampling-fair-coin"><span class="math display">\[
q(x) = 1/2,
\tag{7.37}\]</span></span></p>
<p>but you want to study for the same amount on average. Suppose you decide to do this by importance sampling with the new coin. Now, upon flipping heads or tails, you study for</p>
<p><span id="eq-importance-sampling-study"><span class="math display">\[
\begin{aligned}
\frac{p(\text{heads})}{q(\text{heads})} f(\text{heads}) = \frac{4}{3} \\
\frac{p(\text{tails})}{q(\text{tails})} f(\text{tails}) = \frac{2}{3}
\end{aligned}
\tag{7.38}\]</span></span></p>
<p>hours respectively. Verify that your expected time spent studying is the same as before. Now compute the <em>variance</em> in the time you spend studying. Does it change?</p>
</div>
<p>Returning to the RL setting, we can compute the policy gradient by importance sampling from any trajectory distribution <span class="math inline">\(\rho\)</span>. (All gradients are being taken with respect to <span class="math inline">\(\theta\)</span>.)</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla J(\theta) &amp; = \nabla \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ R(\tau) ] \\
                     &amp; = \nabla \mathop{\mathbb{E}}_{\tau \sim \rho} \left[ \frac{\rho^{\pi_\theta}(\tau)}{\rho(\tau)} R(\tau) \right] \\
                     &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho} \left[ \frac{\nabla \rho^{\pi_\theta}(\tau)}{\rho(\tau)} R(\tau) \right].
\end{aligned}
\]</span></p>
<p>Setting <span class="math inline">\(\rho = \rho^{\pi_\theta}\)</span> reveals <a href="#eq-reinforce-grad-log-prob" class="quarto-xref">eq.&nbsp;<span>7.30</span></a>, and we can then proceed as we did in the previous proof.</p>
</div>
<p>Let us reiterate some intuition into how this method works. Recall that we update our parameters according to</p>
<p><span id="eq-update-reinforce"><span class="math display">\[
\begin{aligned}
    \theta_{t+1} &amp;= \theta_t + \eta \nabla J(\theta_t) \\
    &amp;= \theta^t + \eta \mathop{\mathbb{E}}_{\tau \sim \rho^{\theta_t}} [\nabla \log \rho^{\theta^t}(\tau) R(\tau)].
\end{aligned}
\tag{7.39}\]</span></span></p>
<p>Consider the “good” trajectories where <span class="math inline">\(R(\tau)\)</span> is large. Then <span class="math inline">\(\theta\)</span> gets updated so that these trajectories become more likely. To see why, recall that <span class="math inline">\(\log \rho^\theta(\tau)\)</span> is the log-likelihood of the trajectory <span class="math inline">\(\tau\)</span> under the policy <span class="math inline">\(\pi_\theta\)</span>, so the gradient points in the direction that makes <span class="math inline">\(\tau\)</span> more likely.</p>
<p>However, the REINFORCE gradient estimator <span class="math inline">\(g^\text{R}_\tau(\theta)\)</span> has large variance. Intuitively, this is because it uses the total reward from the entire trajectory, which depends on the entire sequence of interactions with the environment, each step of which introducesrandomness. The rest of this chapter investigates ways to find <em>lower-variance</em> policy gradient estimators.</p>
</section>
<section id="sec-baselines" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="sec-baselines"><span class="header-section-number">7.4.3</span> Baselines and advantages</h3>
<p>A central idea from statistical learning is the <strong>bias-variance decomposition</strong>, which shows that the mean squared error of an estimator is the sum of its squared bias and its variance. All of the policy gradient estimators we will see in this chapter are already unbiased, i.e., their mean over trajectories equals the true policy gradient (<a href="#def-unbiased-estimator" class="quarto-xref">def.&nbsp;<span>7.2</span></a>). Can we construct estimators with lower <em>variance</em> as well?</p>
<p>As a first step, note that the action taken at step <span class="math inline">\(h\)</span> does not causally affect the reward from previous timesteps, since they’re already in the past. So we should only use the reward from the current timestep onwards to estimate the policy gradient.</p>
<div id="thm-unbiased-remaining" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.4 (Using the remaining reward is unbiased)</strong></span> Substituting the reamining reward <span class="math inline">\(\sum_{h'=h}^{H-1} r_{h'}\)</span> for <span class="math inline">\(\psi_h(\tau)\)</span> into the general policy gradient estimator <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> gives an unbiased estimator. That is,</p>
<p><span id="eq-unbiased-remaining"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ g^\text{rem}_\tau(\theta) ] \\
\text{where} \quad
g^\text{rem}_{\tau} (\theta)
&amp;:=
\sum_{h=0}^{H-1}
\left( \sum_{h'=h}^{H-1} r_{h'} \right)
\nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.40}\]</span></span></p>
</div>
<div id="exr-future-rewards" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.4 (Unbiasedness of remaining reward estimator)</strong></span> We leave the proof of <a href="#thm-unbiased-remaining" class="quarto-xref">Theorem&nbsp;<span>7.4</span></a> as an exercise.</p>
</div>
<p>By a conditioning argument, we can replace the remaining reward with the policy’s Q-function, evaluated at the current state. By the same reasoning as above, this also reduces the variance, since the only stochasticity in the expression <span class="math inline">\(Q^{\pi_\theta}(s_h, a_h)\)</span> comes from the current state and action.</p>
<div id="thm-unbiased-q" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.5 (Using the Q function is unbiased)</strong></span> Substituting <span class="math inline">\(Q^{\pi_\theta}(s_h, a_h)\)</span> for <span class="math inline">\(\psi_h(\tau)\)</span> into the general policy gradient estimator <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> gives an unbiased estimator. That is,</p>
<p><span id="eq-estimator-q"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ g^\text{Q}_\tau(\theta) ] \\
\text{where} \quad
g^\text{Q}_{\tau} (\theta)
&amp;:=
\sum_{h=0}^{H-1}
Q^{\pi_\theta}(s_h, a_h)
\nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.41}\]</span></span></p>
</div>
<div id="exr-prove-q-unbiased" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.5 (Unbiasedness of remaining reward estimator)</strong></span> We also leave the proof of <a href="#thm-unbiased-q" class="quarto-xref">Theorem&nbsp;<span>7.5</span></a> as an exercise.</p>
</div>
<p>We can further reduce variance by subtracting a <strong>baseline function</strong> <span class="math inline">\(b_\theta : \mathcal{S} \to \mathbb{R}\)</span>. Note that this function could also depend on the current policy parameters.</p>
<div id="thm-unbiased-baseline" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.6 (Subtracting a baseline function preserves unbiasedness)</strong></span> Let <span class="math inline">\(b_\theta : \mathcal{S} \to \mathbb{R}\)</span> be some baseline function, and let <span class="math inline">\(\psi\)</span> be a gradient coefficient function that yields an unbiased policy gradient estimator (e.g. <a href="#eq-estimator-reinforce" class="quarto-xref">eq.&nbsp;<span>7.25</span></a> or <a href="#eq-estimator-q" class="quarto-xref">eq.&nbsp;<span>7.43</span></a>). Substituting</p>
<p><span id="eq-estimator-baseline"><span class="math display">\[
\psi^{\text{bl}}_h(\tau) := \psi_h(\tau) - b_\theta(s_h)
\tag{7.42}\]</span></span></p>
<p>into the general policy gradient estimator <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> gives an unbiased policy gradient estimator. That is,</p>
<p><span id="eq-estimator-q"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ g^\text{bl}_\tau(\theta) ] \\
\text{where} \quad
g^\text{bl}_{\tau} (\theta)
&amp;:=
\sum_{h=0}^{H-1}
(\psi_h(\tau) - b_\theta(s_h))
\nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.43}\]</span></span></p>
</div>
<div id="exr-prove-baseline-unbiased" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.6 (Unbiasedness of baseline estimator)</strong></span> We leave the proof of <a href="#thm-unbiased-baseline" class="quarto-xref">Theorem&nbsp;<span>7.6</span></a> as an exercise as well.</p>
</div>
<p>For example, we might want <span class="math inline">\(b_h\)</span> to estimate the average remaining reward at a given timestep:</p>
<p><span id="eq-exm-remaining"><span class="math display">\[
b_\theta(s_h) = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{h'=h}^{H-1} r_{h'}
\right].
\tag{7.44}\]</span></span></p>
<p>As a better baseline, we could instead choose the <em>value function</em> of <span class="math inline">\(\pi_{\theta}\)</span>. For any policy <span class="math inline">\(\pi\)</span>, note that the random variable <span class="math inline">\(Q^\pi_h(s, a) - V^\pi_h(s)\)</span>, where the randomness is taken over the action <span class="math inline">\(a\)</span>, is centered around zero. (Recall <span class="math inline">\(V^\pi_h(s) = \mathop{\mathbb{E}}_{a \sim \pi} Q^\pi_h(s, a).\)</span>) This quantity matches the intuition given in <a href="#sec-pg-intro" class="quarto-xref"><span>Section 7.4</span></a>: it is <em>positive</em> for actions that are better than average (in state <span class="math inline">\(s\)</span>), and <em>negative</em> for actions that are worse than average. In fact, it has a particular name: the <strong>advantage function.</strong></p>
<div id="def-advantage" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.4 (Advantage function)</strong></span> For a policy <span class="math inline">\(\pi\)</span>, its advantage function <span class="math inline">\(A^\pi\)</span> at time <span class="math inline">\(h\)</span> is given by</p>
<p><span id="eq-advantage"><span class="math display">\[
A^\pi_h(s, a) := Q^\pi_h(s, a) - V^\pi_h(s).
\tag{7.45}\]</span></span></p>
</div>
<p>Note that for an optimal policy <span class="math inline">\(\pi^\star\)</span>, the advantage of a given state-action pair is always zero or negative.</p>
<p>We can now use <span class="math inline">\(A^{\pi_{\theta}}(s_h, a_h)\)</span> for the gradient coefficients to obtain the ultimate unbiased policy gradient estimator.</p>
<div id="thm-unbiased-adv" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.7 (Using the advnatage function is unbiased)</strong></span> Substituting</p>
<p><span id="eq-estimator-adv"><span class="math display">\[
\psi_h(\tau) := A^{\pi_\theta}(s_h, a_h)
\tag{7.46}\]</span></span></p>
<p>into the general policy gradient estimator <a href="#eq-general-pg" class="quarto-xref">eq.&nbsp;<span>7.23</span></a> gives an unbiased estimator. That is,</p>
<p><span id="eq-estimator-adv"><span class="math display">\[
\begin{aligned}
\nabla J(\theta) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} [ g^\text{adv}_\tau(\theta) ] \\
\text{where} \quad
g^\text{adv}_{\tau} (\theta)
&amp;:=
\sum_{h=0}^{H-1}
A^{\pi_\theta}(s_h, a_h)
\nabla \log \pi_{\theta}(a_h\mid s_h).
\end{aligned}
\tag{7.47}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>This follows directly from <a href="#thm-unbiased-q" class="quarto-xref">Theorem&nbsp;<span>7.5</span></a> and <a href="#thm-unbiased-baseline" class="quarto-xref">Theorem&nbsp;<span>7.6</span></a>.</p>
</div>
<p>Note that to avoid correlations between the gradient estimator and the value estimator (i.e.&nbsp;baseline), we must estimate them with independently sampled trajectories:</p>
<!-- TODO explain _why_ we want to avoid correlations -->
<div id="27f23ec0" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pg_with_learned_baseline(env: gym.Env, pi, eta: <span class="bu">float</span>, theta_init, K: <span class="bu">int</span>, N: <span class="bu">int</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" D"</span>]:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), N)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        V_hat <span class="op">=</span> fit_value(trajectories)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        tau <span class="op">=</span> sample_trajectories(env, pi(theta), <span class="dv">1</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        nabla_hat <span class="op">=</span> jnp.zeros_like(theta)  <span class="co"># gradient estimator</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h, (s, a) <span class="kw">in</span> <span class="bu">enumerate</span>(tau):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> log_likelihood(theta_opt):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> jnp.log(pi(theta_opt)(s, a))</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            nabla_hat <span class="op">=</span> nabla_hat <span class="op">+</span> jax.grad(log_likelihood)(theta) <span class="op">*</span> (return_to_go(tau, h) <span class="op">-</span> V_hat(s))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> nabla_hat</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>latex(pg_with_learned_baseline)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="8">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{pg\_with\_learned\_baseline}(\mathrm{env}: \mathrm{gym}.\mathrm{Env}, \pi, \eta: \mathbb{R}, \theta_{\mathrm{init}}, K: \mathbb{Z}, N: \mathbb{Z}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( K \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), N \mathclose{}\right) \\ \hspace{2em} \widehat{V} \gets \mathrm{fit\_value} \mathopen{}\left( \mathrm{trajectories} \mathclose{}\right) \\ \hspace{2em} \tau \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), 1 \mathclose{}\right) \\ \hspace{2em} \widehat{\nabla} \gets \mathrm{jnp}.\mathrm{zeros\_like} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{2em} \mathbf{for} \ \mathopen{}\left( h, \mathopen{}\left( s, a \mathclose{}\right) \mathclose{}\right) \in \mathrm{enumerate} \mathopen{}\left( \tau \mathclose{}\right) \ \mathbf{do} \\ \hspace{3em} \mathbf{function} \ \mathrm{log\_likelihood}(\theta_{\mathrm{opt}}) \\ \hspace{4em} \mathbf{return} \ \log \pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{3em} \mathbf{end \ function} \\ \hspace{3em} \widehat{\nabla} \gets \widehat{\nabla} + \nabla \mathopen{}\left(\mathrm{log\_likelihood}\mathclose{}\right) \mathopen{}\left( \theta \mathclose{}\right) \cdot \mathopen{}\left( \mathrm{return\_to\_go} \mathopen{}\left( \tau, h \mathclose{}\right) - \widehat{V} \mathopen{}\left( s \mathclose{}\right) \mathclose{}\right) \\ \hspace{2em} \mathbf{end \ for} \\ \hspace{2em} \theta \gets \theta + \eta \widehat{\nabla} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
</div>
<p>Note that you could also generalize this by allowing the learning rate <span class="math inline">\(\eta\)</span> to vary across steps, or take multiple trajectories <span class="math inline">\(\tau\)</span> and compute the sample mean of the gradient estimates.</p>
<p>The baseline estimation step <code>fit_value</code> can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.</p>
<div id="exm-linear-policy-grad" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.5 (Policy gradient for the linear-in-features parameterization)</strong></span> The gradient-log-likelihood for the linear parameterization <a href="#exm-linear-in-features" class="quarto-xref">ex.&nbsp;<span>7.2</span></a> is also quite elegant:</p>
<p><span class="math display">\[
\begin{aligned}
        \nabla \log \pi_\theta(a|s) &amp;= \nabla \left( \theta^\top \phi(s, a) - \log \left( \sum_{a'} \exp(\theta^\top \phi(s, a')) \right) \right) \\
        &amp;= \phi(s, a) - \mathop{\mathbb{E}}_{a' \sim \pi_\theta(s)} \phi(s, a')
\end{aligned}
\]</span></p>
<p>Plugging this into our policy gradient expression, we get</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla J(\theta) &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_h| s_h) A_h^{\pi_\theta}
    \right]                                                                                                                    \\
                     &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{t=0}^{T-1} \left( \phi(s_h, a_h) - \mathop{\mathbb{E}}_{a' \sim \pi(s_h)} \phi(s_h, a') \right) A_h^{\pi_\theta}(s_h, a_h)
    \right]                                                                                                                    \\
                     &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{t=0}^{T-1} \phi(s_h, a_h) A_h^{\pi_\theta} (s_h, a_h) \right]
\end{aligned}
\]</span></p>
<p>Why can we drop the <span class="math inline">\(\mathop{\mathbb{E}}\phi(s_h, a')\)</span> term? By linearity of expectation, consider the dropped term at a single timestep: <span class="math inline">\(\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[ \left( \mathop{\mathbb{E}}_{a' \sim \pi(s_h)} \phi(s, a') \right) A_h^{\pi_\theta}(s_h, a_h) \right].\)</span> By Adam’s Law, we can wrap the advantage term in a conditional expectation on the state <span class="math inline">\(s_h.\)</span> Then we already know that <span class="math inline">\(\mathop{\mathbb{E}}_{a \sim \pi(s)} A_h^{\pi}(s, a) = 0,\)</span> and so this entire term vanishes.</p>
</div>
</section>
</section>
<section id="comparing-policy-gradient-algorithms-to-policy-iteration" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="comparing-policy-gradient-algorithms-to-policy-iteration"><span class="header-section-number">7.5</span> Comparing policy gradient algorithms to policy iteration</h2>
<p>What advantages do policy gradient algorithms have over the policy iteration algorithms covered in <a href="mdps.html#sec-pi" class="quarto-xref"><span>Section 2.4.4.2</span></a>?</p>
<div id="rem-policy-iteration" class="proof remark">
<p><span class="proof-title"><em>Remark 7.7</em> (Policy iteration review). </span>Recall that policy iteration is an algorithm for MDPs with unknown state transitions where we alternate between the following two steps:</p>
<ul>
<li>Estimating the <span class="math inline">\(Q\)</span>-function (or advantage function) of the current policy;</li>
<li>Updating the policy to be greedy with respect to this approximate <span class="math inline">\(Q\)</span>-function (or advantage function).</li>
</ul>
</div>
<p>To analyze the difference between them, we’ll make use of the <strong>performance difference lemma</strong>, which provides an expression for comparing the difference between two value functions.</p>
<div id="thm-pdl" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.8 (Performance difference lemma <span class="citation" data-cites="kakade_approximately_2002">(<a href="references.html#ref-kakade_approximately_2002" role="doc-biblioref">S. Kakade &amp; Langford, 2002</a>, Lemma 6.1)</span>)</strong></span> Suppose Alice is playing a game (an MDP). Bob is spectating, and can evaluate how good an action is compared to his own strategy. (That is, Bob can compute his <em>advantage function</em> <span class="math inline">\(A_h^{\text{Bob}}(s_h, a_h)\)</span>). The performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:</p>
<p><span id="eq-pdl"><span class="math display">\[
V_0^{\text{Alice}}(s) - V_0^{\text{Bob}}(s) = \mathop{\mathbb{E}}_{\tau \sim \rho^\text{Alice}} \left[ \sum_{h=0}^{H-1} A_h^{\text{Bob}} (s_h, a_h) \mid s_0 = s \right]
\tag{7.48}\]</span></span></p>
<p>where <span class="math inline">\(\rho^\text{Alice}\)</span> denotes Alice’s trajectory distribution (<a href="mdps.html#def-trajectory-distribution" class="quarto-xref">def.&nbsp;<span>2.7</span></a>).</p>
</div>
<p>To see why, consider a specific step <span class="math inline">\(h\)</span> in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average. But this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Formally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that</p>
<p><span id="eq-advantage-expansion"><span class="math display">\[
\begin{aligned}
A^\pi_h(s_h, a_h) &amp;= Q^\pi_h(s_h, a_h) - V^\pi_h(s_h) \\
&amp;= r_h(s_h, a_h) + \mathop{\mathbb{E}}_{s_{h+1} \sim P(\cdot \mid s_h, a_h)} [V^\pi_{h+1}(s_{h+1})] - V^\pi_h(s_h)
\end{aligned}
\tag{7.49}\]</span></span></p>
<p>so expanding out the r.h.s. expression of <a href="#eq-pdl" class="quarto-xref">eq.&nbsp;<span>7.48</span></a> and grouping terms together gives</p>
<p><span id="eq-pdl-proof"><span class="math display">\[
\begin{aligned}
\mathop{\mathbb{E}}_{\tau \sim \rho^\text{Alice}} \left[ \sum_{h=0}^{H-1} A_h^{\text{Bob}} (s_h, a_h) \mid s_0 = s \right]
&amp;= \mathop{\mathbb{E}}_{\tau \sim \rho_\text{Alice}} \bigg[
    \left( \sum_{h=0}^{H-1} r_h(s_h, a_h) \right) \\
&amp; \quad {} + \left( V^{\text{Bob}}_1(s_1) + \cdots + V^{\text{Bob}}_H(s_H) \right) \\
&amp; \quad {} - \left( V^{\text{Bob}}_0(s_0) + \cdots + V^{\text{Bob}}_{H-1}(s_{H-1}) \right)
    \mid s_0 = s \bigg] \\
&amp;= V^{\text{Alice}}_0(s) - V^{\text{Bob}}_0(s).
\end{aligned}
\tag{7.50}\]</span></span></p>
<p>as desired. Note that the “inner” expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid. Also note that <span class="math inline">\(V_H^\pi\)</span>, the value after reaching a terminal state, is always zero for any policy <span class="math inline">\(\pi\)</span>.</p>
</div>
<p>The PDL gives insight into why fitted approaches such as PI don’t work as well in the “full” RL setting. To see why, let’s consider a single iteration of policy iteration, where policy <span class="math inline">\(\pi\)</span> gets updated to <span class="math inline">\(\widetilde \pi\)</span>. We’ll assume these policies are deterministic. Define <span class="math inline">\(\Delta_\infty\)</span> to be the most negative advantage:</p>
<p><span id="eq-compare-advantage"><span class="math display">\[
\Delta_\infty = \min_{s \in \mathcal{S}} A^{\pi}_h(s, \widetilde \pi(s)).
\tag{7.51}\]</span></span></p>
<p>Suppose <span class="math inline">\(\Delta_\infty &lt; 0\)</span>, i.e.&nbsp;there exists a state <span class="math inline">\(s\)</span> such that</p>
<p><span id="eq-neg-advantage"><span class="math display">\[
A^\pi(s, \widetilde \pi(s)) &lt; 0,
\tag{7.52}\]</span></span></p>
<p>that is, if <span class="math inline">\(\widetilde \pi\)</span> acts for just one turn from state <span class="math inline">\(s\)</span> and then <span class="math inline">\(\pi\)</span> acts thereafter, the result would be worse on average than allowing <span class="math inline">\(\pi\)</span> to act. Plugging this into the PDL (<a href="#thm-pdl" class="quarto-xref">Theorem&nbsp;<span>7.8</span></a>) gives</p>
<p><span class="math display">\[
\begin{aligned}
V_0^{\widetilde \pi}(s) - V_0^{\pi}(s) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\widetilde \pi}} \left[
\sum_{h=0}^{H-1} A_h^{\pi}(s_h, a_h)
\mid s_0 = s
\right] \\
&amp;\ge H \Delta_\infty \\
V_0^{\widetilde \pi}(s) &amp;\ge V_0^{\pi}(s) - H|\Delta_\infty|.
\end{aligned}
\]</span></p>
<p>That is, for some state <span class="math inline">\(s\)</span>, the lower bound on the performance of <span class="math inline">\(\widetilde \pi\)</span> is <em>lower</em> than the performance of <span class="math inline">\(\pi\)</span>. This doesn’t state that <span class="math inline">\(\widetilde \pi\)</span> <em>will</em> necessarily perform worse than <span class="math inline">\(\pi\)</span>, only suggests that it might be possible. If these worst case states do exist, though, PI does not avoid situations where the new policy often visits them; It does not enforce that the trajectory distributions <span class="math inline">\(\rho^\pi\)</span> and <span class="math inline">\(\rho^{\widetilde \pi}\)</span> be close to each other. In other words, PI falls prey to <strong>distributional shift</strong>: the “training distribution” that our prediction rule is fitted on, <span class="math inline">\(\rho^\pi\)</span>, may differ significantly from the “evaluation distribution” <span class="math inline">\(\rho^{\widetilde \pi}\)</span>.</p>
<p>On the other hand, policy gradient methods <em>do</em>, albeit implicitly, encourage <span class="math inline">\(\rho^\pi\)</span> and <span class="math inline">\(\rho^{\widetilde \pi}\)</span> to be similar. Suppose that the mapping from policy parameters to trajectory distributions is relatively smooth. Then, by adjusting the parameters only a small distance, the new policy will also have a similar trajectory distribution. But this is not very rigorous, and in practice the parameter-to-distribution mapping may not be so smooth. Can we constrain the distance between the resulting distributions more <em>explicitly</em>?</p>
<p>This brings us to the following local policy optimization methods:</p>
<ol type="1">
<li><strong>trust region policy optimization</strong> (TRPO), which explicitly constrains the difference between the distributions before and after each step;</li>
<li>the <strong>natural policy gradient</strong> (NPG), a first-order approximation of TRPO;</li>
<li><strong>proximal policy optimization</strong> (PPO-penalty), a “soft relaxation” of TRPO;</li>
<li>the <strong>clipped surrogate objective</strong> (PPO-clip), a version of PPO that is popular in practice.</li>
</ol>
<div id="rem-ppo-ordering" class="proof remark">
<p><span class="proof-title"><em>Remark 7.8</em> (Ordering of algorithms). </span>Chronologically, NPG was developed first, followed by TRPO and later PPO. We begin with TRPO since it sets up the intuition behind these constrained methods.</p>
</div>
</section>
<section id="sec-trpo" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="sec-trpo"><span class="header-section-number">7.6</span> Trust region policy optimization</h2>
<p>We saw above that policy gradient methods are effective because they implicitly constrain how much the policy changes at each iteration in terms of its trajectory distribution <span class="math inline">\(\rho^\pi\)</span>. What happens if we <em>explicitly</em> constrain the distance between the new and old trajectory distributions? This requires some way to measure the <em>distance</em> between two trajectory distributions. For this, we introduce the <strong>Kullback-Leibler divergence</strong>.</p>
<div id="def-kl-divergence" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.5 (Kullback-Leibler divergence)</strong></span> For two PDFs <span class="math inline">\(p, q\)</span>,</p>
<p><span id="eq-kld"><span class="math display">\[
\mathrm{KL}\left(p\parallel q\right) := \mathop{\mathbb{E}}_{x \sim p} \left[ \log \frac{p(x)}{q(x)} \right].
\tag{7.53}\]</span></span></p>
</div>
<p>The Kullback-Leibler divergence can be interpreted in many different ways, many stemming from information theory. One such interpretation is that <span class="math inline">\(\mathrm{KL}\left(p\parallel q\right)\)</span> describes how much more surprised you are if you <em>think</em> data is being generated by <span class="math inline">\(q\)</span> but it’s actually generated by <span class="math inline">\(p\)</span>, compared to someone who knows the true distribution <span class="math inline">\(p\)</span>. (The <strong>surprise</strong> of an event with probability <span class="math inline">\(p\)</span> is <span class="math inline">\(- \log_2 p\)</span>.)</p>
<p>It can be shown that <span class="math inline">\(\mathrm{KL}\left(p\parallel q\right) = 0\)</span> if and only if <span class="math inline">\(p = q\)</span>. Also note that it is generally <em>not</em> symmetric, that is, <span class="math inline">\(\mathrm{KL}\left(p\parallel q\right) \neq \mathrm{KL}\left(q\parallel p\right)\)</span>. How can we interpret this asymmetry?</p>
<div id="rem-kl-asymmetry" class="proof remark">
<p><span class="proof-title"><em>Remark 7.9</em> (Asymmetry of the Kullback-Leibler divergence). </span>Note that the KL divergence gets large if <span class="math inline">\(p(x)/q(x)\)</span> is very large for some <span class="math inline">\(x\)</span>, that is, <span class="math inline">\(q\)</span> assigns low probability to a common event under <span class="math inline">\(p\)</span>. So if we minimize the KL divergence with respect to the second argument <span class="math inline">\(q\)</span>, the “prediction” distribution, <span class="math inline">\(q\)</span> will “spread out” to cover all common events under <span class="math inline">\(p\)</span>. If we minimize the KL divergence with respect to the first argument <span class="math inline">\(p\)</span>, the data generating distribution, <span class="math inline">\(p\)</span> will “squeeze under” <span class="math inline">\(q\)</span>, so that <span class="math inline">\(p(x)\)</span> is small wherever <span class="math inline">\(q(x)\)</span> is small.</p>
</div>
<div id="cell-fig-exm-kld" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixture_pdf(x, mus, sigmas, weights):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    pdf_val <span class="op">=</span> np.zeros_like(x, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> mu, sigma, w <span class="kw">in</span> <span class="bu">zip</span>(mus, sigmas, weights):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        pdf_val <span class="op">+=</span> w <span class="op">*</span> norm.pdf(x, loc<span class="op">=</span>mu, scale<span class="op">=</span>sigma)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pdf_val</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_divergence(P, Q, x_grid):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    epsilon <span class="op">=</span> <span class="fl">1e-12</span>  <span class="co"># avoid division by zero</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    P_safe <span class="op">=</span> np.maximum(P, epsilon)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    Q_safe <span class="op">=</span> np.maximum(Q, epsilon)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    dx <span class="op">=</span> x_grid[<span class="dv">1</span>] <span class="op">-</span> x_grid[<span class="dv">0</span>]</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(P_safe <span class="op">*</span> np.log(P_safe <span class="op">/</span> Q_safe)) <span class="op">*</span> dx</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Q_pdf(x):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1/3 N(-5, 1) + 2/3 N(2, 1)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mixture_pdf(x, mus<span class="op">=</span>[<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>], sigmas<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>], weights<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>])</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Single Gaussian for the candidate</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> single_gaussian_pdf(x, params):</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    mu, log_sigma <span class="op">=</span> params</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> np.exp(log_sigma)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> norm.pdf(x, loc<span class="op">=</span>mu, scale<span class="op">=</span>sigma)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_objective_PQ(params, x_grid, Q_vals):</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># minimize KL(P_candidate || Q).</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    P_candidate_vals <span class="op">=</span> single_gaussian_pdf(x_grid, params)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kl_divergence(P_candidate_vals, Q_vals, x_grid)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_objective_QP(params, x_grid, Q_vals):</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># minimize KL(Q || P_candidate).</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    P_candidate_vals <span class="op">=</span> single_gaussian_pdf(x_grid, params)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kl_divergence(Q_vals, P_candidate_vals, x_grid)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>x_grid <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">201</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>Q_vals <span class="op">=</span> Q_pdf(x_grid)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Minimize each objective separately</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>init_params <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.0</span>]  <span class="co"># mu=0, log_sigma=0 -&gt; sigma=1</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>res_min_PQ <span class="op">=</span> minimize(kl_objective_PQ, x0<span class="op">=</span>init_params, args<span class="op">=</span>(x_grid, Q_vals), method<span class="op">=</span><span class="st">'L-BFGS-B'</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>res_min_QP <span class="op">=</span> minimize(kl_objective_QP, x0<span class="op">=</span>init_params, args<span class="op">=</span>(x_grid, Q_vals), method<span class="op">=</span><span class="st">'L-BFGS-B'</span>)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">2</span>))</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>plt.plot(x_grid, Q_vals, <span class="st">'-'</span>, label<span class="op">=</span><span class="st">"q"</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>plt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_QP.x), <span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$\arg\min_q \text</span><span class="sc">{KL}</span><span class="vs">(p \parallel q)$"</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>plt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_PQ.x), <span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$\arg\min_p \text</span><span class="sc">{KL}</span><span class="vs">(p \parallel q)$"</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>), plt.ylabel(<span class="st">'PDF'</span>)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-exm-kld" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exm-kld-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-exm-kld-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exm-kld-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.6: Minimizing the forward and backward Kullback-Leibler divergence against a bimodal distribution with respect to a unimodal distribution
</figcaption>
</figure>
</div>
</div>
</div>
<p>For trajectory distributions <span class="math inline">\(\rho^\pi_{\theta^i}\)</span> and <span class="math inline">\(\rho^\pi_{\theta'}\)</span> (<a href="mdps.html#def-trajectory-distribution" class="quarto-xref">def.&nbsp;<span>2.7</span></a>), the KL divergence can be broken down into a sum over timesteps:</p>
<p><span id="eq-kld-trajectories"><span class="math display">\[
\begin{aligned}
\mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right)
&amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \log \rho^{\pi_{\theta^i}}(\tau) - \log \rho^{\pi_{\theta'}}(\tau)
\right] \\
&amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1} \log \pi_{\theta^i}(a_h\mid s_h) - \log \pi_{\theta'}(a_h\mid s_h)
\right] \\
\end{aligned}
\tag{7.54}\]</span></span></p>
<p>since the terms corresponding to the state transitions and initial state distribution cancel out.</p>
<p>We can now use the KL divergence to explicitly constrain the distance between the new and old trajectory distributions:</p>
<p><span id="eq-trpo-intuition"><span class="math display">\[
\begin{aligned}
\theta^{t+1} &amp;\gets \arg\max_{{\theta'}\in \mathbb{R}^D} J({\theta'}) \\
&amp; \text{where } \mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right) &lt; \delta
\end{aligned}
\tag{7.55}\]</span></span></p>
<p>Note that we place <span class="math inline">\(\rho^\pi_{\theta'}\)</span> in the <em>second</em> argument to the KL divergence. This ensures that <span class="math inline">\(\rho^\pi_{\theta'}\)</span> supports all of the trajectories under <span class="math inline">\(\rho^\pi_{\theta^i}\)</span> (see <a href="#rem-kl-asymmetry" class="quarto-xref">Remark&nbsp;<span>7.9</span></a>).</p>
<p>In place of <span class="math inline">\(J\)</span>, if we use the performance difference lemma (<a href="#thm-pdl" class="quarto-xref">Theorem&nbsp;<span>7.8</span></a>) to compare the performance of the new policy to the old one, we obtain an</p>
<div id="def-trpo-update" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.6 (TRPO update rule)</strong></span> Let <span class="math inline">\({\theta^i}\in \mathbb{R}^D\)</span> denote the current policy parameter vector. The TRPO update rule is</p>
<p><span id="eq-trpo"><span class="math display">\[
\begin{aligned}
\theta^{k+1} &amp;\gets \arg\max_{{\theta'}\in \mathbb{R}^D}
\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1}
    \mathop{\mathbb{E}}_{a'_h\sim \pi^{{\theta'}}(\cdot \mid s_h)}
    A^{\pi_{\theta^i}}(s_h, a'_h)
\right] \\
&amp; \text{where } \mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right) &lt; \delta
\end{aligned}
\tag{7.56}\]</span></span></p>
</div>
<div id="rem-curr-states" class="proof remark">
<p><span class="proof-title"><em>Remark 7.10</em> (Drawing states from old policy). </span>Note that we have made a small change to the <a href="#eq-pdl" class="quarto-xref">eq.&nbsp;<span>7.48</span></a>: we use the <em>current</em> policy’s trajectory distribution, and re-sample <em>actions</em> from the <em>updated</em> policy. This allows us to reuse a single batch of trajectories from <span class="math inline">\(\pi_{\theta^i}\)</span> rather than sample new batches from <span class="math inline">\(\pi_{\theta'}\)</span> when solving the optimization problem <a href="#eq-trpo" class="quarto-xref">eq.&nbsp;<span>7.56</span></a>. This approximation also matches the r.h.s. of the PDL to first order in <span class="math inline">\(\theta\)</span>. (We will elaborate more on this later.)</p>
</div>
<div id="cell-fig-alg-trpo" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trpo(env, δ, theta_init, n_interactions):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_interactions)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        A_hat <span class="op">=</span> fit_advantage(trajectories)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> approximate_gain(theta_opt):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>            A_total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tau <span class="kw">in</span> trajectories:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> s, _a, _r <span class="kw">in</span> tau:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> a <span class="kw">in</span> env.action_space:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>                        A_total <span class="op">+=</span> pi(theta)(s, a) <span class="op">*</span> A_hat(s, a)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> A_total</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> constraint(theta_opt):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> kl_div_trajectories(pi, theta, theta_opt, trajectories) <span class="op">&lt;=</span> δ</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> optimize(approximate_gain, constraint)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>latex(trpo)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-alg-trpo" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="10">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alg-trpo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{trpo}(\mathrm{env}, δ, \theta_{\mathrm{init}}, n_{\mathrm{interactions}}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( K \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), n_{\mathrm{interactions}} \mathclose{}\right) \\ \hspace{2em} \widehat{A} \gets \mathrm{fit\_advantage} \mathopen{}\left( \mathrm{trajectories} \mathclose{}\right) \\ \hspace{2em} \mathbf{function} \ \mathrm{approximate\_gain}(\theta_{\mathrm{opt}}) \\ \hspace{3em} A_{\mathrm{total}} \gets 0 \\ \hspace{3em} \mathbf{for} \ \tau \in \mathrm{trajectories} \ \mathbf{do} \\ \hspace{4em} \mathbf{for} \ \mathopen{}\left( s, \mathrm{\_a}, \mathrm{\_r} \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{5em} \mathbf{for} \ a \in \mathrm{env}.\mathrm{action\_space} \ \mathbf{do} \\ \hspace{6em} A_{\mathrm{total}} \gets A_{\mathrm{total}} + \pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \cdot \widehat{A} \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{5em} \mathbf{end \ for} \\ \hspace{4em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{return} \ A_{\mathrm{total}} \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \mathbf{function} \ \mathrm{constraint}(\theta_{\mathrm{opt}}) \\ \hspace{3em} \mathbf{return} \ \mathrm{kl\_div\_trajectories} \mathopen{}\left( \pi, \theta, \theta_{\mathrm{opt}}, \mathrm{trajectories} \mathclose{}\right) \le δ \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \theta \gets \mathrm{optimize} \mathopen{}\left( \mathrm{approximate\_gain}, \mathrm{constraint} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alg-trpo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.7: Pseudocode for trust region policy optimization
</figcaption>
</figure>
</div>
</div>
<p>The above isn’t entirely complete: we still need to solve the actual optimization problem at each step. Unless we know additional properties of the problem, this is still a nonconvex constrained optimization problem that might be inefficient to solve. Do we need to solve for the exact objective function, though? Instead, if we assume that both the objective function and the constraint are somewhat smooth in terms of the policy parameters, we can use their <em>Taylor expansions</em> to give us a simpler optimization problem with a closed-form solution. This brings us to the <strong>natural policy gradient</strong> algorithm.</p>
</section>
<section id="sec-npg" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="sec-npg"><span class="header-section-number">7.7</span> Natural policy gradient</h2>
<p>In some sense, the <em>natural policy gradient</em> algorithm <span class="citation" data-cites="kakade_natural_2001">(<a href="references.html#ref-kakade_natural_2001" role="doc-biblioref">S. M. Kakade, 2001</a>)</span> is an <em>implementation</em> of trust region policy optimization (<a href="#fig-alg-trpo" class="quarto-xref">fig.&nbsp;<span>7.7</span></a>). Recall that in each TRPO update, we seek to maximize the expected total reward while keeping the updated policy close to the current policy in terms of Kullback-Leibler divergence (<a href="#def-kl-divergence" class="quarto-xref">def.&nbsp;<span>7.5</span></a>):</p>
<p><span id="eq-npg-simplify"><span class="math display">\[
\begin{gathered}
\theta^{k+1} \gets \arg\max_{{\theta'}\in \mathbb{R}^D} J({\theta'}) \\
\text{where } \mathrm{KL}\left({\theta^i}\parallel{\theta'}\right) \le \delta
\end{gathered}
\tag{7.57}\]</span></span></p>
<p>NPG uses the following simplification: we take local approximations of the objective and constraint functions, which results in a simple problem with a closed-form solution.</p>
<p>Concretely, we take a first-order Taylor approximation to the objective function about the current iterate <span class="math inline">\({\theta^i}\)</span>:</p>
<p><span id="eq-obj-linear"><span class="math display">\[
J({\theta'}) = J({\theta^i}) + ({\theta'}- {\theta^i})^\top \nabla J({\theta^i}) + O(\|{\theta'}- {\theta^i}\|^2).
\tag{7.58}\]</span></span></p>
<p>We also take a second-order Taylor approximation to the constraint function:</p>
<div id="thm-kl-quadratic" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7.9 (Quadratic approximation to KL divergence.)</strong></span> The second-order approximation to <span class="math inline">\(d^i(\theta) := \mathrm{KL}\left(p_{\theta^i}\parallel p_\theta\right)\)</span> is given by</p>
<p><span id="eq-kl-taylor-small"><span class="math display">\[
\begin{aligned}
d^i(\theta)
&amp;=
\frac{1}{2} (\theta - {\theta^i})^\top F_{\theta^i}(\theta - {\theta^i})
+
O(\|\theta - {\theta^i}\|^3),
\end{aligned}
\tag{7.59}\]</span></span></p>
<p>where <span class="math inline">\(F_{{\theta^i}}\)</span> is the <strong>Fisher information matrix</strong> (FIM) of the trajectory distribution <span class="math inline">\(\rho^{\pi_{\theta^i}}\)</span>. (We define the FIM below in <a href="#def-fisher-matrix" class="quarto-xref">def.&nbsp;<span>7.7</span></a>.)</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We leave the details as an exercise. Here is an outline:</p>
<ol type="1">
<li>Write down the Taylor expansion of <span class="math inline">\(d^i(\theta)\)</span> around <span class="math inline">\({\theta^i}\)</span>.</li>
<li>Show that the zeroth-order term <span class="math inline">\(d^i({\theta^i})\)</span> is zero.</li>
<li>Show that the gradient <span class="math inline">\(\nabla d^i(\theta) \vert_{\theta = {\theta^i}}\)</span> is zero.</li>
<li>Show that the Hessian <span class="math inline">\(\nabla^2 d^i(\theta) \vert_{\theta = {\theta^i}}\)</span> equals <span class="math inline">\(F_{\theta^i}\)</span>.</li>
</ol>
</div>
<div id="def-fisher-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.7 (Fisher information matrix)</strong></span> Let <span class="math inline">\(p_\theta\)</span> denote a distribution parameterized by <span class="math inline">\(\theta\)</span>. Its Fisher information matrix <span class="math inline">\(F_\theta\)</span> can be defined equivalently as:</p>
<p><span id="eq-fisher-matrix"><span class="math display">\[
\begin{aligned}
        F_{\theta} &amp; = \mathop{\mathbb{E}}_{x \sim p_\theta} \left[ (\nabla_\theta \log p_\theta(x)) (\nabla_\theta \log p_\theta(x))^\top \right] \\
                   &amp; = \mathop{\mathbb{E}}_{x \sim p_{\theta}} [- \nabla_\theta^2 \log p_\theta(x)].
\end{aligned}
\tag{7.60}\]</span></span></p>
</div>
<div id="rem-interpret-fisher" class="proof remark">
<p><span class="proof-title"><em>Remark 7.11</em> (Interpretation of the Fisher information matrix). </span>The Fisher information matrix is an important quantity when working with parameterized distributions. It has many possible interpretations. The first expression in <a href="#eq-fisher-matrix" class="quarto-xref">eq.&nbsp;<span>7.60</span></a> shows that it is the covariance matrix of the gradient-log-probability (also known as the <strong>score</strong>), and the second expression shows that it is the expected Hessian matrix of the negative-log-likelihood of the underlying distribution. It can also be shown that it is precisely the Hessian of the KL divergence (with respect to either one of the parameters).</p>
<p>Recall that the Hessian of a function describes its <em>curvature</em>. Concretely, suppose we have some parameter vector <span class="math inline">\(\theta \in \mathbb{R}^D\)</span> and we seek to measure how much <span class="math inline">\(p_\theta\)</span> changes if we shift <span class="math inline">\(\theta\)</span> by <span class="math inline">\(\delta \in \mathbb{R}^D\)</span>. The quantity <span class="math inline">\(\delta^\top F_\theta \delta\)</span> describes how rapidly the negative log-likelihood changes if we move by <span class="math inline">\(\delta\)</span>. (The zeroth and first order terms are both zero in our case due to properties of the KL divergence.)</p>
</div>
<p>Putting this together results in the following update rule:</p>
<div id="def-npg-update" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.8 (Natural policy gradient update)</strong></span> We aim to solve the constrained optimization problem <a href="#eq-npg-simplify" class="quarto-xref">eq.&nbsp;<span>7.57</span></a>. Upon taking first- and second-order approximations of the objective function and constraint function respectively, we obtain the update rule</p>
<p><span id="eq-npg-optimization"><span class="math display">\[
\begin{gathered}
    \theta^{k+1} \gets {\theta^i}+ \arg\max_{\Delta\theta \in \mathbb{R}^D} \nabla J({\theta^i})^\top \Delta\theta \\
    \text{where } \frac{1}{2} \Delta\theta^\top F_{{\theta^i}} \Delta\theta \le \delta,
\end{gathered}
\tag{7.61}\]</span></span></p>
<p>where <span class="math inline">\(F_{\theta^i}\)</span> is the Fisher information matrix of <span class="math inline">\(\rho^\pi_{\theta^i}\)</span>.</p>
</div>
<p><a href="#eq-npg-optimization" class="quarto-xref">eq.&nbsp;<span>7.61</span></a> is a convex optimization problem with a closed-form solution. To see why, it helps to visualize the case where <span class="math inline">\(\theta\)</span> is two-dimensional: the constraint describes the inside of an ellipse, and the objective function is linear, so we can find the extreme point on the boundary of the ellipse by setting the gradient of the Lagrangian to zero:</p>
<p><span id="eq-npg-lagrangian"><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\theta, \alpha)                     &amp; = \nabla J({\theta^i})^\top (\theta - {\theta^i}) - \alpha \left[ \frac{1}{2} (\theta - {\theta^i})^\top F_{{\theta^i}} (\theta - {\theta^i}) - \delta \right] \\
    \nabla \mathcal{L}(\theta^{k+1}, \alpha) &amp; := 0                                                                                                                                                             \\
    \implies \nabla J({\theta^i})        &amp; = \alpha F_{{\theta^i}} (\theta^{k+1} - {\theta^i})
\end{aligned}
\tag{7.62}\]</span></span></p>
<p>Rearranging gives the NPG update rule:</p>
<p><span id="eq-npg-update"><span class="math display">\[
\begin{aligned}
    \theta^{k+1}                           &amp; = {\theta^i}+ \eta F_{{\theta^i}}^{-1} \nabla J({\theta^i})                                                                                             \\
    \text{where } \eta                     &amp; = \sqrt{\frac{2 \delta}{\nabla J({\theta^i})^\top F_{{\theta^i}}^{-1} \nabla J({\theta^i})}}
\end{aligned}
\tag{7.63}\]</span></span></p>
<p>This gives us the closed-form update.</p>
<div id="rem-npg-efficiency" class="proof remark">
<p><span class="proof-title"><em>Remark 7.12</em> (Scalability of NPG). </span>Having a closed-form solution might at first seem like brilliant news. Is there a catch? The challenge lies in computing the inverse Fisher information matrix (FIM) <span class="math inline">\(F_{{\theta^i}}^{-1}\)</span>. Since it is an expectation over trajectories, computing it exactly is typically intractable. Instead, we could collect trajectories from the environment and approximate the expectation by a sample mean, since we can write the Fisher information matrix as</p>
<p><span id="eq-fisher-trajectory"><span class="math display">\[
F_{\theta} = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{h=0}^{H-1} (\nabla \log \pi_\theta (a_h\mid s_h)) (\nabla \log \pi_\theta(a_h\mid s_h))^\top \right].
\tag{7.64}\]</span></span></p>
<p>Note that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.</p>
<p>It turns out that to estimate the FIM to a relative error of <span class="math inline">\(\epsilon\)</span>, we need <span class="math inline">\(O(D / \epsilon^2)\)</span> samples <span class="citation" data-cites="vershynin_high-dimensional_2018">(<a href="references.html#ref-vershynin_high-dimensional_2018" role="doc-biblioref">Vershynin, 2018</a>, Remark 4.7.2)</span>. In order for the estimated FIM to be accurate enough to be useful, this can be too large to be practical. Taking the inverse also takes <span class="math inline">\(O(D^3)\)</span> time, which can be expensive if the parameter space is large.</p>
</div>
<div id="rem-npg-interpretation" class="proof remark">
<p><span class="proof-title"><em>Remark 7.13</em> (NPG accounts for curvature in the parameter space). </span>Let us compare the original policy gradient update (<a href="#eq-pg-optimization" class="quarto-xref">eq.&nbsp;<span>7.17</span></a>) to the NPG update (<a href="#eq-npg-update" class="quarto-xref">eq.&nbsp;<span>7.63</span></a>):</p>
<p><span id="eq-compare-pg-npg"><span class="math display">\[
\begin{aligned}
\theta^{t+1} &amp;= \theta^t + \eta \nabla J(\theta^t) &amp; \text{Policy gradient} \\
\theta^{t+1} &amp;= \theta^t + \eta F_{\theta^t}^{-1} \nabla J(\theta^t) &amp; \text{Natural policy gradient}
\end{aligned}
\tag{7.65}\]</span></span></p>
<p>The NPG update <strong>preconditions</strong> the gradient by the inverse Fisher information matrix. Speaking abstractly, this matrix accounts for the <strong>geometry of the parameter space</strong> in the following sense.</p>
<p>The typical gradient descent algorithm implicitly measures distances between parameters using the typical “flat” <em>Euclidean distance</em>:</p>
<p><span id="eq-exm-distance-euclidean"><span class="math display">\[
\text{distance}(\theta_0, \theta_1) = \| \theta_0 - \theta_1 \|.
\tag{7.66}\]</span></span></p>
<p>The NPG update measures distance between parameters as the KL divergence (<a href="#def-kl-divergence" class="quarto-xref">def.&nbsp;<span>7.5</span></a>) between their induced trajectory distributions:</p>
<p><span id="eq-exm-distance-kl"><span class="math display">\[
\text{distance}(\theta_0, \theta_1) = \mathrm{KL}\left(\rho^{\pi_{\theta_0}}\parallel\rho^{\pi_{\theta_1}}\right).
\tag{7.67}\]</span></span></p>
<p>Using a parameterized policy class is just a means to the end of optimizing the trajectory distribution, so it makes sense to optimize over the trajectory distributions directly. In fact, the NPG update is the only update that is invariant to reparameterizations of the policy space: if instead of <span class="math inline">\(\theta\)</span>, we used some transformation <span class="math inline">\(\phi(\theta)\)</span> to parameterize the policy, the NPG update would remain the same. This is why NPG is called a <strong>coordinate-free</strong> optimization algorithm.</p>
<p>We can illustrate this with the following example:</p>
</div>
<div id="exm-natural-simple" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.6 (Natural gradient on a simple problem)</strong></span> Let’s step away from RL and consider a simple optimization problem over Bernoulli distributions <span class="math inline">\(p_\theta \in \triangle(\{ 0, 1 \})\)</span>. This distribution space is parameterized by the success probability <span class="math inline">\(\theta \in [0, 1]\)</span>. The per-sample objective function is <span class="math inline">\(100\)</span> if the Bernoulli trial succeeds and <span class="math inline">\(1\)</span> if the trial fails.</p>
<p><span id="eq-natural-value"><span class="math display">\[
f(x) = \begin{cases}
100 &amp; x = 1 \\
1 &amp; x = 0.
\end{cases}
\tag{7.68}\]</span></span></p>
<p>The objective <span class="math inline">\(J(\theta)\)</span> is the average of <span class="math inline">\(f\)</span> over <span class="math inline">\(x \sim p_\theta\)</span>:</p>
<p><span id="eq-natural-reward-objective"><span class="math display">\[
\begin{aligned}
    J(\theta) &amp; = \mathop{\mathbb{E}}_{x \sim p_\theta} [f(x)] \\
    &amp;= 100 \theta + 1 (1 - \theta)
\end{aligned}
\tag{7.69}\]</span></span></p>
<p>We can think of the space of such distributions as the line between <span class="math inline">\((0, 1)\)</span> to <span class="math inline">\((1, 0)\)</span> on the Cartesian plane:</p>
<div id="a1b53670" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> x</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\pi(0)$"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\pi(1)$"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pg_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
<figcaption>The space of Bernoulli distributions.</figcaption>
</figure>
</div>
</div>
</div>
<p>Clearly the optimal distribution is the constant one <span class="math inline">\(\pi(1) = 1\)</span>. Suppose we optimize over the parameterized family <span class="math inline">\(\pi_\theta(1) = \frac{\exp(\theta)}{1+\exp(\theta)}\)</span>. Then our optimization algorithm should set <span class="math inline">\(\theta\)</span> to be unboundedly large. Then the “parameter-space” gradient is</p>
<p><span class="math display">\[
\nabla_\theta J(\pi_\theta) = \frac{99 \exp(\theta)}{(1 + \exp(\theta))^2}.
\]</span></p>
<p>Note that as <span class="math inline">\(\theta \to \infty\)</span> that the increments get closer and closer to <span class="math inline">\(0\)</span>; the rate of increase becomes exponentially slow.</p>
<p>However, if we compute the Fisher information “matrix” (which is just a scalar in this case), we can account for the geometry induced by the parameterization.</p>
<p><span class="math display">\[
\begin{aligned}
        F_\theta &amp; = \mathop{\mathbb{E}}_{x \sim \pi_\theta} [ (\nabla_\theta \log \pi_\theta(x))^2 ] \\
                 &amp; = \frac{\exp(\theta)}{(1 + \exp(\theta))^2}.
\end{aligned}
\]</span></p>
<p>This gives the natural gradient update</p>
<p><span class="math display">\[
\begin{aligned}
        \theta^{k+1} &amp; = {\theta^i}+ \eta F_{{\theta^i}}^{-1} \nabla_ \theta J({\theta^i}) \\
                     &amp; = {\theta^i}+ 99 \eta
\end{aligned}
\]</span></p>
<p>which increases at a constant rate, i.e.&nbsp;improves the objective more quickly than parameter-space gradient descent.</p>
</div>
<p>Though the NPG now gives a closed-form optimization step, it requires estimating and computing the inverse Fisher information matrix, which can be difficult or slow, especially as the parameter space gets large (<a href="#rem-npg-efficiency" class="quarto-xref">Remark&nbsp;<span>7.12</span></a>). Can we achieve a similar effect without the inverse Fisher information matrix? This brings us to the <strong>proximal policy optimization</strong> algorithm.</p>
</section>
<section id="sec-ppo-kl" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="sec-ppo-kl"><span class="header-section-number">7.8</span> Penalty-based proximal policy optimization</h2>
<p>We can relax the TRPO optimization problem (<a href="#eq-trpo" class="quarto-xref">eq.&nbsp;<span>7.56</span></a>) in a different way: Rather than imposing a <em>hard constraint</em></p>
<p><span id="eq-hard-constraint"><span class="math display">\[
\mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right) &lt; \delta,
\tag{7.70}\]</span></span></p>
<p>we can instead impose a <em>soft</em> constraint by subtracting <span class="math inline">\(\lambda \mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right)\)</span> from the function to be maximized. <span class="math inline">\(\lambda &gt; 0\)</span> is a coefficient that controls the tradeoff between the two terms. This gives the following objective <span class="citation" data-cites="schulman_proximal_2017">(<a href="references.html#ref-schulman_proximal_2017" role="doc-biblioref">Schulman et al., 2017</a>)</span>:</p>
<p><span id="eq-ppo-penalty"><span class="math display">\[
\begin{aligned}
\theta^{k+1} &amp;\gets \arg\max_{{\theta'}} \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[ \sum_{h=0}^{H-1} \mathop{\mathbb{E}}_{a'_h\sim \pi_{\theta}(s_h)} A^{\pi_{\theta^i}}(s_h, a'_h) \right] - \lambda \mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta'}}\right)
\end{aligned}
\tag{7.71}\]</span></span></p>
<p>This optimization problem is also known as the <strong>Lagrangian</strong> formulation of <a href="#eq-trpo" class="quarto-xref">eq.&nbsp;<span>7.56</span></a>.</p>
<p>How do we solve this optimization? Let us begin by simplifying the <span class="math inline">\(\mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta}}\right)\)</span> term. The state transitions cancel as in <a href="#eq-kld-trajectories" class="quarto-xref">eq.&nbsp;<span>7.54</span></a>, which gives us</p>
<p><span class="math display">\[
\begin{aligned}
    \mathrm{KL}\left(\rho^{\pi_{\theta^i}}\parallel\rho^{\pi_{\theta}}\right) &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[\log \frac{\rho^{\pi_{\theta^i}}(\tau)}{\rho^{\pi_{\theta}}(\tau)}\right]                                                       \\
                                           &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[ \sum_{h=0}^{H-1} \log \frac{\pi_{\theta^i}(a_h\mid s_h)}{\pi_{\theta}(a_h\mid s_h)}\right] \\
                                           &amp; = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[ \sum_{h=0}^{H-1} \log \frac{1}{\pi_{\theta}(a_h\mid s_h)}\right] + c
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c\)</span> doesn’t depend on <span class="math inline">\(\theta\)</span> and can be ignored. This gives the objective</p>
<p><span class="math display">\[
L^k(\theta)
=
\mathop{\mathbb{E}}_{s_0, \dots, s_{H-1} \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1} \mathop{\mathbb{E}}_{a_h\sim \pi_{\theta}(s_h)} A^{\pi_{\theta^i}}(s_h, a_h)
\right] - \lambda \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}}
\left[ \sum_{h=0}^{H-1} \log \frac{1}{\pi_{\theta}(a_h\mid s_h)}\right]
\]</span></p>
<p>Once again, this takes an expectation over trajectories. But here we cannot directly sample trajectories from <span class="math inline">\(\pi_{\theta^i}\)</span>, since in the first term, the actions actually come from <span class="math inline">\(\pi_\theta\)</span>. To make this term line up with the other expectation, we would need the actions to also come from <span class="math inline">\(\pi_{\theta^i}\)</span>. This should sound familiar: we want to estimate an expectation over one distribution by sampling from another. We can use importance sampling (<a href="#thm-importance-sampling" class="quarto-xref">Theorem&nbsp;<span>7.3</span></a>) to rewrite the inner expectation:</p>
<p><span id="eq-likelihood-ratio"><span class="math display">\[
\mathop{\mathbb{E}}_{a_h\sim \pi_{\theta'}(s_h)} A^{\pi_{\theta^i}}(s_h, a_h)
=
\mathop{\mathbb{E}}_{a_h\sim \pi_{\theta^i}(s_h)} \left[
    \frac{\pi_{\theta'}(a_h\mid s_h)}{\pi_{\theta^i}(a_h\mid s_h)} A^{\pi_{\theta^i}}(s_h, a_h)
\right]
\tag{7.72}\]</span></span></p>
<div id="rem-importance-interpretation" class="proof remark">
<p><span class="proof-title"><em>Remark 7.14</em> (Interpretation of likelihood ratio). </span>Suppose <span class="math inline">\(a^+\)</span> is a “good” action for <span class="math inline">\(\pi_{\theta^i}\)</span> in state <span class="math inline">\(s_h\)</span>, i.e.&nbsp;<span class="math inline">\(A^\pi_{\theta^i}(s_h, a^+) &gt; 0\)</span>. Then maximizing <a href="#eq-likelihood-ratio" class="quarto-xref">eq.&nbsp;<span>7.72</span></a> encourages <span class="math inline">\({\theta'}\)</span> to <em>increase</em> the probability ratio <span class="math inline">\(\pi_{\theta'}(a_h\mid s_h) / \pi_{\theta^i}(a_h\mid s_h)\)</span>.</p>
<p>Otherwise, if <span class="math inline">\(a^-\)</span> is a “bad” action for <span class="math inline">\(\pi_{\theta^i}\)</span> in state <span class="math inline">\(s_h\)</span> (i.e.&nbsp;<span class="math inline">\(A^\pi_{\theta^i}(s_h, a^-) &lt; 0\)</span>), then maximizing <a href="#eq-likelihood-ratio" class="quarto-xref">eq.&nbsp;<span>7.72</span></a> encourages <span class="math inline">\({\theta'}\)</span> to <em>decrease</em> the probability ratio <span class="math inline">\(\pi_{\theta'}(a_h\mid s_h) / \pi_{\theta^i}(a_h\mid s_h)\)</span>.</p>
</div>
<p>Now we can combine the expectations together to get the objective</p>
<div id="def-penalty-ppo" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.9 (Penalty objective)</strong></span> <span class="math display">\[
L^k(\theta) = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1} \left(
    \frac{\pi_\theta(a_h\mid s_h)}{\pi_{\theta^i}(a_h\mid s_h)} A^{\pi_{\theta^i}}(s_h, a_h)
    - \lambda \log \frac{1}{\pi_\theta(a_h\mid s_h)}
    \right)
\right]
\]</span></p>
</div>
<p>Now we can estimate this function by a sample mean over trajectories from <span class="math inline">\(\pi_{\theta^i}\)</span>. Remember that to complete a single iteration of PPO, we execute</p>
<p><span class="math display">\[
\theta^{k+1} \gets \arg\max_{\theta} L^k(\theta).
\]</span></p>
<p>If <span class="math inline">\(L^k\)</span> is differentiable, we can optimize it by gradient descent, completing a single iteration of PPO.</p>
<div id="3be510a9" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> TypeVar</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>State <span class="op">=</span> TypeVar(<span class="st">"State"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>Action <span class="op">=</span> TypeVar(<span class="st">"Action"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo_proximal(</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    env,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    pi: Callable[[Float[Array, <span class="st">" D"</span>]], Callable[[State, Action], <span class="bu">float</span>]],</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    λ: <span class="bu">float</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    theta_init: Float[Array, <span class="st">" D"</span>],</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    n_iters: <span class="bu">int</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    n_fit_trajectories: <span class="bu">int</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    n_sample_trajectories: <span class="bu">int</span>,</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        fit_trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_fit_trajectories)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        A_hat <span class="op">=</span> fit(fit_trajectories)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        sample_trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_sample_trajectories)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> objective(theta_opt):</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            total_objective <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tau <span class="kw">in</span> sample_trajectories:</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> s, a, _r <span class="kw">in</span> tau:</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>                    total_objective <span class="op">+=</span> pi(theta_opt)(s, a) <span class="op">/</span> pi(theta)(s, a) <span class="op">*</span> A_hat(s, a) <span class="op">+</span> λ <span class="op">*</span> jnp.log(pi(theta_opt)(s, a))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> total_objective <span class="op">/</span> n_sample_trajectories</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> optimize(objective, theta)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>latex(ppo_proximal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="12">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{ppo\_proximal}(\mathrm{env}, \pi: (\mathbb{R}^{D}) \rightarrow (\mathrm{State} \times \mathrm{Action}) \rightarrow \mathbb{R}, λ: \mathbb{R}, \theta_{\mathrm{init}}: \mathbb{R}^{D}, n_{\mathrm{iters}}: \mathbb{Z}, \mathrm{n\_fit\_trajectories}: \mathbb{Z}, \mathrm{n\_sample\_trajectories}: \mathbb{Z}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( n_{\mathrm{iters}} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{fit\_trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), \mathrm{n\_fit\_trajectories} \mathclose{}\right) \\ \hspace{2em} \widehat{A} \gets \mathrm{fit} \mathopen{}\left( \mathrm{fit\_trajectories} \mathclose{}\right) \\ \hspace{2em} \mathrm{sample\_trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), \mathrm{n\_sample\_trajectories} \mathclose{}\right) \\ \hspace{2em} \mathbf{function} \ \mathrm{objective}(\theta_{\mathrm{opt}}) \\ \hspace{3em} \mathrm{total\_objective} \gets 0 \\ \hspace{3em} \mathbf{for} \ \tau \in \mathrm{sample\_trajectories} \ \mathbf{do} \\ \hspace{4em} \mathbf{for} \ \mathopen{}\left( s, a, \mathrm{\_r} \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{5em} \mathrm{total\_objective} \gets \mathrm{total\_objective} + \frac{\pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right)}{\pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right)} \widehat{A} \mathopen{}\left( s, a \mathclose{}\right) + λ \cdot \log \pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{4em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{return} \ \frac{\mathrm{total\_objective}}{\mathrm{n\_sample\_trajectories}} \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \theta \gets \mathrm{optimize} \mathopen{}\left( \mathrm{objective}, \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
</div>
</section>
<section id="sec-ppo-clip" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="sec-ppo-clip"><span class="header-section-number">7.9</span> Advantage clipping</h2>
<p>Recall that the main point of proximal policy optimization methods (TRPO, NPG, PPO) is to encourage the updated policy (after taking a gradient step) to remain similar to the current one. These methods used the KL divergence to measure the distance between policies. <span class="citation" data-cites="schulman_proximal_2017">Schulman et al. (<a href="references.html#ref-schulman_proximal_2017" role="doc-biblioref">2017</a>)</span> proposed an alternative way to constrain the step size based on <em>clipping</em> a certain objective function. This method, known as “PPO-clip” or the “clipped surrogate” objective function, is the most widely used proximal policy optimization algorithm in practice.</p>
<p>Above, in <a href="#eq-likelihood-ratio" class="quarto-xref">eq.&nbsp;<span>7.72</span></a>, we constructed an objective function by applying importance sampling to the performance difference lemma (<a href="#thm-pdl" class="quarto-xref">Theorem&nbsp;<span>7.8</span></a>). Without the KL divergence penalty, this becomes</p>
<p><span id="eq-penalty-no-kl"><span class="math display">\[
L^k({\theta'}) = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1}
    \frac{\pi_{\theta'}(a_h\mid s_h)}{\pi_{\theta^i}(a_h\mid s_h)} A^{\pi_{\theta^i}}(s_h, a_h)
\right]
\approx V(\pi_{\theta'}) - V(\pi_{\theta^i}).
\tag{7.73}\]</span></span></p>
<p>In the following part, define the policy ratio at time <span class="math inline">\(h\in [H]\)</span> as</p>
<p><span id="eq-policy-ratio"><span class="math display">\[
\Lambda_h({\theta'}, {\theta^i}) = \frac{\pi_{\theta'}(a_h\mid s_h)}{\pi_{\theta^i}(a_h\mid s_h)}.
\tag{7.74}\]</span></span></p>
<p>The clipped surrogate objective function modifies <a href="#eq-penalty-no-kl" class="quarto-xref">eq.&nbsp;<span>7.73</span></a> to remove incentives for <span class="math inline">\(\pi_{\theta'}\)</span> to differ greatly from <span class="math inline">\(\pi_{\theta^i}\)</span>. Specifically, we choose some small <span class="math inline">\(\epsilon &gt; 0\)</span>, and constrain <span class="math inline">\(\Lambda_h({\theta'}, {\theta^i}) \in (1-\epsilon, 1+\epsilon)\)</span>. Formally,</p>
<p><span id="eq-clip-ratio"><span class="math display">\[
\Lambda_h^\text{clipped}({\theta'}, {\theta^i})
=
\text{clip}(\Lambda_h({\theta'}, {\theta^i}), 1-\epsilon, 1+\epsilon),
\tag{7.75}\]</span></span></p>
<p>where</p>
<p><span id="eq-def-clip"><span class="math display">\[
\text{clip}(x, a, b) := \max \{ a, \min \{ x, b \} \}.
\tag{7.76}\]</span></span></p>
<div id="rem-interpret-clip" class="proof remark">
<p><span class="proof-title"><em>Remark 7.15</em> (Interpretation). </span>As in <a href="#rem-importance-interpretation" class="quarto-xref">Remark&nbsp;<span>7.14</span></a>, suppose <span class="math inline">\(a_h^+\)</span> and <span class="math inline">\(a_h^-\)</span> are a “good” and “bad” action in <span class="math inline">\(s_h\)</span>, i.e.&nbsp;<span class="math inline">\(A^\pi_{\theta^i}(s_h, a_h^+) &gt; 0\)</span> and <span class="math inline">\(A^\pi_{\theta^i}(s_h, a_h^-) &lt; 0\)</span>. By clipping <span class="math inline">\(\Lambda_h({\theta'}, {\theta^i})\)</span>, no additional benefit is gained by increasing <span class="math inline">\(\pi_{\theta'}(a_h^+ \mid s_h)\)</span> above <span class="math inline">\((1+\epsilon) \pi_{\theta^i}(a_h^+ \mid s_h)\)</span>, or by decreasing <span class="math inline">\(\pi_{\theta^i}(a_h^- \mid s_h)\)</span> under <span class="math inline">\((1-\epsilon) \pi_{\theta^i}(a_h^- \mid s_h)\)</span>.</p>
</div>
<p>As a final step, we only use this clipped objective if it is smaller than the original objective. For example, if <span class="math inline">\(A^\pi_{\theta^i}(s_h, a_h) &gt; 0\)</span> and <span class="math inline">\(\Lambda_h({\theta'}, {\theta^i}) \ll 1 - \epsilon\)</span>, then the clipped objective would disproportionately incentivize taking action <span class="math inline">\(a_h\)</span>. The same thing happens if <span class="math inline">\(A^\pi_{\theta^i}(s_h, a_h) &lt; 0\)</span> and <span class="math inline">\(\Lambda_h({\theta'}, {\theta^i}) &gt; 1+\epsilon\)</span>.</p>
<p>Putting these together, this results in only clipping the policy ratio on the side that it would normally move towards:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">400</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> jnp.minimum(x, jnp.clip(x, <span class="fl">0.8</span>, <span class="fl">1.2</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="fl">1.0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$\Lambda = 1$"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="fl">1.2</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$1+\epsilon$"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\Lambda$"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\min(\Lambda, \text</span><span class="sc">{clip}</span><span class="vs">(\Lambda, 1-\epsilon, 1+\epsilon))$"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">1.5</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Advantage Positive"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>y_neg <span class="op">=</span> jnp.minimum(<span class="op">-</span>x, jnp.clip(<span class="op">-</span>x, <span class="op">-</span><span class="fl">1.2</span>, <span class="op">-</span><span class="fl">0.8</span>))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_neg)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="fl">1.0</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$\Lambda = 1$"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="fl">0.8</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="vs">r"$1-\epsilon$"</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\Lambda$"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\min(-\Lambda, \text</span><span class="sc">{clip}</span><span class="vs">(-\Lambda, -(1+\epsilon), -(1-\epsilon)))$"</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">0</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Advantage Negative"</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-ppo-clip" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ppo-clip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ppo-clip" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ppo-clip-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ppo-clip-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-ppo-clip-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-ppo-clip">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ppo-clip-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) For a “good” action <span class="math inline">\(a_h^+\)</span> with positive advantage.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ppo-clip" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ppo-clip-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ppo-clip-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pg_files/figure-html/fig-ppo-clip-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-ppo-clip">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ppo-clip-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) For a “poor” action <span class="math inline">\(a_h^-\)</span> with negative advantage.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ppo-clip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.8: Clipped surrogate objective function
</figcaption>
</figure>
</div>
<div id="def-ppo-clip" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.10 (Clipped surrogate objective)</strong></span> The PPO-Clip objective is</p>
<p><span id="eq-clip-objective"><span class="math display">\[
L^i(\theta) = \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi_{\theta^i}}} \left[
    \sum_{h=0}^{H-1}
    \min \left\{
        \Lambda_h(\theta, {\theta^i}) A^{\pi_{\theta^i}}(s_h, a_h),
        \text{clip}(
            \Lambda_h(\theta, {\theta^i}),
            1-\epsilon,
            1+\epsilon
        )
        A^\pi_{\theta^i}(s_h, a_h)
    \right\}
\right]
\tag{7.77}\]</span></span></p>
</div>
<div id="rem-convergence" class="proof remark">
<p><span class="proof-title"><em>Remark 7.16</em> (Convergence of the clipped surrogate objective). </span>From a traditional optimization perspective, this objective function seems challenging to analyze: the clip operation (<a href="#eq-def-clip" class="quarto-xref">eq.&nbsp;<span>7.76</span></a>) is not differentiable</p>
</div>
</section>
<section id="key-takeaways" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">7.10</span> Key takeaways</h2>
<p>Policy gradient methods are a powerful family of algorithms that optimize the expected total reward by iteratively updating the policy parameters. Precisely, we estimate the gradient of the expected total reward (with respect to the parameters), and update the parameters in that direction. But estimating the gradient is a tricky task! We saw many ways to reduce the variance of the gradient estimator, culminating in the advantage-based expression <a href="#eq-estimator-adv" class="quarto-xref">eq.&nbsp;<span>7.47</span></a>.</p>
<p>But updating the parameters doesn’t entirely solve the problem: Sometimes, a small step in the parameters might lead to a big step in the policy. To avoid changing the policy too much at each step, we must account for the curvature in the parameter space. We first did this explicitly with <a href="#sec-trpo" class="quarto-xref"><span>Section 7.6</span></a>, and then saw ways to relax the constraint via the natural policy gradient update (<a href="#sec-npg" class="quarto-xref"><span>Section 7.7</span></a>), the PPO-KL update (<a href="#sec-ppo-kl" class="quarto-xref"><span>Section 7.8</span></a>), and the PPO-Clip update (<a href="#sec-ppo-clip" class="quarto-xref"><span>Section 7.9</span></a>).</p>
<p>These are still popular methods to this day, especially because they efficiently integrate with <em>deep neural networks</em> for representing complex functions.</p>
</section>
<section id="sec-pg-bib" class="level2" data-number="7.11">
<h2 data-number="7.11" class="anchored" data-anchor-id="sec-pg-bib"><span class="header-section-number">7.11</span> Bibliographic notes and further reading</h2>
<p><span class="citation" data-cites="sutton_policy_1999">Sutton et al. (<a href="references.html#ref-sutton_policy_1999" role="doc-biblioref">1999</a>)</span> popularized the term “policy gradient methods”. Policy gradient methods, including REINFORCE <span class="citation" data-cites="williams_simple_1992">(<a href="references.html#ref-williams_simple_1992" role="doc-biblioref">Williams, 1992</a>)</span>, were some of the earliest reinforcement learning methods <span class="citation" data-cites="witten_adaptive_1977 barto_neuronlike_1983 sutton_temporal_1984">(<a href="references.html#ref-barto_neuronlike_1983" role="doc-biblioref">Barto et al., 1983</a>; <a href="references.html#ref-sutton_temporal_1984" role="doc-biblioref">Sutton, 1984</a>; <a href="references.html#ref-witten_adaptive_1977" role="doc-biblioref">Witten, 1977</a>)</span>.</p>
<p><span class="citation" data-cites="kakade_approximately_2002">S. Kakade &amp; Langford (<a href="references.html#ref-kakade_approximately_2002" role="doc-biblioref">2002</a>)</span> introduced the performance difference lemma, which was used to prove theoretical guarantees in several later works.</p>
<p>The expression of the policy gradient in terms of the Q-function was simultaneously observed by <span class="citation" data-cites="marbach_simulation-based_2001">Marbach &amp; Tsitsiklis (<a href="references.html#ref-marbach_simulation-based_2001" role="doc-biblioref">2001</a>)</span> and <span class="citation" data-cites="sutton_policy_1999">Sutton et al. (<a href="references.html#ref-sutton_policy_1999" role="doc-biblioref">1999</a>)</span>.</p>
<p>The natural gradient was suggested by <span class="citation" data-cites="amari_natural_1998">Amari (<a href="references.html#ref-amari_natural_1998" role="doc-biblioref">1998</a>)</span> for general optimization problems and then applied to the policy gradient by <span class="citation" data-cites="kakade_natural_2001">S. M. Kakade (<a href="references.html#ref-kakade_natural_2001" role="doc-biblioref">2001</a>)</span>. The <strong>Natural Actor-Critic</strong> algorithm combines actor-critic learning with the natural policy gradient estimator <span class="citation" data-cites="peters_natural_2005 peters_natural_2008 bhatnagar_natural_2009">(<a href="references.html#ref-bhatnagar_natural_2009" role="doc-biblioref">Bhatnagar et al., 2009</a>; <a href="references.html#ref-peters_natural_2005" role="doc-biblioref">Peters et al., 2005</a>; <a href="references.html#ref-peters_natural_2008" role="doc-biblioref">Peters &amp; Schaal, 2008</a>)</span>.</p>
<p>Natural gradient descent is effective because it accounts for the <em>geometry</em> induced by the parameterization and thereby optimizes over the <em>distributions</em> themselves. Another optimization algorithm that accounts for parameter-space geometry is <strong>mirror descent</strong> <span class="citation" data-cites="nemirovskij_problem_1983 beck_mirror_2003">(<a href="references.html#ref-beck_mirror_2003" role="doc-biblioref">Beck &amp; Teboulle, 2003</a>; <a href="references.html#ref-nemirovskij_problem_1983" role="doc-biblioref">Nemirovskij et al., 1983</a>)</span>. At each optimization step, this maps the parameter vector to a <em>dual space</em> defined to capture the desired geometry. It then takes a gradient descent step on the dual parameters and maps the result back to an updated “primal” parameter vector. One can also apply mirror descent to policy gradient algorithms <span class="citation" data-cites="mahadevan_sparse_2012 mahadevan_basis_2013">(<a href="references.html#ref-mahadevan_basis_2013" role="doc-biblioref">Mahadevan et al., 2013</a>; <a href="references.html#ref-mahadevan_sparse_2012" role="doc-biblioref">Mahadevan &amp; Liu, 2012</a>)</span>.</p>
<p>A key reason for the popularity of policy gradient methods is that they are easily <em>parallelizable</em> across multiple computing threads. Each thread collects rollouts and computes its own gradient estimates, and every once in a while, these are accumulated across threads and used to perform an update of the parameters, which are then synchronized across the different threads. <span class="citation" data-cites="mnih_asynchronous_2016">Mnih et al. (<a href="references.html#ref-mnih_asynchronous_2016" role="doc-biblioref">2016</a>)</span> present multi-threaded variants of policy gradient methods as well as several algorithms from <a href="fitted_dp.html" class="quarto-xref"><span>Chapter 6</span></a>. <span class="citation" data-cites="schulman_trust_2015">Schulman et al. (<a href="references.html#ref-schulman_trust_2015" role="doc-biblioref">2015</a>)</span> proposed TRPO (<a href="#def-trpo-update" class="quarto-xref">def.&nbsp;<span>7.6</span></a>). <span class="citation" data-cites="schulman_high-dimensional_2016">Schulman et al. (<a href="references.html#ref-schulman_high-dimensional_2016" role="doc-biblioref">2016</a>)</span> introduced <strong>generalized advantage estimation</strong> (GAE), which accepts some bias in the gradient estimate in exchange for lower variance. This tradeoff is controlled by the parameter <span class="math inline">\(\lambda\)</span>, the same parameter as in <span class="math inline">\(\text{TD}(\lambda)\)</span>. <span class="citation" data-cites="schulman_proximal_2017">Schulman et al. (<a href="references.html#ref-schulman_proximal_2017" role="doc-biblioref">2017</a>)</span> introduced the PPO-KL and PPO-Clip algorithms. <span class="citation" data-cites="wu_scalable_2017">Wu et al. (<a href="references.html#ref-wu_scalable_2017" role="doc-biblioref">2017</a>)</span> introduced the ACKTR algorithm, which seeks to scale NPG up to deep learning models where naively approximating the Fisher information matrix is impractical. It does so using the Kronecker-factored approximation proposed in <span class="citation" data-cites="martens_optimizing_2015">Martens &amp; Grosse (<a href="references.html#ref-martens_optimizing_2015" role="doc-biblioref">2015</a>)</span>. <span class="citation" data-cites="shao_deepseekmath_2024">Shao et al. (<a href="references.html#ref-shao_deepseekmath_2024" role="doc-biblioref">2024</a>)</span> introduce a variant of PPO known as <strong>Group Relative Policy Optimization</strong> (GRPO).</p>
<p>Policy gradient methods, being on-policy algorithms, inherently have worse sample efficiency than off-policy algorithms, since naively each sample can only be used to update at the value of the parameter vector that it was sampled at. <span class="citation" data-cites="wang_sample_2017">Wang et al. (<a href="references.html#ref-wang_sample_2017" role="doc-biblioref">2017</a>)</span> introduce the <strong>actor-critic with experience replay</strong> (ACER) algorithm, which builds on the <strong>Retrace</strong> Q function estimator <span class="citation" data-cites="munos_safe_2016">(<a href="references.html#ref-munos_safe_2016" role="doc-biblioref">Munos et al., 2016</a>)</span> to estimate policy gradients using previously collected experience <span class="citation" data-cites="lin_self-improving_1992">(<a href="references.html#ref-lin_self-improving_1992" role="doc-biblioref">Lin, 1992</a>)</span>.</p>
<p>The <strong>soft Q-learning</strong> and <strong>soft actor-critic</strong> algorithms introduced in <span class="citation" data-cites="haarnoja_reinforcement_2017">Haarnoja et al. (<a href="references.html#ref-haarnoja_reinforcement_2017" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="haarnoja_soft_2018">Haarnoja et al. (<a href="references.html#ref-haarnoja_soft_2018" role="doc-biblioref">2018</a>)</span> respectively add the <strong>entropy</strong> of the policy to the objective function. This practice is known as <strong>maximum entropy reinforcement learning</strong> since we seek to maximize the policy entropy alongside the expected total reward. This encourages policies that “act more randomly”, which aids exploration.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-amari_natural_1998" class="csl-entry" role="listitem">
Amari, S.-I. (1998). Natural gradient works efficiently in learning. <em>Neural Comput.</em>, <em>10</em>(2), 251–276. <a href="https://doi.org/10.1162/089976698300017746">https://doi.org/10.1162/089976698300017746</a>
</div>
<div id="ref-ansel_pytorch_2024" class="csl-entry" role="listitem">
Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., … Chintala, S. (2024, April). <span>PyTorch</span> 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. <em>29th <span>ACM</span> International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (<span>ASPLOS</span> ’24)</em>. <a href="https://doi.org/10.1145/3620665.3640366">https://doi.org/10.1145/3620665.3640366</a>
</div>
<div id="ref-barto_neuronlike_1983" class="csl-entry" role="listitem">
Barto, A. G., Sutton, R. S., &amp; Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, <em>SMC-13</em>(5), 834–846. <span>IEEE Transactions</span> on <span>Systems</span>, <span>Man</span>, and <span>Cybernetics</span>. <a href="https://doi.org/10.1109/TSMC.1983.6313077">https://doi.org/10.1109/TSMC.1983.6313077</a>
</div>
<div id="ref-baydin_automatic_2018" class="csl-entry" role="listitem">
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M. (2018, February 5). <em>Automatic differentiation in machine learning: A survey</em>. <a href="https://doi.org/10.48550/arXiv.1502.05767">https://doi.org/10.48550/arXiv.1502.05767</a>
</div>
<div id="ref-beck_mirror_2003" class="csl-entry" role="listitem">
Beck, A., &amp; Teboulle, M. (2003). Mirror descent and nonlinear projected subgradient methods for convex optimization. <em>Operations Research Letters</em>, <em>31</em>(3), 167–175. <a href="https://doi.org/10.1016/S0167-6377(02)00231-6">https://doi.org/10.1016/S0167-6377(02)00231-6</a>
</div>
<div id="ref-bhatnagar_natural_2009" class="csl-entry" role="listitem">
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., &amp; Lee, M. (2009). Natural actor–critic algorithms. <em>Automatica</em>, <em>45</em>(11), 2471–2482. <a href="https://doi.org/10.1016/j.automatica.2009.07.008">https://doi.org/10.1016/j.automatica.2009.07.008</a>
</div>
<div id="ref-boyd_convex_2004" class="csl-entry" role="listitem">
Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex optimization</em>. Cambridge University Press. <a href="https://web.stanford.edu/~boyd/cvxbook/">https://web.stanford.edu/~boyd/cvxbook/</a>
</div>
<div id="ref-bradbury_jax_2018" class="csl-entry" role="listitem">
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., &amp; Zhang, Q. (2018). <em><span>JAX</span>: <span>Composable</span> transformations of python+<span>NumPy</span> programs</em> (Version 0.3.13) [Computer software]. <a href="http://github.com/google/jax">http://github.com/google/jax</a>
</div>
<div id="ref-haarnoja_reinforcement_2017" class="csl-entry" role="listitem">
Haarnoja, T., Tang, H., Abbeel, P., &amp; Levine, S. (2017). Reinforcement learning with deep energy-based policies. <em>Proceedings of the 34th <span>International Conference</span> on <span>Machine Learning</span> - <span>Volume</span> 70</em>, 1352–1361.
</div>
<div id="ref-haarnoja_soft_2018" class="csl-entry" role="listitem">
Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: <span class="nocase">Off-policy</span> maximum entropy deep reinforcement learning with a stochastic actor. <em>Proceedings of the 35th <span>International Conference</span> on <span>Machine Learning</span></em>, 1861–1870. <a href="https://proceedings.mlr.press/v80/haarnoja18b.html">https://proceedings.mlr.press/v80/haarnoja18b.html</a>
</div>
<div id="ref-kakade_natural_2001" class="csl-entry" role="listitem">
Kakade, S. M. (2001). A natural policy gradient. <em>Advances in <span>Neural Information Processing Systems</span></em>, <em>14</em>. <a href="https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html</a>
</div>
<div id="ref-kakade_approximately_2002" class="csl-entry" role="listitem">
Kakade, S., &amp; Langford, J. (2002). Approximately optimal approximate reinforcement learning. <em>Proceedings of the <span>Nineteenth International Conference</span> on <span>Machine Learning</span></em>, 267–274.
</div>
<div id="ref-lin_self-improving_1992" class="csl-entry" role="listitem">
Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. <em>Machine Learning</em>, <em>8</em>(3), 293–321. <a href="https://doi.org/10.1007/BF00992699">https://doi.org/10.1007/BF00992699</a>
</div>
<div id="ref-mahadevan_basis_2013" class="csl-entry" role="listitem">
Mahadevan, S., Giguere, S., &amp; Jacek, N. (2013). Basis adaptation for sparse nonlinear reinforcement learning. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>27</em>(1, 1), 654–660. <a href="https://doi.org/10.1609/aaai.v27i1.8665">https://doi.org/10.1609/aaai.v27i1.8665</a>
</div>
<div id="ref-mahadevan_sparse_2012" class="csl-entry" role="listitem">
Mahadevan, S., &amp; Liu, B. (2012). Sparse q-learning with mirror descent. <em>Proceedings of the <span>Twenty-Eighth Conference</span> on <span>Uncertainty</span> in <span>Artificial Intelligence</span></em>, 564–573.
</div>
<div id="ref-marbach_simulation-based_2001" class="csl-entry" role="listitem">
Marbach, P., &amp; Tsitsiklis, J. N. (2001). Simulation-based optimization of markov reward processes. <em>IEEE Transactions on Automatic Control</em>, <em>46</em>(2), 191–209. <span>IEEE Transactions</span> on <span>Automatic Control</span>. <a href="https://doi.org/10.1109/9.905687">https://doi.org/10.1109/9.905687</a>
</div>
<div id="ref-martens_optimizing_2015" class="csl-entry" role="listitem">
Martens, J., &amp; Grosse, R. (2015). Optimizing neural networks with kronecker-factored approximate curvature. <em>Proceedings of the 32nd <span>International Conference</span> on <span>Machine Learning</span></em>, 2408–2417. <a href="https://proceedings.mlr.press/v37/martens15.html">https://proceedings.mlr.press/v37/martens15.html</a>
</div>
<div id="ref-meurer_sympy_2017" class="csl-entry" role="listitem">
Meurer, A., Smith, C. P., Paprocki, M., Čertík, O., Kirpichev, S. B., Rocklin, M., Kumar, A., Ivanov, S., Moore, J. K., Singh, S., Rathnayake, T., Vig, S., Granger, B. E., Muller, R. P., Bonazzi, F., Gupta, H., Vats, S., Johansson, F., Pedregosa, F., … Scopatz, A. (2017). <span>SymPy</span>: Symbolic computing in python. <em>PeerJ Computer Science</em>, <em>3</em>, e103. <a href="https://doi.org/10.7717/peerj-cs.103">https://doi.org/10.7717/peerj-cs.103</a>
</div>
<div id="ref-mnih_asynchronous_2016" class="csl-entry" role="listitem">
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., &amp; Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. <em>Proceedings of <span>The</span> 33rd <span>International Conference</span> on <span>Machine Learning</span></em>, 1928–1937. <a href="https://proceedings.mlr.press/v48/mniha16.html">https://proceedings.mlr.press/v48/mniha16.html</a>
</div>
<div id="ref-munos_safe_2016" class="csl-entry" role="listitem">
Munos, R., Stepleton, T., Harutyunyan, A., &amp; Bellemare, M. G. (2016). Safe and efficient off-policy reinforcement learning. <em>Proceedings of the 30th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em>, 1054–1062.
</div>
<div id="ref-nemirovskij_problem_1983" class="csl-entry" role="listitem">
Nemirovskij, A. S., Judin, D. B., Dawson, E. R., &amp; Nemirovskij, A. S. (1983). <em>Problem complexity and method efficiency in optimization</em>. Wiley.
</div>
<div id="ref-nocedal_numerical_2006" class="csl-entry" role="listitem">
Nocedal, J., &amp; Wright, S. J. (2006). <em>Numerical optimization</em> (2nd ed). Springer.
</div>
<div id="ref-peters_natural_2008" class="csl-entry" role="listitem">
Peters, J., &amp; Schaal, S. (2008). Natural actor-critic. <em>Neurocomputing</em>, <em>71</em>(7), 1180–1190. <a href="https://doi.org/10.1016/j.neucom.2007.11.026">https://doi.org/10.1016/j.neucom.2007.11.026</a>
</div>
<div id="ref-peters_natural_2005" class="csl-entry" role="listitem">
Peters, J., Vijayakumar, S., &amp; Schaal, S. (2005). Natural actor-critic. In J. Gama, R. Camacho, P. B. Brazdil, A. M. Jorge, &amp; L. Torgo (Eds.), <em>Machine <span>Learning</span>: <span>ECML</span> 2005</em> (pp. 280–291). Springer. <a href="https://doi.org/10.1007/11564096_29">https://doi.org/10.1007/11564096_29</a>
</div>
<div id="ref-schulman_trust_2015" class="csl-entry" role="listitem">
Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015). Trust region policy optimization. <em>Proceedings of the 32nd International Conference on Machine Learning</em>, 1889–1897. <a href="https://proceedings.mlr.press/v37/schulman15.html">https://proceedings.mlr.press/v37/schulman15.html</a>
</div>
<div id="ref-schulman_high-dimensional_2016" class="csl-entry" role="listitem">
Schulman, J., Moritz, P., Levine, S., Jordan, M. I., &amp; Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. In Y. Bengio &amp; Y. LeCun (Eds.), <em>4th international conference on learning representations</em>. <a href="http://arxiv.org/abs/1506.02438">http://arxiv.org/abs/1506.02438</a>
</div>
<div id="ref-schulman_proximal_2017" class="csl-entry" role="listitem">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017, August 28). <em>Proximal policy optimization algorithms</em>. <a href="https://doi.org/10.48550/arXiv.1707.06347">https://doi.org/10.48550/arXiv.1707.06347</a>
</div>
<div id="ref-shao_deepseekmath_2024" class="csl-entry" role="listitem">
Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., &amp; Guo, D. (2024, April 27). <em><span>DeepSeekMath</span>: <span>Pushing</span> the limits of mathematical reasoning in open language models</em>. <a href="https://doi.org/10.48550/arXiv.2402.03300">https://doi.org/10.48550/arXiv.2402.03300</a>
</div>
<div id="ref-silver_deterministic_2014" class="csl-entry" role="listitem">
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., &amp; Riedmiller, M. (2014). Deterministic policy gradient algorithms. <em>Proceedings of the 31st <span>International Conference</span> on <span>Machine Learning</span></em>, 387–395. <a href="https://proceedings.mlr.press/v32/silver14.html">https://proceedings.mlr.press/v32/silver14.html</a>
</div>
<div id="ref-sutton_temporal_1984" class="csl-entry" role="listitem">
Sutton, R. S. (1984). <em>Temporal credit assignment in reinforcement learning</em> [PhD thesis]. University of Massachusetts Amherst.
</div>
<div id="ref-sutton_policy_1999" class="csl-entry" role="listitem">
Sutton, R. S., McAllester, D., Singh, S., &amp; Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. <em>Proceedings of the 13th <span>International Conference</span> on <span>Neural Information Processing Systems</span></em>, 1057–1063.
</div>
<div id="ref-vershynin_high-dimensional_2018" class="csl-entry" role="listitem">
Vershynin, R. (2018). <em>High-dimensional probability: <span>An</span> introduction with applications in data science</em>. Cambridge University Press. <a href="https://books.google.com?id=NDdqDwAAQBAJ">https://books.google.com?id=NDdqDwAAQBAJ</a>
</div>
<div id="ref-wang_sample_2017" class="csl-entry" role="listitem">
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., &amp; Freitas, N. de. (2017, February 6). Sample efficient actor-critic with experience replay. <em>5th International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=HyM25Mqel">https://openreview.net/forum?id=HyM25Mqel</a>
</div>
<div id="ref-williams_simple_1992" class="csl-entry" role="listitem">
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. <em>Machine Learning</em>, <em>8</em>(3), 229–256. <a href="https://doi.org/10.1007/BF00992696">https://doi.org/10.1007/BF00992696</a>
</div>
<div id="ref-witten_adaptive_1977" class="csl-entry" role="listitem">
Witten, I. H. (1977). An adaptive optimal controller for discrete-time markov environments. <em>Information and Control</em>, <em>34</em>(4), 286–295. <a href="https://doi.org/10.1016/S0019-9958(77)90354-0">https://doi.org/10.1016/S0019-9958(77)90354-0</a>
</div>
<div id="ref-wu_scalable_2017" class="csl-entry" role="listitem">
Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., &amp; Ba, J. (2017). Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html</a>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/rlbook\.adzc\.ai");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./fitted_dp.html" class="pagination-link" aria-label="Fitted Dynamic Programming Algorithms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./imitation_learning.html" class="pagination-link" aria-label="Imitation Learning">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025 Alexander Cai</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/pg.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/pg.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/adzcai/rlbook" aria-current="page">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>