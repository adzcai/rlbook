{{< include macros.qmd >>}}
# Notation {.unnumbered}

It's worth it to spend a few words discussing the notation used for reinforcement learning.
RL notation can _appear_ quite complicated,
since we often need to index across algorithm iterations, trajectories, and timesteps,
so that certain values can have two or three indices attached to them.
It's important to see beyond the formal notation
and _interpret_ the quantities being symbolized.

We will use the following notation throughout the book.
This notation is inspired by @sutton_reinforcement_2018 and @agarwal_reinforcement_2022.
We use $[N]$ as shorthand for the set $\{ 0, 1, \dots, N-1 \}$.
We try to use each lowercase letter to represent an element of the set
denoted by the corresponding (stylized) uppercase letter.

| Element  |                      Space                      | Definition (of element)                                        |
| :------: | :---------------------------------------------: | :------------------------------------------------------------- |
|   $s$    |                  $\mathcal{S}$                  | A state.                                                       |
|   $a$    |                  $\mathcal{A}$                  | An action.                                                     |
|   $r$    |                      $\R$                       | A reward.                                                      |
| $\gamma$ |                      $[0, 1]$                   | A discount factor.                                             |
|  $\tau$  |                  $\mathcal{T}$                  | A trajectory.                                                  |
|  $\pi$   |                      $\Pi$                      | A policy.                                                      |
| $V^\pi$  |          $\mathcal{S} \to \mathbb{R}$           | The value function of policy $\pi$.                            |
| $Q^\pi$  | $\mathcal{S} \times \mathcal{A} \to \mathbb{R}$ | The action-value function (a.k.a. Q-function) of policy $\pi$. |
| $A^\pi$  | $\mathcal{S} \times \mathcal{A} \to \mathbb{R}$ | The advantage function of policy $\pi$.                        |
|          |            $\triangle(\mathcal{X})$             | A distribution supported on $\mathcal{X}$.                     |
|  $\hi$   |                    $[\hor]$                     | Time horizon index of an MDP (subscript).                      |
|   $k$    |                      $[K]$                      | Arm index of a multi-armed bandit (superscript).               |
|   $t$    |                      $[T]$                      | Number of episodes.                                            |
|   $i$    |                      $[I]$                      | Iteration index of an algorithm (subscript).                   |
| $\theta$ |                    $\Theta$                     | A set of parameters.                                           |

: Table of common notation. {#tbl-notation}

Note that throughout the text, certain symbols will stand for either random variables or fixed values.
We aim to clarify in ambiguous settings.
