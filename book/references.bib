@online{baydin_automatic_2018,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  date = {2018-02-05},
  eprint = {1502.05767},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1502.05767},
  url = {http://arxiv.org/abs/1502.05767},
  urldate = {2024-11-11},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Automatic differentiation in machine learning (2018) - Baydin, Pearlmutter, Radul, Siskind.pdf}
}

@inproceedings{lechner_gigastep_2023,
  title = {Gigastep - One Billion Steps per Second Multi-Agent Reinforcement Learning},
  author = {Lechner, Mathias and Yin, Lianhao and Seyde, Tim and Wang, Tsun-Hsuan and Xiao, Wei and Hasani, Ramin and Rountree, Joshua and Rus, Daniela},
  date = {2023-11-02},
  url = {https://openreview.net/forum?id=UgPAaEugH3},
  urldate = {2023-12-12},
  abstract = {Multi-agent reinforcement learning (MARL) research is faced with a trade-off: it either uses complex environments requiring large compute resources, which makes it inaccessible to researchers with limited resources, or relies on simpler dynamics for faster execution, which makes the transferability of the results to more realistic tasks challenging. Motivated by these challenges, we present Gigastep, a fully vectorizable, MARL environment implemented in JAX, capable of executing up to one billion environment steps per second on consumer-grade hardware. Its design allows for comprehensive MARL experimentation, including a complex, high-dimensional space defined by 3D dynamics, stochasticity, and partial observations. Gigastep supports both collaborative and adversarial tasks, continuous and discrete action spaces, and provides RGB image and feature vector observations, allowing the evaluation of a wide range of MARL algorithms. We validate Gigastep's usability through an extensive set of experiments, underscoring its role in widening participation and promoting inclusivity in the MARL research community.},
  eventtitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Gigastep - One Billion Steps per Second Multi-agent Reinforcement Learning (2023) - Lechner et al.pdf}
}

@online{hausknecht_deep_2017,
  title = {Deep Recurrent Q-Learning for Partially Observable Mdps},
  author = {Hausknecht, Matthew and Stone, Peter},
  date = {2017-01-11},
  eprint = {1507.06527},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.06527},
  url = {http://arxiv.org/abs/1507.06527},
  urldate = {2023-06-04},
  abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textbackslash textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  pubstate = {prepublished},
  annotation = {1274 citations (Semantic Scholar/arXiv) [2023-06-04]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Deep Recurrent Q-Learning for Partially Observable MDPs (2017) - Hausknecht, Stone.pdf}
}

@online{van_hasselt_deep_2018,
  title = {Deep Reinforcement Learning and the Deadly Triad},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=true and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  date = {2018-12-06},
  eprint = {1812.02648},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.02648},
  url = {http://arxiv.org/abs/1812.02648},
  urldate = {2023-03-29},
  abstract = {We know from reinforcement learning theory that temporal difference learning can fail in certain cases. Sutton and Barto (2018) identify a deadly triad of function approximation, bootstrapping, and off-policy learning. When these three properties are combined, learning can diverge with the value estimates becoming unbounded. However, several algorithms successfully combine these three properties, which indicates that there is at least a partial gap in our understanding. In this work, we investigate the impact of the deadly triad in practice, in the context of a family of popular deep reinforcement learning models - deep Q-networks trained with experience replay - analysing how the components of this system play a role in the emergence of the deadly triad, and in the agent's performance},
  pubstate = {prepublished},
  annotation = {152 citations (Semantic Scholar/arXiv) [2023-03-29]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Deep Reinforcement Learning and the Deadly Triad (2018) - van Hasselt et al.pdf}
}

@software{babuschkin_deepmind_2020,
  title = {The {{DeepMind JAX}} Ecosystem},
  author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  date = {2020},
  url = {http://github.com/deepmind},
  keywords = {nosource}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004},
  publisher = {Cambridge University Press},
  url = {https://web.stanford.edu/~boyd/cvxbook/},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2004/Convex Optimization (2004) - Boyd, Vandenberghe 3.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2004/Convex Optimization (2004) - Boyd, Vandenberghe.pdf}
}

@book{nocedal_numerical_2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  date = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {664},
  annotation = {OCLC: ocm68629100},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2006/Numerical optimization (2006) - Nocedal, Wright.pdf}
}

@book{vershynin_high-dimensional_2018,
  title = {High-Dimensional Probability: {{An}} Introduction with Applications in Data Science},
  shorttitle = {High-Dimensional Probability},
  author = {Vershynin, Roman},
  date = {2018-09-27},
  eprint = {NDdqDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Cambridge University Press},
  abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
  isbn = {978-1-108-41519-4},
  langid = {english},
  pagetotal = {299},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/High-Dimensional Probability (2018) - Vershynin.pdf}
}

@online{schulman_proximal_2017,
  title = {Proximal Policy Optimization Algorithms},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2022-09-05},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Proximal Policy Optimization Algorithms (2017) - Schulman et al.pdf;/Users/adzcai/Zotero/storage/NCI3EDQS/1707.html}
}

@article{williams_simple_1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  url = {https://doi.org/10.1007/BF00992696},
  urldate = {2022-09-03},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1992/Simple statistical gradient-following algorithms for connectionist reinforcement learning (1992) - Williams.pdf}
}

@online{islam_reproducibility_2017,
  title = {Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control},
  author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  date = {2017-08-10},
  eprint = {1708.04133},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.04133},
  urldate = {2022-09-01},
  abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control (2017) - Islam, Henderson, Gomrokchi, Precup.pdf;/Users/adzcai/Zotero/storage/DESTVEZK/1708.html}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12},
  journaltitle = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {https://www.nature.com/articles/s41586-020-03051-4},
  urldate = {2022-08-15},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
  issue = {7839},
  langid = {english},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2020/Mastering Atari, Go, chess and shogi by planning with a learned model (2020) - Schrittwieser et al.pdf;/Users/adzcai/Zotero/storage/KLW59Z2L/s41586-020-03051-4.html}
}

@article{silver_general_2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar6404},
  url = {https://www.science.org/doi/10.1126/science.aar6404},
  urldate = {2022-08-15},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play (2018) - Silver et al.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of Go without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2022-08-15},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  issue = {7676},
  langid = {english},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Mastering the game of Go without human knowledge (2017) - Silver et al.pdf;/Users/adzcai/Zotero/storage/6KP4UDZN/nature24270.html}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of Go with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2022-05-08},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  issue = {7587},
  langid = {english},
  keywords = {broken},
  annotation = {9998 citations (Semantic Scholar/DOI) [2022-05-18]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Mastering the game of Go with deep neural networks and tree search (2016) - Silver et al.pdf;/Users/adzcai/Zotero/storage/STY6YT82/nature16961.html}
}

@article{degrave_magnetic_2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and family=Casas, given=Diego, prefix=de las, useprefix=true and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  date = {2022-02},
  journaltitle = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  url = {https://www.nature.com/articles/s41586-021-04301-9},
  urldate = {2023-05-21},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  issue = {7897},
  langid = {english},
  annotation = {230 citations (Semantic Scholar/DOI) [2023-05-21]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Magnetic control of tokamak plasmas through deep reinforcement learning (2022) - Degrave et al.pdf}
}

@book{sussman_functional_2013,
  title = {Functional Differential Geometry},
  author = {Sussman, Gerald Jay and Wisdom, Jack and Farr, Will},
  date = {2013},
  publisher = {The MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-01934-7},
  pagetotal = {228},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/Functional differential geometry (2013) - Sussman, Wisdom, Farr.pdf}
}

@book{hastie_elements_2013,
  title = {The Elements of Statistical Learning: {{Data}} Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2013-11-11},
  eprint = {yPfZBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Springer Science \& Business Media},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  isbn = {978-0-387-21606-5},
  langid = {english},
  pagetotal = {545},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/The Elements of Statistical Learning (2013) - Hastie, Tibshirani, Friedman.pdf}
}

@online{adaptive_agent_team_human-timescale_2023,
  title = {Human-Timescale Adaptation in an Open-Ended Task Space},
  author = {Adaptive Agent Team and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves, Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
  date = {2023-01-18},
  eprint = {2301.07608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.07608},
  urldate = {2023-02-21},
  abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.},
  pubstate = {prepublished},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-02-20]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Human-Timescale Adaptation in an Open-Ended Task Space (2023) - Adaptive Agent Team et al.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  url = {http://incompleteideas.net/book/RLbook2020trimmed.pdf},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Reinforcement learning (2018) - Sutton, Barto.pdf}
}

@book{kochenderfer_algorithms_2022,
  title = {Algorithms for Decision Making},
  author = {Kochenderfer, Mykel J and Wheeler, Tim A and Wray, Kyle H},
  date = {2022-08-16},
  url = {https://mitpress.mit.edu/9780262047012/algorithms-for-decision-making/},
  urldate = {2022-10-23},
  abstract = {A broad introduction to algorithms for decision making under uncertainty, introducing the underlying mathematical problem formulations and the algorithms for...},
  isbn = {978-0-262-04701-2},
  langid = {american},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Algorithms for Decision Making (2022) - Kochenderfer, Wheeler, Wray.pdf}
}

@article{schultz_neural_1997,
  title = {A Neural Substrate of Prediction and Reward},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  date = {1997-03-14},
  journaltitle = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.275.5306.1593},
  url = {https://www.science.org/doi/10.1126/science.275.5306.1593},
  urldate = {2022-11-28},
  abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
  annotation = {7847 citations (Semantic Scholar/DOI) [2022-11-27]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1997/A Neural Substrate of Prediction and Reward (1997) - Schultz, Dayan, Montague.pdf}
}

@book{albrecht_multi-agent_2023,
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  author = {Albrecht, Stefano V. and Christianos, Filippos and Schäfer, Lukas},
  date = {2023},
  publisher = {MIT Press},
  url = {https://www.marl-book.com},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Multi-agent reinforcement learning (2023) - Albrecht, Christianos, Schäfer.pdf}
}

@inproceedings{kakade_approximately_2002,
  title = {Approximately Optimal Approximate Reinforcement Learning},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Machine Learning}}},
  author = {Kakade, Sham and Langford, John},
  date = {2002-07-08},
  series = {{{ICML}} '02},
  pages = {267--274},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {978-1-55860-873-3},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2002/Approximately Optimal Approximate Reinforcement Learning (2002) - Kakade, Langford 3.pdf}
}

@software{bradbury_jax_2018,
  title = {{{JAX}}: {{Composable}} Transformations of python+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  date = {2018},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  keywords = {nosource}
}

@inproceedings{freeman_brax_2021,
  title = {Brax – a Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  booktitle = {{{NeurIPS Datasets}} and {{Benchmarks}} 2021},
  author = {Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  date = {2021-06-24},
  eprint = {2106.13281},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.13281},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract-round1.html},
  urldate = {2023-06-26},
  abstract = {We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.},
  eventtitle = {{{NeurIPS Datasets}} and {{Benchmarks}}},
  pubstate = {preprint | DBLP: https://dblp.org/rec/conf/nips/FreemanFRGMB21},
  annotation = {151 citations (Semantic Scholar/arXiv) [2023-07-22]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Brax – A Differentiable Physics Engine for Large Scale Rigid Body Simulation (2021) - Freeman et al.pdf}
}

@unpublished{li_deep_2018,
  title = {Deep Reinforcement Learning},
  author = {Li, Yuxi},
  date = {2018-10-15},
  eprint = {1810.06339},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.06339},
  url = {http://arxiv.org/abs/1810.06339},
  urldate = {2022-05-18},
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  pubstate = {prepublished},
  keywords = {broken},
  annotation = {200 citations (Semantic Scholar/arXiv) [2022-05-18]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Deep Reinforcement Learning (2018) - Li.pdf;/Users/adzcai/Zotero/storage/U58RHL7G/1810.html}
}

@article{lai_asymptotically_1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  date = {1985-03-01},
  journaltitle = {Advances in Applied Mathematics},
  shortjournal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
  urldate = {2023-10-23},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1985/Asymptotically efficient adaptive allocation rules (1985) - Lai, Robbins.pdf}
}

@book{nielsen_neural_2015,
  title = {Neural Networks and Deep Learning},
  author = {Nielsen, Michael A.},
  date = {2015},
  publisher = {Determination Press},
  url = {http://neuralnetworksanddeeplearning.com/},
  urldate = {2024-03-10},
  keywords = {nosource}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-31073-2},
  pagetotal = {738},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2006/Pattern recognition and machine learning (2006) - Bishop.pdf}
}

@book{james_introduction_2023,
  title = {An Introduction to Statistical Learning: With Applications in Python},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
  date = {2023},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-38747-0},
  url = {https://link.springer.com/10.1007/978-3-031-38747-0},
  urldate = {2024-02-07},
  isbn = {978-3-031-38746-3 978-3-031-38747-0},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/An Introduction to Statistical Learning (2023) - James et al.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2021},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition},
  publisher = {Pearson},
  location = {Hoboken},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  isbn = {978-0-13-461099-3},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Artificial intelligence (2021) - Russell, Norvig.pdf}
}

@book{heath_scientific_2018,
  title = {Scientific Computing: An Introductory Survey},
  shorttitle = {Scientific Computing},
  author = {Heath, Michael T.},
  date = {2018},
  series = {Classics in Applied Mathematics},
  edition = {Revised second edition, SIAM edition},
  number = {80},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {Philadelphia},
  isbn = {978-1-61197-557-4},
  pagetotal = {567},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Scientific computing (2018) - Heath 2.pdf}
}

@inproceedings{ross_reduction_2010,
  title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
  author = {Ross, Stéphane and Gordon, Geoffrey J. and Bagnell, J.},
  date = {2010-11-02},
  url = {https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e},
  urldate = {2024-08-08},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2010/A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (2010) - Ross, Gordon, Bagnell.pdf}
}

@online{zhang_deep_2015,
  title = {Deep Learning with Elastic Averaging {{SGD}}},
  author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
  date = {2015-10-25},
  eprint = {1412.6651},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1412.6651},
  url = {http://arxiv.org/abs/1412.6651},
  urldate = {2024-07-01},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Deep learning with Elastic Averaging SGD (2015) - Zhang, Choromanska, LeCun.pdf;/Users/adzcai/Zotero/storage/M4LFKVWK/1412.html}
}

@inreference{achiam_spinning_2018,
  title = {Spinning up in Deep Reinforcement Learning},
  author = {Achiam, Joshua},
  date = {2018},
  url = {https://spinningup.openai.com/en/latest/index.html},
  urldate = {2024-07-01},
  keywords = {broken},
  file = {/Users/adzcai/Zotero/storage/UPUMW6XV/index.html}
}

@online{zhang_adaptable_2024,
  title = {Adaptable Logical Control for Large Language Models},
  author = {Zhang, Honghua and Kung, Po-Nien and Yoshida, Masahiro and family=Broeck, given=Guy Van, prefix=den, useprefix=false and Peng, Nanyun},
  date = {2024-06-19},
  eprint = {2406.13892},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.13892},
  url = {http://arxiv.org/abs/2406.13892},
  urldate = {2024-07-01},
  abstract = {Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable framework that facilitates tractable and flexible control of LLM generation to reliably follow logical constraints. Ctrl-G combines any production-ready LLM with a Hidden Markov Model, enabling LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of interactive text editing: specifically, for the task of generating text insertions/continuations following logical constraints, Ctrl-G achieves over 30\% higher satisfaction rate in human evaluation compared to GPT4. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts for constrained generation by large margins on standard benchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on the Grade School Math benchmark to assist LLM reasoning, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Adaptable Logical Control for Large Language Models (2024) - Zhang et al.pdf;/Users/adzcai/Zotero/storage/38W8T74Y/2406.html}
}

@online{zhai_fine-tuning_2024,
  title = {Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning},
  author = {Zhai, Yuexiang and Bai, Hao and Lin, Zipeng and Pan, Jiayi and Tong, Shengbang and Zhou, Yifei and Suhr, Alane and Xie, Saining and LeCun, Yann and Ma, Yi and Levine, Sergey},
  date = {2024-05-16},
  eprint = {2405.10292},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.10292},
  url = {http://arxiv.org/abs/2405.10292},
  urldate = {2024-07-01},
  abstract = {Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning (2024) - Zhai et al.pdf;/Users/adzcai/Zotero/storage/2X2WJU4D/2405.html}
}

@online{welleck_decoding_2024,
  title = {From Decoding to Meta-Generation: {{Inference-time}} Algorithms for Large Language Models},
  shorttitle = {From Decoding to Meta-Generation},
  author = {Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  date = {2024-06-24},
  eprint = {2406.16838},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.16838},
  url = {http://arxiv.org/abs/2406.16838},
  urldate = {2024-07-01},
  abstract = {One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/From Decoding to Meta-Generation (2024) - Welleck et al.pdf;/Users/adzcai/Zotero/storage/S4Y984R4/2406.html}
}

@online{sun_easy--hard_2024,
  title = {Easy-to-Hard Generalization: {{Scalable}} Alignment beyond Human Supervision},
  shorttitle = {Easy-to-Hard Generalization},
  author = {Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang},
  date = {2024-03-14},
  eprint = {2403.09472},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.09472},
  url = {http://arxiv.org/abs/2403.09472},
  urldate = {2024-07-01},
  abstract = {Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textbackslash textit\{easy-to-hard generalization\}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \textbackslash textit\{easy-to-hard generalization from evaluators\} can enable \textbackslash textit\{easy-to-hard generalizations in generators\} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\textbackslash\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Easy-to-Hard Generalization (2024) - Sun et al.pdf;/Users/adzcai/Zotero/storage/J52D59AK/2403.html}
}

@article{mnih_playing_2013,
  title = {Playing Atari with Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.5602},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2024-06-21},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/Playing Atari with Deep Reinforcement Learning (2013) - Mnih et al.pdf}
}

@book{agarwal_reinforcement_2022,
  title = {Reinforcement Learning: {{Theory}} and Algorithms},
  shorttitle = {{{AJKS}}},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  date = {2022-01-31},
  url = {https://rltheorybook.github.io/rltheorybook_AJKS.pdf},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Reinforcement Learning (2022) - Agarwal, Jiang, Kakade, Sun.pdf}
}

@inproceedings{azar_minimax_2017,
  title = {Minimax Regret Bounds for Reinforcement Learning},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, Rémi},
  date = {2017-07-17},
  pages = {263--272},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/azar17a.html},
  urldate = {2024-06-21},
  abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \$\textbackslash tilde \{O\}( \textbackslash sqrt\{HSAT\} + H\textasciicircum 2S\textasciicircum 2A+H\textbackslash sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \$\textbackslash tilde \{O\}(HS \textbackslash sqrt\{AT\})\$ achieved by the UCRL2 algorithm. The key significance of our new results is that when \$T\textbackslash geq H\textasciicircum 3S\textasciicircum 3A\$ and \$SA\textbackslash geq H\$, it leads to a regret of \$\textbackslash tilde\{O\}(\textbackslash sqrt\{HSAT\})\$ that matches the established lower bound of \$\textbackslash Omega(\textbackslash sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based “exploration bonuses” that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Minimax Regret Bounds for Reinforcement Learning (2017) - Azar, Osband, Munos.pdf}
}

@inproceedings{kakade_natural_2001,
  title = {A Natural Policy Gradient},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kakade, Sham M},
  date = {2001},
  volume = {14},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
  urldate = {2024-06-20},
  abstract = {We provide a  natural gradient method that represents the steepest  descent  direction based on the underlying structure of the param(cid:173) eter space.  Although gradient methods cannot make large changes  in  the  values  of the  parameters,  we  show  that  the  natural  gradi(cid:173) ent is moving toward choosing a greedy optimal action rather than  just a  better action.  These greedy optimal  actions  are those  that  would  be  chosen  under  one  improvement  step  of  policy  iteration  with  approximate,  compatible  value  functions,  as  defined  by  Sut(cid:173) ton  et al.  [9].  We  then show drastic performance improvements in  simple MDPs and in the more challenging MDP of Tetris.},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2001/A Natural Policy Gradient (2001) - Kakade.pdf}
}

@article{deng_mnist_2012,
  title = {The {{MNIST}} Database of Handwritten Digit Images for Machine Learning Research},
  author = {Deng, Li},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  url = {https://ieeexplore.ieee.org/document/6296535},
  urldate = {2025-01-02},
  abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  file = {/Users/adzcai/Zotero/storage/394GTFDU/6296535.html}
}

@software{allaire_quarto_2024,
  title = {Quarto},
  author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
  date = {2024-02},
  doi = {10.5281/zenodo.5960048},
  url = {https://github.com/quarto-dev/quarto-cli},
  version = {1.4},
  keywords = {nosource}
}

@artwork{gpa_photo_archive_robotic_2017,
  title = {Robotic Arm},
  author = {{GPA Photo Archive}},
  date = {2017-07-25},
  url = {https://www.flickr.com/photos/iip-photo-archive/36123310136/},
  urldate = {2025-01-05},
  abstract = {NASA's Marshall Space Flight Center Follow Robotics Work on Space Station Set Up Thursday Spacewalk  The Pressurized Mating Adapter-3 (PMA-3) is in the grip of the International Space Station's Canadarm2 robotic arm during its relocation and attachment to the station's Harmony module on March 26, 2017. The adapter was prepared for this move on Friday, March 24, during a successful spacewalk by Expedition 50 Commander Shane Kimbrough of NASA and Flight Engineer Thomas Pesquet of ESA (European Space Agency). The second spacewalk by Kimbrough and Flight Engineer Peggy Whitson of NASA, which began at 7:29 a.m. on Thursday, March 30, will finalize the PMA-3 cable connections on Harmony.  Photo credit: NASA {$<$}a href="http://www.flickr.com/photos/iip-photo-archive/36123310136/in/photolist-V4sDRQ-mJHgiM-TbVeu4-X36pA1-iD8YmZ-VaWXAB-nUZYbb-V4sEaL-Raxdtc-eX1zwL-amFsJE-8cThYi-69LKFA-T41G6A-T96Eed-a1PnMo-RboyLE-Sc8vEH-TtbedA-kPmFzk-Su5Rw2-ogj9rw-2cyWHU-qK5viN-pvQ1ww-at2134-7w1KCT-qU8mkJ-RxeF38-oSRtfb-ohhGHv-pZMmx5-pw4vBi-qRURyA-6iQLJU-m3Asiq-7PMF1B-8UUSxz-SxtYZo-9KKHbV-C1vqkF-nUZWEq-U5h3JP-ocqj1w-pGAfL3-69LKCb-SqEefQ-qBKEeg-V7eaa8-ViUXY8"{$>$}www.flickr.com/photos/iip-photo-archive/36123310136/in/ph...{$<$}/a{$>$}},
  file = {/Users/adzcai/Zotero/storage/2TVTE9D9/Robotic arm (2017) - Archive.jpg}
}

@artwork{frans_berkelaar_container_2009,
  title = {Container Ship {{MSC}} Davos - Westerschelde - Zeeland},
  author = {{Frans Berkelaar}},
  date = {2009-05-29},
  url = {https://www.flickr.com/photos/28169156@N03/52957948820/},
  urldate = {2025-01-05},
  abstract = {OLYMPUS DIGITAL CAMERA},
  file = {/Users/adzcai/Zotero/storage/VLKCH5QJ/Container ship MSC Davos - Westerschelde - Zeeland (2009) - Frans Berkelaar.jpg}
}

@artwork{guy_chess_2006,
  title = {Chess},
  author = {Guy, Romain},
  date = {2006-08-04},
  url = {https://www.flickr.com/photos/romainguy/230416692/},
  urldate = {2025-01-05},
  abstract = {We played some chess while waiting for our room to be ready when we got at our hotel in Hangzhou.},
  file = {/Users/adzcai/Zotero/storage/WEYSCFYL/Chess (2006) - Guy.jpg}
}

@article{white_survey_1993,
  title = {A Survey of Applications of Markov Decision Processes},
  author = {White, D. J.},
  date = {1993},
  journaltitle = {The Journal of the Operational Research Society},
  volume = {44},
  number = {11},
  eprint = {2583870},
  eprinttype = {jstor},
  pages = {1073--1096},
  publisher = {Palgrave Macmillan Journals},
  issn = {0160-5682},
  doi = {10.2307/2583870},
  url = {https://www.jstor.org/stable/2583870},
  urldate = {2025-01-05},
  abstract = {A collection of papers on the application of Markov decision processes is surveyed and classified according to the use of real life data, structural results and special computational schemes. Observations are made about various features of the applications.},
  keywords = {nosource}
}

@online{foster_statistical_2023,
  title = {The Statistical Complexity of Interactive Decision Making},
  author = {Foster, Dylan J. and Kakade, Sham M. and Qian, Jian and Rakhlin, Alexander},
  date = {2023-07-11},
  eprint = {2112.13487},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.13487},
  url = {http://arxiv.org/abs/2112.13487},
  urldate = {2025-01-23},
  abstract = {A fundamental challenge in interactive learning and decision making, ranging from bandit problems to reinforcement learning, is to provide sample-efficient, adaptive learning algorithms that achieve near-optimal regret. This question is analogous to the classical problem of optimal (supervised) statistical learning, where there are well-known complexity measures (e.g., VC dimension and Rademacher complexity) that govern the statistical complexity of learning. However, characterizing the statistical complexity of interactive learning is substantially more challenging due to the adaptive nature of the problem. The main result of this work provides a complexity measure, the Decision-Estimation Coefficient, that is proven to be both necessary and sufficient for sample-efficient interactive learning. In particular, we provide: 1. a lower bound on the optimal regret for any interactive decision making problem, establishing the Decision-Estimation Coefficient as a fundamental limit. 2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which transforms any algorithm for supervised estimation into an online algorithm for decision making. E2D attains a regret bound that matches our lower bound up to dependence on a notion of estimation performance, thereby achieving optimal sample-efficient learning as characterized by the Decision-Estimation Coefficient. Taken together, these results constitute a theory of learnability for interactive decision making. When applied to reinforcement learning settings, the Decision-Estimation Coefficient recovers essentially all existing hardness results and lower bounds. More broadly, the approach can be viewed as a decision-theoretic analogue of the classical Le Cam theory of statistical estimation; it also unifies a number of existing approaches -- both Bayesian and frequentist.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/The Statistical Complexity of Interactive Decision Making (2023) - Foster, Kakade, Qian, Rakhlin.pdf;/Users/adzcai/Zotero/storage/M7UNCKRF/2112.html}
}

@online{shao_deepseekmath_2024,
  title = {{{DeepSeekMath}}: {{Pushing}} the Limits of Mathematical Reasoning in Open Language Models},
  shorttitle = {{{DeepSeekMath}}},
  author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
  date = {2024-04-27},
  eprint = {2402.03300},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.03300},
  url = {http://arxiv.org/abs/2402.03300},
  urldate = {2025-02-06},
  abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/DeepSeekMath (2024) - Shao et al.pdf;/Users/adzcai/Zotero/storage/JYYAQBDU/2402.html}
}

@article{meurer_sympy_2017,
  title = {{{SymPy}}: Symbolic Computing in Python},
  shorttitle = {{{SymPy}}},
  author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and Čertík, Ondřej and Kirpichev, Sergey B. and Rocklin, Matthew and family=Kumar, given=AmiT, given-i={{Am}} and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Roučka, Štěpán and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
  date = {2017-01-02},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {3},
  pages = {e103},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.103},
  url = {https://peerj.com/articles/cs-103},
  urldate = {2025-02-06},
  abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/SymPy (2017) - Meurer et al.pdf}
}

@inproceedings{ansel_pytorch_2024,
  title = {{{PyTorch}} 2: Faster Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle = {29th {{ACM}} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 ({{ASPLOS}} '24)},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and family=Luk, given=CK, given-i=CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  date = {2024-04},
  publisher = {ACM},
  doi = {10.1145/3620665.3640366},
  url = {https://pytorch.org/assets/pytorch2-2.pdf},
  keywords = {nosource}
}

@article{carvalho_predictive_2024,
  title = {Predictive Representations: Building Blocks of Intelligence},
  shorttitle = {Predictive Representations},
  author = {Carvalho, Wilka and Tomov, Momchil S. and family=Cothi, given=William, prefix=de, useprefix=true and Barry, Caswell and Gershman, Samuel J.},
  date = {2024-10-11},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {36},
  number = {11},
  pages = {2225--2298},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01705},
  url = {https://doi.org/10.1162/neco_a_01705},
  urldate = {2025-02-07},
  abstract = {Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This review integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation and its generalizations, which have been widely applied as both engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Predictive Representations (2024) - Carvalho et al.pdf;/Users/adzcai/Zotero/storage/KH9JNQTF/Predictive-Representations-Building-Blocks-of.html}
}

@online{deepseek-ai_deepseek-r1_2025,
  title = {{{DeepSeek-R1}}: Incentivizing Reasoning Capability in Llms via Reinforcement Learning},
  shorttitle = {{{DeepSeek-R1}}},
  author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  date = {2025-01-22},
  eprint = {2501.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.12948},
  url = {http://arxiv.org/abs/2501.12948},
  urldate = {2025-02-07},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2025/DeepSeek-R1 (2025) - DeepSeek-AI et al.pdf;/Users/adzcai/Zotero/storage/3MYSD8TV/2501.html}
}

@article{maxwell_governors_1867,
  title = {On Governors},
  author = {Maxwell, J. Clerk},
  date = {1867},
  journaltitle = {Proceedings of the royal society of London},
  volume = {16},
  eprint = {112510},
  eprinttype = {jstor},
  pages = {270--283},
  publisher = {Royal Society},
  issn = {0370-1662},
  url = {https://www.jstor.org/stable/112510},
  urldate = {2025-03-08},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1867/On Governors (1867) - Maxwell.pdf}
}

@inproceedings{schulman_trust_2015,
  title = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  date = {2015-06-01},
  pages = {1889--1897},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/schulman15.html},
  urldate = {2025-03-09},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Trust region policy optimization (2015) - Schulman et al 2.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Trust region policy optimization (2015) - Schulman et al.pdf}
}

@artwork{danilyuk_robot_2021,
  title = {A Robot Imitating a Girl's Movement},
  author = {Danilyuk, Pavel},
  date = {2021-05-19},
  url = {https://www.pexels.com/photo/a-robot-imitating-a-girl-s-movement-8294811/}
}

@inproceedings{huang_ppo-clip_2024,
  title = {{{PPO-clip}} Attains Global Optimality: Towards Deeper Understandings of Clipping},
  shorttitle = {{{PPO-clip}} Attains Global Optimality},
  booktitle = {Proceedings of the Thirty-Eighth {{AAAI}} Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
  author = {Huang, Nai-Chieh and Hsieh, Ping-Chun and Ho, Kuo-Hao and Wu, I-Chen},
  date = {2024-02-20},
  series = {{{AAAI}}'24/{{IAAI}}'24/{{EAAI}}'24},
  volume = {38},
  pages = {12600--12607},
  publisher = {AAAI Press},
  doi = {10.1609/aaai.v38i11.29154},
  url = {https://doi.org/10.1609/aaai.v38i11.29154},
  urldate = {2025-03-19},
  abstract = {Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the O(1/√T) min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis, we streamline convergence analysis by introducing a two-step policy improvement approach. This decouples policy search from complex neural policy parameterization using a regression-based update scheme. Furthermore, we gain deeper insights into the efficacy of PPO-Clip by interpreting these generalized objectives. Our theoretical findings also mark the first characterization of the influence of the clipping mechanism on PPO-Clip convergence. Importantly, the clipping range affects only the pre-constant of the convergence rate.},
  isbn = {978-1-57735-887-9},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/PPO-clip attains global optimality (2024) - Huang, Hsieh, Ho, Wu.pdf}
}

@article{silver_reward_2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  date = {2021-10-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
  urldate = {2025-03-19},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  keywords = {Artificial general intelligence,Artificial intelligence,Reinforcement learning,Reward},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Reward is enough (2021) - Silver, Singh, Precup, Sutton.pdf;/Users/adzcai/Zotero/storage/VCSD52BK/S0004370221000862.html}
}

@book{bertsekas_neuro-dynamic_1996,
  title = {Neuro-Dynamic Programming},
  author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  date = {1996},
  series = {Optimization and Neural Computation Series},
  publisher = {Athena scientific},
  location = {Belmont (Mass.)},
  isbn = {978-1-886529-10-6},
  langid = {english}
}

@inproceedings{hasselt_double_2010,
  title = {Double Q-Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hasselt, Hado},
  date = {2010},
  volume = {23},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html},
  urldate = {2025-03-20},
  abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2010/Double Q-learning (2010) - Hasselt.pdf}
}

@inproceedings{wang_dueling_2016,
  title = {Dueling Network Architectures for Deep Reinforcement Learning},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  date = {2016-06-11},
  pages = {1995--2003},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v48/wangf16.html},
  urldate = {2025-03-20},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Dueling Network Architectures for Deep Reinforcement Learning (2016) - Wang et al.pdf}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/document/726791},
  urldate = {2025-03-20},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1998/Gradient-based learning applied to document recognition (1998) - Lecun, Bottou, Bengio, Haffner.pdf;/Users/adzcai/Zotero/storage/YMDDDNJS/726791.html}
}

@inproceedings{schaul_prioritized_2016,
  title = {Prioritized Experience Replay},
  booktitle = {4th International Conference on Learning Representations},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2016-05-02},
  location = {San Juan, Puerto Rico},
  url = {http://arxiv.org/abs/1511.05952},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  keywords = {⛔ No DOI found},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Prioritized experience replay (2016) - Schaul et al.pdf}
}

@inproceedings{farebrother_stop_2024,
  title = {Stop Regressing: Training Value Functions via Classification for Scalable Deep {{RL}}},
  shorttitle = {Stop Regressing},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  author = {Farebrother, Jesse and Orbay, Jordi and Vuong, Quan and Taïga, Adrien Ali and Chebotar, Yevgen and Xiao, Ted and Irpan, Alex and Levine, Sergey and Castro, Pablo Samuel and Faust, Aleksandra and Kumar, Aviral and Agarwal, Rishabh},
  date = {2024-07-21},
  series = {{{ICML}}'24},
  volume = {235},
  pages = {13049--13071},
  publisher = {JMLR.org},
  location = {Vienna, Austria},
  abstract = {Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a crossentropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multitask RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Stop regressing (2024) - Farebrother et al.pdf}
}

@inproceedings{hasselt_deep_2016,
  title = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence},
  author = {family=Hasselt, given=Hado, prefix=van, useprefix=false and Guez, Arthur and Silver, David},
  date = {2016-02-12},
  series = {{{AAAI}}'16},
  pages = {2094--2100},
  publisher = {AAAI Press},
  location = {Phoenix, Arizona},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Deep reinforcement learning with double Q-Learning (2016) - Hasselt, Guez, Silver.pdf}
}

@inproceedings{mnih_asynchronous_2016,
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-11},
  pages = {1928--1937},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v48/mniha16.html},
  urldate = {2025-03-20},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Asynchronous Methods for Deep Reinforcement Learning (2016) - Mnih et al.pdf}
}

@inproceedings{schulman_high-dimensional_2016,
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  booktitle = {4th International Conference on Learning Representations},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2016-05-02},
  location = {San Juan, Puerto Rico},
  url = {http://arxiv.org/abs/1506.02438},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  keywords = {⛔ No DOI found},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/High-dimensional continuous control using generalized advantage estimation (2016) - Schulman et al.pdf}
}

@inproceedings{wu_scalable_2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using Kronecker-Factored Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html},
  urldate = {2025-03-20},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also the method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (2017) - Wu et al.pdf}
}

@inproceedings{wang_sample_2017,
  title = {Sample Efficient Actor-Critic with Experience Replay},
  booktitle = {5th International Conference on Learning Representations},
  author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and family=Freitas, given=Nando, prefix=de, useprefix=false},
  date = {2017-02-06},
  url = {https://openreview.net/forum?id=HyM25Mqel},
  urldate = {2025-03-20},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Sample Efficient Actor-Critic with Experience Replay (2017) - Wang et al.pdf}
}

@inproceedings{haarnoja_soft_2018,
  title = {Soft Actor-Critic: {{Off-policy}} Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  shorttitle = {Soft Actor-Critic},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  date = {2018-07-03},
  pages = {1861--1870},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/haarnoja18b.html},
  urldate = {2025-03-20},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Soft Actor-Critic (2018) - Haarnoja, Zhou, Abbeel, Levine_1.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Soft Actor-Critic (2018) - Haarnoja, Zhou, Abbeel, Levine_2.pdf}
}

@inproceedings{silver_deterministic_2014,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  date = {2014-01-27},
  pages = {387--395},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v32/silver14.html},
  urldate = {2025-03-20},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2014/Deterministic Policy Gradient Algorithms (2014) - Silver et al.pdf}
}

@book{szepesvari_algorithms_2010,
  title = {Algorithms for Reinforcement Learning},
  author = {Szepesvári, Csaba},
  date = {2010},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-01551-9},
  url = {https://link.springer.com/10.1007/978-3-031-01551-9},
  urldate = {2025-03-20},
  isbn = {978-3-031-00423-0 978-3-031-01551-9},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2010/Algorithms for Reinforcement Learning (2010) - Szepesvári.pdf}
}

@book{plaat_deep_2022,
  title = {Deep Reinforcement Learning},
  author = {Plaat, Aske},
  date = {2022},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-19-0638-1},
  url = {https://link.springer.com/10.1007/978-981-19-0638-1},
  urldate = {2025-03-20},
  isbn = {978-981-19-0637-4 978-981-19-0638-1},
  langid = {english},
  keywords = {Alpha Go,artificial intelligence,deep learning,deep reinforcement learning,meta-learning,multi-agent learning,optimal control,reinforcement learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Deep Reinforcement Learning (2022) - Plaat.pdf}
}

@book{murphy_probabilistic_2023,
  title = {Probabilistic Machine Learning: Advanced Topics},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  date = {2023},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts London},
  abstract = {"An advanced book for researchers and graduate students working in machine learning and statistics that reflects the influence of deep learning"--},
  isbn = {978-0-262-04843-9 978-0-262-37599-3 978-0-262-37600-6},
  langid = {english},
  pagetotal = {1},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Probabilistic machine learning (2023) - Murphy.pdf}
}

@online{ivanov_modern_2019,
  title = {Modern Deep Reinforcement Learning Algorithms},
  author = {Ivanov, Sergey and D'yakonov, Alexander},
  date = {2019-07-06},
  eprint = {1906.10025},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.10025},
  url = {http://arxiv.org/abs/1906.10025},
  urldate = {2025-03-21},
  abstract = {Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2019/Modern Deep Reinforcement Learning Algorithms (2019) - Ivanov, D'yakonov.pdf;/Users/adzcai/Zotero/storage/KLSM92FX/1906.html}
}

@book{puterman_markov_1994,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  shorttitle = {Markov Decision Processes},
  author = {Puterman, Martin L.},
  date = {1994},
  series = {Wiley Series in Probability and Mathematical Statistics. {{Applied}} Probability and Statistics Section},
  publisher = {Wiley},
  location = {New York},
  isbn = {978-0-471-61977-2},
  pagetotal = {649},
  keywords = {Dynamic programming,Markov processes,Statistical decision}
}

@book{li_reinforcement_2023,
  title = {Reinforcement Learning for Sequential Decision and Optimal Control},
  author = {Li, Shengbo Eben},
  date = {2023},
  publisher = {Springer Nature},
  location = {Singapore},
  doi = {10.1007/978-981-19-7784-8},
  url = {https://link.springer.com/10.1007/978-981-19-7784-8},
  urldate = {2025-03-21},
  isbn = {978-981-19-7783-1 978-981-19-7784-8},
  langid = {english},
  keywords = {Artificial Intelligence,Deep Learning,Engineering Application,Machine Learning,Optimal Control,Reinforcement Learning}
}

@book{rao_foundations_2022,
  title = {Foundations of Reinforcement Learning with Applications in Finance},
  author = {Rao, Ashwin and Jelvis, Tikhon},
  date = {2022-12-15},
  publisher = {{Chapman and Hall/CRC}},
  location = {New York},
  doi = {10.1201/9781003229193},
  abstract = {Foundations of Reinforcement Learning with Applications in Finance aims to demystify Reinforcement Learning, and to make it a practically useful tool for those studying and working in applied areas — especially finance. Reinforcement Learning is emerging as a powerful technique for solving a variety of complex problems across industries that involve Sequential Optimal Decisioning under Uncertainty. Its penetration in high-profile problems like self-driving cars, robotics, and strategy games points to a future where Reinforcement Learning algorithms will have decisioning abilities far superior to humans. But when it comes getting educated in this area, there seems to be a reluctance to jump right in, because Reinforcement Learning appears to have acquired a reputation for being mysterious and technically challenging. This book strives to impart a lucid and insightful understanding of the topic by emphasizing the foundational mathematics and implementing models and algorithms in well-designed Python code, along with robust coverage of several financial trading problems that can be solved with Reinforcement Learning. This book has been created after years of iterative experimentation on the pedagogy of these topics while being taught to university students as well as industry practitioners. Features Focus on the foundational theory underpinning Reinforcement Learning and software design of the corresponding models and algorithms Suitable as a primary text for courses in Reinforcement Learning, but also as supplementary reading for applied/financial mathematics, programming, and other related courses Suitable for a professional audience of quantitative analysts or data scientists Blends theory/mathematics, programming/algorithms and real-world financial nuances while always striving to maintain simplicity and to build intuitive understanding To access the code base for this book, please go to: https://github.com/TikhonJelvis/RL-book},
  isbn = {978-1-003-22919-3},
  pagetotal = {522},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Foundations of Reinforcement Learning with Applications in Finance (2022) - Rao, Jelvis.pdf}
}

@book{mannor_reinforcement_2024,
  title = {Reinforcement Learning: Foundations},
  author = {Mannor, Shie and Mansour, Yishay and Tamar, Aviv},
  date = {2024},
  url = {https://sites.google.com/view/rlfoundations/home},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Reinforcement learning (2024) - Mannor, Mansour, Tamar.pdf}
}

@article{arulkumaran_deep_2017,
  title = {Deep Reinforcement Learning: A Brief Survey},
  shorttitle = {Deep Reinforcement Learning},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  date = {2017-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  pages = {26--38},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2743240},
  url = {https://ieeexplore.ieee.org/document/8103164},
  urldate = {2025-03-21},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Artificial intelligence,Learning (artificial intelligence),Machine learning,Neural networks,Signal processing algorithms,Visualization},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Deep Reinforcement Learning (2017) - Arulkumaran, Deisenroth, Brundage, Bharath.pdf;/Users/adzcai/Zotero/storage/K9ATXLD4/8103164.html}
}

@article{weng2018bandit,
  title = {A (Long) Peek into Reinforcement Learning},
  author = {Weng, Lilian},
  date = {2018},
  journaltitle = {lilianweng.github.io},
  url = {https://lilianweng.github.io/posts/2018-02-19-rl-overview/},
  keywords = {⛔ No DOI found}
}

@book{powell_reinforcement_2022,
  title = {Reinforcement Learning and Stochastic Optimization: A Unified Framework for Sequential Decisions},
  shorttitle = {Reinforcement Learning and Stochastic Optimization},
  author = {Powell, Warren B.},
  date = {2022},
  publisher = {Wiley},
  location = {Hoboken, New Jersey},
  abstract = {"The first step in sequential decision problems is to understand what decisions are being made. It is surprising how often it is that people faced with complex problems, which spans scientists in a lab to people trying to solve major health problems, are not able to identify the decisions they face. We then want to find a method for making decisions. There are at least 45 words in the English language that are equivalent to "method for making a decision," but the one we have settled on is policy. The term policy is very familiar to fields such as Markov decision processes and reinforcement learning, but with a much narrower interpretation than we will use. Other fields do not use the term at all. Designing effective policies will be the focus of most of this book. Even more subtle is identifying the different sources of uncertainty. It can be hard enough trying to identify potential decisions, but thinking about all the random events that might affect whatever it is that you are managing, whether it is reducing disease, managing inventories, or making investments, can seem like a hopeless challenge"--},
  isbn = {978-1-119-81503-7 978-1-119-81504-4 978-1-119-81505-1 978-1-119-81506-8},
  langid = {english},
  pagetotal = {1}
}

@inproceedings{agarwal_deep_2021,
  title = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  date = {2021},
  volume = {34},
  pages = {29304--29320},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html},
  urldate = {2025-03-21},
  abstract = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field's confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field. This work received an outstanding paper award at NeurIPS 2021.},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Deep reinforcement learning at the edge of the statistical precipice (2021) - Agarwal et al_1.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Deep Reinforcement Learning at the Edge of the Statistical Precipice (2021) - Agarwal et al.pdf}
}

@online{openai_introducing_2022,
  title = {Introducing {{ChatGPT}}},
  author = {{OpenAI}},
  date = {2022-11-30},
  url = {https://openai.com/index/chatgpt/},
  urldate = {2025-03-21},
  abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  langid = {american},
  organization = {OpenAI News},
  file = {/Users/adzcai/Zotero/storage/4INTYPBS/chatgpt.html}
}

@article{bertsekas_model_2024,
  title = {Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming},
  shorttitle = {Model Predictive Control and Reinforcement Learning},
  author = {Bertsekas, Dimitri P.},
  date = {2024-01-01},
  journaltitle = {IFAC-PapersOnLine},
  shortjournal = {IFAC-PapersOnLine},
  series = {8th {{IFAC Conference}} on {{Nonlinear Model Predictive Control NMPC}} 2024},
  volume = {58},
  number = {18},
  pages = {363--383},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2024.09.056},
  url = {https://www.sciencedirect.com/science/article/pii/S2405896324014356},
  urldate = {2025-03-21},
  abstract = {In this paper we describe a new conceptual framework that connects approximate Dynamic Programming (DP), Model Predictive Control (MPC), and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method. We call them the off-line training and the on-line play algorithms. The names are borrowed from some of the major successes of RL involving games; primary examples are the recent (2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents. Significantly, the synergy between off-line training and on-line play also underlies MPC (as well as other major classes of sequential decision problems), and indeed the MPC design architecture is very similar to the one of AlphaZero and TD-Gammon. This conceptual insight provides a vehicle for bridging the cultural gap between RL and MPC, and sheds new light on some fundamental issues in MPC. These include the enhancement of stability properties through rollout, the treatment of uncertainty through the use of certainty equivalence, the resilience of MPC in adaptive control settings that involve changing system parameters, and the insights provided by the superlinear performance bounds implied by Newton's method.},
  keywords = {Adaptive Control,Dynamic Programming,Model Predictive Control,Newton's Method,Reinforcement Learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Model Predictive Control and Reinforcement Learning (2024) - Bertsekas.pdf;/Users/adzcai/Zotero/storage/69APCYBK/S2405896324014356.html}
}

@article{turing_intelligent_1948,
  title = {Intelligent Machinery},
  author = {Turing, Alan},
  date = {1948},
  journaltitle = {National physical laboratory},
  url = {https://weightagnostic.github.io/papers/turing1948.pdf},
  urldate = {2025-03-21},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/_/turing1948 () - .pdf}
}

@artwork{pixabay_20_2016,
  title = {20 Mg Label Blister Pack},
  author = {{Pixabay}},
  date = {2016-10-10},
  url = {https://www.pexels.com/photo/20-mg-label-blister-pack-208512/},
  file = {/Users/adzcai/Zotero/storage/JZGULUG7/pexels-pixabay-208512.jpg}
}

@artwork{negative_space_photo_2015,
  title = {Photo of Commercial District during Dawn},
  author = {{Negative Space}},
  date = {2015-01-08},
  url = {https://www.pexels.com/photo/photo-of-commercial-district-during-dawn-34639/},
  file = {/Users/adzcai/Zotero/storage/8WR3IMDN/pexels-negativespace-34639.jpg}
}

@article{bubeck_pure_2011,
  title = {Pure Exploration in Finitely-Armed and Continuous-Armed Bandits},
  author = {Bubeck, Sébastien and Munos, Rémi and Stoltz, Gilles},
  date = {2011-04-22},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Algorithmic {{Learning Theory}} ({{ALT}} 2009)},
  volume = {412},
  number = {19},
  pages = {1832--1852},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2010.12.059},
  url = {https://www.sciencedirect.com/science/article/pii/S030439751000767X},
  urldate = {2025-03-21},
  abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.},
  keywords = {Continuous-armed bandits,Efficient exploration,Multi-armed bandits,Simple regret},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2011/Pure exploration in finitely-armed and continuous-armed bandits (2011) - Bubeck, Munos, Stoltz.pdf;/Users/adzcai/Zotero/storage/X6Z4IFMA/S030439751000767X.html}
}

@inproceedings{dann_unifying_2017,
  title = {Unifying {{PAC}} and Regret: Uniform {{PAC}} Bounds for Episodic Reinforcement Learning},
  shorttitle = {Unifying {{PAC}} and Regret},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html},
  urldate = {2025-03-21},
  abstract = {Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Unifying PAC and Regret (2017) - Dann, Lattimore, Brunskill.pdf}
}

@online{razin_what_2025,
  title = {What Makes a Reward Model a Good Teacher? {{An}} Optimization Perspective},
  shorttitle = {What Makes a Reward Model a Good Teacher?},
  author = {Razin, Noam and Wang, Zixuan and Strauss, Hubert and Wei, Stanley and Lee, Jason D. and Arora, Sanjeev},
  date = {2025-03-19},
  eprint = {2503.15477},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.15477},
  url = {http://arxiv.org/abs/2503.15477},
  urldate = {2025-03-22},
  abstract = {The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. While this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient optimization.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2025/What Makes a Reward Model a Good Teacher (2025) - Razin et al.pdf;/Users/adzcai/Zotero/storage/PXP47NW9/2503.html}
}

@artwork{pixabay_coins_2016,
  title = {Coins on Brown Wood},
  author = {{Pixabay}},
  date = {2016-09-10},
  url = {https://www.pexels.com/photo/coins-on-brown-wood-210600/},
  file = {/Users/adzcai/Zotero/storage/5AKG6AW5/pexels-pixabay-210600.jpg}
}

@book{gittins_multi-armed_2011,
  title = {Multi-Armed Bandit Allocation Indices},
  author = {Gittins, John C.},
  namea = {Glazebrook, Kevin D. and Weber, Richard},
  nameatype = {collaborator},
  date = {2011},
  edition = {2nd ed},
  publisher = {Wiley},
  location = {Chichester},
  isbn = {978-1-119-99021-5},
  langid = {english},
  pagetotal = {293}
}

@book{sontag_mathematical_1998,
  title = {Mathematical Control Theory},
  author = {Sontag, Eduardo D.},
  editor = {Marsden, J. E. and Sirovich, L. and Golubitsky, M. and Jäger, W.},
  editortype = {redactor},
  date = {1998},
  series = {Texts in {{Applied Mathematics}}},
  volume = {6},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-0577-7},
  url = {http://link.springer.com/10.1007/978-1-4612-0577-7},
  urldate = {2025-03-23},
  isbn = {978-1-4612-6825-3 978-1-4612-0577-7},
  keywords = {Algebra,control,differential equation,knowledge,Lie-Algebra,linear algebra,mathematics,Optimal control,stability,systems theory}
}

@inproceedings{li_contextual-bandit_2010,
  title = {A Contextual-Bandit Approach to Personalized News Article Recommendation},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
  date = {2010-04-26},
  series = {{{WWW}} '10},
  pages = {661--670},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1772690.1772758},
  url = {https://doi.org/10.1145/1772690.1772758},
  urldate = {2025-03-23},
  abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5\% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
  isbn = {978-1-60558-799-8},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2010/A contextual-bandit approach to personalized news article recommendation (2010) - Li, Chu, Langford, Schapire.pdf}
}

@article{auer_using_2002,
  title = {Using Confidence Bounds for Exploitation-Exploration Trade-Offs},
  author = {Auer, Peter},
  date = {2002},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {397--422},
  issn = {ISSN 1533-7928},
  url = {https://www.jmlr.org/papers/v3/auer02a.html},
  urldate = {2025-03-23},
  abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
  issue = {Nov},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2002/Using Confidence Bounds for Exploitation-Exploration Trade-offs (2002) - Auer.pdf}
}

@article{thompson_likelihood_1933,
  title = {On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
  author = {Thompson, William R.},
  date = {1933},
  journaltitle = {Biometrika},
  volume = {25},
  number = {3/4},
  eprint = {2332286},
  eprinttype = {jstor},
  pages = {285--294},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2332286},
  url = {https://www.jstor.org/stable/2332286},
  urldate = {2025-03-24},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1933/On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples (1933) - Thompson.pdf}
}

@article{thompson_theory_1935,
  title = {On the Theory of Apportionment},
  author = {Thompson, William R.},
  date = {1935},
  journaltitle = {American Journal of Mathematics},
  volume = {57},
  number = {2},
  eprint = {2371219},
  eprinttype = {jstor},
  pages = {450--456},
  publisher = {The Johns Hopkins University Press},
  issn = {0002-9327},
  doi = {10.2307/2371219},
  url = {https://www.jstor.org/stable/2371219},
  urldate = {2025-03-24},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1935/On the Theory of Apportionment (1935) - Thompson.pdf}
}

@book{stigler_history_2003,
  title = {The History of Statistics: The Measurement of Uncertainty before 1900},
  shorttitle = {The History of Statistics},
  author = {Stigler, Stephen M.},
  date = {2003},
  edition = {9. print},
  publisher = {Belknap Pr. of Harvard Univ. Pr},
  location = {Cambridge, Mass.},
  isbn = {978-0-674-40341-3 978-0-674-40340-6},
  langid = {english},
  pagetotal = {410},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2003/The history of statistics (2003) - Stigler.djvu}
}

@incollection{fisher_statistical_1992,
  title = {Statistical Methods for Research Workers},
  booktitle = {Breakthroughs in {{Statistics}}: {{Methodology}} and {{Distribution}}},
  author = {Fisher, R. A.},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  date = {1992},
  pages = {66--70},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_6},
  url = {https://doi.org/10.1007/978-1-4612-4380-9_6},
  urldate = {2025-03-24},
  abstract = {The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.},
  isbn = {978-1-4612-4380-9},
  langid = {english},
  keywords = {Manure Plot,Manurial Response,Manurial Treatment,Rothamsted Experimental Station,Total Yield},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1992/Breakthroughs in Statistics, Statistical Methods for Research Workers (1992) - Fisher, Kotz, Johnson.pdf}
}

@book{fisher_statistical_1925,
  title = {Statistical Methods for Research Workers, 11th Ed. Rev},
  author = {Fisher, R.A.},
  date = {1925},
  series = {Statistical Methods for Research Workers, 11th Ed. Rev},
  publisher = {Edinburgh},
  location = {Oliver and Boyd},
  abstract = {Contains revisions of probability formulas and treatment of correlations.  Harvard Book List (edited) 1955 \#94 (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/Users/adzcai/Zotero/storage/TQKK8E4C/1925-15003-000.html}
}

@article{robbins_aspects_1952,
  title = {Some Aspects of the Sequential Design of Experiments},
  author = {Robbins, Herbert},
  date = {1952-09},
  journaltitle = {Bulletin of the American Mathematical Society},
  volume = {58},
  number = {5},
  pages = {527--535},
  publisher = {American Mathematical Society},
  issn = {0002-9904, 1936-881X},
  url = {https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full},
  urldate = {2025-03-24},
  abstract = {Bulletin (New Series) of the American Mathematical Society},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1952/Some aspects of the sequential design of experiments (1952) - Robbins.pdf}
}

@article{wald_statistical_1949,
  title = {Statistical Decision Functions},
  author = {Wald, Abraham},
  date = {1949-06},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {20},
  number = {2},
  pages = {165--205},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177730030},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-20/issue-2/Statistical-Decision-Functions/10.1214/aoms/1177730030.full},
  urldate = {2025-03-24},
  abstract = {The foundations of a general theory of statistical decision functions, including the classical non-sequential case as well as the sequential case, was discussed by the author in a previous publication [3]. Several assumptions made in [3] appear, however, to be unnecessarily restrictive (see conditions 1-7, pp. 297 in [3]). These assumptions, moreover, are not always fulfilled for statistical problems in their conventional form. In this paper the main results of [3], as well as several new results, are obtained from a considerably weaker set of conditions which are fulfilled for most of the statistical problems treated in the literature. It seemed necessary to abandon most of the methods of proofs used in [3] (particularly those in section 4 of [3]) and to develop the theory from the beginning. To make the present paper self-contained, the basic definitions already given in [3] are briefly restated in section 2.1. In [3] it is postulated (see Condition 3, p. 207) that the space \$\textbackslash Omega\$ of all admissible distribution functions \$F\$ is compact. In problems where the distribution function \$F\$ is known except for the values of a finite number of parameters, i.e., where \$\textbackslash Omega\$ is a parametric class of distribution functions, the compactness condition will usually not be fulfilled if no restrictions are imposed on the possible values of the parameters. For example, if \$\textbackslash Omega\$ is the class of all univariate normal distributions with unit variance, \$\textbackslash Omega\$ is not compact. It is true that by restricting the parameter space to a bounded and closed subset of the unrestricted space, compactness of \$\textbackslash Omega\$ will usually be attained. Since such a restriction of the parameter space can frequently be made in applied problems, the condition of compactness may not be too restrictive from the point of view of practical applications. Nevertheless, it seems highly desirable from the theoretical point of view to eliminate or to weaken the condition of compactness of \$\textbackslash Omega\$. This is done in the present paper. The compactness condition is completely omitted in the discrete case (Theorems 2.1-2.5), and replaced by the condition of separability of \$\textbackslash Omega\$ in the continuous case (Theorems 3.1-3.4). The latter condition is fulfilled in most of the conventional statistical problems. Another restriction postulated in [3] (Condition 4, p. 297) is the continuity of the weight function \$W(F, d)\$ in \$F\$. As explained in section 2.1 of the present paper, the value of \$W(F, d)\$ is interpreted as the loss suffered when \$F\$ happens to be the true distribution of the chance variables under consideration and the decision \$d\$ is made by the statistician. While the assumption of continuity of \$W(F, d)\$ in \$F\$ may seem reasonable from the point of view of practical application, it is rather undesirable from the theoretical point of view for the following reasons. It is of considerable theoretical interest to consider simplified weight functions \$W(F, d)\$ which can take only the values 0 and 1 (the value 0 corresponds to a correct decision, and the value 1 to a wrong decision). Frequently, such weight functions are necessarily discontinuous. Consider, for example, the problem of testing the hypothesis \$H\$ that the mean \$\textbackslash theta\$ of a normally distributed chance variable \$X\$ with unit variance is equal to zero. Let \$d\_1\$ denote the decision to accept \$H\$, and \$d\_2\$ the decision to reject \$H\$. Assigning the value zero to the weight \$W\$ whenever a correct decision is made, and the value 1 whenever a wrong decision is made, we have: \$W(\textbackslash theta, d\_1) = 0 \textbackslash text\{for\} \textbackslash theta = 0, \textbackslash text\{and\} = 1 \textbackslash text\{for\} \textbackslash theta \textbackslash neq 0; W (\textbackslash theta, d\_2) = 0 \textbackslash text\{for\} \textbackslash theta \textbackslash neq 0, \textbackslash text\{and\} = 1 \textbackslash text\{for\} \textbackslash theta = 0.\$ This weight function is obviously discontinuous. In the present paper the main results (Theorems 2.1-2.5 and Theorems 3.1-3.4) are obtained without making any continuity assumption regarding \$W(F, d)\$. The restrictions imposed in the present paper on the cost function of experimentation are considerably weaker than those formulated in [3]. Condition 5 [3, p. 297] concerning the class \$\textbackslash Omega\$ of admissible distribution functions, and condition 7 [3, p. 298] concerning the class of decision functions at the disposal of the statistician are omitted here altogether. One of the new results obtained here is the establishment of the existence of so called minimax solutions under rather weak conditions (Theorems 2.3 and 3.2). This result is a simple consequence of two lemmas (Lemmas 2.4 and 3.3) which seem to be of interest in themselves. The present paper consists of three sections. In the first section several theorems are given concerning zero sum two person games which go somewhat beyond previously published results. The results in section 1 are then applied to statistical decision functions in sections 2 and 3. Section 2 treats the case of discrete chance variables, while section 3 deals with the continuous case. The two cases have been treated separately, since the author was not able to find any simple and convenient way of combining them into a single more general theory.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1949/Statistical Decision Functions (1949) - Wald.pdf}
}

@article{bellman_problem_1956,
  title = {A Problem in the Sequential Design of Experiments},
  author = {Bellman, Richard},
  date = {1956},
  journaltitle = {Sankhyā: The Indian Journal of Statistics (1933-1960)},
  volume = {16},
  number = {3/4},
  eprint = {25048278},
  eprinttype = {jstor},
  pages = {221--229},
  publisher = {Springer},
  issn = {0036-4452},
  url = {https://www.jstor.org/stable/25048278},
  urldate = {2025-03-24},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1956/A Problem in the Sequential Design of Experiments (1956) - Bellman.pdf}
}

@book{vapnik_nature_2000,
  title = {The Nature of Statistical Learning Theory},
  author = {Vapnik, Vladimir N.},
  date = {2000},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-3264-1},
  url = {http://link.springer.com/10.1007/978-1-4757-3264-1},
  urldate = {2025-03-24},
  isbn = {978-1-4419-3160-3 978-1-4757-3264-1},
  keywords = {cognition,Conditional probability,control,learning,pattern recognition,Statistical Learning,Statistical Theory,statistics}
}

@book{berry_bandit_1985,
  title = {Bandit Problems},
  author = {Berry, Donald A. and Fristedt, Bert},
  date = {1985},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-015-3711-7},
  url = {http://link.springer.com/10.1007/978-94-015-3711-7},
  urldate = {2025-03-27},
  isbn = {978-94-015-3713-1 978-94-015-3711-7},
  langid = {english},
  keywords = {Calculation,Counting,graphs,management,Mathematica,mathematics,minimum,operations research,probability,proof,statistics,types}
}

@article{auer_finite-time_2002-1,
  title = {Finite-Time Analysis of the Multiarmed Bandit Problem},
  author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
  date = {2002-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {47},
  number = {2},
  pages = {235--256},
  issn = {1573-0565},
  doi = {10.1023/A:1013689704352},
  url = {https://doi.org/10.1023/A:1013689704352},
  urldate = {2025-03-27},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  keywords = {adaptive allocation rules,Artificial Intelligence,bandit problems,finite horizon regret},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2002/Finite-time Analysis of the Multiarmed Bandit Problem (2002) - Auer, Cesa-Bianchi, Fischer.pdf}
}

@article{witten_adaptive_1977,
  title = {An Adaptive Optimal Controller for Discrete-Time Markov Environments},
  author = {Witten, Ian H.},
  date = {1977-08-01},
  journaltitle = {Information and Control},
  shortjournal = {Information and Control},
  volume = {34},
  number = {4},
  pages = {286--295},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(77)90354-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0019995877903540},
  urldate = {2025-03-27},
  abstract = {This paper describes an adaptive controller for discrete-time stochastic environments. The controller receives the environment's current state and a reward signal which indicates the desirability of that state. In response, it selects an appropriate control action and notes its effect. The cycle repeats indefinitely. The control environments to be tackled include the well-known n-armed bandit problem, and the adaptive controller comprises an ensemble of n-armed bandit controllers, suitably interconnected. The design of these constituent elements is not discussed. It is shown that, under certain conditions, the controller's actions eventually become optimal for the particular control task with which it is faced, in the sense that they maximize the expected reward obtained in the future.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1977/An adaptive optimal controller for discrete-time Markov environments (1977) - Witten.pdf;/Users/adzcai/Zotero/storage/JC2QFXP9/S0019995877903540.html}
}

@article{barto_neuronlike_1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  date = {1983-09},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  number = {5},
  pages = {834--846},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1983.6313077},
  url = {https://ieeexplore.ieee.org/document/6313077},
  urldate = {2025-03-27},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  keywords = {Adaptive systems,Biological neural networks,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1983/Neuronlike adaptive elements that can solve difficult learning control problems (1983) - Barto, Sutton, Anderson.pdf;/Users/adzcai/Zotero/storage/8LU269S4/6313077.html}
}

@thesis{sutton_temporal_1984,
  type = {phdthesis},
  title = {Temporal Credit Assignment in Reinforcement Learning},
  author = {Sutton, Richard Stuart},
  date = {1984},
  institution = {University of Massachusetts Amherst},
  abstract = {This dissertation describes computational experiments comparing the performance of a range of reinforcement-learning algorithms. The experiments are designed to focus on aspects of the credit-assignment problem having to do with determining when the behavior that deserves credit occurred. The issues of knowledge representation involved in developing new features or refining existing ones are not addressed. The algorithms considered include some from learning automata theory, mathematical learning theory, early "cybernetic" approaches to learning, Samuel's checker-playing program, Michie and Chambers's "Boxes" system, and a number of new algorithms. The tasks were selected so as to involve, first in isolation and then in combination, the issues of misleading generalizations, delayed reinforcement, unbalanced reinforcement, and secondary reinforcement. The tasks range from simple, abstract "two-armed bandit" tasks to a physically realistic pole-balancing task. The results indicate several areas where the algorithms presented here perform substantially better than those previously studied. An unbalanced distribution of reinforcement, misleading generalizations, and delayed reinforcement can greatly retard learning and in some cases even make it counterproductive. Performance can be substantially improved in the presence of these common problems through the use of mechanisms of reinforcement comparison and secondary reinforcement. We present a new algorithm similar to the "learning-by-generalization" algorithm used for altering the static evaluation function in Samuel's checker-playing program. Simulation experiments indicate that the new algorithm performs better than a version of Samuel's algorithm suitably modified for reinforcement learning tasks. Theoretical analysis in terms of an "ideal reinforcement signal" sheds light on the relationship between these two algorithms and other temporal credit-assignment algorithms.},
  pagetotal = {223},
  annotation = {AAI8410337}
}

@article{sutton_learning_1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  author = {Sutton, Richard S.},
  date = {1988-08-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  url = {https://doi.org/10.1007/BF00115009},
  urldate = {2025-03-27},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1988/Learning to predict by the methods of temporal differences (1988) - Sutton.pdf}
}

@article{samuel_studies_1959,
  title = {Some Studies in Machine Learning Using the Game of Checkers},
  author = {Samuel, A. L.},
  date = {1959-07},
  journaltitle = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646},
  doi = {10.1147/rd.33.0210},
  url = {https://ieeexplore.ieee.org/document/5392560},
  urldate = {2025-03-27},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  eventtitle = {{{IBM Journal}} of {{Research}} and {{Development}}},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1959/Some Studies in Machine Learning Using the Game of Checkers (1959) - Samuel.pdf;/Users/adzcai/Zotero/storage/ZQR6ZSH9/5392560.html}
}

@book{klopf_brain_1972,
  title = {Brain Function and Adaptive Systems: A Heterostatic Theory},
  shorttitle = {Brain Function and Adaptive Systems},
  author = {Klopf, A. Harry},
  date = {1972},
  eprint = {rkmSPwAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Air Force Cambridge Research Laboratories, Air Force Systems Command, United States Air Force},
  langid = {english},
  pagetotal = {84},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1972/Brain Function and Adaptive Systems (1972) - Klopf.pdf}
}

@inproceedings{werbos_neural_1989,
  title = {Neural Networks for Control and System Identification},
  booktitle = {Proceedings of the 28th {{IEEE Conference}} on {{Decision}} and {{Control}},},
  author = {Werbos, P.J.},
  date = {1989-12},
  pages = {260-265 vol.1},
  doi = {10.1109/CDC.1989.70114},
  url = {https://ieeexplore.ieee.org/document/70114},
  urldate = {2025-03-27},
  abstract = {A review is presented of the field of neuroengineering as a whole, highlighting the importance of neurocontrol and neuroidentification. Then a description is given of the five major architectures in use today in neurocontrol (in robotics, in particular) and a few areas for future research. The author concludes with comments on neuroidentification.{$<>$}},
  eventtitle = {Proceedings of the 28th {{IEEE Conference}} on {{Decision}} and {{Control}},},
  keywords = {Biological neural networks,Computer architecture,Control systems,Control theory,Educational institutions,Hardware,Neural engineering,Neural networks,Optical computing,System identification},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1989/Neural networks for control and system identification (1989) - Werbos.pdf;/Users/adzcai/Zotero/storage/AQB9V93Y/70114.html}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2025-03-27},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  langid = {english},
  keywords = {Computer science},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Human-level control through deep reinforcement learning (2015) - Mnih et al.pdf}
}

@inproceedings{rummery_-line_1994,
  title = {On-Line q-Learning Using Connectionist Systems},
  author = {Rummery, Gavin Adrian and Niranjan, M.},
  date = {1994},
  url = {https://www.semanticscholar.org/paper/On-line-Q-learning-using-connectionist-systems-Rummery-Niranjan/7a09464f26e18a25a948baaa736270bfb84b5e12},
  urldate = {2025-03-27},
  abstract = {Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete (cid:12)nite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to gener-alise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of di(cid:11)erent algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Di(cid:11)erence algorithm (Sutton 1988), including a new algorithm (Modi(cid:12)ed Connectionist Q-Learning), and Q((cid:21)) (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each trial before updating can occur. On-line updating is found to be more robust to the choice of training parameters than backward replay, and also enables the algorithms to be used in continuously operating systems where no end of trial conditions occur. We compare the performance of these algorithms on a realistic robot navigation problem, where a simulated mobile robot is trained to guide itself to a goal position in the presence of obstacles. The robot must rely on limited sensory feedback from its surroundings, and make decisions that can be generalised to arbitrary layouts of obstacles. These simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q((cid:21)) are more robust than standard Q-learning updates.},
  keywords = {⛔ No DOI found}
}

@book{thorndike_animal_1911,
  title = {Animal Intelligence:  {{Experimental}} Studies},
  shorttitle = {Animal Intelligence},
  author = {Thorndike, Edward Lee},
  date = {1911},
  series = {Animal Intelligence:  {{Experimental}} Studies},
  pages = {viii, 297},
  publisher = {Macmillan Press},
  location = {Lewiston, NY, US},
  doi = {10.5962/bhl.title.55072},
  pagetotal = {viii, 297},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1911/Animal intelligence (1911) - Thorndike.pdf;/Users/adzcai/Zotero/storage/Y2AUVKSN/2003-00027-000.html}
}

@inproceedings{martens_optimizing_2015,
  title = {Optimizing Neural Networks with Kronecker-Factored Approximate Curvature},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  date = {2015-06-01},
  pages = {2408--2417},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/martens15.html},
  urldate = {2025-03-27},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network’s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC’s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Optimizing Neural Networks with Kronecker-factored Approximate Curvature (2015) - Martens, Grosse.pdf}
}

@inproceedings{maei_convergent_2009,
  title = {Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maei, Hamid and Szepesvári, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S},
  date = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2009/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html},
  urldate = {2025-03-27},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2009/Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation (2009) - Maei et al.pdf}
}

@inproceedings{haarnoja_reinforcement_2017,
  title = {Reinforcement Learning with Deep Energy-Based Policies},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  date = {2017-08-06},
  series = {{{ICML}}'17},
  pages = {1352--1361},
  publisher = {JMLR.org},
  location = {Sydney, NSW, Australia},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Reinforcement learning with deep energy-based policies (2017) - Haarnoja, Tang, Abbeel, Levine.pdf}
}

@article{wise_dopamine_2004,
  title = {Dopamine, Learning and Motivation},
  author = {Wise, Roy A.},
  date = {2004-06},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {5},
  number = {6},
  pages = {483--494},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn1406},
  url = {https://www.nature.com/articles/nrn1406},
  urldate = {2025-03-28},
  abstract = {Brain dopamine has been linked to both motor and motivational functions. Several motivational hypotheses have been challenged and found inadequate, but it remains clear that dopamine is vital for the 'stamping-in' of stimulus–reward and response–reward associations.Stimulus–reward associations are, in turn, crucial for the subsequent motivation in a previous-reward situation. Response habits are triggered by environmental stimuli that have been previously associated with reward, and the initiation of such response habits is not dependent on immediate dopamine function. If repeated with dopamine function blocked, however, the old stimulus–reward associations are extinguished and response motivation progressively weakens.While the motivational effectiveness of reward-associated stimuli does not require immediate dopamine function, phasic dopamine elevations can nonetheless amplify stimulus effectiveness. This amplification is thought to be a dopamine function in the nucleus accumbens.The role of dopamine in the stamping-in of reward associations might be much less localized. Dopamine seems to have important roles in the consolidation of memory in various structures — structures that are linked to different kinds of learning or to the learning of different things.A full appreciation of the role of dopamine in motivation must be on the basis of an understanding of not only the role of dopamine in immediate behavioural arousal, but also its role in the learning and memory of learned motivational stimuli.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences}
}

@inproceedings{munos_safe_2016,
  title = {Safe and Efficient Off-Policy Reinforcement Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Munos, Rémi and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G.},
  date = {2016-12-05},
  series = {{{NIPS}}'16},
  pages = {1054--1062},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(λ), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q* without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(λ), which was an open problem since 1989. We illustrate the benefits of Retrace(λ) on a standard suite of Atari 2600 games.},
  isbn = {978-1-5108-3881-9},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Safe and efficient off-policy reinforcement learning (2016) - Munos, Stepleton, Harutyunyan, Bellemare.pdf}
}

@article{lin_self-improving_1992,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {293--321},
  issn = {1573-0565},
  doi = {10.1007/BF00992699},
  url = {https://doi.org/10.1007/BF00992699},
  urldate = {2025-03-28},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  langid = {english},
  keywords = {Artificial Intelligence,connectionist networks,planning,Reinforcement learning,teaching},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1992/Self-improving reactive agents based on reinforcement learning, planning and teaching (1992) - Lin.pdf}
}

@article{amari_natural_1998,
  title = {Natural Gradient Works Efficiently in Learning},
  author = {Amari, Shun-Ichi},
  date = {1998-02-15},
  journaltitle = {Neural Comput.},
  volume = {10},
  number = {2},
  pages = {251--276},
  issn = {0899-7667},
  doi = {10.1162/089976698300017746},
  url = {https://doi.org/10.1162/089976698300017746},
  urldate = {2025-03-28}
}

@article{peters_natural_2008,
  title = {Natural Actor-Critic},
  author = {Peters, Jan and Schaal, Stefan},
  date = {2008-03-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  series = {Progress in {{Modeling}}, {{Theory}}, and {{Application}} of {{Computational Intelligenc}}},
  volume = {71},
  number = {7},
  pages = {1180--1190},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2007.11.026},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231208000532},
  urldate = {2025-03-28},
  abstract = {In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.},
  keywords = {Actor-Critic methods,Compatible function approximation,Natural gradients,Policy-gradient methods,Reinforcement learning,Robot learning},
  file = {/Users/adzcai/Zotero/storage/EDEKAILG/S0925231208000532.html}
}

@inproceedings{peters_natural_2005,
  title = {Natural Actor-Critic},
  booktitle = {Machine {{Learning}}: {{ECML}} 2005},
  author = {Peters, Jan and Vijayakumar, Sethu and Schaal, Stefan},
  editor = {Gama, João and Camacho, Rui and Brazdil, Pavel B. and Jorge, Alípio Mário and Torgo, Luís},
  date = {2005},
  pages = {280--291},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11564096_29},
  abstract = {This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.},
  isbn = {978-3-540-31692-3},
  langid = {english},
  keywords = {Fisher Information Matrix,Imitation Learning,Motor Primitive,Natural Gradient,Reinforcement Learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2005/Natural Actor-Critic (2005) - Peters et al.pdf}
}

@inproceedings{park_rls-based_2005,
  title = {An {{RLS-based}} Natural Actor-Critic Algorithm for Locomotion of a Two-Linked Robot Arm},
  booktitle = {Computational {{Intelligence}} and {{Security}}},
  author = {Park, Jooyoung and Kim, Jongho and Kang, Daesung},
  editor = {Hao, Yue and Liu, Jiming and Wang, Yuping and Cheung, Yiu-ming and Yin, Hujun and Jiao, Licheng and Ma, Jianfeng and Jiao, Yong-Chang},
  date = {2005},
  pages = {65--72},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11596448_9},
  abstract = {Recently, actor-critic methods have drawn much interests in the area of reinforcement learning, and several algorithms have been studied along the line of the actor-critic strategy. This paper studies an actor-critic type algorithm utilizing the RLS(recursive least-squares) method, which is one of the most efficient techniques for adaptive signal processing, together with natural policy gradient. In the actor part of the studied algorithm, we follow the strategy of performing parameter update via the natural gradient method, while in its update for the critic part, the recursive least-squares method is employed in order to make the parameter estimation for the value functions more efficient. The studied algorithm was applied to locomotion of a two-linked robot arm, and showed better performance compared to the conventional stochastic gradient ascent algorithm.},
  isbn = {978-3-540-31599-5},
  langid = {english},
  keywords = {Adaptive Signal Processing,Ascent Algorithm,Policy Distribution,Policy Gradient,Reinforcement Learning}
}

@article{beck_mirror_2003,
  title = {Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization},
  author = {Beck, Amir and Teboulle, Marc},
  date = {2003-05-01},
  journaltitle = {Operations Research Letters},
  shortjournal = {Operations Research Letters},
  volume = {31},
  number = {3},
  pages = {167--175},
  issn = {0167-6377},
  doi = {10.1016/S0167-6377(02)00231-6},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637702002316},
  urldate = {2025-03-28},
  abstract = {The mirror descent algorithm (MDA) was introduced by Nemirovsky and Yudin for solving convex optimization problems. This method exhibits an efficiency estimate that is mildly dependent in the decision variables dimension, and thus suitable for solving very large scale optimization problems. We present a new derivation and analysis of this algorithm. We show that the MDA can be viewed as a nonlinear projected-subgradient type method, derived from using a general distance-like function instead of the usual Euclidean squared distance. Within this interpretation, we derive in a simple way convergence and efficiency estimates. We then propose an Entropic mirror descent algorithm for convex minimization over the unit simplex, with a global efficiency estimate proven to be mildly dependent in the dimension of the problem.},
  keywords = {Complexity analysis,Global rate of convergence,Mirror descent algorithms,Nonlinear projections,Nonsmooth convex minimization,Projected subgradient methods,Relative entropy},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2003/Mirror descent and nonlinear projected subgradient methods for convex optimization (2003) - Beck, Teboulle.pdf;/Users/adzcai/Zotero/storage/DPEEWVP5/S0167637702002316.html}
}

@book{nemirovskij_problem_1983,
  title = {Problem Complexity and Method Efficiency in Optimization},
  author = {Nemirovskij, Arkadij S. and Judin, David B. and Dawson, E. R. and Nemirovskij, Arkadij S.},
  date = {1983},
  series = {Wiley-{{Interscience}} Series in Discrete Mathematics},
  publisher = {Wiley},
  location = {Chichester New York},
  isbn = {978-0-471-10345-5},
  langid = {english},
  pagetotal = {388},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1983/Problem complexity and method efficiency in optimization (1983) - Nemirovskij, Judin, Dawson, Nemirovskij_1.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1983/Problem complexity and method efficiency in optimization (1983) - Nemirovskij, Judin, Dawson, Nemirovskij.pdf}
}

@inproceedings{mahadevan_sparse_2012,
  title = {Sparse Q-Learning with Mirror Descent},
  booktitle = {Proceedings of the {{Twenty-Eighth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Mahadevan, Sridhar and Liu, Bo},
  date = {2012-08-14},
  series = {{{UAI}}'12},
  pages = {564--573},
  publisher = {AUAI Press},
  location = {Arlington, Virginia, USA},
  abstract = {This paper explores a new framework for reinforcement learning based on online convex optimization, in particular mirror descent and related algorithms. Mirror descent can be viewed as an enhanced gradient method, particularly suited to minimization of convex functions in high-dimensional spaces. Unlike traditional gradient methods, mirror descent undertakes gradient updates of weights in both the dual space and primal space, which are linked together using a Legendre transform. Mirror descent can be viewed as a proximal algorithm where the distance generating function used is a Bregman divergence. A new class of proximal-gradient based temporal-difference (TD) methods are presented based on different Bregman divergences, which are more powerful than regular TD learning. Examples of Bregman divergences that are studied include p-norm functions, and Mahalanobis distance based on the covariance of sample gradients. A new family of sparse mirror-descent reinforcement learning methods are proposed, which are able to find sparse fixed points of an l1-regularized Bellman equation at significantly less computational cost than previous methods based on second-order matrix methods. An experimental study of mirror-descent reinforcement learning is presented using discrete and continuous Markov decision processes.},
  isbn = {978-0-9749039-8-9}
}

@article{mahadevan_basis_2013,
  title = {Basis Adaptation for Sparse Nonlinear Reinforcement Learning},
  author = {Mahadevan, Sridhar and Giguere, Stephen and Jacek, Nicholas},
  date = {2013-06-30},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {27},
  number = {1},
  pages = {654--660},
  issn = {2374-3468},
  doi = {10.1609/aaai.v27i1.8665},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/8665},
  urldate = {2025-03-28},
  abstract = {This paper presents a new approach to representation discovery in reinforcement learning (RL) using basis adaptation. We introduce a general framework for basis adaptation as \{\textbackslash em nonlinear separable least-squares value function approximation\} based on finding Frechet gradients of an error function using variable projection functionals.  We then present a scalable proximal gradient-based approach for basis adaptation  using the recently proposed mirror-descent framework for RL.  Unlike traditional temporal-difference (TD) methods for RL, mirror descent based RL methods undertake proximal gradient updates of weights in a dual space, which is linked together with the primal  space using a Legendre transform involving the gradient of a strongly convex function. Mirror descent RL can be viewed as a proximal TD algorithm using Bregman divergence as the distance generating function.  We present a new class of regularized proximal-gradient based TD methods, which combine feature selection through sparse L1 regularization and basis adaptation. Experimental results are provided to illustrate and validate the approach.},
  issue = {1},
  langid = {english},
  keywords = {basis adaptation,mirror descent,Reinforcement learning},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/Basis Adaptation for Sparse Nonlinear Reinforcement Learning (2013) - Mahadevan, Giguere, Jacek.pdf}
}

@inproceedings{sutton_policy_1999,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
  date = {1999-11-29},
  series = {{{NIPS}}'99},
  pages = {1057--1063},
  publisher = {MIT Press},
  location = {Cambridge, MA, USA},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1999/Policy gradient methods for reinforcement learning with function approximation (1999) - Sutton, McAllester, Singh, Mansour.pdf}
}

@article{bhatnagar_natural_2009,
  title = {Natural Actor–Critic Algorithms},
  author = {Bhatnagar, Shalabh and Sutton, Richard S. and Ghavamzadeh, Mohammad and Lee, Mark},
  date = {2009-11-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {45},
  number = {11},
  pages = {2471--2482},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2009.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0005109809003549},
  urldate = {2025-03-28},
  abstract = {We present four new reinforcement learning algorithms based on actor–critic, natural-gradient and function-approximation ideas, and we provide their convergence proofs. Actor–critic reinforcement learning methods are online approximations to policy iteration in which the value-function parameters are estimated using temporal difference learning and the policy parameters are updated by stochastic gradient descent. Methods based on policy gradients in this way are of special interest because of their compatibility with function-approximation methods, which are needed to handle large or infinite state spaces. The use of temporal difference learning in this way is of special interest because in many applications it dramatically reduces the variance of the gradient estimates. The use of the natural gradient is of interest because it can produce better conditioned parameterizations and has been shown to further reduce variance in some cases. Our results extend prior two-timescale convergence results for actor–critic methods by Konda and Tsitsiklis by using temporal difference learning in the actor and by incorporating natural gradients. Our results extend prior empirical studies of natural actor–critic methods by Peters, Vijayakumar and Schaal by providing the first convergence proofs and the first fully incremental algorithms.},
  keywords = {Actor–critic reinforcement learning algorithms,Approximate dynamic programming,Function approximation,Natural gradient,Policy-gradient methods,Temporal difference learning,Two-timescale stochastic approximation},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2009/Natural actor–critic algorithms (2009) - Bhatnagar, Sutton, Ghavamzadeh, Lee.pdf;/Users/adzcai/Zotero/storage/LYD47RRG/S0005109809003549.html}
}

@article{marbach_simulation-based_2001,
  title = {Simulation-Based Optimization of {{Markov}} Reward Processes},
  author = {Marbach, P. and Tsitsiklis, J.N.},
  date = {2001-02},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {46},
  number = {2},
  pages = {191--209},
  issn = {1558-2523},
  doi = {10.1109/9.905687},
  url = {https://ieeexplore.ieee.org/document/905687},
  urldate = {2025-03-28},
  abstract = {This paper proposes a simulation-based algorithm for optimizing the average reward in a finite-state Markov reward process that depends on a set of parameters. As a special case, the method applies to Markov decision processes where optimization takes place within a parametrized set of policies. The algorithm relies on the regenerative structure of finite-state Markov processes, involves the simulation of a single sample path, and can be implemented online. A convergence result (with probability 1) is provided.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Computational modeling,Convergence,Decision making,Dynamic programming,Laboratories,Markov processes,Optimization methods,State-space methods,Stochastic processes,Uncertainty},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2001/Simulation-based optimization of Markov reward processes (2001) - Marbach, Tsitsiklis.pdf;/Users/adzcai/Zotero/storage/XWPNQCRG/905687.html}
}
