@online{baydin_automatic_2018,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  date = {2018-02-05},
  eprint = {1502.05767},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1502.05767},
  url = {http://arxiv.org/abs/1502.05767},
  urldate = {2024-11-11},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Automatic differentiation in machine learning (2018) - Baydin, Pearlmutter, Radul, Siskind.pdf}
}

@inproceedings{lechner_gigastep_2023,
  title = {Gigastep - {{One Billion Steps}} per {{Second Multi-agent Reinforcement Learning}}},
  author = {Lechner, Mathias and Yin, Lianhao and Seyde, Tim and Wang, Tsun-Hsuan and Xiao, Wei and Hasani, Ramin and Rountree, Joshua and Rus, Daniela},
  date = {2023-11-02},
  url = {https://openreview.net/forum?id=UgPAaEugH3},
  urldate = {2023-12-12},
  abstract = {Multi-agent reinforcement learning (MARL) research is faced with a trade-off: it either uses complex environments requiring large compute resources, which makes it inaccessible to researchers with limited resources, or relies on simpler dynamics for faster execution, which makes the transferability of the results to more realistic tasks challenging. Motivated by these challenges, we present Gigastep, a fully vectorizable, MARL environment implemented in JAX, capable of executing up to one billion environment steps per second on consumer-grade hardware. Its design allows for comprehensive MARL experimentation, including a complex, high-dimensional space defined by 3D dynamics, stochasticity, and partial observations. Gigastep supports both collaborative and adversarial tasks, continuous and discrete action spaces, and provides RGB image and feature vector observations, allowing the evaluation of a wide range of MARL algorithms. We validate Gigastep's usability through an extensive set of experiments, underscoring its role in widening participation and promoting inclusivity in the MARL research community.},
  eventtitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Gigastep - One Billion Steps per Second Multi-agent Reinforcement Learning (2023) - Lechner et al.pdf}
}

@online{hausknecht_deep_2017,
  title = {Deep {{Recurrent Q-Learning}} for {{Partially Observable MDPs}}},
  author = {Hausknecht, Matthew and Stone, Peter},
  date = {2017-01-11},
  eprint = {1507.06527},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.06527},
  url = {http://arxiv.org/abs/1507.06527},
  urldate = {2023-06-04},
  abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textbackslash textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  pubstate = {prepublished},
  annotation = {1274 citations (Semantic Scholar/arXiv) [2023-06-04]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Deep Recurrent Q-Learning for Partially Observable MDPs (2017) - Hausknecht, Stone.pdf}
}

@software{babuschkin_deepmind_2020,
  title = {The {{DeepMind JAX Ecosystem}}},
  author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  date = {2020},
  url = {http://github.com/deepmind},
  keywords = {nosource}
}

@book{boyd_convex_2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004},
  publisher = {Cambridge University Press},
  url = {https://web.stanford.edu/~boyd/cvxbook/},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2004/Convex Optimization (2004) - Boyd, Vandenberghe 3.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2004/Convex Optimization (2004) - Boyd, Vandenberghe.pdf}
}

@book{nocedal_numerical_2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  date = {2006},
  series = {Springer Series in Operations Research},
  edition = {2nd ed},
  publisher = {Springer},
  location = {New York},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {664},
  annotation = {OCLC: ocm68629100},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2006/Numerical optimization (2006) - Nocedal, Wright.pdf}
}

@book{vershynin_high-dimensional_2018,
  title = {High-{{Dimensional Probability}}: {{An Introduction}} with {{Applications}} in {{Data Science}}},
  shorttitle = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  date = {2018-09-27},
  eprint = {NDdqDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Cambridge University Press},
  abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
  isbn = {978-1-108-41519-4},
  langid = {english},
  pagetotal = {299},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/High-Dimensional Probability (2018) - Vershynin.pdf}
}

@online{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2022-09-05},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Proximal Policy Optimization Algorithms (2017) - Schulman et al.pdf;/Users/adzcai/Zotero/storage/NCI3EDQS/1707.html}
}

@article{williams_simple_1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  url = {https://doi.org/10.1007/BF00992696},
  urldate = {2022-09-03},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1992/Simple statistical gradient-following algorithms for connectionist reinforcement learning (1992) - Williams.pdf}
}

@online{islam_reproducibility_2017,
  title = {Reproducibility of {{Benchmarked Deep Reinforcement Learning Tasks}} for {{Continuous Control}}},
  author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
  date = {2017-08-10},
  eprint = {1708.04133},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.04133},
  urldate = {2022-09-01},
  abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control (2017) - Islam, Henderson, Gomrokchi, Precup.pdf;/Users/adzcai/Zotero/storage/DESTVEZK/1708.html}
}

@article{schrittwieser_mastering_2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12},
  journaltitle = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {https://www.nature.com/articles/s41586-020-03051-4},
  urldate = {2022-08-15},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
  issue = {7839},
  langid = {english},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2020/Mastering Atari, Go, chess and shogi by planning with a learned model (2020) - Schrittwieser et al.pdf;/Users/adzcai/Zotero/storage/KLW59Z2L/s41586-020-03051-4.html}
}

@article{silver_general_2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  date = {2018-12-07},
  journaltitle = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar6404},
  url = {https://www.science.org/doi/10.1126/science.aar6404},
  urldate = {2022-08-15},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play (2018) - Silver et al.pdf}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2022-08-15},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  issue = {7676},
  langid = {english},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Mastering the game of Go without human knowledge (2017) - Silver et al.pdf;/Users/adzcai/Zotero/storage/6KP4UDZN/nature24270.html}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  url = {https://www.nature.com/articles/nature16961},
  urldate = {2022-05-08},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  issue = {7587},
  langid = {english},
  keywords = {broken},
  annotation = {9998 citations (Semantic Scholar/DOI) [2022-05-18]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2016/Mastering the game of Go with deep neural networks and tree search (2016) - Silver et al.pdf;/Users/adzcai/Zotero/storage/STY6YT82/nature16961.html}
}

@article{degrave_magnetic_2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and family=Casas, given=Diego, prefix=de las, useprefix=true and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  date = {2022-02},
  journaltitle = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  url = {https://www.nature.com/articles/s41586-021-04301-9},
  urldate = {2023-05-21},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  issue = {7897},
  langid = {english},
  annotation = {230 citations (Semantic Scholar/DOI) [2023-05-21]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Magnetic control of tokamak plasmas through deep reinforcement learning (2022) - Degrave et al.pdf}
}

@inproceedings{silver_deterministic_2014,
  title = {Deterministic Policy Gradient Algorithms},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 32},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  date = {2014-06-21},
  series = {{{ICML}}'14},
  pages = {I-387--I-395},
  publisher = {JMLR.org},
  location = {Beijing, China},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2014/Deterministic policy gradient algorithms (2014) - Silver et al.pdf}
}

@book{sussman_functional_2013,
  title = {Functional Differential Geometry},
  author = {Sussman, Gerald Jay and Wisdom, Jack and Farr, Will},
  date = {2013},
  publisher = {The MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-01934-7},
  pagetotal = {228},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/Functional differential geometry (2013) - Sussman, Wisdom, Farr.pdf}
}

@book{hastie_elements_2013,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2013-11-11},
  eprint = {yPfZBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Springer Science \& Business Media},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  isbn = {978-0-387-21606-5},
  langid = {english},
  pagetotal = {545},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/The Elements of Statistical Learning (2013) - Hastie, Tibshirani, Friedman.pdf}
}

@online{adaptive_agent_team_human-timescale_2023,
  title = {Human-{{Timescale Adaptation}} in an {{Open-Ended Task Space}}},
  author = {Adaptive Agent Team and Bauer, Jakob and Baumli, Kate and Baveja, Satinder and Behbahani, Feryal and Bhoopchand, Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves, Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander and Zhang, Lei},
  date = {2023-01-18},
  eprint = {2301.07608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.07608},
  urldate = {2023-02-21},
  abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.},
  pubstate = {prepublished},
  annotation = {1 citations (Semantic Scholar/arXiv) [2023-02-20]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/Human-Timescale Adaptation in an Open-Ended Task Space (2023) - Adaptive Agent Team et al.pdf}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  url = {http://incompleteideas.net/book/RLbook2020trimmed.pdf},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Reinforcement learning (2018) - Sutton, Barto.pdf}
}

@book{kochenderfer_algorithms_2022,
  title = {Algorithms for {{Decision Making}}},
  author = {Kochenderfer, Mykel J and Wheeler, Tim A and Wray, Kyle H},
  date = {2022-08-16},
  url = {https://mitpress.mit.edu/9780262047012/algorithms-for-decision-making/},
  urldate = {2022-10-23},
  abstract = {A broad introduction to algorithms for decision making under uncertainty, introducing the underlying mathematical problem formulations and the algorithms for...},
  isbn = {978-0-262-04701-2},
  langid = {american},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Algorithms for Decision Making (2022) - Kochenderfer, Wheeler, Wray.pdf}
}

@inproceedings{schulman_high-dimensional_2018,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  booktitle = {{{ICLR}} 2016},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  date = {2018-10-20},
  eprint = {1506.02438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02438},
  urldate = {2023-06-21},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  pubstate = {preprint},
  keywords = {broken},
  annotation = {2253 citations (Semantic Scholar/arXiv) [2023-07-22]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/High-Dimensional Continuous Control Using Generalized Advantage Estimation (2018) - Schulman et al2.pdf}
}

@inproceedings{kakade_approximately_2002,
  title = {Approximately {{Optimal Approximate Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Machine Learning}}},
  author = {Kakade, Sham and Langford, John},
  date = {2002-07-08},
  series = {{{ICML}} '02},
  pages = {267--274},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  isbn = {978-1-55860-873-3},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2002/Approximately Optimal Approximate Reinforcement Learning (2002) - Kakade, Langford 3.pdf}
}

@software{bradbury_jax_2018,
  title = {{{JAX}}: {{Composable}} Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and Zhang, Qiao},
  date = {2018},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  keywords = {nosource}
}

@inproceedings{freeman_brax_2021,
  title = {Brax – {{A Differentiable Physics Engine}} for {{Large Scale Rigid Body Simulation}}},
  booktitle = {{{NeurIPS Datasets}} and {{Benchmarks}} 2021},
  author = {Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  date = {2021-06-24},
  eprint = {2106.13281},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.13281},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract-round1.html},
  urldate = {2023-06-26},
  abstract = {We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.},
  eventtitle = {{{NeurIPS Datasets}} and {{Benchmarks}}},
  pubstate = {preprint | DBLP: https://dblp.org/rec/conf/nips/FreemanFRGMB21},
  annotation = {151 citations (Semantic Scholar/arXiv) [2023-07-22]},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Brax – A Differentiable Physics Engine for Large Scale Rigid Body Simulation (2021) - Freeman et al.pdf}
}

@article{lai_asymptotically_1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  date = {1985-03-01},
  journaltitle = {Advances in Applied Mathematics},
  shortjournal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
  urldate = {2023-10-23},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1985/Asymptotically efficient adaptive allocation rules (1985) - Lai, Robbins.pdf}
}

@book{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  date = {2015},
  publisher = {Determination Press},
  url = {http://neuralnetworksanddeeplearning.com/},
  urldate = {2024-03-10},
  keywords = {nosource}
}

@book{james_introduction_2023,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{Python}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan},
  date = {2023},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-031-38747-0},
  url = {https://link.springer.com/10.1007/978-3-031-38747-0},
  urldate = {2024-02-07},
  isbn = {978-3-031-38746-3 978-3-031-38747-0},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2023/An Introduction to Statistical Learning (2023) - James et al.pdf}
}

@book{russell_artificial_2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2021},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition},
  publisher = {Pearson},
  location = {Hoboken},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  isbn = {978-0-13-461099-3},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2021/Artificial intelligence (2021) - Russell, Norvig.pdf}
}

@book{heath_scientific_2018,
  title = {Scientific Computing: An Introductory Survey},
  shorttitle = {Scientific Computing},
  author = {Heath, Michael T.},
  date = {2018},
  series = {Classics in Applied Mathematics},
  edition = {Revised second edition, SIAM edition},
  number = {80},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {Philadelphia},
  isbn = {978-1-61197-557-4},
  pagetotal = {567},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2018/Scientific computing (2018) - Heath 2.pdf}
}

@inproceedings{ross_reduction_2010,
  title = {A {{Reduction}} of {{Imitation Learning}} and {{Structured Prediction}} to {{No-Regret Online Learning}}},
  author = {Ross, Stéphane and Gordon, Geoffrey J. and Bagnell, J.},
  date = {2010-11-02},
  url = {https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e},
  urldate = {2024-08-08},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2010/A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (2010) - Ross, Gordon, Bagnell.pdf}
}

@article{barto_neuronlike_1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  date = {1983-09},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  number = {5},
  pages = {834--846},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1983.6313077},
  url = {https://ieeexplore.ieee.org/document/6313077},
  urldate = {2024-07-01},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  keywords = {broken},
  file = {/Users/adzcai/Zotero/storage/GHD9WZXL/6313077.html}
}

@online{zhang_deep_2015,
  title = {Deep Learning with {{Elastic Averaging SGD}}},
  author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
  date = {2015-10-25},
  eprint = {1412.6651},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1412.6651},
  url = {http://arxiv.org/abs/1412.6651},
  urldate = {2024-07-01},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Deep learning with Elastic Averaging SGD (2015) - Zhang, Choromanska, LeCun.pdf;/Users/adzcai/Zotero/storage/M4LFKVWK/1412.html}
}

@inreference{achiam_spinning_2018,
  title = {Spinning {{Up}} in {{Deep Reinforcement Learning}}},
  author = {Achiam, Joshua},
  date = {2018},
  url = {https://spinningup.openai.com/en/latest/index.html},
  urldate = {2024-07-01},
  keywords = {broken},
  file = {/Users/adzcai/Zotero/storage/UPUMW6XV/index.html}
}

@online{zhang_adaptable_2024,
  title = {Adaptable {{Logical Control}} for {{Large Language Models}}},
  author = {Zhang, Honghua and Kung, Po-Nien and Yoshida, Masahiro and family=Broeck, given=Guy Van, prefix=den, useprefix=false and Peng, Nanyun},
  date = {2024-06-19},
  eprint = {2406.13892},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.13892},
  url = {http://arxiv.org/abs/2406.13892},
  urldate = {2024-07-01},
  abstract = {Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, an adaptable framework that facilitates tractable and flexible control of LLM generation to reliably follow logical constraints. Ctrl-G combines any production-ready LLM with a Hidden Markov Model, enabling LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when applied to a TULU2-7B model, outperforms GPT3.5 and GPT4 on the task of interactive text editing: specifically, for the task of generating text insertions/continuations following logical constraints, Ctrl-G achieves over 30\% higher satisfaction rate in human evaluation compared to GPT4. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts for constrained generation by large margins on standard benchmarks. Additionally, as a proof-of-concept study, we experiment Ctrl-G on the Grade School Math benchmark to assist LLM reasoning, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Adaptable Logical Control for Large Language Models (2024) - Zhang et al.pdf;/Users/adzcai/Zotero/storage/38W8T74Y/2406.html}
}

@online{zhai_fine-tuning_2024,
  title = {Fine-{{Tuning Large Vision-Language Models}} as {{Decision-Making Agents}} via {{Reinforcement Learning}}},
  author = {Zhai, Yuexiang and Bai, Hao and Lin, Zipeng and Pan, Jiayi and Tong, Shengbang and Zhou, Yifei and Suhr, Alane and Xie, Saining and LeCun, Yann and Ma, Yi and Levine, Sergey},
  date = {2024-05-16},
  eprint = {2405.10292},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.10292},
  url = {http://arxiv.org/abs/2405.10292},
  urldate = {2024-07-01},
  abstract = {Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning (2024) - Zhai et al.pdf;/Users/adzcai/Zotero/storage/2X2WJU4D/2405.html}
}

@online{welleck_decoding_2024,
  title = {From {{Decoding}} to {{Meta-Generation}}: {{Inference-time Algorithms}} for {{Large Language Models}}},
  shorttitle = {From {{Decoding}} to {{Meta-Generation}}},
  author = {Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  date = {2024-06-24},
  eprint = {2406.16838},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.16838},
  url = {http://arxiv.org/abs/2406.16838},
  urldate = {2024-07-01},
  abstract = {One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/From Decoding to Meta-Generation (2024) - Welleck et al.pdf;/Users/adzcai/Zotero/storage/S4Y984R4/2406.html}
}

@online{sun_easy--hard_2024,
  title = {Easy-to-{{Hard Generalization}}: {{Scalable Alignment Beyond Human Supervision}}},
  shorttitle = {Easy-to-{{Hard Generalization}}},
  author = {Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang},
  date = {2024-03-14},
  eprint = {2403.09472},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.09472},
  url = {http://arxiv.org/abs/2403.09472},
  urldate = {2024-07-01},
  abstract = {Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textbackslash textit\{easy-to-hard generalization\}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \textbackslash textit\{easy-to-hard generalization from evaluators\} can enable \textbackslash textit\{easy-to-hard generalizations in generators\} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0\textbackslash\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.},
  pubstate = {prepublished},
  keywords = {broken},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Easy-to-Hard Generalization (2024) - Sun et al.pdf;/Users/adzcai/Zotero/storage/J52D59AK/2403.html}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
  date = {2013},
  journaltitle = {CoRR},
  volume = {abs/1312.5602},
  eprint = {1312.5602},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1312.5602},
  urldate = {2024-06-21},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2013/Playing Atari with Deep Reinforcement Learning (2013) - Mnih et al.pdf}
}

@book{agarwal_reinforcement_2022,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {{{AJKS}}},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  date = {2022-01-31},
  url = {https://rltheorybook.github.io/rltheorybook_AJKS.pdf},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2022/Reinforcement Learning (2022) - Agarwal, Jiang, Kakade, Sun.pdf}
}

@inproceedings{azar_minimax_2017,
  title = {Minimax {{Regret Bounds}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, Rémi},
  date = {2017-07-17},
  pages = {263--272},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/azar17a.html},
  urldate = {2024-06-21},
  abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \$\textbackslash tilde \{O\}( \textbackslash sqrt\{HSAT\} + H\textasciicircum 2S\textasciicircum 2A+H\textbackslash sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \$\textbackslash tilde \{O\}(HS \textbackslash sqrt\{AT\})\$ achieved by the UCRL2 algorithm. The key significance of our new results is that when \$T\textbackslash geq H\textasciicircum 3S\textasciicircum 3A\$ and \$SA\textbackslash geq H\$, it leads to a regret of \$\textbackslash tilde\{O\}(\textbackslash sqrt\{HSAT\})\$ that matches the established lower bound of \$\textbackslash Omega(\textbackslash sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based “exploration bonuses” that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/Minimax Regret Bounds for Reinforcement Learning (2017) - Azar, Osband, Munos.pdf}
}

@inproceedings{kakade_natural_2001,
  title = {A {{Natural Policy Gradient}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kakade, Sham M},
  date = {2001},
  volume = {14},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
  urldate = {2024-06-20},
  abstract = {We provide a  natural gradient method that represents the steepest  descent  direction based on the underlying structure of the param(cid:173) eter space.  Although gradient methods cannot make large changes  in  the  values  of the  parameters,  we  show  that  the  natural  gradi(cid:173) ent is moving toward choosing a greedy optimal action rather than  just a  better action.  These greedy optimal  actions  are those  that  would  be  chosen  under  one  improvement  step  of  policy  iteration  with  approximate,  compatible  value  functions,  as  defined  by  Sut(cid:173) ton  et al.  [9].  We  then show drastic performance improvements in  simple MDPs and in the more challenging MDP of Tetris.},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2001/A Natural Policy Gradient (2001) - Kakade.pdf}
}

@article{deng_mnist_2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  url = {https://ieeexplore.ieee.org/document/6296535},
  urldate = {2025-01-02},
  abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  file = {/Users/adzcai/Zotero/storage/394GTFDU/6296535.html}
}

@software{allaire_quarto_2024,
  title = {Quarto},
  author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
  date = {2024-02},
  doi = {10.5281/zenodo.5960048},
  url = {https://github.com/quarto-dev/quarto-cli},
  version = {1.4},
  keywords = {nosource}
}

@artwork{gpa_photo_archive_robotic_2017,
  title = {Robotic Arm},
  author = {{GPA Photo Archive}},
  date = {2017-07-25},
  url = {https://www.flickr.com/photos/iip-photo-archive/36123310136/},
  urldate = {2025-01-05},
  abstract = {NASA's Marshall Space Flight Center Follow Robotics Work on Space Station Set Up Thursday Spacewalk  The Pressurized Mating Adapter-3 (PMA-3) is in the grip of the International Space Station's Canadarm2 robotic arm during its relocation and attachment to the station's Harmony module on March 26, 2017. The adapter was prepared for this move on Friday, March 24, during a successful spacewalk by Expedition 50 Commander Shane Kimbrough of NASA and Flight Engineer Thomas Pesquet of ESA (European Space Agency). The second spacewalk by Kimbrough and Flight Engineer Peggy Whitson of NASA, which began at 7:29 a.m. on Thursday, March 30, will finalize the PMA-3 cable connections on Harmony.  Photo credit: NASA {$<$}a href="http://www.flickr.com/photos/iip-photo-archive/36123310136/in/photolist-V4sDRQ-mJHgiM-TbVeu4-X36pA1-iD8YmZ-VaWXAB-nUZYbb-V4sEaL-Raxdtc-eX1zwL-amFsJE-8cThYi-69LKFA-T41G6A-T96Eed-a1PnMo-RboyLE-Sc8vEH-TtbedA-kPmFzk-Su5Rw2-ogj9rw-2cyWHU-qK5viN-pvQ1ww-at2134-7w1KCT-qU8mkJ-RxeF38-oSRtfb-ohhGHv-pZMmx5-pw4vBi-qRURyA-6iQLJU-m3Asiq-7PMF1B-8UUSxz-SxtYZo-9KKHbV-C1vqkF-nUZWEq-U5h3JP-ocqj1w-pGAfL3-69LKCb-SqEefQ-qBKEeg-V7eaa8-ViUXY8"{$>$}www.flickr.com/photos/iip-photo-archive/36123310136/in/ph...{$<$}/a{$>$}},
  file = {/Users/adzcai/Zotero/storage/2TVTE9D9/Robotic arm (2017) - Archive.jpg}
}

@artwork{frans_berkelaar_container_2009,
  title = {Container Ship {{MSC Davos}} - {{Westerschelde}} - {{Zeeland}}},
  author = {{Frans Berkelaar}},
  date = {2009-05-29},
  url = {https://www.flickr.com/photos/28169156@N03/52957948820/},
  urldate = {2025-01-05},
  abstract = {OLYMPUS DIGITAL CAMERA},
  file = {/Users/adzcai/Zotero/storage/VLKCH5QJ/Container ship MSC Davos - Westerschelde - Zeeland (2009) - Frans Berkelaar.jpg}
}

@artwork{guy_chess_2006,
  title = {Chess},
  author = {Guy, Romain},
  date = {2006-08-04},
  url = {https://www.flickr.com/photos/romainguy/230416692/},
  urldate = {2025-01-05},
  abstract = {We played some chess while waiting for our room to be ready when we got at our hotel in Hangzhou.},
  file = {/Users/adzcai/Zotero/storage/WEYSCFYL/Chess (2006) - Guy.jpg}
}

@article{white_survey_1993,
  title = {A {{Survey}} of {{Applications}} of {{Markov Decision Processes}}},
  author = {White, D. J.},
  date = {1993},
  journaltitle = {The Journal of the Operational Research Society},
  volume = {44},
  number = {11},
  eprint = {2583870},
  eprinttype = {jstor},
  pages = {1073--1096},
  publisher = {Palgrave Macmillan Journals},
  issn = {0160-5682},
  doi = {10.2307/2583870},
  url = {https://www.jstor.org/stable/2583870},
  urldate = {2025-01-05},
  abstract = {A collection of papers on the application of Markov decision processes is surveyed and classified according to the use of real life data, structural results and special computational schemes. Observations are made about various features of the applications.},
  keywords = {nosource}
}

@article{meurer_sympy_2017,
  title = {{{SymPy}}: Symbolic Computing in {{Python}}},
  shorttitle = {{{SymPy}}},
  author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and Čertík, Ondřej and Kirpichev, Sergey B. and Rocklin, Matthew and family=Kumar, given=AmiT, given-i={{Am}} and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Roučka, Štěpán and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
  date = {2017-01-02},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {3},
  pages = {e103},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.103},
  url = {https://peerj.com/articles/cs-103},
  urldate = {2025-02-06},
  abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2017/SymPy (2017) - Meurer et al.pdf}
}

@inproceedings{ansel_pytorch_2024,
  title = {{{PyTorch}} 2: {{Faster}} Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle = {29th {{ACM}} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 ({{ASPLOS}} '24)},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and family=Luk, given=CK, given-i=CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  date = {2024-04},
  publisher = {ACM},
  doi = {10.1145/3620665.3640366},
  url = {https://pytorch.org/assets/pytorch2-2.pdf},
  keywords = {nosource}
}

@article{carvalho_predictive_2024,
  title = {Predictive {{Representations}}: {{Building Blocks}} of {{Intelligence}}},
  shorttitle = {Predictive {{Representations}}},
  author = {Carvalho, Wilka and Tomov, Momchil S. and family=Cothi, given=William, prefix=de, useprefix=true and Barry, Caswell and Gershman, Samuel J.},
  date = {2024-10-11},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {36},
  number = {11},
  pages = {2225--2298},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01705},
  url = {https://doi.org/10.1162/neco_a_01705},
  urldate = {2025-02-07},
  abstract = {Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This review integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation and its generalizations, which have been widely applied as both engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Predictive Representations (2024) - Carvalho et al.pdf;/Users/adzcai/Zotero/storage/KH9JNQTF/Predictive-Representations-Building-Blocks-of.html}
}

@online{deepseek-ai_deepseek-r1_2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  date = {2025-01-22},
  eprint = {2501.12948},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.12948},
  url = {http://arxiv.org/abs/2501.12948},
  urldate = {2025-02-07},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2025/DeepSeek-R1 (2025) - DeepSeek-AI et al.pdf;/Users/adzcai/Zotero/storage/3MYSD8TV/2501.html}
}

@online{farebrother_stop_2024,
  title = {Stop {{Regressing}}: {{Training Value Functions}} via {{Classification}} for {{Scalable Deep RL}}},
  shorttitle = {Stop {{Regressing}}},
  author = {Farebrother, Jesse and Orbay, Jordi and Vuong, Quan and Taïga, Adrien Ali and Chebotar, Yevgen and Xiao, Ted and Irpan, Alex and Levine, Sergey and Castro, Pablo Samuel and Faust, Aleksandra and Kumar, Aviral and Agarwal, Rishabh},
  date = {2024-03-06},
  eprint = {2403.03950},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.03950},
  url = {http://arxiv.org/abs/2403.03950},
  urldate = {2025-02-07},
  abstract = {Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.},
  pubstate = {prepublished},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2024/Stop Regressing (2024) - Farebrother et al.pdf;/Users/adzcai/Zotero/storage/ZLQNN42D/2403.html}
}

@article{maxwell_governors_1867,
  title = {On {{Governors}}},
  author = {Maxwell, J. Clerk},
  date = {1867},
  journaltitle = {Proceedings of the Royal Society of London},
  volume = {16},
  eprint = {112510},
  eprinttype = {jstor},
  pages = {270--283},
  publisher = {Royal Society},
  issn = {0370-1662},
  url = {https://www.jstor.org/stable/112510},
  urldate = {2025-03-08},
  keywords = {⛔ No DOI found},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/1867/On Governors (1867) - Maxwell.pdf}
}

@inproceedings{schulman_trust_2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  date = {2015-06-01},
  pages = {1889--1897},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/schulman15.html},
  urldate = {2025-03-09},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Trust Region Policy Optimization (2015) - Schulman et al_Supplementary.pdf;/Users/adzcai/Library/Mobile Documents/com~apple~CloudDocs/assets/2015/Trust Region Policy Optimization (2015) - Schulman et al.pdf}
}
