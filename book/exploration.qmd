{{< include _macros.tex >}}

# Exploration in MDPs {#sec-exploration}

## Introduction

One of the key challenges of reinforcement learning is the *exploration-exploitation tradeoff*.
Should we *exploit* actions we know will give high reward,
or should we *explore* different actions to discover potentially better strategies?
An algorithm that doesn't explore effectively might easily *overfit* to certain areas of the state space,
and fail to generalize once they enter a region they haven't yet seen.

In the multi-armed bandit setting (@sec-bandits),
we studied the upper confidence bound (UCB) algorithm (@sec-ucb)
that incentivizes the learner to explore arms that it is uncertain about.
In particular, UCB relies on **optimism in the face of uncertainty**:
it chooses arms based on an _overestimate_ of the arm's true mean reward.
In this chapter, we will see how to generalize this idea to the MDP setting.

```{python}
from utils import latex
```



### Sparse reward {#sec-sparse-reward}

Exploration is crucial in **sparse reward** problems
where $r(s, a) = 0$ for most (or nearly all) states and actions.
Often, the agent must take a specific sequence of actions
before any reward is observed.

:::: {#exm-sparse-reward-mdp}
#### Chain MDP

Here's a simple example of an MDP with sparse rewards:

![An illustration of the chain MDP environment.](shared/sparse_reward_mdp.png)

There are $|\mathcal{S}|$ cells arranged in a chain.
The agent starts in the leftmost cell.
The rightmost state is a terminal state.
In every state, there are three possible actions,
two of which move the agent left and one which moves the agent right.
(The two "left" actions do nothing in the leftmost cell.)
The reward function gives a reward of $1$
for taking the action that enters the rightmost cell,
and zero otherwise.
::::

The problem of sparse rewards is especially prevalent in RL
compared to supervised learning.
In most supervised learning tasks,
every labelled sample provides some useful signal.
However, in RL,
algorithms that don't *systematically* explore new states
may fail to learn anything meaningful within a reasonable amount of time.

Consider the algorithms we've covered so far for unknown environments:
policy gradient methods (@sec-pg)
and fitted DP methods (@sec-fit).
How would these do on this problem?

::: {#rem-sparse-rl}
#### Policy gradient methods fail on sparse reward

Policy gradient algorithms require the gradient to be *nonzero* in order to learn.
If we never observe any reward,
the gradient will always be zero,
and the policy will never change or improve.
If we think of the expected total reward as a function $J(\theta)$ of the policy parameters,
we can visualize the _graph_ of $J$ as being mostly flat,
making it impossible to "climb the hill"
from almost every random initialization.
:::

::: {#rem-fitted-dp-sparse}
#### Fitted DP methods fail on sparse reward

Fitted DP algorithms run into a similar issue:
as we randomly interact with the environment,
we never observe any reward,
and so the reward model simply gives zero for every state-action pair.
In expectation,
it would take a computationally infeasible number of rollouts
to observe the reward by chance.
:::

This is quite disheartening!
The sophisticated methods we've developed,
which can exceed human-level performance on a wide variety of tasks,
fail on this problem that seems almost trivial.

Of course,
a simple way to solve the "chain MDP" in @exm-sparse-reward-mdp
is to actively visit unseen states.
For a policy that visits a new state in each rollout,
the final cell can be reached in $O(|\mathcal{S}|)$ rollouts
(i.e. $O(|\mathcal{S}|^2)$ time).
The rest of this chapter will consider ways
to _explicitly_ explore unknown states.


### Reward shaping

One workaround to sparse reward problems in practice
is to _shape_ the reward function using domain knowledge.
For example, in @exm-sparse-reward-mdp,
we (that is, the practitioners)
know that travelling to the right is the correct action,
so we could design a reward function that provides a reward of $0.1$
for the action that moves to the right.
A similar strategy is used in practice for many chess or board game algorithms
where capturing the opponent's pieces earns some positive reward.

Though this might seem obvious,
designing a useful reward function can be challenging in practice.
The agent may learn to exploit the intermediate rewards
rather than solve the original goal.
A famous example is the agent trained to play the CoastRunners game,
in which players race boats around a racetrack.
However, the algorithm found that it could achieve higher reward
(i.e. in-game score)
by refusing to go around the racetrack
and instead collecting bonus points in a loop!

![An RL agent collects bonus points instead of participating in the race. Image from @clark_faulty_2024.](./shared/faulty-reward-functions.jpg.webp)




## Exploration in deterministic MDPs {#sec-explore-det-mdp}

Let us address the exploration problem in a *deterministic* MDP,
that is, where taking action $a$ in state $s$ always leads to the state $P(s, a) \in \mathcal{S}$.
How can we methodically visit every single state-action pair?

In the multi-armed bandit setting (@sec-bandits),
there are no states,
so it's trivial to visit every "state-action pair":
just pull each arm once.
But in the MDP setting,
in order to achieve a particular state-action pair $(s, a)$,
one must plan out a path from the initial state.

We can do this by constructing an MDP where only unseen state-action pairs are rewarded,
and using value iteration/dynamic programming (@sec-finite-opt-dp)
to reach the unknown states in $M_\mathcal{D}$.
Concretely,
we keep a set $\mathcal{D}$ of all the $(s, a, r, s')$ tuples we've observed.
Each episode,
we use $\mathcal{D}$ to construct a fully known MDP, $M_\mathcal{D}$,
in which only unseen state-action pairs are rewarded.

:::: {#def-explore-then-exploit}
#### Explore-then-exploit algorithm

Suppose that every state can be reached from the initial state within a single episode.

1. $\mathcal{D} \gets \emptyset$
2.  For $T = 0, 1, 2, \dots$ (until the entire MDP has been explored):
    1. Construct $M_\mathcal{D}$ using $\mathcal{D}$. That is, the state transitions are set to those observed in $\mathcal{D}$, and the reward is set to $0$ for all state-action pairs in $\mathcal{D}$, and $1$ otherwise.
    2. Execute DP (@sec-finite-opt-dp) on the known MDP $M_\mathcal{D}$ to compute the optimal policy $\pi^\star_{\mathcal{D}}$.
    3. Execute $\pi^\star_{\mathcal{D}}$ in $M_\mathcal{D}$. This will visit some $(s, a)$ not yet in $\mathcal{D}$,
        and observe the reward $r(s, a)$ and next state $P(s, a)$.
    4. $\mathcal{D} \gets \mathcal{D} \cup \{ (s, a, r, s') \}$, where $s' = P(s, a), r = r(s, a)$ are the observed state transition and reward.
::::

::: {#rem-explore-bfs}
#### Path planning is graph traversal

Review the dynamic programming algorithm for a finite-horizon MDP (@sec-finite-opt-dp).
Note that in the constructed MDP $M_\mathcal{D}$,
this is identical to a **breadth-first search**
beginning from the desired state at the final timestep:
each state-timestep pair is a node in the graph,
and the state transitions determine the (directed) edges.
Each state-timestep pair from which it is possible to reach the desired state
is assigned a value of $1$.
The policy serves to backtrack through these state-timestep pairs,
returning to the root node of the search:
the desired state.
:::

We can easily measure the **per-episode regret** of this algorithm.

::: {#def-per-episode-regret}
#### Per-episode regret

We aim to evaluate some iterative policy optimization algorithm.
Let $\pi^\ep$ be the policy returned by the algorithm after $\ep$ iterations.
The per-episode regret across $\Ep$ iterations is given by

$$
\text{Regret}_\Ep = \E_{s_0 \sim P_0}\left[ \sum_{\ep = 0}^{\Ep - 1} V^\star_0(s_0) - V^{\pi^\ep}_0(s_0) \right]
$$ {#eq-per-episode-regret}

where the randomness is in the initial state distribution.
:::

::: {#rem-mab-mdp}
#### MDP policies as MAB arms

What does this have to do with the definition of regret in the MAB setting (@def-regret)?
Here, policies are arms,
and the "mean reward" is the expected total reward of a trajectory.
We'll make this connection more explicit in @sec-mdp-mab.
:::

::: {#thm-explore-then-exploit-performance}
#### Performance of explore-then-exploit

The regret of the explore-then-exploit algorithm (@def-explore-then-exploit)
can be upper-bounded by

$$
\sum_{\ep = 0}^{\Ep - 1} V^\star_0 - V_0^{\pi^\ep} \le |\mathcal{S}||\mathcal{A}| \Step.
$$ {#eq-explore-exploit-performance}

(This MDP and algorithm are deterministic, assuming there is a single starting state, so the regret is not random.)
:::

::: {.proof}
As long as every state can be reached from $s_0$ within a single episode,
i.e. $|\mathcal{S}| \le \Step$,
@def-per-episode-regret will eventually be able to explore all $|\mathcal{S}| |\mathcal{A}|$ state-action pairs,
adding one new transition per episode.

Let $M$ denote the original MDP that we aim to solve.
We know it will take at most $|\mathcal{S}| |\mathcal{A}|$ iterations to explore the entire MDP,
after which $M_\mathcal{D} = M$ and $\pi^\star_{\mathcal{D}}$ is the optimal policy in $M$,
incurring no additional regret.
For each "shortest-path" policy $\pi^\star_{\mathcal{D}}$ up until then,
its value will differ from that of $\pi^\star$ by at most $\Step$,
since the policies will differ by at most $1$ reward at each timestep. 
:::


## Treating an unknown MDP as a MAB {#sec-mdp-mab}

We explored the exploration-exploitation tradeoff
in the multi-armed bandits setting (@sec-bandits).
Can we apply the MAB algorithms we discovered to MDPs as well?
Let us formally describe an unknown MDP as an MAB problem.

In a MAB problem,
we want to find the *arm* with the highest mean reward.
In an MDP,
we want to find the *policy* that achieves the highest expected total reward.
So if we want to apply MAB techniques to solving an MDP,
it makes sense to draw an equivalence between *arms* and *policies*.
We can summarize this equivalence in the following table:


| MAB                                                    | MDP                                                                                                                      |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------ |
| $K$ arms                                               | $(\vert\mathcal{A}\vert^{\vert\mathcal{S}\vert})^\Step$ deterministic policies                                           |
| unknown reward distributions $\nu^k$                   | unknown trajectory distributions $\rho^\pi$                                                                              |
| $k^\star = \arg\max_{k \in [K]} \E_{r \sim \nu^k} [r]$ | $\pi^\star = \arg\max_{\pi \in \Pi} \E_{\tau \sim \rho^\pi} \left[ \sum_{\step=0}^{\Step-1} r(s_\step, a_\step) \right]$ |
| pull arm $k$ and observe reward                        | roll out with $\pi$ and observe total reward                                                                             |

: Treating an MDP with finite states and actions as a MAB. {#tbl-mdp-mab}

(For the sake of this example,
assume that the MDP's reward function is stochastic,
so that the MAB reward distributions are nondegenerate.)

Recall that UCB incurs regret $\widetilde{O}(\sqrt{TK})$,
where $\Ep$ is the number of pulls and $K$ is the number of arms.
So in the MDP-as-MAB problem, using UCB for $\Ep$ episodes would achieve regret

$$
\widetilde{O}\left(
    \sqrt{|\mathcal{A}|^{|\mathcal{S}|\Step} T}
\right)
$$ {#eq-mdp-as-mab}

This scales *exponentially* in $|\mathcal{S}|$ and $\Step$,
which quickly becomes intractable.
Notably,
this method treats each policy as entirely independent from the others,
but the performance of different policies are typically correlated.
We can illustrate this with the following example:



::: {#exm-ineffective-mdp}
#### Treating an MDP as a MAB

Consider a "coin MDP" with two states "heads" and "tails",
two actions "Y" and "N", and a time horizon of $\Step=2$.
The state transition flips the coin,
and doesn't depend on the action.
The reward only depends on the action:
Taking action Y gives reward $1$,
and taking action N gives reward $0$.

Suppose we collect data from the two constant policies $\pi_{\text{Y}}(s) = \text{Y}$ and $\pi_{\text{N}}(s) = \text{N}$.
Now we want to learn about the policy $\widetilde{\pi}$ that takes action Y and then N.
Do we need to collect data from $\widetilde{\pi}$ to evaluate it?
No: Since the reward only depends on the action,
we can infer its value from our data on the policies $\pi_{\text{Y}}$ and $\pi_{\text{N}}$.
However,
if we treat the MDP as a bandit in which $\widetilde{\pi}$ is a new,
unknown arm,
we ignore the known correlation between the action and the reward.
:::



## Upper confidence bound value iteration {#sec-ucbvi}

We shouldn't need to consider all $|\mathcal{A}|^{|\mathcal{S}| \Step}$ deterministic policies
to achieve low regret.
Rather, all we need to describe the optimal policy is $Q^\star$,
which has $H |\mathcal{S}||\mathcal{A}|$ entries to be learned.
In this section,
we'll study the upper confidence bound value iteration (UCBVI) algorithm [@azar_minimax_2017],
which indeed achieves _polynomial_ regret in $|\mathcal{S}|$, $|\mathcal{A}|$, and $\Step$.

As its name suggests,
UCBVI combines the upper confidence bound (UCB) algorithm
from the multi-armed bandits setting (@sec-ucb)
with value iteration (VI) from the MDP setting (@sec-finite-opt-dp):

-   UCB strikes a good exploration-exploitation tradeoff in an (unknown) MAB;
-   VI (what we simply call DP in the finite-horizon setting)
    computes the optimal value function in a known MDP
    (with finite states and actions).

Let us briefly review these two algorithms:

::: {#rem-ucb-review}
#### Review of UCB

At each iteration $\ep$,
for each arm $k$,
we construct a *confidence interval* for the mean of arm $k$'s reward distribution.
We then choose the arm with the highest upper confidence bound:

$$
\begin{aligned}
k_{t+1} &\gets \arg\max_{k \in [K]} \text{ucb}^k_t \\
\text{where } \text{ucb}^k_t &= \frac{R^{k}_t}{N^{k}_t} + \sqrt{\frac{\ln(2t/\delta)}{2 N^{k}_t}}
\end{aligned}
$$ {#eq-ucb-review}

where $N_t^k$ indicates the number of times arm $k$ has been pulled up until time $\ep$,
$R_t^k$ indicates the total reward obtained by pulling arm $k$ up until time $\ep$,
and $\delta > 0$ controls the width of the confidence interval.
:::

We can treat the upper confidence bound as a "proxy reward"
that is the estimated mean reward plus a bonus **exploration term**.
Since the size of the bonus term is proportional to our uncertainty (i.e. predicted variance) about that arm's mean,
this is called an **optimistic** bonus.
In UCBVI,
we will extend this idea to the case of an unknown MDP $\mathcal{M}$
by adding an _exploration term_ to the reward function.
Then, we will use DP to solve for the optimal policy in $\widetilde{\mathcal{M}}$.

::: {#rem-vi-review}
#### Review of VI/DP

Value iteration (VI) is a dynamic programming (DP) algorithm
for computing the optimal policy and value function
in an MDP where the state transitions and reward function are known.
We begin at the final timestep,
where $V_\Step^\star(s) = 0$,
and work backwards using Bellman's optimality equations (@thm-bellman-opt):

For $\step = \Step-1, \dots, 0$:

$$
\begin{aligned}
Q_\step^\star(s, a) &= r(s, a) + \E_{s' \sim P(\cdot \mid s, a)}[V_{\step+1}^\star(s')] \\
\pi_\step^\star(s) &= \arg\max_{a \in \mathcal{A}} Q_\step^\star(s, a) \\
V_\step^\star(s) &= Q_\step^\star(s, \pi_\step^\star(s)).
\end{aligned}
$$ {#eq-vi-review}
:::

**Assumptions**:
We will consider the general case of a **time-varying** MDP
where the transition and reward functions may change over time.
Recall our convention that $P_\step$ is the distribution of $s_{h+1} \mid s_{h}, a_{h}$
and $r_\step$ is applied to $s_\step, a_\step$.


::: {#def-ucbvi}
#### UCBVI

At a high level, the UCBVI algorithm can be described as follows:
For $i = 0, \dots, I-1$:

1.  **Modeling:** Use previously collected data to model the state transitions $\widehat{P}_0, \dots, \widehat{P}_{\Step-1}$ and reward functions $\widehat{r}_0, \dots, \widehat{r}_{\Step-1}$.
2.  **Reward bonus:** Design a reward bonus $b_\step(s, a) \in \mathbb{R}$ to encourage exploration, analogous to the UCB term.
3.  **Optimistic planning:** Use VI (i.e. DP) to compute the optimal policy $\widehat \pi$ in the modelled MDP

$$
\widetilde{\mathcal{M}} = (\mathcal{S}, \mathcal{A}, \{ \widehat{P}_\step \}_{h \in [H]}, \{ \widehat{r}_\step + b_\step \}_{h \in [H]}, H).
$$

4.  **Execution:** Use $\widehat \pi$ to collect a new trajectory.

We detail each of these steps below.
:::


### Modeling the transitions

Recall that we _don't know_ the state transitions or reward function
of the MDP we aim to solve.
We seek to approximate

$$
P_\step(s_{\step + 1} \mid s_\step, a_\step) = \frac{\pr(s_\step, a_\step, s_{\step + 1})}{\pr(s_\step, a_\step)},
$$ {#eq-transitions-factor}

where $\pr$ denotes the true joint probabilities.
We can estimate these using their sample probabilities across a set of collected transitions.
That is, define

$$
\begin{aligned}
    N_\step^i(s, a, s') & := \sum_{i'=0}^{i-1} \ind{ (s_\step^{i'}, a_\step^{i'}, s_{h+1}^{i'}) = (s, a, s') } \\
    N_\step^i(s, a)     & := \sum_{i'=0}^{i-1} \ind{ (s_\step^{i'}, a_\step^{i'}) = (s, a) }                \\
\end{aligned}
$$ {#eq-count-transitions}

to be the number of times the tuple $s, a, s'$ appears in the collected data,
and similar for the state-action pair $s, a$.
Then we can model

$$
\widehat{P}_\step^t(s' \mid s, a) = \frac{N_\step^t(s, a, s')}{N_\step^t(s, a)}.
$$ {#eq-average-transition}

Similarly, we can model the rewards by the sample mean in each state-action pair:

$$
\widehat{r}_\step^t(s, a) = \frac{N_\step^t(s, a)} \sum_{t'=0}^{t-1} \ind{ (s_\step^i, a_\step^i) = (s, a) } r_\step^i.
$$ {#eq-average-reward}

This is a fairly naive, nonparametric estimator
that doesn't assume any underlying structure of the MDP.
We'll see how to incorporate assumptions about the MDP in the following section.



### Reward bonus {#sec-reward-bonus}

To motivate the reward bonus term,
recall how we designed the reward bonus term for UCB (@sec-ucb):

1.  We used Hoeffding's inequality to bound,
    with high probability,
    how far the sample mean $\widehat \mu_t^k$ deviated from the true mean $\mu^k$.
2.  By inverting this inequality,
    we obtained a $(1-\delta)$-confidence interval for the true mean,
    centered at our estimate.
3.  To make this bound *uniform* across all timesteps $t \in [T]$,
    we applied the union bound and multiplied $\delta$ by a factor of $\Ep$.

We'd like to do the same for UCBVI,
and construct the bonus term
such that $V^\star_\step(s) \le \widehat{V}_\step^t(s)$ with high probability.
However, our construction will be more complex than the MAB case,
since $\widehat{V}_\step^t(s)$ depends on the bonus $b_\step^t(s, a)$ implicitly via DP.
We claim that the bonus term that gives the proper bound is

$$
b_\step^\iter(s, a) = 2 \Step \sqrt{\frac{\log( |\mathcal{S}||\mathcal{A}|\Step \Iter/\delta )}{N_\step^t(s, a)}}.
$$ {#eq-ucbvi-bonus}

We provide a heuristic sketch of the proof in @sec-ucbvi-proof; see @agarwal_reinforcement_2022, Section 7.3 for a full proof.

### Performance of UCBVI

How exactly does UCBVI strike a good balance between exploration and exploitation? In UCB for MABs, the bonus exploration term is simple to interpret: It encourages the learner to take actions with a high exploration term. Here, the policy depends on the bonus term indirectly: The policy is obtained by planning in an MDP where the bonus term is added to the reward function. Note that the bonuses *propagate backwards* in DP, effectively enabling the learner to *plan to explore* unknown states. This effect takes some further interpretation.

Recall we constructed $b^t_\step$ so that, with high probability, $V^\star_\step(s) \le \widehat{V}_\step^t(s)$ and so

$$
V^\star_\step(s) - V^{\pi^\ep}_\step(s) \le \widehat{V}_\step^t(s) - V^{\pi^\ep}_\step(s).
$$

That is, the l.h.s. measures how suboptimal policy $\pi^\ep$ is in the true environment, while the r.h.s. is the difference in the policy's value when acting in the modelled MDP $\widetilde{\mathcal{M}}^t$ instead of the true one $\mathcal{M}$.

If the r.h.s. is *small*, this implies that the l.h.s. difference is also small, i.e. that $\pi^\ep$ is *exploiting* actions that are giving high reward.

If the r.h.s. is *large*, then we have overestimated the value:
$\pi^\ep$, the optimal policy of $\widetilde{\mathcal{M}}^t$, does not perform well in the true environment $\mathcal{M}$. This indicates that one of the $b_h^t(s, a)$ terms must be large, or some $\widehat P^t_\step(\cdot \mid s, a)$ must be inaccurate, indicating a state-action pair with a low visit count $N^t_\step(s, a)$ that the learner was encouraged to explore.

It turns out that UCBVI achieves a regret of

::: {#thm-ucbvi-regret}
#### UCBVI regret

The expected regret of UCBVI satisfies

$$
\E \left[
    \sum_{\ep = 0}^{\Ep - 1} \left(V^\star_0(s_0) - V^{\pi^\ep}_0(s_0) \right)
\right] =
\widetilde{O}(\Step^2 \sqrt{|\mathcal{S}| |\mathcal{A}| \Iter})
$$
:::

Comparing this to the UCB regret bound $\widetilde{O}(\sqrt{T K})$, where $K$ is the number of arms of the MAB, we see that we've reduced the number of effective arms from $|\mathcal{A}|^{|\mathcal{S}|\Step}$ (in @eq-mdp-as-mab) to $H^4 |\mathcal{S}||\mathcal{A}|$, which is indeed polynomial in $|\mathcal{S}|$, $|\mathcal{A}|$, and $\Step$, as desired. This is also roughly the number of episodes it takes to achieve constant-order average regret:

$$
\frac{1}{T} \E[\text{Regret}_\Ep] = \widetilde{O}\left(\sqrt{\frac{H^4 |\mathcal{S}||\mathcal{A}|}{T}}\right)
$$

Note that the time-dependent transition matrix has $H |\mathcal{S}|^2 |\mathcal{A}|$ entries. Assuming $H \ll |\mathcal{S}|$, this shows that it's possible to achieve low regret, and achieve a near-optimal policy, while only understanding a $1/|\mathcal{S}|$ fraction of the world's dynamics.


## Linear MDPs

A polynomial dependency on $|\mathcal{S}|$ and $|\mathcal{A}|$ is manageable when the state and action spaces are small. But for large or continuous state and action spaces, even this polynomial factor will become intractable. Can we find algorithms that don't depend on $|\mathcal{S}|$ or $|\mathcal{A}|$ at all, effectively reducing the dimensionality of the MDP? In this section, we'll explore **linear MDPs**: an example of a *parameterized* MDP where the rewards and state transitions depend only on some parameter space of dimension $d$ that is independent from $|\mathcal{S}|$ or $|\mathcal{A}|$.

::: {#def-linear-mdp}
#### Linear MDP

We assume that the transition probabilities and rewards are *linear* in some feature vector

$\phi(s, a) \in \mathbb{R}^d$:

$$
\begin{aligned}
        P_\step(s' \mid s, a) & = \phi(s, a)^\top \mu^\star_\step(s') \\
        r_\step(s, a)         & = \phi(s, a)^\top \theta_\step^\star
\end{aligned}
$$

Note that we can also think of $P_\step(\cdot \mid s, a) = \mu_\step^\star$ as an $|\mathcal{S}| \times d$ matrix, and think of $\mu^\star_\step(s')$ as indexing into the $s'$-th row of this matrix (treating it as a column vector). Thinking of $V^\star_{\step+1}$ as an $|\mathcal{S}|$-dimensional vector, this allows us to write

$$
\E_{s' \sim P_\step(\cdot \mid s, a)}[V^\star_{\step+1}(s)] = (\mu^\star_\step \phi(s, a))^\top V^\star_{\step+1}.
$$

The $\phi$ feature mapping can be designed to capture interactions between the state $s$ and action $a$. In this book, we'll assume that the feature map $\phi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}^d$ and the reward function (described by $\theta_\step^\star$) are known to the learner.
:::

### Planning in a linear MDP

It turns out that $Q^\star_\step$ is also linear with respect to this feature mapping. We can prove this by simply computing it using DP. We initialize the value function at the end of the time horizon by setting $V_{H}^\star(s) = 0$ for all states $s$. Then we iterate:

$$
\begin{aligned}
    Q^\star_\step(s, a)  & = r_\step(s, a) + \E_{s' \sim P_\step(\cdot \mid s, a)} [V^\star_{h+1}(s')]                          \\
                     & = \phi(s, a)^\top \theta_\step^\star + (\mu_\step^\star \phi(s, a))^\top V^\star_{h+1}               \\
                     & = \phi(s, a)^\top \underbrace{( \theta_\step^\star + (\mu_\step^\star)^\top  V^\star_{h+1})}_{w_\step} \\
    V^\star_\step(s)     & = \max_a Q^\star_\step(s, a)                                                                       \\
    \pi^\star_\step(s) & = \arg\max_a Q^\star_\step(s, a)
\end{aligned}
$$

::: {#exr-linear-q}
#### Action-value function is linear in features

Show that $Q^\pi_\step$ is also linear with respect to $\phi(s, a)$ for any policy $\pi$.
:::



### UCBVI in a linear MDP {#sec-lin-ucbvi}

#### Modeling the transitions

This linear assumption on the MDP will also allow us to model the unknown dynamics $P^?_\step(s' \mid s, a)$ with techniques from **supervised learning** (SL). Recall that SL is useful for estimating conditional expectations by minimizing mean squared error. We can rephrase the estimation of $P^?_\step(s' \mid s, a)$ as a least-squares problem as follows: Write $\delta_s$ to denote a one-hot vector in $\mathbb{R}^{|\mathcal{S}|}$, with a $1$ in the $s$-th entry and $0$ everywhere else. Note that

$$
\E_{s' \sim P_h(\cdot \mid s, a)} [\delta_{s'}] = P_h(\cdot \mid s, a) = \mu_h^\star \phi(s, a).
$$

Furthermore, since the expectation here is linear with respect to $\phi(s, a)$, we can directly apply least-squares multi-target linear regression to construct the estimate

$$
\widehat \mu = \arg\min_{\mu \in \mathbb{R}^{|\mathcal{S}| \times d}} \sum_{\ep = 0}^{\Ep - 1} \|\mu \phi(s_h^i, a_h^i) - \delta_{s_{h+1}^i} \|_2^2.
$$

This has a well-known closed-form solution:

$$
\begin{aligned}
    \widehat \mu^\top            & = (A_h^t)^{-1} \sum_{i=0}^{t-1} \phi(s_h^i, a_h^i) \delta_{s_{h+1}^i}^\top \\
    \text{where} \quad A_h^t & = \sum_{i=0}^{t-1} \phi(s_h^i, a_h^i) \phi(s_h^i, a_h^i)^\top + \lambda I
\end{aligned}
$$

where we include a $\lambda I$ term to ensure that the matrix $A^t_h$ is invertible. (This can also be derived by adding a $\lambda \|\mu\|_{\text{F}}^2$ regularization term to the objective.) We can directly plug in this estimate into $\widehat{P}^t_h(\cdot \mid s, a) = \widehat \mu^t_h \phi(s, a)$.



#### Reward bonus

Now, to design the reward bonus, we can't apply Hoeffding's inequality anymore, since the terms no longer involve sample means of bounded random variables; Instead, we're incorporating information across different states and actions. Rather, we can construct an upper bound using *Chebyshev's inequality* in the same way we did for the LinUCB algorithm in the MAB setting @sec-lin-ucb:

$$
b^t_\step(s, a) = \beta \sqrt{\phi(s, a)^\top (A^t_h)^{-1} \phi(s, a)}, \quad \beta = \widetilde O(d \Step).
$$

Note that this isn't explicitly inversely proportional to $N_h^t(s, a)$ as in the original UCBVI bonus term @eq-ucbvi-bonus. Rather, it is inversely proportional to the amount that the direction $\phi(s, a)$ has been explored in the history. That is, if $A-h^t$ has a large component in the direction $\phi(s, a)$, implying that this direction is well explored, then the bonus term will be small, and vice versa.

We can now plug in these transition estimates and reward bonuses into the UCBVI algorithm @def-ucbvi.


::: {#thm-lin-ucbvi-regret}
#### LinUCBVI regret

The LinUCBVI algorithm achieves expected regret

$$
\E[\text{Regret}_\Ep] = \E\left[\sum_{\ep = 0}^{\Ep - 1} V^\star_0(s_0) - V^{\pi^\ep}_0(s_0) \right] \le \widetilde O(H^2 d^{1.5} \sqrt{T})
$$
:::

Comparing this to our bound for UCBVI in an environment without this linear assumption, we see that we go from a sample complexity of $\widetilde \Omega(H^4 |\mathcal{S}||\mathcal{A}|)$ to $\widetilde \Omega(H^4 d^{3})$. This new sample complexity only depends on the feature dimension and not on the state or action space of the MDP!

## Key takeaways

We first discussed the explore-then-exploit algorithm (@def-explore-then-exploit),
a simple way to explore a deterministic MDP by visiting all state-action pairs.
This is essentially a graph traversal algorithm,
where each state represents an edge of the graph.
We then discussed how to treat an unknown MDP as a MAB (@sec-mdp-mab),
and how this approach is inefficient since it doesn't make use of correlations between different policies.
We then introduced the UCBVI algorithm (@def-ucbvi),
the key algorithm of this chapter,
which models the unknown MDP by a proxy MDP with a reward bonus term that encourages exploration.
Finally, assuming that the transitions and rewards are linear
with respect to a feature transformation of the state and action,
we introduced the LinUCBVI algorithm (@sec-lin-ucbvi),
which has a sample complexity independent of the size of the state and action spaces.
This makes it possible to scale up UCBVI to large problems
that have a simple underlying structure.


## Bibliographic notes and further reading {#sec-exploration-bib}

Sparse reward problems are frequent throughout reinforcement learning.
The chain MDP example is from @thrun_efficient_1992.
One of the most famous sparse reward problems is **Montezuma's Revenge**,
one of the tasks in the popular **arcade learning environment** (ALE) benchmark of Atari 2600 games [@bellemare_arcade_2013, @machado_revisiting_2018].
These were first solved by algorithms that explicitly encourage exploration [@burda_exploration_2018].

The UCBVI algorithm was first presented in @azar_minimax_2017.
UCBVI extends the UCB algorithm from multi-armed bandits
to the MDP
by estimating a model of the environment.
Later work by @drago_refined_2025 improved the regret bound on UCBVI.
Other model-based methods for strategic exploration
have been studied at least since @schmidhuber_curious_1991 and @meyer_possibility_1991.
UCBVI computes the reward bonus using the _count_ of the number of times
that state-action pair has been visited.
@tang_exploration_2017 surveys other such count-based exploration algorithms.

It is also possible to encourage model-free algorithms to strategically explore.
@badia_agent57_2020 designed a Q-learning algorithm with exploration incentives
that surpassed the human baseline on the challenging Atari tasks.

We refer the reader to @ladosz_exploration_2022 for a recent survey of exploration in RL.



::: {.content-visible when-profile="thesis"}
## Differences from course

The first example in this chapter,
that of the "chain MDP",
serves to disillusion students from the excitement of the previous chapters:
though the methods we discussed up to now are quite sophisticated,
they fail to solve such a simple problem!


:::