{{< include _macros.tex >}}

# Linear Quadratic Regulators {#sec-control}

## Introduction

In @sec-mdps,
we considered decision problems with finitely many states and actions.
However, in many applications,
states and actions may take on _continuous_ values.
For example,
consider autonomous driving,
controlling a robot's joints,
and automated manufacturing.
How can we teach computers to solve these kinds of problems?
This is the task of **continuous control**.

::: {layout-ncol="2"}
![Solving a Rubik’s Cube with a robot hand](shared/rubiks_cube.jpg)

![Boston Dynamics’s Spot robot](shared/boston_dynamics.jpg)
:::

Aside from the change in the state and action spaces,
the general problem setup remains the same:
we seek to construct an *optimal policy* that outputs actions to solve the desired task.
We will see that many key ideas and algorithms,
in particular dynamic programming algorithms (@sec-finite-opt-dp),
carry over to this new setting.


| Chapter | State space | Action space | Policies | Knowledge of environment | Key algorithms |
| --- | --- | --- | --- | --- |
| @sec-mdps (MDPs) | $\{1, \dots, \vert \mathcal{S} \vert \}$ | $\{ 1, \dots, \vert \mathcal{A} \vert \}$ | deterministic | known | dynamic programming, policy iteration |
| @sec-control (Control) | $\R^{n_\st}$ | $\R^{n_\act}$ | deterministic | unknown | (iterative) linear quadratic regulator |

: A comparison of the control setting with the MDP setting. {#tbl-overview-chapters-control}

This chapter introduces a fundamental tool to solve a simple class of continuous control problems:
the **linear quadratic regulator** (LQR).
We can use the LQR model as a building block for solving more complex problems.

::: {#rem-differences-rl}
#### Control vs RL

Control theory is often considered a distinct, though related, field from RL.
In both fields,
the central aim is to solve sequential decision problems,
i.e. find a strategy for taking actions in the environment
in order to achieve some goal
that is measured by a scalar signal.
The two fields arose rather independently
and with rather different problem settings,
and use different mathematical terminology as a result.

Control theory has close ties to electrical and mechanical engineering.
Humans have been designing control systems since antiquity.
The first attempt to formalize control theory as a field is attributed to @maxwell_governors_1867.
Maxwell (well known for Maxwell's equations) demonstrated the utility
of analyzing mathematical models to design stable systems.
Control theorists typically work in a _continuous-time_ setting
in which the dynamics can be described by systems of differential equations.
The goal is typically to ensure _stability_ of the system
and minimize some notion of "cost"
(e.g. wasted energy in controlling the system).
Rather than learning the system from data,
one typically supposes a particular structure of the environment
and solves for the optimal controller using analytical or numerical methods.
As such, control theory algorithms are more accurately referred to as _planning methods_
that do not depend on _data_;
that is, there is no "learning" patterns from data involved.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt
from utils import latex
import numpy as np
import matplotlib.pyplot as plt
import control
```

## Optimal control

Let's first look at a simple example of a continuous control problem:

:::: {#exm-cart-pole}
#### CartPole

Try to balance a pencil on its point on a flat surface. It's much more
difficult than it may first seem: the position of the pencil varies
continuously, and the state transitions governing the system, i.e. the
laws of physics, are highly complex. This task is equivalent to the
classic control problem known as *CartPole*:

![A snapshot of the cart pole environment](shared/cart_pole.png){width=200}

The state $\st \in \mathbb{R}^4$ can be described by four real variables:

1.  the position of the cart;
2.  the velocity of the cart;
3.  the angle of the pole;
4.  the angular velocity of the pole.

We can *control* the cart by applying a horizontal force $\act \in \mathbb{R}$.

**Goal:** Stabilize the cart around an ideal state and action
$(\st^\star, \act^\star)$.
::::

A continuous control environment is a special case of an MDP (@def-finite-horizon-mdp).
Recall that a finite-horizon MDP

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P_0, P, r, \Step).
$$ {#eq-recall-finite-mdp}

is defined by its state space $\mathcal{S}$,
action space $\mathcal{A}$,
initial state distribution $P_0$,
state transitions $P$,
reward function $r$,
and time horizon $\Step$.
These each have equivalents in the control setting.

::: {#def-continuous-control}
#### Continuous control environment

A continuous control environment is defined by the following components:

-   The state and action spaces $\mathcal{S}, \mathcal{A}$ are *continuous* rather than finite.
    That is, $\mathcal{S} \subseteq \mathbb{R}^{n_\st}$ and $\mathcal{A} \subseteq \mathbb{R}^{n_\act}$,
    where $n_\st$ and $n_\act$ are the number of coordinates required to specify a single state or action respectively.
    For example, in robotic control,
    $n_\st$ might be the number of sensors
    and $n_\act$ might be the number of actuators on the robot.
-   $P_0 \in \triangle(\mathcal{S})$ denotes the initial state distribution.
-   We call the state transitions the **dynamics** of the system and denote them by $f$:
    $$
    \st_{\step+1} = f_\step(\st_\step, \act_\step, w_\step),
    $$ {#eq-lqr-dynamics}
    where $w_\step$ denotes noise drawn from the distribution $\nu_\step \in \triangle(\mathbb{R}^{n_w})$.
    Note that the dynamics may vary at each timestep $\step$.
-   Instead of maximizing the reward function, we seek to minimize the **cost function** $c_\step: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$.
    Often, the cost function describes *how far away* we are from a **target state-action pair** $(\st^\star, \act^\star)$.
-   The agent acts over a *finite time horizon* $\Step \in \mathbb{N}$.

Together, these constitute a description of the environment

$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P_0, f, \nu, c, \Step).
$$ {#eq-continuous-control}
:::

::: {#def-optimal-control}
#### Optimal control

For a continuous control environment $\mathcal{M}$,
the optimal control problem
is to compute a policy $\pi$
that minimizes the total cost

$$
\begin{aligned}
    \min_{\pi_0, \dots, \pi_{\Step-1} : \mathcal{S} \to \mathcal{A}} \quad & \E \left[
        \sum_{\step=0}^{\Step-1} c_\step(\st_\step, \act_\step)
    \right] \\
    \text{where} \quad & \st_{\step+1} = f_\step(\st_\step, \act_\step, w_\step), \\
    & \act_\step = \pi_\step(\st_\step) \\
    & \st_0 \sim P_0 \\
    & w_\step \sim \nu_\step.
\end{aligned}
$$ {#eq-optimal-control}
:::

In this chapter, we will only consider *deterministic, time-dependent* policies

$$
\pi = (\pi_0, \dots, \pi_{\Step-1})
\quad \text{where} \quad
\pi_h : \mathcal{S} \to \mathcal{A}
\text{ for each }
\step \in [\Step].
$$ {#eq-det-h-policy}

To make the explicit conditioning more concise,
we let $\rho^\pi$ denote the trajectory distribution
induced by the policy $\pi$.
That is, if we sample initial states from $P_0$,
act according to $\pi$,
and transition according to the dynamics $f$,
the resulting distribution over trajectories is $\rho^\pi$.
Put another way,
the procedure for sampling from $\rho^\pi$
is to take rollouts in the environment using $\pi$.

::: {#thm-continuous-control-trajectories}
#### Trajectory distributions

Let $\pi = \{ \pi_0, \dots, \pi_{\Step-1} \}$ be a deterministic, time-dependent policy.
We include states $\st_\step$, actions $\act_\step$, and noise terms $w_\step$ in the trajectory.
Then the density of a trajectory $\tau = (\st_0, \act_0, w_0, \dots, w_{\Step-2}, \st_{\Step-1}, \act_{\Step-1})$
can be expressed as follows:

$$
\begin{aligned}
\rho^{\pi}(\tau)
&:= P_0(\st_0) \times \ind{\act_0 = \pi_0(\st_0)} \\
&\quad {} \times \nu(w_0) \times \ind{\st_1 = f(\st_0, \act_0, w_0)} \\
&\quad {} \times \cdots \\
&\quad {} \times \nu(w_{\Step-2})
    \times \ind{\st_{\Step-1} = f(\st_{\Step-2}, \act_{\Step-2}, w_{\Step-2})}
    \times \ind{\act_{\Step-1} = \pi_{\Step-1}(\st_{\Step-1})} \\
&= P_0(\st_0) \ind{\act_0 = \pi_0(\st_0)}
    \prod_{\step=1}^{\Step-1} \nu(w_{\step-1})
    \ind{\st_{\step} = f(\st_{\step-1}, \act_{\step-1}, w_{\step-1})}
    \ind{\act_{\step} = \pi_{\step}(\st_{\step})}
\end{aligned}
$$ {#eq-continuous-autoregressive-trajectories}
:::

This expression may seem intimidating,
but on closer examination,
it simply uses indicator variables to enforce
that only valid trajectories have nonzero probability.
Henceforth, we will write $\tau \sim \rho^\pi$
instead of the explicit conditioning in @eq-optimal-control.

As in @sec-mdps,
_value functions_ will be crucial for constructing the optimal policy via **dynamic programming.**

::: {#def-value-lqr}
#### Value functions in LQR

Given a policy $\mathbf{\pi} = (\pi_0, \dots, \pi_{\Step-1})$,
we can define its value function $V^\pi_\step : \mathcal{S} \to \mathbb{R}$ at time $\step \in [\Step]$
as the **cost-to-go** incurred by that policy in expectation:

$$
V^\pi_\step (\st) = \E_{\tau \sim \rho^\pi} \left[
    \sum_{\step'=\step}^{\Step-1} c(\st_{\step'}, \act_{\step'})
    \mid \st_\step = \st
\right]
$$ {#eq-value-control}
:::

::: {#def-q-lqr}
#### Q function in LQR

The Q-function additionally conditions on the first action we take:

$$
Q^\pi_\step (\st, \act) = \E_{\tau \sim \rho^\pi} \left[
    \sum_{\step'=\step}^{\Step-1} c(\st_{\step'}, \act_{\step'})
    \mid (\st_\step, \act_\step) = (\st, \act)
\right]
$$ {#eq-q-lqr}
:::

Note that since we use a _cost function_ $c$ instead of a _reward function_ $r$,
the best policies in a given state (or state-action pair) are the ones with _smaller_ $V^\pi$ and $Q^\pi$.

### A first attempt: Discretization

Can we solve this problem using tools from the finite MDP setting?
If $\mathcal{S}$ and $\mathcal{A}$ were finite,
then we could use dynamic programming (@fig-optimal-policy) to compute the optimal policy exactly.
This inspires us to try *discretizing* the problem.

Suppose $\mathcal{S}$ and $\mathcal{A}$ are bounded,
that is, $\max_{\st \in \mathcal{S}} \|\st\| \le B_\st$
and $\max_{\act \in \mathcal{A}} \|\act\| \le B_\act$.
To make $\mathcal{S}$ and $\mathcal{A}$ finite,
let's choose some small positive $\epsilon$,
and simply round each coordinate to the nearest multiple of $\epsilon$.
For example, if $\epsilon = 0.01$,
then we round each element of $\st$ and $\act$ to two decimal spaces.

What goes wrong?
The discretized $\widetilde{\mathcal{S}}$ and $\widetilde{\mathcal{A}}$ may be finite,
but for all practical purposes,
they are far too large:
we must divide *each dimension* into intervals of length $\varepsilon$,
resulting in the state and action spaces

$$
|\widetilde{\mathcal{S}}| = (B_\st/\varepsilon)^{n_\st}
\text{ and }
|\widetilde{\mathcal{A}}| = (B_\act/\varepsilon)^{n_\act}.
$$ {#eq-epsilon-size}

To get a sense of how quickly this grows,
consider $\varepsilon = 0.01, n_\st = n_\act = 4$, and $B_\st = B_\act = 1$.
Then the number of elements in the transition matrix would be
$|\widetilde{\mathcal{S}}|^2 |\widetilde{\mathcal{A}}| = (100^{4})^2 (100^{4}) = 10^{24}$!
(That's a million million million million.)

What properties of the problem could we instead make use of?
Note that by discretizing the state and action spaces,
we implicitly assumed that rounding each state or action vector by some tiny amount $\varepsilon$
wouldn't change the behaviour of the system by much;
namely, that the cost and dynamics were relatively *continuous*.
Can we use this continuous structure in other ways?
Yes! We will see that the **linear quadratic regulator** makes better use of this assumption.

## The Linear Quadratic Regulator {#sec-lqr}

The optimal control problem @def-optimal-control is quite general.
Is there a relevant simplification that we can solve,
where the dynamics $f$ and cost function $c$ have some special structure?
The **linear quadratic regulator** (LQR) is a special case of a continuous control environment (@def-continuous-control)
where the dynamics $f$ are _linear_
and the cost function $c$ is an _upward-curved quadratic_.
We also assume that the environment is _time-invariant_.
This is an important environment class
in which the optimal policy and value function can be solved for exactly.

::: {#def-lqr}
#### Linear quadratic regulator

The dynamics $f$ are specified by the matrices
$A \in \mathbb{R}^{n_\st \times n_\st}$ and $B \in \mathbb{R}^{n_\st \times n_\act}$:

$$
x_{\step+1} = f(\st_\step, \act_\step, w_\step) = A \st_\step + B \act_\step + w_\step
$$ {#eq-lqr-linear-dynamics}

for all $\step \in [\Step-1]$,
where $w_\step \sim \mathcal{N}(0, \sigma^2) I$.

The cost function is specified by the symmetric positive definite matrices
$Q \in \mathbb{R}^{n_\st \times n_\st}$ and $R \in \mathbb{R}^{n_\act \times n_\act}$:

$$
c(\st_\step, \act_\step) = \st_\step^\top Q \st_\step + \act_\step^\top R \act_\step
$$ {#eq-lqr-cost-quadratic}

for all $\step \in [\Step]$.
(See @sec-bg-linear-algebra if you're unfamiliar with symmetric positive definite matrices.)
We use the same letter as the Q-function (@def-q-lqr);
the context should make it clear which is intended.
:::

::: {#rem-lqr-properties}
#### Properties of LQR

Recall that $w_\step$ is a noise term that makes the dynamics random.
By setting its covariance matrix to $\sigma^2 I$,
we mean that there is Gaussian noise of standard deviation $\sigma$
added independently to each coordinate of the state.
Setting $\sigma = 0$ gives us **deterministic** state transitions.
Surprisingly, the optimal policy doesn't depend on the amount of noise,
although the optimal value function and Q-function do.
(We will show this in a later derivation.)

The LQR cost function attempts to stabilize the state and action
to $(s^\star, a^\star) = (0, 0)$.
We require $Q$ and $R$ to both be _positive definite_ matrices
so that $c$ has a unique minimum.
We can furthermore assume without loss of generality that they are both _symmetric_ (see exercise below).
This greatly simplifies later computations.
:::

::: {#exr-symmetric-q-r}
#### Symmetric $Q$ and $R$

Show that replacing $Q$ and $R$ with $(Q + Q^\top) / 2$ and $(R + R^\top) / 2$ (which are symmetric) yields the same cost function.
:::

::: {#exm-lqr-system}
#### Example LQR system

Consider a ball moving along a line.
From elementary physics,
its velocity $\dot x$ is the instantaneous change in its position,
and its acceleration $\ddot x$ is the instantaneous change in its velocity.
Suppose we can apply a force to accelerate the ball.
:::

```{python}
#| label: fig-lqr-accelerate
#| fig-cap: Visualization of an LQR system.

# 1. Continuous-time double integrator
A_c = np.array([[0, 1],
                [0, 0]])
B_c = np.array([[0],
                [1]])

# 2. Discretize the system
dt = 0.1
sys_c = control.ss(A_c, B_c, np.eye(2), 0)
sys_d = control.c2d(sys_c, dt)
A_d, B_d, C_d, D_d = control.ssdata(sys_d)

# 3. Define LQR cost (discrete-time) and compute gain
Q = np.eye(2)
R = np.array([[0.1]])
K, S, E = control.dlqr(A_d, B_d, Q, R)

# 4. Simulate discrete-time closed-loop system
N = 50                 # number of discrete steps
x = np.zeros((2, N+1)) # state array: 2 rows (pos & vel), N+1 columns (time steps)
x[:, 0] = [1.0, 0.0]   # initial condition: position=1, velocity=0
u = np.zeros((N, 1))   # control input array with correct shape (N, 1)

for k in range(N):
    # u[k] = -K x[k]
    u[k] = -K @ x[:, k].reshape(2, 1)  # Reshape state to column vector
    # x[k+1] = A_d x[k] + B_d u[k]
    x[:, k+1] = A_d @ x[:, k] + B_d @ u[k].flatten()  # Flatten u[k] to make dimensions compatible

# 5. Plot states over time and control input
time = np.arange(N+1) * dt

plt.figure(figsize=(10,6))

# (A) Position and velocity vs. time
plt.subplot(2,1,1)
plt.plot(time, x[0, :], 'b-o', label='Position')
plt.plot(time, x[1, :], 'g-o', label='Velocity')
plt.title('Discrete-Time LQR: State Evolution Over Time')
plt.xlabel('Time [s]')
plt.ylabel('State Values')
plt.grid(True)
plt.legend()

# (B) Control input vs. time
# We only have N control values for the N intervals
plt.subplot(2,1,2)
plt.step(time[:-1], u.flatten(), where='post', color='r', label='Control Input')
plt.title('Control Input Over Time')
plt.xlabel('Time [s]')
plt.ylabel('u(k)')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# 6. Phase-plane plot (position vs. velocity) to visualize discrete steps
plt.figure(figsize=(6,6))
plt.plot(x[0, :], x[1, :], 'o-', label='State Trajectory')
for k in range(N):
    # Draw an arrow from x[k] to x[k+1]
    dx = x[0, k+1] - x[0, k]
    dy = x[1, k+1] - x[1, k]
    plt.arrow(x[0, k], x[1, k], dx, dy, 
              head_width=0.02, length_includes_head=True, color='gray', alpha=0.6)

plt.grid(True)
plt.xlabel('Position')
plt.ylabel('Velocity')
plt.title('Phase-Plane Trajectory (Discrete Steps)')
plt.legend()
plt.show()
```

## Optimality and the Riccati Equation {#sec-optimal-lqr}

In this section,
we'll compute the optimal value function $V^\star_h$,
Q-function $Q^\star_h$,
and policy $\pi^\star_h$ for an LQR problem (@def-lqr) using **dynamic programming**.
The algorithm is identical to the one in @sec-eval-dp-finite-horizon,
except here states and actions are vector-valued.
Recall the definition of the optimal value function:

::: {#def-optimal-value-lqr}
#### Optimal value function in LQR

The **optimal value function** is the one that,
at any time and in any state,
achieves *minimum cost* across all policies $\pi$ that are *history-independent, time-dependent, and deterministic*.

$$
\begin{aligned}
    V^\star_\step(\st) &= \min_{\pi} V^\pi_\step(\st) \\
    &= \min_{\pi} \E_{\tau \sim \rho^\pi} \left[
            \sum_{\step'=\step}^{\Step-1} \left( \st_{\step'}^\top Q \st_{\step'} + \act_{\step'}^\top R \act_{\step'} \right)
            \mid \st_\step = \st
        \right] \\
\end{aligned}
$$ {#eq-optimal-value-lqr}
:::

::: {#def-optimal-q-lqr}
#### Optimal Q function in LQR

The optimal Q-function is defined in the same way,
conditioned on the starting state-action pair:

$$
\begin{aligned}
    Q^\star_\step(\st, \act) &= \min_{\pi} Q^\pi_\step(\st, \act) \\
    &= \min_{\pi} \E \left[
        \sum_{\step'=\step}^{\Step-1}\left(  \st_{\step'}^\top Q \st_{\step'} + \act_{\step'}^\top R \act_{\step'} \right)
        \mid \st_\step = \st, \act_\step = \act
    \right] \\
\end{aligned}
$$ {#eq-optimal-q-lqr}
:::

::: {#rem-lqr-restriction-generality}
#### Minimum over all policies

By @thm-optimal-greedy,
we do not lose any generality
by only considering policies that are history-independent, time-dependent, and deterministic.
That is, the optimal policy within this class
is also globally optimal across all policies,
in the sense that it achieves lower cost
for any given trajectory prefix.
:::

The solution has very simple structure:
$V_h^\star$ and $Q^\star_h$ are *upward-curved quadratics*
and $\pi_h^\star$ is *linear* and furthermore does not depend on the amount of noise!

::: {#thm-optimal-value-lqr-quadratic}
#### Optimal value function in LQR is an upward-curved quadratic

At each timestep $\step \in [\Step]$,

$$
V^\star_\step(\st) = \st^\top P_\step \st + p_\step
$$ {#eq-optimal-value-lqr-quadratic}

for some symmetric, positive-definite $n_\st \times n_\st$ matrix $P_\step$ and scalar
$p_\step \in \mathbb{R}$.
:::

::: {#thm-optimal-policy-lqr-linear}
#### Optimal policy in LQR is linear

At each timestep $\step \in [\Step]$, 

$$
\pi^\star_\step (\st) = - K_\step \st
$$ {#eq-optimal-policy-lqr-linear}

for some $K_\step \in \mathbb{R}^{n_\act \times n_\st}$.
(The negative is due to convention.)
:::

The construction (and inductive proof)
proceeds similarly to the one in the MDP setting (@sec-eval-dp-finite-horizon).
We leave the full proof,
which involves a significant amount of linear algebra,
to @sec-proof-lqr.

```{mermaid}
%%| label: fig-lqr-dp-optimal-finite
%%| fig-cap: Illustrating a dynamic programming algorithm for computing the optimal policy
%%|          in an LQR environment.
graph RL
    Q0["$$Q^\star_{H-1} = r$$"] -- "$$\max_{a \in \mathcal{A}}$$" --> V0
    Q0 -- "$$\arg\max_{a \in \mathcal{A}}$$" --> P0["$$\pi^\star_{H-1}$$"]
    V0["$$V^\star_{H-1}$$"] -- "Bellman equations" --> Q1
    
    Q1["$$Q^\star_{H-2}$$"] -- "$$\max_{a \in \mathcal{A}}$$" --> V1
    Q1 -- "$$\arg\max_{a \in \mathcal{A}}$$" --> P1["$$\pi^\star_{H-2}$$"]
    V1["$$V^\star_{H-2}$$"] -- "Bellman equations" --> Q2

    Q2["..."]
```

Here we provide the concrete $P_\step, p_\step$, and $K_\step$
used to compute the quantities above.

::: {#def-riccati}
#### Riccati equation

It turns out that the matrices $P_0, \dots, P_{\Step-1}$ used to define $V^\star$
obey a recurrence relation known as the **Riccati equation**:

$$
P_\step = Q + A^\top P_{\step+1} A - A^\top P_{\step+1} B (R + B^\top P_{\step+1} B)^{-1} B^\top P_{\step+1} A,
$$ {#eq-riccati}

where $A \in \mathbb{R}^{n_\st \times n_\st}$ and $B \in \mathbb{R}^{n_\st \times n_\act}$
are the matrices used to define the dynamics $f$
and $Q \in \mathbb{R}^{n_\st \times n_\st}$ and $R \in \mathbb{R}^{n_\act \times n_\act}$
are the matrices used to define the cost function $c$ (@def-lqr).
:::

The scalars $p_\step$ obey the recurrence relation

$$
p_\step = \mathrm{Tr}(\sigma^2 P_{\step+1}) + p_{\step+1}
$$ {#eq-lqr-proof-scalar}

and the matrices $K_0, \dots, K_{\Step-1}$ for defining the policy satisfy

$$
K_\step = (R + B^\top P_{\step+1} B)^{-1} B^\top P_{\step+1} A.
$$ {#eq-k-pi}

By setting $P_\Step = 0$ and $p_\Step = 0$,

```{python}
def dp_lqr(A, B, Q, R, sigma, H):
    n_x, n_u = B.shape
    P = jnp.zeros(n_x, n_x)
    p = 0
    P_ary, p_ary = [], []
    for h in range(H):
        P = Q + A.T @ P @ A - A.T @ P @ B @ np.inv(R + B.T @ P @ B) @ B.T @ P @ A
        p = sigma ** 2 * np.trace(P) + p
        P_ary.append(P)
        p_ary.append(p)
    return reversed(P_ary), reversed(p)

latex(dp_lqr)
```

### Expected state at time $\step$

Let's consider how the
state at time $\step$ behaves when we act according to this optimal policy.
In the field of control theory,
which is often used in high-stakes circumstances such as aviation or architecture,
it is important to understand the way the state evolves
under the proposed controls.

How can we compute the expected state at time $\step$ when acting
according to the optimal policy? Let's first express $\st_\step$ in a
cleaner way in terms of the history. Note that having linear dynamics
makes it easy to expand terms backwards in time: 

$$
\begin{aligned}
    \st_\step & = A \st_{\step-1} + B \act_{\step-1} + w_{\step-1}                                 \\
            & = A (A\st_{\step-2} + B \act_{\step-2} + w_{\step-2}) + B \act_{\step-1} + w_{\step-1} \\
            & = \cdots                                                                     \\
            & = A^\step \st_0 + \sum_{i=0}^{\step-1} A^i (B \act_{\step-i-1} + w_{\step-i-1}).
\end{aligned}
$$ {#eq-expected-state-derive}

Let's consider the *average state* at this time, given all the past
states and actions. Since we assume that $\E [w_\step] = 0$ (this is the
zero vector in $d$ dimensions), when we take an expectation, the $w_\step$
term vanishes due to linearity, and so we're left with

$$
\E [\st_\step \mid \st_{0:(\step-1)}, \act_{0:(\step-1)}] = A^\step \st_0 + \sum_{i=0}^{\step-1} A^i B \act_{\step-i-1}.
$$ {#eq-expected-state}


::: {#exr-expected-state}
#### Expected state

Show that if we choose actions according to the optimal policy @lem-pi-linear, @eq-expected-state becomes

$$
\E [\st_\step \mid \st_0, \act_i = \pi^\star_i(\st_i)\quad \forall i \le \step] = \left( \prod_{i=0}^{\step-1} (A - B K_i) \right) \st_0.
$$
:::

This introdces the quantity $A - B K_i$, which shows up frequently in
control theory. For example, one important question is: will $\st_\step$
remain bounded, or will it go to infinity as time goes on? To answer
this, let's imagine for simplicity that these $K_i$s are equal (call
this matrix $K$). Then the expression above becomes $(A-BK)^\step \st_0$.
Now consider the maximum eigenvalue $\lambda_{\max}$ of $A - BK$. If
$|\lambda_{\max}| > 1$, then there's some nonzero initial state
$\bar \st_0$, the corresponding eigenvector, for which


$$
\lim_{\step \to \infty} (A - BK)^\step \bar \st_0
    = \lim_{\step \to \infty} \lambda_{\max}^\step \bar \st_0
    = \infty.
$$
    
Otherwise, if $|\lambda_{\max}| < 1$, then it's impossible for your original state to explode as dramatically.

## Extensions

We've now formulated an optimal solution for the time-homogeneous LQR
and computed the expected state under the optimal policy.
However, real world tasks rarely have such simple dynamics,
and we may wish to design more complex cost functions.
In this section, we'll consider more general extensions of LQR where some of the assumptions we made above
are relaxed. Specifically, we'll consider:

1.  **Time-dependency**, where the dynamics and cost function might
    change depending on the timestep.

2.  **General quadratic cost**, where we allow for linear terms and a
    constant term.

3.  **Tracking a goal trajectory** rather than aiming for a single goal
    state-action pair.

Combining these will allow us to use the LQR solution to solve more
complex setups by taking *Taylor approximations* of the dynamics and
cost functions.

### Time-dependent dynamics and cost function {#sec-time-dep-lqr}

So far, we've considered the *time-homogeneous* case,
where the dynamics and cost function stay the same at every timestep.
However, this might not always be the case.
As an example, in many sports,
the rules and scoring system might change during an overtime period.
To address such tasks,
we can loosen the time-homogeneous restriction,
and consider the case where the dynamics and cost function are *time-dependent.*
Our analysis remains almost identical;
in fact, we can simply add a time index to the matrices $A$ and $B$
that determine the dynamics
and the matrices $Q$ and $R$ that determine the cost.

The modified problem is now defined as follows:

::: {#def-time-dependent-lqr}
#### Time-dependent LQR

$$
\begin{aligned}
        \min_{\pi_{0}, \dots, \pi_{\Step-1}} \quad & \E \left[ \left( \sum_{\step=0}^{\Step-1} (\st_\step^\top Q_\step \st_\step) + \act_\step^\top R_\step \act_\step \right) + \st_\Step^\top Q_\Step \st_\Step \right] \\
        \textrm{where} \quad                      & \st_{\step+1} = f_\step(\st_\step, \act_\step, w_\step) = A_\step \st_\step + B_\step \act_\step + w_\step                                                             \\
                                                  & \st_0 \sim \mu_0                                                                                                                                   \\
                                                  & \act_\step = \pi_\step (\st_\step)                                                                                                                       \\
                                                  & w_\step \sim \mathcal{N}(0, \sigma^2 I).
\end{aligned}
$$


:::

The derivation of the optimal value functions and the optimal policy
remains almost exactly the same, and we can modify the Riccati equation
accordingly:

::: {#def-riccati-time-dependent}
#### Time-dependent Riccati Equation

$$
P_\step = Q_\step + A_\step^\top P_{\step+1} A_\step - A_\step^\top P_{\step+1} B_\step (R_\step + B_\step^\top P_{\step+1} B_\step)^{-1} B_\step^\top P_{\step+1} A_\step.
$$

Note that this is just the time-homogeneous Riccati equation
(@def-riccati), but with the time index added to each of the
relevant matrices.
:::

::: {#exr-time-dependent}
#### Time dependent LQR proof

Walk through the proof in @sec-optimal-lqr to verify that we can simply add $\step$ for the time-dependent case.
:::

Additionally, by allowing the dynamics to vary across time, we gain the
ability to *locally approximate* nonlinear dynamics at each timestep.
We'll discuss this later in the chapter.

### More general quadratic cost functions

Our original cost function had only second-order terms with respect to
the state and action, incentivizing staying as close as possible to
$(\st^\star, \act^\star) = (0, 0)$. We can also consider more general
quadratic cost functions that also have first-order terms and a constant
term. Combining this with time-dependent dynamics results in the
following expression, where we introduce a new matrix $M_\step$ for the
cross term, linear coefficients $q_\step$ and $r_\step$ for the state and
action respectively, and a constant term $c_\step$:

$$
c_\step(\st_\step, \act_\step) = ( \st_\step^\top Q_\step \st_\step + \st_\step^\top M_\step \act_\step + \act_\step^\top R_\step \act_\step ) + (\st_\step^\top q_\step + \act_\step^\top r_\step) + c_\step.
$$ {#eq-general-quadratic-cost}

Similarly, we can also include a
constant term $v_\step \in \mathbb{R}^{n_\st}$ in the dynamics (note that this is
*deterministic* at each timestep, unlike the stochastic noise $w_\step$):


$$
\st_{\step+1} = f_\step(\st_\step, \act_\step, w_\step) = A_\step \st_\step + B_\step \act_\step + v_\step + w_\step.
$$



::: {#exr-general-lqr}

#### General cost function

Derive the optimal solution. You will need to slightly modify the
proof in @sec-optimal-lqr.
:::

### Tracking a predefined trajectory

Consider applying LQR to a task like autonomous driving, where the
target state-action pair changes over time. We might want the vehicle to
follow a predefined *trajectory* of states and actions
$(\st_\step^\star, \act_\step^\star)_{\step=0}^{\Step-1}$. To express this as a
control problem, we'll need a corresponding time-dependent cost
function:


$$
c_\step(\st_\step, \act_\step) = (\st_\step - \st^\star_\step)^\top Q (\st_\step - \st^\star_\step) + (\act_\step - \act^\star_\step)^\top R (\act_\step - \act^\star_\step).
$$


Note that this punishes states and actions that are far from the
intended trajectory. By expanding out these multiplications, we can see
that this is actually a special case of the more general quadratic cost
function above @eq-general-quadratic-cost:


$$
M_\step = 0, \qquad q_\step = -2Q \st^\star_\step, \qquad r_\step = -2R \act^\star_\step, \qquad c_\step = (\st^\star_\step)^\top Q (\st^\star_\step) + (\act^\star_\step)^\top R (\act^\star_\step).
$$



## Approximating nonlinear dynamics {#sec-approx-nonlinear}

The LQR algorithm solves for the optimal policy when the dynamics are
*linear* and the cost function is an *upward-curved quadratic*.
However, real settings are rarely this simple!
Let's return to the CartPole example from the start of the chapter (@exm-cart-pole).
The dynamics (physics) aren't linear.
How can we approximate this by an LQR problem?

Concretely, let's consider a *noise-free* problem since,
as we saw, the noise doesn't factor into the optimal policy.
Let's assume the dynamics and cost function are stationary,
and ignore the terminal state for simplicity:

::: {#def-nonlinear-control}
#### Nonlinear control problem


$$
\begin{aligned}
        \min_{\pi_0, \dots, \pi_{\Step-1} : \mathcal{S} \to \mathcal{A}} \quad
        & \E_{\st_0 \sim P_0} \left[
            \sum_{\step=0}^{\Step-1} c(\st_\step, \act_\step)
        \right] \\
        \text{where} \quad                                  & \st_{\step+1} = f(\st_\step, \act_\step)                                   \\
                                                            & \act_\step = \pi_\step(\st_\step)                                          \\
                                                            & \st_0 \sim \mu_0                                                     \\
                                                            & c(\st, \act) = d(\st, \st^\star) + d(\act, \act^\star).
\end{aligned}
$$

Here, $d$ denotes a function
that measures the "distance" between its two arguments.
:::

This is now only slightly simplified
from the general optimal control problem
(see @def-optimal-control).
Here, we don't know an analytical form
for the dynamics $f$ or the cost function $c$,
but we assume that we're able to *query/sample/simulate* them
to get their values at a given state and action.
To clarify, consider the case where the dynamics are given by real world physics.
We can't (yet) write down an expression for the dynamics that we can differentiate or integrate analytically.
However, we can still *simulate* the dynamics and cost function
by running a real-world experiment and measuring the resulting states and costs.
How can we adapt LQR to this more general nonlinear case?

### Local linearization

How can we apply LQR when the dynamics are nonlinear or the cost function is more complex?
We'll exploit the useful fact that we can take a function that's *locally continuous* around $(s^\star, a^\star)$ and
approximate it nearby with low-order polynomials (i.e. its Taylor
approximation). In particular, as long as the dynamics $f$ are
differentiable around $(\st^\star, \act^\star)$ and the cost function
$c$ is twice differentiable at $(\st^\star, \act^\star)$, we can take a
linear approximation of $f$ and a quadratic approximation of $c$ to
bring us back to the regime of LQR.

Linearizing the dynamics around $(\st^\star, \act^\star)$ gives:

$$
\begin{gathered}
    f(\st, \act) \approx f(\st^\star, \act^\star) + \nabla_\st f(\st^\star, \act^\star) (\st - \st^\star) + \nabla_\act f(\st^\star, \act^\star) (\act - \act^\star) \\
    (\nabla_\st f(\st, \act))_{ij} = \frac{d f_i(\st, \act)}{d \st_j}, \quad i, j \le n_\st \qquad (\nabla_\act f(\st, \act))_{ij} = \frac{d f_i(\st, \act)}{d \act_j}, \quad i \le n_\st, j \le n_\act
\end{gathered}
$$

and quadratizing the cost function around
$(\st^\star, \act^\star)$ gives: 

$$
\begin{aligned}
    c(\st, \act) & \approx c(\st^\star, \act^\star) \quad \text{constant term}                                                                                      \\
                 & \qquad + \nabla_\st c(\st^\star, \act^\star) (\st - \st^\star) + \nabla_\act c(\st^\star, \act^\star) (a - \act^\star) \quad \text{linear terms} \\
                 & \left. \begin{aligned}
                               & \qquad + \frac{1}{2} (\st - \st^\star)^\top \nabla_{\st \st} c(\st^\star, \act^\star) (\st - \st^\star)       \\
                               & \qquad + \frac{1}{2} (\act - \act^\star)^\top \nabla_{\act \act} c(\st^\star, \act^\star) (\act - \act^\star) \\
                               & \qquad + (\st - \st^\star)^\top \nabla_{\st \act} c(\st^\star, \act^\star) (\act - \act^\star)
                          \end{aligned} \right\} \text{quadratic terms}
\end{aligned}
$$

where the gradients and Hessians are defined as

$$
\begin{aligned}
    (\nabla_\st c(\st, \act))_{i}         & = \frac{d c(\st, \act)}{d \st_i}, \quad i \le n_\st
                                          & (\nabla_\act c(\st, \act))_{i}                                               & = \frac{d c(\st, \act)}{d \act_i}, \quad i \le n_\act               \\
    (\nabla_{\st \st} c(\st, \act))_{ij}  & = \frac{d^2 c(\st, \act)}{d \st_i d \st_j}, \quad i, j \le n_\st
                                          & (\nabla_{\act \act} c(\st, \act))_{ij}                                       & = \frac{d^2 c(\st, \act)}{d \act_i d \act_j}, \quad i, j \le n_\act \\
    (\nabla_{\st \act} c(\st, \act))_{ij} & = \frac{d^2 c(\st, \act)}{d \st_i d \act_j}. \quad i \le n_\st, j \le n_\act
\end{aligned}
$$

::: {#exr-quadratic}
#### Expressing as quadratic
Note that this cost can be expressed in the general quadratic form seen in @eq-general-quadratic-cost.
Derive the corresponding quantities $Q, R, M, q, r, c$.
:::

### Finite differencing

To calculate these gradients and Hessians in practice,
we use a method known as **finite differencing** for numerically computing derivatives.
Namely, we can simply use the limit definition of the derivative, and
see how the function changes as we add or subtract a tiny $\delta$ to
the input.


$$
\frac{d}{dx} f(x) = \lim_{\delta \to 0} \frac{f(x + \delta) - f(x)}{\delta}
$$


Note that this only requires us to be able to *query* the function, not
to have an analytical expression for it, which is why it's so useful in
practice.

### Local convexification

However, simply taking the second-order approximation of the cost
function is insufficient, since for the LQR setup we required that the
$Q$ and $R$ matrices were positive definite, i.e. that all of their
eigenvalues were positive.

One way to naively *force* some symmetric matrix $D$ to be positive definite
is to set any non-positive eigenvalues to some small positive value $\varepsilon > 0$.
Recall that any real symmetric matrix $D \in \mathbb{R}^{n \times n}$ has an basis of eigenvectors $u_1, \dots, u_n$
with corresponding eigenvalues $\lambda_1, \dots, \lambda_n$
such that $D u_i = \lambda_i u_i$.
Then we can construct the positive definite approximation by

$$
\widetilde{D} = \left( \sum_{i=1, \dots, n \mid \lambda_i > 0} \lambda_i u_i u_i^\top \right) + \varepsilon I.
$$



**Exercise:** Convince yourself that $\widetilde{D}$ is indeed positive
definite.

Note that Hessian matrices are generally symmetric, so we can apply this
process to $Q$ and $R$ to obtain the positive definite approximations
$\widetilde{Q}$ and $\widetilde{R}$.
Now that we have an upward-curved
quadratic approximation to the cost function, and a linear approximation
to the state transitions, we can simply apply the time-homogenous LQR
methods from @sec-optimal-lqr.

But what happens when we enter states far away from $\st^\star$ or want
to use actions far from $\act^\star$? A Taylor approximation is only
accurate in a *local* region around the point of linearization, so the
performance of our LQR controller will degrade as we move further away.
We'll see how to address this in the next section using the **iterative LQR** algorithm.

![Local linearization might only be accurate in a small region around the
point of linearization](shared/log_taylor.png)

### Iterative LQR {#sec-iterative-lqr}

To address these issues with local linearization, we'll use an iterative
approach, where we repeatedly linearize around different points to
create a *time-dependent* approximation of the dynamics, and then solve
the resulting time-dependent LQR problem to obtain a better policy. This
is known as **iterative LQR** or **iLQR**:

::: {#def-ilqr}
#### Iterative LQR

For each iteration of the algorithm:

1. Form a time-dependent LQR problem around the current candidate
trajectory using local linearization.
2. Compute the optimal policy using @sec-time-dep-lqr.
3. Generate a new series of actions using this policy.
4. Compute a better candidate trajectory by interpolating between the
current and proposed actions.
:::

Now let's go through the details of each step. We'll use superscripts to
denote the iteration of the algorithm. We'll also denote
$\bar \st_0 = \E_{\st_0 \sim \mu_0} [\st_0]$ as the expected initial
state.

At iteration $i$ of the algorithm, we begin with a **candidate**
trajectory
$\bar \tau^i = (\bar \st^i_0, \bar \act^i_0, \dots, \bar \st^i_{\Step-1}, \bar \act^i_{\Step-1})$.

**Step 1: Form a time-dependent LQR problem.** At each timestep
$\step \in [\Step]$, we use the techniques from
[](#approx_nonlinear) to linearize the dynamics and
quadratize the cost function around $(\bar \st^i_\step, \bar \act^i_\step)$:


$$
\begin{aligned}
    f_\step(\st, \act) & \approx f(\bar {\st}^i_\step, \bar {\act}^i_\step) + \nabla_{\st } f(\bar {\st}^i_\step, \bar {\act}^i_\step)(\st - \bar {\st}^i_\step) + \nabla_{\act } f(\bar {\st}^i_\step, \bar {\act}^i_\step)(\act - \bar {\act}^i_\step)                         \\
    c_\step(\st, \act) & \approx c(\bar {\st}^i_\step, \bar {\act}^i_\step) + \begin{bmatrix}
                                                              \st - \bar {\st }^i_\step& \act - \bar {\act}^i_\step
                                                          \end{bmatrix} \begin{bmatrix}
                                                                            \nabla_{\st } c(\bar {\st}^i_\step, \bar {\act}^i_\step)\\
                                                                            \nabla_{\act} c(\bar {\st}^i_\step, \bar {\act}^i_\step)
                                                                        \end{bmatrix}                                                      \\
                     & \qquad + \frac{1}{2} \begin{bmatrix}
                                                \st - \bar {\st }^i_\step& \act - \bar {\act}^i_\step
                                            \end{bmatrix} \begin{bmatrix}
                                                              \nabla_{\st \st} c(\bar {\st}^i_\step, \bar {\act}^i_\step)  & \nabla_{\st \act} c(\bar {\st}^i_\step, \bar {\act}^i_\step)  \\
                                                              \nabla_{\act \st} c(\bar {\st}^i_\step, \bar {\act}^i_\step) & \nabla_{\act \act} c(\bar {\st}^i_\step, \bar {\act}^i_\step)
                                                          \end{bmatrix}
    \begin{bmatrix}
        \st - \bar {\st }^i_\step\\
        \act - \bar {\act}^i_\step
    \end{bmatrix}.
\end{aligned}
$$



**Step 2: Compute the optimal policy.** We can now solve the
time-dependent LQR problem using the Riccati equation from
@sec-time-dep-lqr to compute the optimal policy
$\pi^i_0, \dots, \pi^i_{\Step-1}$.

**Step 3: Generate a new series of actions.** We can then generate a new
sample trajectory by taking actions according to this optimal policy:


$$
\bar \st^{i+1}_0 = \bar \st_0, \qquad \widetilde \act_\step = \pi^i_\step(\bar \st^{i+1}_\step), \qquad \bar \st^{i+1}_{\step+1} = f(\bar \st^{i+1}_\step, \widetilde \act_\step).
$$


Note that the states are sampled according to the *true* dynamics, which
we assume we have query access to.

**Step 4: Compute a better candidate trajectory.**, Note that we've
denoted these actions as $\widetilde \act_\step$ and aren't directly using
them for the next iteration $\bar \act^{i+1}_\step$. Rather, we want to
*interpolate* between them and the actions from the previous iteration
$\bar \act^i_0, \dots, \bar \act^i_{\Step-1}$. This is so that the cost
will *increase monotonically,* since if the new policy turns out to
actually be worse, we can stay closer to the previous trajectory. (Can
you think of an intuitive example where this might happen?)

Formally, we want to find $\alpha \in [0, 1]$ to generate the next
iteration of actions
$\bar \act^{i+1}_0, \dots, \bar \act^{i+1}_{\Step-1}$ such that the cost
is minimized: 

$$
\begin{aligned}
    \min_{\alpha \in [0, 1]} \quad & \sum_{\step=0}^{\Step-1} c(\st_\step, \bar \act^{i+1}_\step)                     \\
    \text{where} \quad             & \st_{\step+1} = f(\st_\step, \bar \act^{i+1}_\step)                             \\
                                   & \bar \act^{i+1}_\step = \alpha \bar \act^i_\step + (1-\alpha) \widetilde \act_\step \\
                                   & \st_0 = \bar \st_0.
\end{aligned}
$$

 Note that this optimizes over the closed interval
$[0, 1]$, so by the Extreme Value Theorem, it's guaranteed to have a
global maximum.

The final output of this algorithm is a policy $\pi^{n_\text{steps}}$
derived after $n_\text{steps}$ of the algorithm. Though the proof is
somewhat complex, one can show that for many nonlinear control problems,
this solution converges to a locally optimal solution (in the policy
space).

## Key takeaways

This chapter introduced some approaches to solving different variants of
the optimal control problem
@def-optimal-control. We began with the simple case of linear
dynamics and an upward-curved quadratic cost. This model is called the
LQR and we solved for the optimal policy using dynamic programming. We
then extended these results to the more general nonlinear case via local
linearization. We finally saw the iterative LQR algorithm for solving
nonlinear control problems.


::: {.content-visible when-profile="thesis"}
## Differences from course

This chapter is presented 
:::
