{{< include macros.qmd >}}

# Policy Gradient Methods {#sec-pg}

## Introduction

The core task of RL is finding the **optimal policy** in a given environment.
This is essentially an *optimization problem*
(i.e. computing a minimum or maximum):
out of some class of policies $\Pi$,
we want to find the one that achieves the maximum expected total reward:

$$
\hat \pi = \arg\max_{\pi \in \Pi} \E_{\tau \sim \rho^\pi} [R(\tau)] \quad \text{where} \quad R(\tau) := \sum_{\hi=0}^{\hor-1} r(s_\hi, a_\hi).
$$ {#eq-policy-optimization-goal}

In a known environment with a finite set of states and actions,
we can compute the optimal policy
using dynamic programming in $O( H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)$ steps (@sec-finite-opt-dp).
In more general settings, though,
such as when the environment is _unknown_,
it is typically intractable to compute the optimal policy exactly.
Instead, we start from some random policy,
and then iteratively *improve* it by interacting with the environment.
Policy iteration (@sec-pi) and iterative LQR (@sec-iterative-lqr)
are two algorithms we've seen that do this.

In this chapter,
we will explore _policy gradient algorithms,_
which also iteratively improve a policy.
The _gradient_ of a function at a specific input point
is the direction that increases the output value most rapidly.
If we think of the expected total reward as a function of the policy,
we can repeatedly shift the policy by the gradient of that function
to obtain policies that achieve higher expected total rewards.

| Algorithm                                                   | State space                              | Action space                              | Policies                      | Knowledge of environment |
| ----------------------------------------------------------- | ---------------------------------------- | ----------------------------------------- | ----------------------------- | ------------------------ |
| dynamic programming (@sec-finite-opt-dp)                    | finite | finite | deterministic                 | known                    |
| policy iteration (@sec-pi)                                  | finite | finite | deterministic                 | known                    |
| linear quadratic regulator (@sec-lqr)                       | $\R^{n_\st}$                             | $\R^{n_\act}$                             | deterministic                 | known                    |
| (iterative) linear quadratic regulator (@sec-iterative-lqr) | $\R^{n_\st}$                             | $\R^{n_\act}$                             | deterministic                 | unknown                  |
| upper confidence bound (@sec-ucb)                           | none                                     | $\{ 1, \dots, K \}$                       | deterministic or stochastic   | unknown                  |
| Thompson sampling (@sec-thompson-sampling)                  | none                                     | $\{ 1, \dots, K \}$                       | deterministic or stochastic   | unknown                  |
| fitted policy iteration (@sec-fit-pi)                       | arbitrary                                | $\{ 1, \dots, \vert \mathcal{A} \vert \}$ | deterministic                 | unknown                  |
| Q-learning (@sec-q-learning)                                | arbitrary                                | $\{ 1, \dots, \vert \mathcal{A} \vert \}$ | deterministic                 | unknown                  |
| REINFORCE (@sec-reinforce)                                  | arbitrary                                | arbitrary                                 | stochastic (or deterministic) | unknown                  |
| Natural policy gradient (@sec-npg)                          | arbitrary                                | arbitrary                                 | stochastic (or deterministic) | unknown                  |

: An overview of problem settings discussed so far. {#tbl-overview-chapters-pg}

Policy gradient methods are responsible for groundbreaking applications including AlphaGo,
OpenAI Five,
and large language models.
This chapter will explore:

1. Examples of _parameterized policies_
   that can be expressed in terms of a finite set of _parameters._
1. _Gradient descent,_
   a general optimization algorithm for differentiable functions.
1. Different estimators of the _policy gradient,_
   enabling us to apply (stochastic) gradient descent to RL.
1. *Proximal (policy) optimization* techniques
   that ensure the steps taken are "not too large".
   These are some of the most popular and widely used RL algorithms at the time of writing.

::: {#rem-conventions}
#### Conventions

Policy gradient algorithms typically optimize over _stochastic policies_ (@def-policy).
In this chapter,
we will only discuss the case of stochastic policies,
though there exist policy gradient algorithms
for deterministic policies as well [@silver_deterministic_2014].

To ease notation,
we will treat the horizon $H$ as finite
and avoid adding a discount factor $\gamma$.
We make the policy's dependence on the timestep $h$ implicit
by assuming that the state space conveys information about the timestep.
We also use

$$
R(\tau) := \sum_{\hi=0}^{\hor-1} r_\hi
$$ {#eq-total-reward}

to denote the total reward in a trajectory.
:::

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
from utils import Array, Float, Callable, jax, jnp, rng, latex, gym
from jaxtyping import UInt
import jax.random as jr
import functools as ft
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from scipy.optimize import minimize

rng = rng(184)
```

## Parameterized policies {#sec-parameterizations}

Optimizing over the entire space of policies is usually intractable:
there's just too many!
There are there are $|\mathcal{A}|^{|\mathcal{S}|}$ deterministic mappings
from state to actions.
In continuous state spaces,
$|\mathcal{S}|$ is infinite,
so the space of policies becomes _infinite-dimensional_.
Infinite-dimensional spaces are usually hard to optimize over.
Instead, we choose a _parameterized policy class_ with a finite number $D$ of adjustable parameters.
Each parameter vector corresponds to a mapping from states to actions.
(We also discussed the notion of a parameterized function class in @sec-sl-parameterized).
The following examples seek to make this more concrete.

::: {#exm-tabular-repr}
#### Tabular representation

If both the state and action spaces are finite,
perhaps we could simply learn a preference value $\theta_{s,a}$ for each state-action pair,
resulting in a table of $|\mathcal{S}| |\mathcal{A}|$ values.
These are the $D$ _parameters_ of the policy.
Then, for each state,
to compute the distribution over actions,
we perform a **softmax** operation:
we exponentiate each of the values,
and then normalize to form a valid distribution.

$$
\pi^\text{softmax}_\theta(a \mid s) = \frac{\exp(\theta_{s,a})}{\sum_{s,a'} \exp (\theta_{s,a'})}.
$$ {#eq-parameterized-tabular}

In this way, we turn a vector of length $|\mathcal{S}| |\mathcal{A}|$
into a mapping from $\mathcal{S} \to \triangle(\mathcal{A})$.
However, this doesn't make use of any structure in the states or actions,
so while this is flexible, it is also prone to overfitting.
For small state and action spaces,
it makes more sense to use a dynamic programming algorithm from @sec-mdps.
:::

For a given _parameterization,_ such as the one above,
we call the set of resulting policies the **parameterized policy class,**
that is,

$$
\{ \pi_\theta \mid \theta \in \R^D \},
$$ {#eq-parameterized-class}

where $D$ is the fixed number of parameters.
Let us explore some more examples of parameterized policy classes.

::: {#exm-linear-in-features}
#### Linear in features

Another approach is to map each state-action pair into some **feature space** $\phi(s, a) \in \mathbb{R}^D$.
Then, to map a feature vector to a probability,
we take a linear combination of the features and take a softmax.
The parameters $\theta$ are the coefficients of the linear combination:

$$
\pi^\text{linear}_{\theta}(a \mid s) = \frac{\exp(\theta^\top \phi(s, a))}{\sum_{a'} \exp(\theta^\top \phi(s, a'))}.
$$ {#eq-parameterized-linear}

Another interpretation is that $\theta$ represents the feature vector of the "desired" state-action pair,
as state-action pairs whose features align closely with $\theta$ are given higher probability.
:::

::: {#exm-neural-policy}
#### Neural policies

More generally,
we could map states and actions to unnormalized scores
via some parameterized function $f_\theta : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$,
such as a neural network, and choose actions according to a softmax:

$$
\pi^\text{neural}_\theta(a \mid s) = \frac{\exp(f_{\theta}(s, a))}{\sum_{a'} \exp(f_{\theta}(s, a'))}.
$$ {#eq-parameterized-neural}
:::

::: {#rem-why-softmax}
#### Why softmax

The three parameterized policy classes above all use the _softmax_ operation.
Of course, this isn't the only way to turn a list of values
into a probability distribution:
for example, you could also subtract by the minimum value
and divide by the range.
Why is softmax preferred in practice?
One reason is that it is a _smooth, differentiable_ function of the input values.
It is also nice to work with analytically,
and has other interpretations from physics,
where it is known as the _Gibbs_ or _Boltzmann_ distribution,
and economics,
where it is known as the _Bradley-Terry model_ for ranking choices.
:::

::: {#exm-gaussian-policy}
#### Diagonal Gaussian policies for continuous action spaces

Consider an $N$-dimensional action space $\mathcal{A} = \mathbb{R}^{N}$.
Then for a stochastic policy,
we could predict the *mean* action
and then add some random noise to it.
For example,
we could use a linear model to predict the mean action
and then add some noise $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ to it.


$$
\pi_\theta(\cdot \mid s) = \mathcal{N}(\theta^\top \phi(s), \sigma^2 I).
$$
:::

Now we've seen some examples of _parameterized policies._
Optimizing over a parameterized policy class
makes the policy optimization problem
finite-dimensional:

$$
\begin{aligned}
\hat \theta &= \arg\max_{\theta \in \R^D} J(\theta) \\
\text{where}\quad J(\theta) &:= \E_{\tau \sim \rho^{\pi_\theta}} \sum_{\hi=0}^{\hor-1} r(s_\hi, a_\hi).
\end{aligned}
$$ {#eq-policy-optimization-parameters}

This enables us to apply one of the most popular and general optimization algorithms:
_gradient descent_.

## Gradient descent {#sec-gd}

**Gradient descent** is an optimization algorithm
that can be applied to any differentiable function.
That is, it is a tool for solving

$$
\arg\min_{\theta \in \R^D} J(\theta),
$$ {#eq-opt-min}

where $J(\theta) \in \R$ is the function to be minimized.
For two-dimensional inputs,
a suitable analogy for this algorithm is making your way down a hill (with no cliffs),
where you keep taking steps in the steepest direction downwards from your current position.
Your horizontal position $(x, z)$ is the input
and your vertical position $y$ is the function to be minimized.
The *slope* of the mountain at your current position
can be expressed using the *gradient*,
written $\nabla y(x, z) \in \mathbb{R}^2$.
This can be computed as the vector of partial derivatives,

$$
\nabla y(x, z) = \begin{pmatrix}
\frac{\partial y}{\partial x} \\
\frac{\partial y}{\partial z}
\end{pmatrix}.
$$ {#eq-grad-partial}

```{python}
#| label: fig-exm-grad-descent
#| fig-cap: Example of gradient descent on the Rosenbrock function.

def f(x, y):
    return 100.0 * (y - x**2)**2 + (1 - x)**2

# Create a grid of points
X, Y = jnp.mgrid[0:1:40j, 0:1:40j]
Z = f(X, Y)

fig, ax = plt.subplots(figsize=(6, 4))
contourf = ax.contourf(X, Y, Z, levels=20, alpha=0.6)
fig.colorbar(contourf, ax=ax)

tx, ty = 0.5, 0.8
gx, gy = -0.001 * jnp.asarray(jax.grad(f, argnums=(0, 1))(tx, ty))

ax.arrow(tx, ty, gx, gy, fc='blue', ec='blue', width=0.01, head_width=0.05, head_length=0.1, zorder=5)
ax.scatter(tx, ty, color='red', s=100, zorder=5)

ax.set_xlabel(r"$x$")
ax.set_ylabel(r"$y$")

plt.show()
```

To calculate the *slope* (aka "directional derivative") of the mountain in a given direction $(\Delta x, \Delta z)$,
you take the dot product of the difference vector with the gradient:

$$
\Delta y = \begin{pmatrix}
\Delta x \\ \Delta z
\end{pmatrix}
\cdot
\nabla y(x, z),
$$ {#eq-grad-dot}

where $x, z$ is your current position.
What direction should we walk to go down the hill as quickly as possible?
That is, we want to find the direction $(\Delta x, \Delta z)$ that minimizes the slope:

$$
\arg\min_{\Delta x, \Delta z} \Delta y.
$$ {#eq-grad-argmin}

We use the useful fact that,
for a given vector $v$,
the direction $u$ that minimizes the dot product $u \cdot v$
points in the opposite direction to $v$.
This should make intuitive sense,
since $u \cdot v = \|u\| \|v\| \cos \theta$,
where $\theta$ is the angle between $u$ and $v$,
and the cosine function is minimized at $\theta = \pi$.
Applying this to @eq-grad-dot,
we see that the direction of steepest decrease is opposite to the gradient.
"Walking" in that direction
corresponds to subtracting a multiple of the gradient from your current position:

$$
\begin{pmatrix}
x^{t+1} \\ z^{t+1}
\end{pmatrix}
= 
\begin{pmatrix}
x^{t} \\ z^{t}
\end{pmatrix}
-
\eta \nabla y(x^{t}, z^{t})
$$

where $t$ denotes the iteration of the algorithm
and $\eta > 0$ is a "step size" hyperparameter that controls the size of the steps we take.
(Note that we could also vary the step size across iterations,
that is, $\eta^0, \dots, \eta^T$.)

::: {#rem-ascent-vs-descent}
#### Minimization vs maximization

Optimization problems are usually posed as *minimization* problems by convention.
To solve a *maximization* problem with gradient descent,
you could simply take steps in the direction of the _positive_ gradient.
This also corresponds to flipping the sign of the objective function.
:::

The case of a two-dimensional input is easy to visualize.
The analogy to climbing a hill (if we were maximizing the function)
is why gradient descent is sometimes called **hill climbing**
and the graph of the objective function is called the **loss landscape**.
(The term "loss" comes from supervised learning,
which you can read about in @sec-sl.)
But this idea can be straightforwardly extended to higher-dimensional inputs.
From now on,
we'll use $J$ to denote the function we're trying to maximize,
and $\theta$ to denote the parameters being optimized over.
(In the above example, $\theta = \begin{pmatrix} x & z \end{pmatrix}^\top$).
Let's summarize the algorithm in general:

::: {#def-grad-descent}
#### Gradient descent

Suppose we are trying to solve the optimization problem

$$
\theta^\star = \arg\min_{\theta \in \R^D} J(\theta),
$$ {#eq-opt}

where $J(\theta) \in \R$ is the differentiable function to be minimized.
Gradient descent starts with an initial guess $\theta^0$
and then takes steps in the direction of $- \nabla J(\theta^t)$,
where $\theta^t$ is the current iterate:

$$
\theta^{t+1} = \theta^t - \eta \nabla J(\theta^t).
$$ {#eq-grad-descent}

Note that we scale $\nabla J(\theta^t)$ by the **step size** $\eta > 0$,
also known as the learning rate.
:::

```{python}
#| label: fig-grad-descent
#| fig-cap: Pseudocode for gradient descent.

def gradient_descent(
    theta_init: Float[Array, " D"],
    objective: Callable[[Float[Array, " D"]], float],
    eta: float,
    n_steps: UInt,
):
    # `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.
    theta = theta_init
    history = [theta]
    for step in range(n_steps):
        theta = theta + eta * jax.grad(objective)(theta)
        history.append(theta)
    return theta, jnp.array(history)

latex(gradient_descent, id_to_latex={"objective": "J"})
```

Notice that the parameters will stop changing once $\nabla J(\theta) = 0$.
Once we reach this **stationary point,**
our current parameters are 'locally optimal'
(assuming we're at a local minimum):
it's impossible to increase the function by moving in any direction.
If $J$ is *convex*
(i.e. the function looks like an upward-curved bowl),
then the only point where this happens is at the *global optimum.*
Otherwise, if $J$ is nonconvex,
the best we can hope for is a *local optimum.*
We won't go deeper into the theory of convex functions here.
For more details, refer to the textbook of @boyd_convex_2004.

::: {#rem-sgd-limitations}
#### Limitations of gradient descent

Gradient descent and its variants are responsible for most of the major achievements in modern machine learning.
It is important to note,
however, that for many problems,
gradient descent is _not_ a good optimization algorithm to reach for!
If you have more information about the problem,
such as access to _second_ derivatives,
you can apply more powerful optimization algorithms
that converge much more rapidly.
Read @nocedal_numerical_2006
if you're curious about the field of numerical optimization.
:::

### Computing derivatives {#sec-computing-derivatives}

How does a computer compute the gradient of a function?

One way is **symbolic differentiation,**
which is similar to the way you might compute it by hand:
the computer applies a list of rules to transform the *symbols* involved.
Python's `sympy` package [@meurer_sympy_2017] supports symbolic differentiation.
However, functions implemented as algorithms in code
may not always have a straightforward symbolic representation.

Another way is **numerical differentiation,**
which is based on the limit definition of a (directional) derivative:

$$
\nabla_{\boldsymbol{u}} J(\boldsymbol{x}) = \lim_{\varepsilon \to 0}
\frac{J(\boldsymbol{x} + \varepsilon \boldsymbol{u}) - J(\boldsymbol{x})}{\varepsilon}
$$ {#eq-numerical-differentiation}

Then, we can substitute a small value of $\varepsilon$ on the r.h.s. to approximate the directional derivative.
How small, though?
Depending on how smooth the function is and how accurate our estimate needs to be,
we may need such a small value of $\varepsilon$
that typical computers will run into _rounding errors_.
Also, to compute the full gradient,
we would need to compute the r.h.s. once for each input dimension.
This is an issue if computing $J$ is expensive.

**Automatic differentiation** achieves the best of both worlds.
Like symbolic differentiation,
we manually implement the derivative rules for a few basic operations.
However, instead of executing these on the *symbols*,
we execute them on the *values* when the function gets called,
like in numerical differentiation.
This allows us to differentiate through programming constructs such as branches or loops,
and doesn't involve any arbitrarily small values.
@baydin_automatic_2018 provides an accessible survey of automatic differentiation.
At the time of writing,
all of the popular Python libraries for machine learning,
such as PyTorch [@ansel_pytorch_2024] and Jax [@bradbury_jax_2018],
use automatic differentiation.

### Stochastic gradient descent {#sec-sgd}

In real applications,
computing the gradient of the target function is not so simple.
As an example from supervised learning,
$J(\theta)$ might be the sum of squared prediction errors across an entire training dataset.
If our dataset is very large,
it might not fit into our computer's memory,
making it impossible to evaluate $\nabla J(\theta)$ at once.
We will see that computing the exact gradient in RL faces a similar challenge
where computing the gradient would require computing a complicated integral.

In these cases,
we can compute some *gradient estimate*

$$
g_x(\theta) \approx \nabla J(\theta)
$$ {#eq-grad-estimator-notation}

of the gradient at each step,
using some observed data $x$,
and walk in that direction instead.
This is called **stochastic** gradient descent.
In the SL example above,
we might randomly choose a *minibatch* of samples
and use them to estimate the true prediction error.

```{python}
#| label: fig-sgd
#| fig-cap: Pseudocode for stochastic gradient descent.

def sgd(
    rng: jr.PRNGKey,
    theta_init: Float[Array, " D"],
    estimate_gradient: Callable[[jr.PRNGKey, Float[Array, " D"]], Float[Array, " D"]],
    eta: float,
    n_steps: int,
):
    # Perform `n_steps` steps of SGD.

    # `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.
    theta = theta_init
    rngs = jr.split(rng, n_steps)
    history = [theta]
    for step in range(n_steps):
        theta = theta + eta * estimate_gradient(rngs[step], theta)
        history.append(theta)
    return theta, jnp.array(history)


latex(sgd, id_to_latex={"estimate_gradient": "g"})
```

```{python}
#| label: fig-sgd-demo
#| layout-ncol: 2
#| fig-cap: GA and SGD on a two-dimensional convex optimization task. Note that a larger batch size reduces the variance in the step direction.
#| fig-subcap:
#|   - Full-batch GA.
#|   - SGD with a batch size of 1.
#|   - SGD with a batch size of 5.
#|   - SGD with a batch size of 10.

X = 2 * jr.normal(next(rng), (100, 2))
w_true = jnp.ones(2)
y = X @ w_true
w_init = jnp.zeros(2)
lr = 0.1
n_steps = 10

def estimate_gradient(rng, theta, batch_size):
    batch_idx = jr.randint(rng, batch_size, 0, X.shape[0])
    X_batch = X[batch_idx]
    y_batch = y[batch_idx]
    return - (1 / batch_size) * X_batch.T @ (X_batch @ theta - y_batch)

def loss(theta):
    return - jnp.mean((X @ theta - y) ** 2)

# loss contour
w_lim = slice(-0.2, 1.5, 20j)
w0, w1 = jnp.mgrid[w_lim, w_lim]
thetas = jnp.stack([w0, w1], axis=2)
z = jax.vmap(jax.vmap(loss))(thetas)

def plot_history(history):
    plt.contourf(w0, w1, z, levels=10, alpha=0.3)
    for i in range(len(history) - 1):
        plt.arrow(history[i, 0], history[i, 1], history[i+1, 0] - history[i, 0], history[i+1, 1] - history[i, 1],
                width=0.03, fc='black', ec='black', zorder=5)
    plt.xlabel(r"$w_0$")
    plt.ylabel(r"$w_1$")
    plt.xlim(w_lim.start, w_lim.stop)
    plt.ylim(w_lim.start, w_lim.stop)
    plt.gcf().set_size_inches((3, 2))
    plt.show()

# also plot full gradient
_, history_gd = gradient_descent(w_init, loss, lr, n_steps)
_, history_sgd_1 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=1), lr, n_steps)
_, history_sgd_5 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=5), lr, n_steps)
_, history_sgd_10 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=10), lr, n_steps)

plot_history(history_gd)
plot_history(history_sgd_1)
plot_history(history_sgd_5)
plot_history(history_sgd_10)
```

What makes one gradient estimator better than another?
Ideally, we want this estimator to be _unbiased;_
that is, on average,
it matches a single true gradient step.

::: {#def-unbiased-estimator}
#### Unbiased gradient estimator

We call the gradient estimator $g$ **unbiased** if,
for all $\theta \in \R^D$,

$$
\E_{x \sim p_\theta} [g_x(\theta)] = \nabla J(\theta),
$$ {#eq-gd-unbiased}

where $p_\theta$ denotes the distribution of the observed data $x$.
:::

We also want the *variance* of the estimator to be low so that its performance doesn't change drastically at each step.

We can actually show that, for many "nice" functions, in a finite number of steps, SGD will find a $\theta$ that is "close" to a stationary point. In another perspective, for such functions, the local "landscape" of $J$ around $\theta$ becomes flatter and flatter the longer we run SGD.

::: {#thm-sgd-convergence}
#### SGD convergence

More formally, suppose we run SGD for $K$ steps, using an unbiased gradient estimator.
Let the step size $\eta^k$ scale as $O(1/\sqrt{k}).$
Then if $J$ is bounded and $\beta$-smooth (see below),
and the *norm* of the gradient estimator has a bounded second moment $\sigma^2,$

$$
\|\nabla J(\thetacurr)\|^2 \le O \left( M \beta \sigma^2 / K\right).
$$

We call a function $\beta$-smooth if its gradient is Lipschitz continuous with constant $\beta$:

$$
\|\nabla J(\theta) - \nabla J(\theta')\| \le \beta \|\theta - \theta'\|.
$$
:::

We'll now see a concrete application of stochastic gradient descent in the context of policy optimization.

## Policy (stochastic) gradient descent {#sec-pg-intro}

Policy gradient methods boil down to applying gradient descent to the policy optimization problem
for a chosen parameterized policy class (@eq-parameterized-class):

$$
\theta^{t+1} = \theta^t + \eta \nabla J(\theta^t)
\quad \text{where} \quad
J(\theta) = \E_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{\hi=0}^{\hor-1} r(s_\hi, a_\hi) \right].
$$ {#eq-pg-optimization}

The challenges lie mostly in computing unbiased gradient estimators (@def-unbiased-estimator)
and in _constraining_ the size of each update
to improve stability of the learning algorithm.

To provide some intuition for the gradient estimators we will later derive,
the following section constructs a general policy gradient algorithm in a "bottom-up" way.
The remainder of the chapter focuses on methods for improving the stability
of the policy gradient algorithm.

::: {#rem-pg-on-policy}
#### On-policy

Note that since the gradient estimator is computed using data collected by the current policy,
policy gradient algorithms are generally _on-policy_ (@def-on-off-policy).
Note that on-policy algorithms generally suffer
from _worse sample efficiency_ than off-policy algorithms:
while off-policy algorithms (such as Q-learning (@sec-q-learning))
can use data collected by any method,
on-policy algorithms can only make use of data from the _current_ policy.
:::

### Introduction to policy gradient methods {#sec-pg-intuition}

In this section,
we'll intuitively construct an iterative algorithm for improving the parameters.
We'll build up to true policy gradient estimators
by considering a series of update rules.

Remember that in RL,
the primary goal is to find the *optimal policy* that achieves the highest total reward.
Put simply,
we want policies that _take better actions more often._
Breaking that down, we need

1. a way to shift the policy to make certain actions more likely, and
2. a way to measure how good an action is in a given state.

Let's tackle the first task:
getting the policy to take action $a$ in state $s$.
Back in the tabular setting, for a deterministic policy $\pi$,
we simply assigned $\pi(s) \gets a$.
Now that we're using a parameterized policy class,
we can't directly assign a value to $\pi(s)$.
Instead, we adjust the parameters $\theta$ of the policy
to maximize the probability of taking $a$:

$$
\hat \theta = \arg\max_{\theta} \pi_{\theta}(a \mid s).
$$ {#eq-max-action-likelihood}

Assuming the output of the policy is differentiable with respect to its parameters,
we can apply gradient descent:

$$
\begin{aligned}
\theta^{t+1} &= \theta^t + \eta g_{s, a}(\theta^t) \\
\text{where} \quad g_{s, a}(\theta) &:= \nabla \pi_{\theta}(a \mid s).
\end{aligned}
$$ {#eq-gd-max-action-likelihood}

The notation $g$ is chosen to remind you of a gradient estimator (@eq-grad-estimator-notation).
We will draw the relationship between our intuitive approach and SGD more concretely later on.

Now let's approach the second task:
how do we choose which actions to take more often?
Suppose we have some random variable $\psi$
(that's the Greek letter "psi")
that is correlated with how "good" action $a$ is in state $s$.
Later in the chapter,
we'll explore concrete choices for $\psi$,
but for now,
we'll leave its identity a mystery.
Then, to update the policy,
we could sample an action $a \sim \picurr(\cdot \mid s)$,
and weight the corresponding gradient by $\psi$.
We will therefore call $\psi$ the **gradient coefficient**.

$$
g_{s, a}(\theta) := \psi \nabla \pi_{\theta}(a \mid s).
$$ {#eq-gd-weighted}

To illustrate this, suppose the policy takes one action $a_0$ and obtains a coefficient of $\psi_0 = 2$
and then takes a second action $a_1$ with a coefficient of $\psi_1 = -1$.
Then we would update $\theta \gets \theta + 2 \eta \nabla \pi_{\theta}(a_0 \mid s)$
and then $\theta \gets \theta - \eta \nabla \pi_{\theta}(a_1 \mid s)$.

::: {#exr-solve-directly}
#### An alternative approach

Compare this with the policy iteration update (@sec-pi),
where we updated the deterministic policy according to
$\pi(s) = \arg\max_{a \in \mathcal{A}} Q^\pi(s, a)$.
In our current setting,
why not just solve for $\theta^{t+1} = \arg\max_{\theta} \pi_{\theta}(a^\star \mid s)$,
where $a^\star = \arg\max_{a \in \mathcal{A}} Q^{\picurr}(a \mid s)$,
instead of sampling an action from our policy?
What type of action space does this approach assume?
What does the added stochasticity grant you?
:::

But the gradient estimator @eq-gd-weighted has an issue:
the amount that we encourage action $a$ depends on how *often* the policy takes it.
This could lead to a positive feedback loop where the most common action becomes more and more likely,
regardless of its quality.
To cancel out this factor,
we divide by the action's likelihood:

$$
\begin{aligned}
g_{s, a}(\theta) &:= \psi \frac{\nabla \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
&= \psi \nabla \log \pi_{\theta}(a \mid s).
\end{aligned}
$$ {#eq-gd-log-step}

Now we can extend this across the entire time horizon.
Suppose we use $\picurr$ to roll out a trajectory $\tau = (s_0, a_0, \dots, s_{\hor-1}, a_{\hor-1})$
and compute @eq-gd-log-step at each step of the trajectory.
We compute a gradient coefficient at each timestep,
so we denote each instance by $\psi_\hi(\tau)$.

$$
g_\tau(\theta)
:=
\sum_{\hi=0}^{\hor-1}
\psi_\hi(\tau) \nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
$$ {#eq-general-pg-single}

To reduce the variance,
we could roll out multiple trajectories,
and average the gradient steps across them.
This gives us the general form of the **policy gradient algorithm**:

::: {#def-policy-gradient}
#### General policy gradient algorithm

Suppose we are given an expression for the gradient coefficients $\psi_\hi(\tau)$.
Then we can perform policy gradient optimization as follows.

At each iteration $t = 0, \dots, T-1$ of the algorithm,
we sample $N$ trajectories
$\tau^n = (s^n_0, a^n_0, r^n_0, \dots, s^n_{\hor-1}, a^n_{\hor-1}, r^n_{\hor-1})$,
and compute the update rule

$$
\begin{aligned}
\theta^{t+1} &= \theta^t + \eta \frac{1}{N} \sum_{n=1}^N g_{\tau^n}(\theta^t) \\
\text{where} \quad g_{\tau}(\theta) &= \sum_{\hi=0}^{\hor-1}
\psi_\hi(\tau) \nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-general-pg}
:::

This algorithm allows us to optimize a policy
by sampling trajectories from it
and computing the gradient-log-likelihoods (sometimes called the **scores**) of the chosen actions.
Then we can update the parameters $\theta$ in the direction given by @eq-general-pg
to obtain a new policy that chooses better actions more often.

```{python}
#| label: fig-policy-grad-general
#| fig-cap: Pseudocode for the general policy gradient algorithm.

def policy_gradient(env: gym.Env, pi, theta: Float[Array, " D"], get_psi: Callable[[list["Transition"]], Float[Array, " H"]]):
    """Estimate the policy gradient using REINFORCE."""
    g = jnp.zeros_like(theta)
    tau = sample_trajectory(env, pi(theta))
    psis = get_psi(pi(theta), tau)
    for (s, a, r), psi in zip(tau, psis):
        def policy_log_likelihood(theta: Float[Array, " D"]) -> float:
            return log(pi(theta)(s, a))
        g += psi * jax.grad(policy_log_likelihood)(theta)
    return g

latex(policy_gradient, id_to_latex={"jax.grad": r"\nabla"})
```

::: {#rem-policy-grad-summary}
#### Summary of intuitive derivation

Let us review how we arrived at this expression:

1. We take the gradient of the policy to encourage certain actions (@eq-gd-max-action-likelihood).
2. We encourage actions proportionally to their advantage (@eq-gd-weighted).
3. We correct for the policy's sampling distribution (@eq-gd-log-step).
4. We extend this to each step of the trajectory (@eq-general-pg-single).
5. We sample multiple trajectories to reduce variance (@eq-general-pg).

The last piece is to figure out what $\psi$ stands for.
:::

::: {#exr-grad-coefficient}
#### Brainstorming

Can you think of any possibilities?
$\psi_\hi(\tau)$ should correlate with the quality of the action taken at time $\hi$.
It may depend on the current policy $\picurr$
or any component of the trajectory $(s_0, a_0, r_0, \dots, s_{\hor-1}, a_{\hor-1}, r_{\hor-1})$.
:::

We won't keep you waiting:
it turns out that if we set $\psi_\hi(\tau)$ to

1. $R(\tau)$ (the total reward of the trajectory) (@eq-estimator-reinforce),
1. $\sum_{\hi'=\hi}^{\hor-1} r(s_{\hi'}, a_{\hi'})$ (the remaining reward in the trajectory),
1. $Q^{\picurr}(s_\hi, a_\hi)$ (the current policy's Q-function), or
1. $A^{\picurr}(s_\hi, a_\hi)$ (the current policy's advantage function),

among other possibilities,
the gradient term of @eq-general-pg is actually
an _unbiased_ estimator (@def-unbiased-estimator) of the true "policy gradient" $\nabla J(\theta)$
(see @eq-policy-optimization-parameters).
That is, for any of the $\psi$ above,
updating the parameters according to the general policy gradient algorithm @eq-general-pg
_is_ (minibatch) stochastic gradient descent on the expected total reward $J(\theta)$,
with the gradient estimator

$$
\begin{aligned}
g_{\tau}(\theta)
&=
\sum_{\hi=0}^{\hor-1}
\psi_\hi(\tau) \nabla \log \pi_{\theta}(a_\hi \mid s_\hi) \\
\text{where} \quad
\E_{\tau \sim \rho^{\pi_\theta}}[ g_{\tau}(\theta) ]
&=
\nabla J(\theta).
\end{aligned}
$$ {#eq-sgd-pg-estimator}

### The REINFORCE policy gradient {#sec-reinforce}

We begin by showing that setting $\psi_\hi = \sum_{\hi'=0}^{\hor-1} r_{\hi'}$,
i.e. the total reward of the trajectory,
provides an unbiased gradient estimator.

::: {#thm-unbiased-reinforce}
#### Using the total reward is unbiased

Substituting

$$
\psi_\hi(\tau) := R(\tau) := \sum_{\hi'=0}^{\hor-1} r_{\hi'}
$$ {#eq-estimator-reinforce}

into the general policy gradient estimator @eq-general-pg gives an unbiased estimator.
That is,

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}} [ g^\text{R}_\tau(\theta) ] \\
\text{where} \quad
g^\text{R}_{\tau} (\theta)
&:=
\sum_{\hi=0}^{\hor-1}
R(\tau)
\nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-estimator-reinforce-unbiased}
:::

The "R" stands for REINFORCE,
which stands for "REward Increment $=$ Nonnegative Factor $\times$ Offset Reinforcement $\times$ Characteristic Eligibility" [@williams_simple_1992, p. 234].
(We will not elaborate further on this etymology.)

::: {.proof}
#### Proof via calculus

As our first step towards constructing an unbiased policy gradient estimator,
let us simplify the expression

$$
\nabla J(\theta) = \nabla \E_{\tau \sim \rho^{\pi_\theta}} [R(\tau)].
$$ {#eq-objective-def}

In supervised learning,
we were able to swap the gradient and expectation.
That was because the _function_ being averaged depended on the parameters,
not the distribution itself:

$$
\nabla \E_{(x, y) \sim p}[
    L(f_\theta(x), y)
] = \E_{(x, y) \sim p} [\nabla L(f_\theta(x), y) ].
$$ {#eq-sl-example-swap}

Here, though, the distribution depends on the parameters,
and the function being averaged does not.
One way to compute this type of derivative is to use the identity

$$
\nabla \rho^{\pi_\theta}(\tau)
=
\rho^{\pi_\theta}(\tau) \nabla \log \rho^{\pi_\theta}(\tau).
$$ {#eq-grad-log-prob}

By expanding the definition of expected value,
we can compute the correct value to be

$$
\begin{aligned}
\nabla J(\theta) & = \nabla \E_{\tau \sim \rho^{\pi_\theta}} [R(\tau)] \\
& = \int \nabla \rho^{\pi_\theta}(\tau) R(\tau) \\
& = \int \rho^{\pi_\theta}(\tau) \nabla \log \rho^{\pi_\theta}(\tau) R(\tau) \\
& = \E_{\tau \sim \rho^{\pi_\theta}} [ \nabla \log \rho^{\pi_\theta}(\tau) R(\tau) ].
\end{aligned}
$$ {#eq-reinforce-grad-log-prob}

Now we deal with the $\nabla \log \rho^{\pi_\theta}(\tau)$ term,
that is, the gradient-log-likelihood (aka **score**)
of the trajectory.
Recall @thm-autoregressive-trajectories,
in which we showed that when the state transitions are Markov
(i.e. $s_{\hi}$ only depends on $s_{\hi-1}, a_{\hi-1}$)
and the policy is history-independent
(i.e. $a_\hi \sim \pi_\theta (\cdot \mid s_\hi)$),
we can autoregressively write out the likelihood of a trajectory under the policy $\pi_\theta$.
Taking the log of the trajectory likelihood turns the products into sums:

$$
\log \rho^{\pi_\theta}(\tau) = \log P_0(s_0) + \sum_{\hi=0}^{\hor-1} \Big(
    \log \pi_\theta(a_\hi \mid s_\hi) + \log P(s_{\hi+1} \mid s_\hi, a_\hi)
\Big)
$$ {#eq-log-autoregressive}

When we take the gradient with respect to the parameters $\theta$,
only the $\log \pi_\theta(a_\hi \mid s_\hi)$ terms depend on $\theta$:

$$
\nabla \log \rho^{\pi_\theta}(\tau)
=
\sum_{\hi=0}^{\hor-1} \nabla \log \pi_\theta(a_\hi \mid s_\hi).
$$ {#eq-grad-log-autoregressive}

Substituting this into @eq-reinforce-grad-log-prob gives

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}}
\left[
    \sum_{\hi=0}^{\hor-1} R(\tau) \nabla \log \pi_\theta(a_\hi \mid s_\hi)
\right] \\
&= \E_{\tau \sim \rho^{\pi_\theta}} [g^\text{R}_\theta(\tau)],
\end{aligned}
$$

showing that the REINFORCE policy gradient (@eq-estimator-reinforce) is unbiased.
:::

:::: {.proof}
#### Proof via importance sampling.

Another way of deriving @thm-unbiased-reinforce
involves a technique known as **importance sampling**.
We'll demonstrate this approach here since it will come in handy later on.
Importance sampling is useful when we want to estimate an expectation
over a distribution that is hard to sample from.
Instead, we can sample from a different distribution that supports the same values,
and then reweight the samples according to the likelihood ratio between the two distributions.

::: {#thm-importance-sampling}
#### Importance sampling

Consider some random variable $x \in \mathcal{X}$ with density function $p$.
Let $q$ be the density function of another distribution on $\mathcal{X}$
that supports all of $p$,
that is, $q(x) = 0$ only if $p(x) = 0$.
Then

$$
\E_{x \sim p}[f(x)] = \E_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right].
$$ {#eq-importance-sampling}
:::

::: {.proof}
We expand the definition of expected value:

$$
\begin{aligned}
\E_{x \sim p}[f(x)] &= \sum_{x \in \mathcal{X}} f(x) p(x) \\
&= \sum_{x \in \mathcal{X}} f(x) \frac{p(x)}{q(x)} q(x) \\
&= \E_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right].
\end{aligned}
$$ {#eq-importance-sampling-derivation}
:::

::: {#exr-importance-sampling}
#### Importance sampling for a biased coin

Suppose you are a student and you determine your study routine by flipping a biased coin.
Let $x \in \{ \text{heads}, \text{tails} \}$ be the result of the coin flip.
The coin shows heads twice as often as it shows tails:

$$
p(x) = \begin{cases}
2/3 & x = \text{heads} \\
1/3 & x = \text{tails}.
\end{cases}
$$ {#eq-importance-sampling-biased-coin}

Suppose you study for $f(x)$ hours, where

$$
f(x) = \begin{cases}
1 & x = \text{heads} \\
2 & x = \text{tails}.
\end{cases}
$$ {#eq-importance-sampling-f}

One day, you lose your coin,
and have to replace it with a fair one, i.e.

$$
q(x) = 1/2,
$$ {#eq-importance-sampling-fair-coin}

but you want to study for the same amount on average.
Suppose you decide to do this by importance sampling with the new coin.
Now, upon flipping heads or tails, you study for

$$
\begin{aligned}
\frac{p(\text{heads})}{q(\text{heads})} f(\text{heads}) = \frac{4}{3} \\
\frac{p(\text{tails})}{q(\text{tails})} f(\text{tails}) = \frac{2}{3}
\end{aligned}
$$ {#eq-importance-sampling-study}

hours respectively.
Verify that your expected time spent studying
is the same as before.
Now compute the _variance_ in the time you spend studying.
Does it change?
:::

Returning to the RL setting,
we can compute the policy gradient by importance sampling from any trajectory distribution $\rho$.
(All gradients are being taken with respect to $\theta$.)

$$
\begin{aligned}
    \nabla J(\theta) & = \nabla \E_{\tau \sim \rho^{\pi_\theta}} [ R(\tau) ] \\
                     & = \nabla \E_{\tau \sim \rho} \left[ \frac{\rho^{\pi_\theta}(\tau)}{\rho(\tau)} R(\tau) \right] \\
                     & = \E_{\tau \sim \rho} \left[ \frac{\nabla \rho^{\pi_\theta}(\tau)}{\rho(\tau)} R(\tau) \right].
\end{aligned}
$$

Setting $\rho = \rho^{\pi_\theta}$ reveals @eq-reinforce-grad-log-prob,
and we can then proceed as we did in the previous proof.
::::

Let us reiterate some intuition into how this method works.
Recall that we update our parameters according to

$$
\begin{aligned}
    \theta_{t+1} &= \theta_t + \eta \nabla J(\theta_t) \\
    &= \theta^t + \eta \E_{\tau \sim \rho^{\theta_t}} [\nabla \log \rho^{\theta^t}(\tau) R(\tau)].
\end{aligned}
$$ {#eq-update-reinforce}

Consider the "good" trajectories where $R(\tau)$ is large.
Then $\theta$ gets updated so that these trajectories become more likely.
To see why,
recall that $\log \rho^\theta(\tau)$ is the log-likelihood of the trajectory $\tau$ under the policy $\pi_\theta$,
so the gradient points in the direction that makes $\tau$ more likely.

However,
the REINFORCE gradient estimator $g^\text{R}_\tau(\theta)$ has large variance.
Intuitively,
this is because it uses the total reward from the entire trajectory,
which depends on the entire sequence of interactions with the environment,
each step of which introducesrandomness.
The rest of this chapter investigates ways to find *lower-variance* policy gradient estimators.

### Baselines and advantages {#sec-baselines}

A central idea from statistical learning is the **bias-variance decomposition**,
which shows that the mean squared error of an estimator is the sum of its squared bias and its variance.
All of the policy gradient estimators we will see in this chapter are already unbiased,
i.e.,
their mean over trajectories equals the true policy gradient (@def-unbiased-estimator).
Can we construct estimators with lower *variance* as well?

As a first step,
note that the action taken at step $\hi$
does not causally affect the reward from previous timesteps,
since they're already in the past.
So we should only use the reward from the current timestep onwards
to estimate the policy gradient.

::: {#thm-unbiased-remaining}
#### Using the remaining reward is unbiased

Substituting the reamining reward $\sum_{\hi'=\hi}^{\hor-1} r_{\hi'}$
for $\psi_\hi(\tau)$
into the general policy gradient estimator @eq-general-pg gives an unbiased estimator.
That is,

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}} [ g^\text{rem}_\tau(\theta) ] \\
\text{where} \quad
g^\text{rem}_{\tau} (\theta)
&:=
\sum_{\hi=0}^{\hor-1}
\left( \sum_{\hi'=\hi}^{\hor-1} r_{\hi'} \right)
\nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-unbiased-remaining}
:::

::: {#exr-future-rewards}
#### Unbiasedness of remaining reward estimator

We leave the proof of @thm-unbiased-remaining as an exercise.
:::

By a conditioning argument,
we can replace the remaining reward with the policy's Q-function,
evaluated at the current state.
By the same reasoning as above,
this also reduces the variance,
since the only stochasticity in the expression $Q^{\pi_\theta}(s_\hi, a_\hi)$
comes from the current state and action.

::: {#thm-unbiased-q}
#### Using the Q function is unbiased

Substituting $Q^{\pi_\theta}(s_\hi, a_\hi)$
for $\psi_\hi(\tau)$
into the general policy gradient estimator @eq-general-pg gives an unbiased estimator.
That is,

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}} [ g^\text{Q}_\tau(\theta) ] \\
\text{where} \quad
g^\text{Q}_{\tau} (\theta)
&:=
\sum_{\hi=0}^{\hor-1}
Q^{\pi_\theta}(s_\hi, a_\hi)
\nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-estimator-q}
:::

::: {#exr-prove-q-unbiased}
#### Unbiasedness of remaining reward estimator

We also leave the proof of @thm-unbiased-q as an exercise.
:::

We can further reduce variance by subtracting a **baseline function**
$b_\theta : \mathcal{S} \to \mathbb{R}$.
Note that this function could also depend on the current policy parameters.

::: {#thm-unbiased-baseline}
#### Subtracting a baseline function preserves unbiasedness

Let $b_\theta : \mathcal{S} \to \mathbb{R}$ be some baseline function,
and let $\psi$ be a gradient coefficient function
that yields an unbiased policy gradient estimator
(e.g. @eq-estimator-reinforce or @eq-estimator-q).
Substituting

$$
\psi^{\text{bl}}_\hi(\tau) := \psi_\hi(\tau) - b_\theta(s_\hi)
$$ {#eq-estimator-baseline}

into the general policy gradient estimator @eq-general-pg
gives an unbiased policy gradient estimator.
That is,

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}} [ g^\text{bl}_\tau(\theta) ] \\
\text{where} \quad
g^\text{bl}_{\tau} (\theta)
&:=
\sum_{\hi=0}^{\hor-1}
(\psi_\hi(\tau) - b_\theta(s_\hi))
\nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-estimator-q}
:::

::: {#exr-prove-baseline-unbiased}
#### Unbiasedness of baseline estimator

We leave the proof of @thm-unbiased-baseline as an exercise as well.
:::

For example, we might want $b_\hi$ to estimate the average remaining reward at a given timestep:

$$
b_\theta(s_\hi) = \E_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{\hi'=\hi}^{\hor-1} r_{\hi'}
\right].
$$ {#eq-exm-remaining}

As a better baseline, we could instead choose the *value function* of $\pi_{\theta}$.
For any policy $\pi$,
note that the random variable $Q^\pi_\hi(s, a) - V^\pi_\hi(s)$,
where the randomness is taken over the action $a$,
is centered around zero.
(Recall $V^\pi_\hi(s) = \E_{a \sim \pi} Q^\pi_\hi(s, a).$)
This quantity matches the intuition given in @sec-pg-intro:
it is *positive* for actions that are better than average (in state $s$),
and *negative* for actions that are worse than average.
In fact, it has a particular name:
the **advantage function.**

::: {#def-advantage}
#### Advantage function

For a policy $\pi$,
its advantage function $A^\pi$ at time $\hi$ is given by

$$
A^\pi_\hi(s, a) := Q^\pi_\hi(s, a) - V^\pi_\hi(s).
$$ {#eq-advantage}
:::

Note that for an optimal policy $\pi^\star$,
the advantage of a given state-action pair is always zero or negative.

We can now use $A^{\pi_{\theta}}(s_\hi, a_\hi)$
for the gradient coefficients
to obtain the ultimate unbiased policy gradient estimator.

::: {#thm-unbiased-adv}
#### Using the advnatage function is unbiased

Substituting

$$
\psi_\hi(\tau) := A^{\pi_\theta}(s_\hi, a_\hi)
$$ {#eq-estimator-adv}

into the general policy gradient estimator @eq-general-pg gives an unbiased estimator.
That is,

$$
\begin{aligned}
\nabla J(\theta) &= \E_{\tau \sim \rho^{\pi_\theta}} [ g^\text{adv}_\tau(\theta) ] \\
\text{where} \quad
g^\text{adv}_{\tau} (\theta)
&:=
\sum_{\hi=0}^{\hor-1}
A^{\pi_\theta}(s_\hi, a_\hi)
\nabla \log \pi_{\theta}(a_\hi \mid s_\hi).
\end{aligned}
$$ {#eq-estimator-adv}
:::

::: {.proof}
This follows directly from @thm-unbiased-q and @thm-unbiased-baseline.
:::

Note that to avoid correlations between the gradient estimator and the value estimator (i.e. baseline),
we must estimate them with independently sampled trajectories:

<!-- TODO explain _why_ we want to avoid correlations -->

```{python}
def pg_with_learned_baseline(env: gym.Env, pi, eta: float, theta_init, K: int, N: int) -> Float[Array, " D"]:
    theta = theta_init
    for k in range(K):
        trajectories = sample_trajectories(env, pi(theta), N)
        V_hat = fit_value(trajectories)
        tau = sample_trajectories(env, pi(theta), 1)
        nabla_hat = jnp.zeros_like(theta)  # gradient estimator

        for h, (s, a) in enumerate(tau):
            def log_likelihood(theta_opt):
                return jnp.log(pi(theta_opt)(s, a))
            nabla_hat = nabla_hat + jax.grad(log_likelihood)(theta) * (return_to_go(tau, h) - V_hat(s))
        
        theta = theta + eta * nabla_hat
    return theta

latex(pg_with_learned_baseline)
```

Note that you could also generalize this by allowing the learning rate $\eta$ to vary across steps, or take multiple trajectories $\tau$ and compute the sample average of the gradient estimates.

The baseline estimation step `fit_value` can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.

::: {#exm-linear-policy-grad}
#### Policy gradient for the linear-in-features parameterization

The gradient-log-likelihood for the linear parameterization @exm-linear-in-features is also quite elegant:

$$
\begin{aligned}
        \nabla \log \pi_\theta(a|s) &= \nabla \left( \theta^\top \phi(s, a) - \log \left( \sum_{a'} \exp(\theta^\top \phi(s, a')) \right) \right) \\
        &= \phi(s, a) - \E_{a' \sim \pi_\theta(s)} \phi(s, a')
\end{aligned}
$$

Plugging this into our policy gradient expression, we get

$$
\begin{aligned}
    \nabla J(\theta) & = \E_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_\hi | s_\hi) A_\hi^{\pi_\theta}
    \right]                                                                                                                    \\
                     & = \E_{\tau \sim \rho^{\pi_\theta}} \left[
    \sum_{t=0}^{T-1} \left( \phi(s_\hi, a_\hi) - \E_{a' \sim \pi(s_\hi)} \phi(s_\hi, a') \right) A_\hi^{\pi_\theta}(s_\hi, a_\hi)
    \right]                                                                                                                    \\
                     & = \E_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{t=0}^{T-1} \phi(s_\hi, a_\hi) A_\hi^{\pi_\theta} (s_\hi, a_\hi) \right]
\end{aligned}
$$

Why can we drop the $\E \phi(s_\hi, a')$ term? By linearity of expectation, consider the dropped term at a single timestep: $\E_{\tau \sim \rho^{\pi_\theta}} \left[ \left( \E_{a' \sim \pi(s_\hi)} \phi(s, a') \right) A_\hi^{\pi_\theta}(s_\hi, a_\hi) \right].$ By Adam's Law, we can wrap the advantage term in a conditional expectation on the state $s_\hi.$ Then we already know that $\E_{a \sim \pi(s)} A_\hi^{\pi}(s, a) = 0,$ and so this entire term vanishes.
:::

## Comparing policy gradient algorithms to policy iteration

What advantages do policy gradient algorithms have
over the policy iteration algorithms covered in @sec-pi?

::: {#rem-policy-iteration}
#### Policy iteration review

Recall that policy iteration is an algorithm for MDPs with unknown state transitions
where we alternate between the following two steps:

-   Estimating the $Q$-function (or advantage function) of the current policy;
-   Updating the policy to be greedy with respect to this approximate $Q$-function (or advantage function).
:::

To analyze the difference between them, we'll make use of the **performance difference lemma**,
which provides an expression for comparing the difference between two value functions.

::: {#thm-pdl}
#### Performance difference lemma [@kakade_approximately_2002, Lemma 6.1]

Suppose Alice is playing a game (an MDP). Bob is spectating, and can evaluate how good an action is compared to his own strategy. (That is, Bob can compute his *advantage function* $A_\hi^{\text{Bob}}(s_\hi, a_\hi)$). The performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:

$$
V_0^{\text{Alice}}(s) - V_0^{\text{Bob}}(s) = \E_{\tau \sim \rho^\text{Alice}} \left[ \sum_{\hi=0}^{\hor-1} A_\hi^{\text{Bob}} (s_\hi, a_\hi) \mid s_0 = s \right]
$$ {#eq-pdl}

where $\rho^\text{Alice}$ denotes Alice's trajectory distribution (@def-trajectory-distribution).
:::

To see why, consider a specific step $\hi$ in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average. But this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!

::: {.proof}
Formally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that

$$
\begin{aligned}
A^\pi_\hi(s_\hi, a_\hi) &= Q^\pi_\hi(s_\hi, a_\hi) - V^\pi_\hi(s_\hi) \\
&= r_\hi(s_\hi, a_\hi) + \E_{s_{\hi+1} \sim P(\cdot \mid s_\hi, a_\hi)} [V^\pi_{\hi+1}(s_{\hi+1})] - V^\pi_\hi(s_\hi)
\end{aligned}
$$ {#eq-advantage-expansion}

so expanding out the r.h.s. expression of @eq-pdl and grouping terms together gives

$$
\begin{aligned}
\E_{\tau \sim \rho^\text{Alice}} \left[ \sum_{\hi=0}^{\hor-1} A_\hi^{\text{Bob}} (s_\hi, a_\hi) \mid s_0 = s \right]
&= \E_{\tau \sim \rho_\text{Alice}} \bigg[
    \left( \sum_{\hi=0}^{\hor-1} r_\hi(s_\hi, a_\hi) \right) \\
& \quad {} + \left( V^{\text{Bob}}_1(s_1) + \cdots + V^{\text{Bob}}_\hor(s_\hor) \right) \\
& \quad {} - \left( V^{\text{Bob}}_0(s_0) + \cdots + V^{\text{Bob}}_{\hor-1}(s_{\hor-1}) \right)
    \mid s_0 = s \bigg] \\
&= V^{\text{Alice}}_0(s) - V^{\text{Bob}}_0(s).
\end{aligned}
$$ {#eq-pdl-proof}

as desired.
Note that the "inner" expectation from expanding the advantage function
has the same distribution as the outer one,
so omitting it here is valid.
Also note that $V_\hor^\pi$,
the value after reaching a terminal state,
is always zero for any policy $\pi$.
:::

The PDL gives insight into why fitted approaches such as PI don't work as well in the "full" RL setting.
To see why, let's consider a single iteration of policy iteration,
where policy $\pi$ gets updated to $\widetilde \pi$.
We'll assume these policies are deterministic.
Define $\Delta_\infty$ to be the most negative advantage:

$$
\Delta_\infty = \min_{s \in \mathcal{S}} A^{\pi}_\hi(s, \widetilde \pi(s)).
$$ {#eq-compare-advantage}

Suppose $\Delta_\infty < 0$, i.e. there exists a state $s$ such that

$$
A^\pi(s, \widetilde \pi(s)) < 0,
$$ {#eq-neg-advantage}

that is,
if $\widetilde \pi$ acts for just one turn from state $s$ and then $\pi$ acts thereafter,
the result would be worse on average than allowing $\pi$ to act.
Plugging this into the PDL (@thm-pdl) gives

$$
\begin{aligned}
V_0^{\widetilde \pi}(s) - V_0^{\pi}(s) &= \E_{\tau \sim \rho^{\widetilde \pi}} \left[
\sum_{\hi=0}^{\hor-1} A_\hi^{\pi}(s_\hi, a_\hi)
\mid s_0 = s
\right] \\
&\ge H \Delta_\infty \\
V_0^{\widetilde \pi}(s) &\ge V_0^{\pi}(s) - H|\Delta_\infty|.
\end{aligned}
$$

That is, for some state $s$,
the lower bound on the performance of $\widetilde \pi$ is *lower* than the performance of $\pi$.
This doesn't state that $\widetilde \pi$ *will* necessarily perform worse than $\pi$,
only suggests that it might be possible.
If these worst case states do exist,
though, PI does not avoid situations where the new policy often visits them;
It does not enforce that the trajectory distributions $\rho^\pi$ and $\rho^{\widetilde \pi}$ be close to each other.
In other words,
PI falls prey to **distributional shift**:
the "training distribution" that our prediction rule is fitted on, $\rho^\pi$,
may differ significantly from the "evaluation distribution" $\rho^{\widetilde \pi}$.

On the other hand, policy gradient methods *do*,
albeit implicitly,
encourage $\rho^\pi$ and $\rho^{\widetilde \pi}$ to be similar.
Suppose that the mapping from policy parameters to trajectory distributions is relatively smooth.
Then, by adjusting the parameters only a small distance,
the new policy will also have a similar trajectory distribution.
But this is not very rigorous,
and in practice the parameter-to-distribution mapping may not be so smooth.
Can we constrain the distance between the resulting distributions more *explicitly*?

This brings us to the following local policy optimization methods:

1.  **trust region policy optimization** (TRPO) [@schulman_trust_2015],
    which explicitly constrains the difference between the distributions
    before and after each step;
2.  the **natural policy gradient** (NPG) [@kakade_natural_2001],
    a first-order approximation of TRPO;
3.  **proximal policy optimization** (PPO-penalty) [@schulman_proximal_2017],
    a "soft relaxation" of TRPO;
4.  the **clipped surrogate objective** (PPO-clip) [@schulman_proximal_2017],
    a version of PPO that is popular in practice.

::: {#rem-ppo-ordering}
#### Ordering of algorithms

Chronologically,
NPG was developed first,
followed by TRPO and later PPO.
We begin with TRPO since it sets up the intuition behind these constrained methods.
:::

## Trust region policy optimization {#sec-trpo}

We saw above that policy gradient methods are effective
because they implicitly constrain how much the policy changes at each iteration
in terms of its trajectory distribution $\rho^\pi$.
What happens if we *explicitly* constrain the distance between the new and old trajectory distributions?
This requires some way to measure the _distance_ between two trajectory distributions.
For this,
we introduce the **Kullback-Leibler divergence**.

::: {#def-kl-divergence}
#### Kullback-Leibler divergence

For two PDFs $p, q$,

$$
\kl{p}{q} := \E_{x \sim p} \left[ \log \frac{p(x)}{q(x)} \right].
$$ {#eq-kld}
:::

The Kullback-Leibler divergence can be interpreted in many different ways,
many stemming from information theory.
One such interpretation
is that $\kl{p}{q}$ describes how much more surprised you are
if you *think* data is being generated by $q$ but it's actually generated by $p$,
compared to someone who knows the true distribution $p$.
(The **surprise** of an event with probability $p$ is $- \log_2 p$.)

It can be shown that $\kl{p}{q} = 0$ if and only if $p = q$.
Also note that it is generally *not* symmetric,
that is,
$\kl{p}{q} \neq \kl{q}{p}$.
How can we interpret this asymmetry?

::: {#rem-kl-asymmetry}
#### Asymmetry of the Kullback-Leibler divergence

Note that the KL divergence gets large
if $p(x)/q(x)$ is very large for some $x$,
that is, $q$ assigns low probability to a common event under $p$.
So if we minimize the KL divergence with respect to the second argument $q$,
the "prediction" distribution,
$q$ will "spread out" to cover all common events under $p$.
If we minimize the KL divergence with respect to the first argument $p$,
the data generating distribution,
$p$ will "squeeze under" $q$,
so that $p(x)$ is small wherever $q(x)$ is small.
:::

```{python}
#| label: fig-exm-kld
#| fig-cap: Minimizing the forward and backward Kullback-Leibler divergence against a bimodal distribution with respect to a unimodal distribution

def mixture_pdf(x, mus, sigmas, weights):
    pdf_val = np.zeros_like(x, dtype=float)
    for mu, sigma, w in zip(mus, sigmas, weights):
        pdf_val += w * norm.pdf(x, loc=mu, scale=sigma)
    return pdf_val

def kl_divergence(P, Q, x_grid):
    epsilon = 1e-12  # avoid division by zero
    P_safe = np.maximum(P, epsilon)
    Q_safe = np.maximum(Q, epsilon)
    dx = x_grid[1] - x_grid[0]
    return np.sum(P_safe * np.log(P_safe / Q_safe)) * dx

def Q_pdf(x):
    # 1/3 N(-5, 1) + 2/3 N(2, 1)
    return mixture_pdf(x, mus=[-5, 2], sigmas=[1, 1], weights=[1/3, 2/3])

# Single Gaussian for the candidate
def single_gaussian_pdf(x, params):
    mu, log_sigma = params
    sigma = np.exp(log_sigma)
    return norm.pdf(x, loc=mu, scale=sigma)

def kl_objective_PQ(params, x_grid, Q_vals):
    # minimize KL(P_candidate || Q).
    P_candidate_vals = single_gaussian_pdf(x_grid, params)
    return kl_divergence(P_candidate_vals, Q_vals, x_grid)

def kl_objective_QP(params, x_grid, Q_vals):
    # minimize KL(Q || P_candidate).
    P_candidate_vals = single_gaussian_pdf(x_grid, params)
    return kl_divergence(Q_vals, P_candidate_vals, x_grid)

x_grid = np.linspace(-10, 10, 201)
Q_vals = Q_pdf(x_grid)

# Minimize each objective separately
init_params = [0.0, 0.0]  # mu=0, log_sigma=0 -> sigma=1
res_min_PQ = minimize(kl_objective_PQ, x0=init_params, args=(x_grid, Q_vals), method='L-BFGS-B')
res_min_QP = minimize(kl_objective_QP, x0=init_params, args=(x_grid, Q_vals), method='L-BFGS-B')

plt.figure(figsize=(6, 4))
plt.plot(x_grid, Q_vals, '-', label="q")
plt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_QP.x), '--', label=r"$\arg\min_q \text{KL}(p \parallel q)$")
plt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_PQ.x), '--', label=r"$\arg\min_p \text{KL}(p \parallel q)$")
plt.xlabel('x'), plt.ylabel('PDF')
plt.legend()
plt.show()
```

For trajectory distributions $\rho^\picurr$ and $\rho^\pinext$ (@def-trajectory-distribution),
the KL divergence can be broken down into a sum over timesteps:

$$
\begin{aligned}
\kl{\rho^{\picurr}}{\rho^{\pinext}}
&= \E_{\tau \sim \rho^{\picurr}} \left[
    \log \rho^{\picurr}(\tau) - \log \rho^{\pinext}(\tau)
\right] \\
&= \E_{\tau \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1} \log \picurr(a_\hi \mid s_\hi) - \log \pinext(a_\hi \mid s_\hi)
\right] \\
\end{aligned}
$$ {#eq-kld-trajectories}

since the terms corresponding to the state transitions and initial state distribution cancel out.

We can now use the KL divergence
to explicitly constrain the distance between the new and old trajectory distributions:

$$
\begin{aligned}
\theta^{t+1} &\gets \arg\max_{\thetanext \in \mathbb{R}^D} J(\thetanext) \\
& \text{where } \kl{\rho^{\picurr}}{\rho^{\pinext}} < \delta
\end{aligned}
$$ {#eq-trpo-intuition}

Note that we place $\rho^\pinext$ in the _second_ argument to the KL divergence.
This ensures that $\rho^\pinext$ supports all of the trajectories under $\rho^\picurr$
(see @rem-kl-asymmetry).

In place of $J$,
if we use the performance difference lemma (@thm-pdl)
to compare the performance of the new policy to the old one,
we obtain an

::: {#def-trpo-update}
#### TRPO update rule

Let $\thetacurr \in \R^D$ denote the current policy parameter vector.
The TRPO update rule is

$$
\begin{aligned}
\theta^{k+1} &\gets \arg\max_{\thetanext \in \mathbb{R}^D}
\E_{\tau \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1}
    \E_{a'_\hi \sim \pi^{\thetanext}(\cdot \mid s_\hi)}
    A^{\picurr}(s_\hi, a'_\hi)
\right] \\
& \text{where } \kl{\rho^{\picurr}}{\rho^{\pinext}} < \delta
\end{aligned}
$$ {#eq-trpo}
:::

::: {#rem-curr-states}
#### Drawing states from old policy

Note that we have made a small change to the @eq-pdl:
we use the _current_ policy's trajectory distribution,
and re-sample *actions* from the _updated_ policy.
This allows us to reuse a single batch of trajectories from $\picurr$
rather than sample new batches from $\pinext$
when solving the optimization problem @eq-trpo.
This approximation also matches the r.h.s. of the PDL to first order in $\theta$.
(We will elaborate more on this later.)
:::

```{python}
#| label: fig-alg-trpo
#| fig-cap: Pseudocode for trust region policy optimization

def trpo(env, δ, theta_init, n_interactions):
    theta = theta_init
    for k in range(K):
        trajectories = sample_trajectories(env, pi(theta), n_interactions)
        A_hat = fit_advantage(trajectories)
        
        def approximate_gain(theta_opt):
            A_total = 0
            for tau in trajectories:
                for s, _a, _r in tau:
                    for a in env.action_space:
                        A_total += pi(theta)(s, a) * A_hat(s, a)
            return A_total
        
        def constraint(theta_opt):
            return kl_div_trajectories(pi, theta, theta_opt, trajectories) <= δ
        
        theta = optimize(approximate_gain, constraint)

    return theta

latex(trpo)
```

The above isn't entirely complete:
we still need to solve the actual optimization problem at each step.
Unless we know additional properties of the problem,
this is still a nonconvex constrained optimization problem
that might be inefficient to solve.
Do we need to solve for the exact objective function, though?
Instead, if we assume that both the objective function and the constraint
are somewhat smooth in terms of the policy parameters,
we can use their *Taylor expansions* to give us a simpler optimization problem with a closed-form solution.
This brings us to the **natural policy gradient** algorithm.

## Natural policy gradient {#sec-npg}

In some sense,
the _natural policy gradient_ algorithm [@kakade_natural_2001] is an _implementation_ of trust region policy optimization (@fig-alg-trpo).
Recall that in each TRPO update,
we seek to maximize the expected total reward
while keeping the updated policy close to the current policy in terms of Kullback-Leibler divergence (@def-kl-divergence):

$$
\begin{gathered}
\theta^{k+1} \gets \arg\max_{\thetanext \in \R^D} J(\thetanext) \\
\text{where } \kl{\thetacurr}{\thetanext} \le \delta
\end{gathered}
$$ {#eq-npg-simplify}

NPG uses the following simplification:
we take local approximations
of the objective and constraint functions,
which results in a simple problem with a closed-form solution.

Concretely, we take a first-order Taylor approximation to the objective function
about the current iterate $\thetacurr$:

$$
J(\thetanext) = J(\thetacurr) + (\thetanext - \thetacurr)^\top \nabla J(\thetacurr) + O(\|\thetanext - \thetacurr\|^2).
$$ {#eq-obj-linear}

We also take a second-order Taylor approximation to the constraint function:

::: {#thm-kl-quadratic}
#### Quadratic approximation to KL divergence.

The second-order approximation to $d^\ki(\theta) := \kl{p_\thetacurr}{p_\theta}$
is given by

$$
\begin{aligned}
d^\ki(\theta)
&=
\frac{1}{2} (\theta - \thetacurr)^\top F_\thetacurr (\theta - \thetacurr) 
+
O(\|\theta - \thetacurr\|^3),
\end{aligned}
$$ {#eq-kl-taylor-small}

where $F_{\thetacurr}$ is the **Fisher information matrix** (FIM) of the trajectory distribution $\rho^{\picurr}$.
(We define the FIM below in @def-fisher-matrix.)
:::

::: {.proof}
We leave the details as an exercise.
Here is an outline:

1.  Write down the Taylor expansion of $d^\ki(\theta)$ around $\thetacurr$.
1.  Show that the zeroth-order term $d^\ki(\thetacurr)$ is zero.
1.  Show that the gradient $\nabla d^\ki(\theta) \vert_{\theta = \thetacurr}$ is zero.
1.  Show that the Hessian $\nabla^2 d^\ki(\theta) \vert_{\theta = \thetacurr}$ equals $F_\thetacurr$.
:::

::: {#def-fisher-matrix}
#### Fisher information matrix

Let $p_\theta$ denote a distribution parameterized by $\theta$.
Its Fisher information matrix $F_\theta$ can be defined equivalently as:

$$
\begin{aligned}
        F_{\theta} & = \E_{x \sim p_\theta} \left[ (\nabla_\theta \log p_\theta(x)) (\nabla_\theta \log p_\theta(x))^\top \right] \\
                   & = \E_{x \sim p_{\theta}} [- \nabla_\theta^2 \log p_\theta(x)].
\end{aligned}
$$ {#eq-fisher-matrix}
:::

::: {#rem-interpret-fisher}
#### Interpretation of the Fisher information matrix

The Fisher information matrix is an important quantity
when working with parameterized distributions.
It has many possible interpretations.
The first expression in @eq-fisher-matrix shows that it is the covariance matrix of the gradient-log-probability
(also known as the **score**),
and the second expression shows that it is the expected Hessian matrix of the negative-log-likelihood.
See @sec-hessians for more details.
:::

Putting this together results in the following update rule:

::: {#def-npg-update}
#### Natural policy gradient update

We aim to solve the constrained optimization problem @eq-npg-simplify.
Upon taking first- and second-order approximations
of the objective function and constraint function respectively,
we obtain the update rule

$$
\begin{gathered}
    \theta^{k+1} \gets \thetacurr + \arg\max_{\Delta\theta \in \R^D} \nabla J(\thetacurr)^\top \Delta\theta \\
    \text{where } \frac{1}{2} \Delta\theta^\top F_{\thetacurr} \Delta\theta \le \delta,
\end{gathered}
$$ {#eq-npg-optimization}

where $F_\thetacurr$ is the Fisher information matrix of $\rho^\picurr$.
:::

@eq-npg-optimization is a convex optimization problem with a closed-form solution.
To see why,
it helps to visualize the case where $\theta$ is two-dimensional:
the constraint describes the inside of an ellipse,
and the objective function is linear,
so we can find the extreme point on the boundary of the ellipse
by setting the gradient of the Lagrangian to zero:

$$
\begin{aligned}
    \mathcal{L}(\theta, \alpha)                     & = \nabla J(\thetacurr)^\top (\theta - \thetacurr) - \alpha \left[ \frac{1}{2} (\theta - \thetacurr)^\top F_{\thetacurr} (\theta - \thetacurr) - \delta \right] \\
    \nabla \mathcal{L}(\theta^{k+1}, \alpha) & := 0                                                                                                                                                             \\
    \implies \nabla J(\thetacurr)        & = \alpha F_{\thetacurr} (\theta^{k+1} - \thetacurr)
\end{aligned}
$$ {#eq-npg-lagrangian}

Rearranging gives the NPG update rule:

$$
\begin{aligned}
    \theta^{k+1}                           & = \thetacurr + \eta F_{\thetacurr}^{-1} \nabla J(\thetacurr)                                                                                             \\
    \text{where } \eta                     & = \sqrt{\frac{2 \delta}{\nabla J(\thetacurr)^\top F_{\thetacurr}^{-1} \nabla J(\thetacurr)}}
\end{aligned}
$$ {#eq-npg-update}

This gives us the closed-form update.

::: {#rem-npg-efficiency}
#### Scalability of NPG

Having a closed-form solution might at first seem like brilliant news.
Is there a catch?
The challenge lies in computing
the inverse Fisher information matrix (FIM) $F_{\thetacurr}^{-1}$.
Since it is an expectation over trajectories,
computing it exactly is typically intractable.
Instead, we could collect trajectories from the environment
and approximate the expectation by a sample average,
since we can write the Fisher information matrix as

$$
F_{\theta} = \E_{\tau \sim \rho^{\pi_\theta}} \left[ \sum_{\hi=0}^{\hor-1} (\nabla \log \pi_\theta (a_\hi \mid s_\hi)) (\nabla \log \pi_\theta(a_\hi \mid s_\hi))^\top \right].
$$ {#eq-fisher-trajectory}

Note that we've used the Markov property to cancel out the cross terms corresponding to two different time steps.

It turns out that to estimate the FIM to a relative error of $\epsilon$,
we need $O(D / \epsilon^2)$ samples [@vershynin_high-dimensional_2018, Remark 4.7.2].
In order for the estimated FIM to be accurate enough to be useful,
this can be too large to be practical.
Taking the inverse also takes $O(D^3)$ time,
which can be expensive if the parameter space is large.
:::

::: {#rem-npg-interpretation}
#### NPG accounts for curvature in the parameter space

Let us compare the original policy gradient update (@eq-pg-optimization) to the NPG update (@eq-npg-update):

$$
\begin{aligned}
\theta^{t+1} &= \theta^t + \eta \nabla J(\theta^t) & \text{Policy gradient} \\
\theta^{t+1} &= \theta^t + \eta F_{\theta^t}^{-1} \nabla J(\theta^t) & \text{Natural policy gradient}
\end{aligned}
$$ {#eq-compare-pg-npg}

The NPG update **preconditions** the gradient by the inverse Fisher information matrix.
Speaking abstractly,
this matrix accounts for the **geometry of the parameter space** in the following sense.

The typical gradient descent algorithm implicitly measures distances between parameters
using the typical "flat" *Euclidean distance*:

$$
\text{distance}(\theta_0, \theta_1) = \| \theta_0 - \theta_1 \|.
$$ {#eq-exm-distance-euclidean}

The NPG update measures distance between parameters
as the KL divergence (@def-kl-divergence) between their induced trajectory distributions:

$$
\text{distance}(\theta_0, \theta_1) = \kl{\rho^{\pi_{\theta_0}}}{\rho^{\pi_{\theta_1}}}.
$$ {#eq-exm-distance-kl}

Using a parameterized policy class
is just a means to the end of optimizing the trajectory distribution,
so it makes sense to optimize over the trajectory distributions directly.
In fact, the NPG update is the only update that is invariant to reparameterizations of the policy space:
if instead of $\theta$, we used some transformation $\phi(\theta)$ to parameterize the policy,
the NPG update would remain the same.
This is why NPG is called a **coordinate-free** optimization algorithm.

We can illustrate this with the following example:
:::

::: {#exm-natural-simple}
#### Natural gradient on a simple problem

Let's step away from RL and consider a simple optimization problem
over Bernoulli distributions $p_\theta \in \Delta(\{ 0, 1 \})$.
This distribution space is parameterized by the success probability $\theta \in [0, 1]$.
The per-sample objective function is $100$ if the Bernoulli trial succeeds
and $1$ if the trial fails.

$$
f(x) = \begin{cases}
100 & x = 1 \\
1 & x = 0.
\end{cases}
$$ {#eq-natural-value}

The objective $J(\theta)$
is the average of $f$ over $x \sim p_\theta$:

$$
\begin{aligned}
    J(\theta) & = \E_{x \sim p_\theta} [f(x)] \\
    &= 100 \theta + 1 (1 - \theta)
\end{aligned}
$$ {#eq-natural-reward-objective}

We can think of the space of such distributions as the line between $(0, 1)$ to $(1, 0)$ on the Cartesian plane:

```{python}
#| fig-cap: A line from (0, 1) to (1, 0)
#| fig-align: center

x = jnp.linspace(0, 1, 50)
y = 1 - x
plt.plot(x, y)
plt.xlabel(r"$\pi(0)$")
plt.ylabel(r"$\pi(1)$")
plt.title("Space of Bernoulli distributions")
plt.show()
```

Clearly the optimal distribution is the constant one $\pi(1) = 1$. Suppose we optimize over the parameterized family $\pi_\theta(1) = \frac{\exp(\theta)}{1+\exp(\theta)}$. Then our optimization algorithm should set $\theta$ to be unboundedly large. Then the "parameter-space" gradient is

$$
\nabla_\theta J(\pi_\theta) = \frac{99 \exp(\theta)}{(1 + \exp(\theta))^2}.
$$

Note that as $\theta \to \infty$ that the increments get closer and closer to $0$; the rate of increase becomes exponentially slow.

However, if we compute the Fisher information "matrix" (which is just a scalar in this case), we can account for the geometry induced by the parameterization.

$$
\begin{aligned}
        F_\theta & = \E_{x \sim \pi_\theta} [ (\nabla_\theta \log \pi_\theta(x))^2 ] \\
                 & = \frac{\exp(\theta)}{(1 + \exp(\theta))^2}.
\end{aligned}
$$

This gives the natural gradient update

$$
\begin{aligned}
        \theta^{k+1} & = \thetacurr + \eta F_{\thetacurr}^{-1} \nabla_ \theta J(\thetacurr) \\
                     & = \thetacurr + 99 \eta
\end{aligned}
$$

which increases at a constant rate, i.e. improves the objective more quickly than parameter-space gradient descent.
::::

Though the NPG now gives a closed-form optimization step,
it requires estimating and computing the inverse Fisher information matrix,
which can be difficult or slow,
especially as the parameter space gets large (@rem-npg-efficiency).
Can we achieve a similar effect without the inverse Fisher information matrix?
This brings us to the **proximal policy optimization** algorithm.

## Penalty-based proximal policy optimization {#sec-ppo-kl}

We can relax the TRPO optimization problem (@eq-trpo) in a different way:
Rather than imposing a _hard constraint_

$$
\kl{\rho^{\picurr}}{\rho^{\pinext}} < \delta,
$$ {#eq-hard-constraint}

we can instead impose a *soft* constraint by subtracting
$\lambda \kl{\rho^{\picurr}}{\rho^{\pinext}}$
from the function to be maximized.
$\lambda > 0$ is a coefficient
that controls the tradeoff between the two terms.
This gives the following objective [@schulman_proximal_2017]:

$$
\begin{aligned}
\theta^{k+1} &\gets \arg\max_{\thetanext} \E_{\tau \sim \rho^{\picurr}} \left[ \sum_{\hi=0}^{\hor-1} \E_{a'_\hi \sim \pi_{\theta}(s_\hi)} A^{\picurr}(s_\hi, a'_\hi) \right] - \lambda \kl{\rho^{\picurr}}{\rho^{\pinext}}
\end{aligned}
$$ {#eq-ppo-penalty}

This optimization problem is also known
as the **Lagrangian** formulation of @eq-trpo.

How do we solve this optimization?
Let us begin by simplifying the $\kl{\rho^{\picurr}}{\rho^{\pi_{\theta}}}$ term.
The state transitions cancel as in @eq-kld-trajectories,
which gives us

$$
\begin{aligned}
    \kl{\rho^{\picurr}}{\rho^{\pi_{\theta}}} & = \E_{\tau \sim \rho^{\picurr}} \left[\log \frac{\rho^{\picurr}(\tau)}{\rho^{\pi_{\theta}}(\tau)}\right]                                                       \\
                                           & = \E_{\tau \sim \rho^{\picurr}} \left[ \sum_{\hi=0}^{\hor-1} \log \frac{\picurr(a_\hi \mid s_\hi)}{\pi_{\theta}(a_\hi \mid s_\hi)}\right] \\
                                           & = \E_{\tau \sim \rho^{\picurr}} \left[ \sum_{\hi=0}^{\hor-1} \log \frac{1}{\pi_{\theta}(a_\hi \mid s_\hi)}\right] + c
\end{aligned}
$$

where $c$ doesn't depend on $\theta$ and can be ignored.
This gives the objective

$$
L^k(\theta)
=
\E_{s_0, \dots, s_{\hor-1} \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1} \E_{a_\hi \sim \pi_{\theta}(s_\hi)} A^{\picurr}(s_\hi, a_\hi)
\right] - \lambda \E_{\tau \sim \rho^{\picurr}}
\left[ \sum_{\hi=0}^{\hor-1} \log \frac{1}{\pi_{\theta}(a_\hi \mid s_\hi)}\right]
$$

Once again,
this takes an expectation over trajectories.
But here we cannot directly sample trajectories from $\picurr$,
since in the first term, the actions actually come from $\pi_\theta$.
To make this term line up with the other expectation,
we would need the actions to also come from $\picurr$.
This should sound familiar:
we want to estimate an expectation over one distribution by sampling from another.
We can use importance sampling (@thm-importance-sampling) to rewrite the inner expectation:

$$
\E_{a_\hi \sim \pinext(s_\hi)} A^{\picurr}(s_\hi, a_\hi)
=
\E_{a_\hi \sim \picurr(s_\hi)} \left[
    \frac{\pinext(a_\hi \mid s_\hi)}{\picurr(a_\hi \mid s_\hi)} A^{\picurr}(s_\hi, a_\hi)
\right]
$$ {#eq-likelihood-ratio}

::: {#rem-importance-interpretation}
#### Interpretation of likelihood ratio

Suppose $a^+$ is a "good" action for $\picurr$ in state $s_\hi$,
i.e. $A^\picurr(s_\hi, a^+) > 0$.
Then maximizing @eq-likelihood-ratio encourages $\thetanext$ to _increase_ the probability ratio
$\pinext(a_\hi \mid s_\hi) / \picurr(a_\hi \mid s_\hi)$.

Otherwise,
if $a^-$ is a "bad" action for $\picurr$ in state $s_\hi$
(i.e. $A^\picurr(s_\hi, a^-) < 0$),
then maximizing @eq-likelihood-ratio encourages $\thetanext$ to _decrease_ the probability ratio
$\pinext(a_\hi \mid s_\hi) / \picurr(a_\hi \mid s_\hi)$.
:::

Now we can combine the expectations together to get the objective

::: {#def-penalty-ppo}
#### Penalty objective

$$
L^k(\theta) = \E_{\tau \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1} \left(
    \frac{\pi_\theta(a_\hi \mid s_\hi)}{\picurr(a_\hi \mid s_\hi)} A^{\picurr}(s_\hi, a_\hi)
    - \lambda \log \frac{1}{\pi_\theta(a_\hi \mid s_\hi)}
    \right)
\right]
$$
:::

Now we can estimate this function by a sample average over trajectories from $\picurr$. Remember that to complete a single iteration of PPO, we execute

$$
\theta^{k+1} \gets \arg\max_{\theta} L^k(\theta).
$$

If $L^k$ is differentiable, we can optimize it by gradient descent, completing a single iteration of PPO.

```{python}
from typing import TypeVar

State = TypeVar("State")
Action = TypeVar("Action")

def ppo_proximal(
    env,
    pi: Callable[[Float[Array, " D"]], Callable[[State, Action], float]],
    λ: float,
    theta_init: Float[Array, " D"],
    n_iters: int,
    n_fit_trajectories: int,
    n_sample_trajectories: int,
):
    theta = theta_init
    for k in range(n_iters):
        fit_trajectories = sample_trajectories(env, pi(theta), n_fit_trajectories)
        A_hat = fit(fit_trajectories)

        sample_trajectories = sample_trajectories(env, pi(theta), n_sample_trajectories)
        
        def objective(theta_opt):
            total_objective = 0
            for tau in sample_trajectories:
                for s, a, _r in tau:
                    total_objective += pi(theta_opt)(s, a) / pi(theta)(s, a) * A_hat(s, a) + λ * jnp.log(pi(theta_opt)(s, a))
            return total_objective / n_sample_trajectories
        
        theta = optimize(objective, theta)

    return theta

latex(ppo_proximal)
```

## Advantage clipping {#sec-ppo-clip}

Recall that the main point of proximal policy optimization methods (TRPO, NPG, PPO)
is to encourage the updated policy (after taking a gradient step)
to remain similar to the current one.
These methods used the KL divergence to measure the distance between policies.
@schulman_proximal_2017 proposed an alternative way to constrain the step size
based on _clipping_ a certain objective function.
This method, known as "PPO-clip" or the "clipped surrogate" objective function,
is the most widely used proximal policy optimization algorithm in practice.

Above, in @eq-likelihood-ratio,
we constructed an objective function
by applying importance sampling
to the performance difference lemma (@thm-pdl).
Without the KL divergence penalty,
this becomes

$$
L^k(\thetanext) = \E_{\tau \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1}
    \frac{\pi_\thetanext(a_\hi \mid s_\hi)}{\picurr(a_\hi \mid s_\hi)} A^{\picurr}(s_\hi, a_\hi)
\right]
\approx V(\pi_\thetanext) - V(\picurr).
$$ {#eq-penalty-no-kl}

In the following part,
define the policy ratio at time $\hi \in [\hor]$ as

$$
\Lambda_\hi(\thetanext, \thetacurr) = \frac{\pinext(a_\hi \mid s_\hi)}{\picurr(a_\hi \mid s_\hi)}.
$$ {#eq-policy-ratio}

The clipped surrogate objective function modifies @eq-penalty-no-kl
to remove incentives
for $\pinext$ to differ greatly from $\picurr$.
Specifically, we choose some small $\epsilon > 0$,
and constrain $\Lambda_\hi(\thetanext, \thetacurr) \in (1-\epsilon, 1+\epsilon)$.
Formally,

$$
\Lambda_\hi^\text{clipped}(\thetanext, \thetacurr)
=
\text{clip}(\Lambda_\hi(\thetanext, \thetacurr), 1-\epsilon, 1+\epsilon),
$$ {#eq-clip-ratio}

where

$$
\text{clip}(x, a, b) := \max \{ a, \min \{ x, b \} \}.
$$ {#eq-def-clip}

::: {#rem-interpret-clip}
#### Interpretation

As in @rem-importance-interpretation,
suppose $a_\hi^+$ and $a_\hi^-$ are a "good" and "bad" action in $s_\hi$,
i.e. $A^\picurr(s_\hi, a_\hi^+) > 0$ and $A^\picurr(s_\hi, a_\hi^-) < 0$.
By clipping $\Lambda_\hi(\thetanext, \thetacurr)$,
no additional benefit is gained by increasing $\pinext(a_\hi^+ \mid s_\hi)$
above $(1+\epsilon) \picurr(a_\hi^+ \mid s_\hi)$,
or by decreasing $\picurr(a_\hi^- \mid s_\hi)$
under $(1-\epsilon) \picurr(a_\hi^- \mid s_\hi)$.
:::

As a final step,
we only use this clipped objective if it is smaller than the original objective.
For example, if $A^\picurr(s_\hi, a_\hi) > 0$ and $\Lambda_\hi(\thetanext, \thetacurr) \ll 1 - \epsilon$,
then the clipped objective would disproportionately incentivize taking action $a_\hi$.
The same thing happens if $A^\picurr(s_\hi, a_\hi) < 0$ and $\Lambda_\hi(\thetanext, \thetacurr) > 1+\epsilon$.

Putting these together,
this results in only clipping the policy ratio
on the side that it would normally move towards:

```{python}
#| label: fig-ppo-clip
#| fig-cap: Clipped surrogate objective function
#| fig-subcap:
#| - For a "good" action $a_\hi^+$ with positive advantage.
#| - For a "poor" action $a_\hi^-$ with negative advantage.
#| layout-ncol: 2

x = jnp.linspace(0, 2, 400)
y = jnp.minimum(x, jnp.clip(x, 0.8, 1.2))

plt.plot(x, y)
plt.axvline(1.0, color='gray', linestyle='--', label=r"$\Lambda = 1$")
plt.axvline(1.2, color='red', linestyle='--', label=r"$1+\epsilon$")
plt.xlabel(r"$\Lambda$")
plt.ylabel(r"$\min(\Lambda, \text{clip}(\Lambda, 1-\epsilon, 1+\epsilon))$")
plt.legend()
plt.xlim(0, 2)
plt.ylim(0, 1.5)
plt.title("Advantage Positive")

plt.show()

y_neg = jnp.minimum(-x, jnp.clip(-x, -1.2, -0.8))

plt.plot(x, y_neg)
plt.axvline(1.0, color='gray', linestyle='--', label=r"$\Lambda = 1$")
plt.axvline(0.8, color='red', linestyle='--', label=r"$1-\epsilon$")
plt.xlabel(r"$\Lambda$")
plt.ylabel(r"$\min(-\Lambda, \text{clip}(-\Lambda, -(1+\epsilon), -(1-\epsilon)))$")
plt.legend()
plt.xlim(0, 2)
plt.ylim(-1.5, 0)
plt.title("Advantage Negative")

plt.show()
```

::: {#def-ppo-clip}
#### Clipped surrogate objective

The PPO-Clip objective is

$$
L^\ki(\theta) = \E_{\tau \sim \rho^{\picurr}} \left[
    \sum_{\hi=0}^{\hor-1}
    \min \left\{
        \Lambda_\hi(\theta, \thetacurr) A^{\picurr}(s_\hi, a_\hi),
        \text{clip}(
            \Lambda_\hi(\theta, \thetacurr),
            1-\epsilon,
            1+\epsilon
        )
        A^\picurr(s_\hi, a_\hi)
    \right\}
\right]
$$ {#eq-clip-objective}
:::


::: {#rem-convergence}
#### Convergence of the clipped surrogate objective

From a traditional optimization perspective,
this objective function seems challenging to analyze:
the clip operation (@eq-def-clip) is not differentiable
:::



## Key takeaways

Policy gradient methods are a powerful family of algorithms
that optimize the expected total reward
by iteratively updating the policy parameters.
Precisely,
we estimate the gradient of the expected total reward (with respect to the parameters),
and update the parameters in that direction.
But estimating the gradient is a tricky task!
We saw many ways to reduce the variance of the gradient estimator,
culminating in the advantage-based expression @eq-estimator-adv.

But updating the parameters doesn't entirely solve the problem:
Sometimes, a small step in the parameters might lead to a big step in the policy.
To avoid changing the policy too much at each step,
we must account for the curvature in the parameter space.
We first did this explicitly with @sec-trpo,
and then saw ways to relax the constraint
via the natural policy gradient update (@sec-npg),
the PPO-KL update (@sec-ppo-kl),
and the PPO-Clip update (@sec-ppo-clip).

These are still popular methods to this day,
especially because they efficiently integrate with *deep neural networks* for representing complex functions.

## Bibliographic notes {#sec-pg-bib}

- @kakade_approximately_2002

from spinning up:
- @mnih_asynchronous_2016 present multi-threaded variants of one-step Sarsa, n-step Q-learning, and advantage actor-critic.
The gradient estimates are accumulated across learners and periodically updated and synchronized across all learners.
All policy gradient methods can be parallelized in this way.
- @schulman_trust_2015
- @schulman_high-dimensional_2016
- @schulman_proximal_2017
- @wu_scalable_2017
- @wang_sample_2017
- @haarnoja_soft_2018




::: {.content-visible when-profile="thesis"}
## Differences from course

:::
