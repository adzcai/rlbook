{{< include _macros.tex >}}

::: {.hidden}
\providecommand{\regret}{\text{Regret}_\Ep}
\providecommand{\remainder}{\mathbin{\%}}
:::

# Multi-Armed Bandits {#sec-bandits}

## Introduction

The **multi-armed bandit** (MAB\index{multi-armed bandit}) problem is a simple setting for studying
the basic challenges of sequential decision-making.
In this setting,
an agent repeatedly chooses from a fixed set of actions,
called **arms**,
each of which has an associated reward distribution.
The agent's goal is to maximize the total reward it receives over some time period.
Here are a few examples:


:::: {layout-ncol="2"}
::: {#exm-advertising}
#### Online advertising

Let's suppose you, the agent, are an advertising company.
You have $\Arm$ different ads that you can show to users.
Let's suppose you are targeting a single user.
You receive $1$ reward if the user clicks the ad, and $0$ otherwise.
Thus, the unknown *reward distribution* associated to each ad
is a Bernoulli distribution defined by the probability that the user clicks on the ad.
Your goal is to maximize the total number of clicks by the user.
:::

![Deciding which ad to put on a billboard can be treated as a MAB problem. Image from @negative_space_photo_2015.](shared/advertisements.jpg)
::::

:::: {layout-ncol="2"}
::: {#exm-clinical-trials}
#### Clinical trials

Suppose you're a pharmaceutical company, and you're testing a new drug.
You have $\Arm$ different dosages of the drug that you can administer to patients.
You receive $1$ reward if the patient recovers, and $0$ otherwise.
Thus, the unknown reward distribution associated with each dosage
is a Bernoulli distribution defined by the probability that the patient recovers.
Your goal is to maximize the total number of patients that recover.
:::

![Optimizing between different medications can be treated as a MAB problem. Image from @pixabay_20_2016.](shared/medication.jpg)
::::

The MAB is the simplest setting for exploring the **exploration-exploitation tradeoff**:
should the agent choose new actions to learn more about the environment,
or should it choose actions that it already knows to be good?

In this chapter, we will introduce the multi-armed bandits setting,
and discuss some of the challenges that arise when trying to solve problems in this setting.
We will also introduce some of the key concepts that we will use throughout the book,
such as regret and exploration-exploitation tradeoffs.


```{python}
from jaxtyping import Float, Array
import numpy as np
import latexify
from typing import Callable, Union
import matplotlib.pyplot as plt
import utils

import solutions.bandits as solutions

np.random.seed(184)

def random_argmax(ary: Array) -> int:
    """Take an argmax and randomize between ties."""
    max_idx = np.flatnonzero(ary == ary.max())
    return np.random.choice(max_idx).item()


# used as decorator
latex = latexify.algorithmic(
    trim_prefixes={"mab"},
    id_to_latex={"arm": "a_\ep", "reward": "r", "means": "\mu", "t": r"\ep", "T": r"\Ep"},
    use_math_symbols=True,
)
```

:::: {#rem-multi-armed}
#### Etymology

The name “multi-armed bandit” comes from slot machines in casinos,
which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.
::::

| Environment is...                                 | known (planning/optimal control)   | unknown (learning from experience) |
| ------------------------------------------------- | ---------------------------------- | ---------------------------------- |
| stateless/invariant ($\vert\mathcal{S}\vert = 1$) | (not interesting)                  | multi-armed bandits                |
| stateful/dynamic                                  | MDP planning (dynamic programming) | "full" RL                          |

: How bandits leads into the full RL problem. {#tbl-bandits-full}

## The multi-armed bandit problem {#sec-bandit-problem}

Let's frame the MAB problem mathematically.
In both @exm-advertising and @exm-clinical-trials,
the outcome of each decision is binary (i.e. $0$ or $1$).
This is the **Bernoulli bandit** problem,
which we'll use throughout the chapter.

:::: {layout-ncol="2"}
::: {#def-bernoulli-bandit}
#### Bernoulli bandit problem\index{Bernoulli bandit}

Let $\Arm$ denote the number of arms.
We'll label them $0, \dots, \Arm-1$ and use *superscripts* to indicate the arm index $\arm$.
Since we seldom need to raise a number to a power,
this won't cause much confusion.

Let $\mu^\arm$ be the mean of arm $\arm$,
such that pulling arm $\arm$ either returns reward $1$ with probability $\mu^\arm$ or $0$ otherwise.

The agent gets $\Ep$ chances to pull an arm.
The task is to determine a strategy for maximizing the total reward.
:::

![Each arm $\arm$ in a Bernoulli bandit is a biased coin with probability $\mu^\arm$ of showing heads. Each "pull" corresponds to flipping a coin. The goal is to maximize the number of heads given $\Ep$ flips. Image from @pixabay_coins_2016.](shared/flipping-coins.jpg)
::::

```{python}
class MAB:
    """The Bernoulli multi-armed bandit environment."""

    def __init__(self, means: Float[Array, " K"], T: int):
        """Constructor.

        :param means: the means (success probabilities) of the reward distributions for each arm
        :param T: the time horizon
        """
        assert all(0 <= p <= 1 for p in means)
        self.means = means
        self.T = T
        self.K = self.means.size
        self.best_arm = random_argmax(self.means)

    def pull(self, k: int) -> int:
        """Pull the `k`-th arm and sample from its (Bernoulli) reward distribution."""
        reward = np.random.rand() < self.means[k].item()
        return +reward
```

```{python}
mab = MAB(means=np.array([0.1, 0.8, 0.4]), T=100)
```

In pseudocode, the agent's interaction with the MAB environment
can be described by the following process:

```{python}
#| label: fig-mab-interaction
#| fig-cap: The MAB interaction loop. $a_\ep \in [\Arm]$ denotes the $\ep$th arm pulled.

def mab_loop(mab: MAB, agent: "Agent") -> int:
    for t in range(mab.T):
        arm = agent.choose_arm()  # in 0, ..., K-1
        reward = mab.pull(arm)
        agent.update_history(arm, reward)


latex(mab_loop)
```

```{python}
class Agent:
    """Stores the pull history and uses it to decide which arm to pull next.
        
    Since we are working with Bernoulli bandit,
    we can summarize the pull history concisely in a (K, 2) array.
    """

    def __init__(self, K: int, T: int):
        """The MAB agent that decides how to choose an arm given the past history."""
        self.K = K
        self.T = T
        self.rewards = []  # for plotting
        self.choices = []
        self.history = np.zeros((K, 2), dtype=int)

    def choose_arm(self) -> int:
        """Choose an arm of the MAB. Algorithm-specific."""
        ...

    def count(self) -> int:
        """The number of pulls made. Also the current step index."""
        return len(self.rewards)

    def update_history(self, arm: int, reward: int):
        self.rewards.append(reward)
        self.choices.append(arm)
        self.history[arm, reward] += 1
```

What's the *optimal* strategy for the agent,
i.e. the one that achieves the highest expected reward?
Convince yourself that the agent should try
to always pull the arm with the highest expected reward:

::: {#def-optimal-arm}
#### Optimal arm

We call the arm whose reward distribution has the highest mean
the **optimal arm**:

$$
\begin{aligned}
\arm^\star &:= \arg\max_{\arm \in [\Arm]} \mu^\arm \\
\mu^\star &:= \mu^{\arm^\star} = \max_{\arm \in [\Arm]} \mu^\arm.
\end{aligned}
$$ {#eq-optimal-arm}
:::

### Regret

When analyzing an MAB algorithm,
rather than measuring its performance in terms of the (expected) total reward,
we instead compare it to the "oracle" strategy
that knows the optimal arm in advance.
This gives rise to a measure of performance
known as **regret**,
which answers,
how much better _could_ you have done
had you known the optimal arm from the beginning?
This provides a more meaningful baseline than the expected total reward,
whose magnitude may vary widely depending on the bandit problem.

::: {#exm-regret}
#### Regret provides a more meaningful baseline

Consider the problem of grade inflation or deflation.
If you went to a different school or university,
you might have gotten a very different grade,
independently of your actual ability.
This is like comparing bandit algorithms based on the expected total reward:
the baseline varies depending on the problem (i.e. school),
making it hard to compare the algorithms (i.e. students) themselves.
Instead, it makes more sense to measure you relative to the (theoretical) best student from your school.
This is analogous to measuring algorithm performance using regret.
:::

:::: {#def-regret}
#### Regret\index{regret}

The agent's (cumulative) **regret** after $\Ep$ pulls is defined as

$$
\regret := \sum_{\ep=0}^{\Ep-1} \mu^\star - \mu^{a_\ep},
$$ {#eq-regret}

where $a_\ep \in [\Arm]$ is the arm chosen on the $\ep$th pull.
::::

```{python}
def regret_per_step(mab: MAB, agent: Agent):
    """Get the difference from the average reward of the optimal arm. The sum of these is the regret."""
    return [mab.means[mab.best_arm] - mab.means[arm] for arm in agent.choices]
```

This depends on the *true means* of the pulled arms,
and is independent of the observed rewards.
The regret $\regret$ is a random variable
where the randomness comes from the agent's strategy
(i.e. the sequence of actions $a_0, \dots, a_{\Arm-1}$).

Also note that we care about the _total_ regret across all decisions,
rather than just the final decision.
If we only cared about the quality of the final decision,
the best strategy would be to learn as much as possible about the environment,
and then use that knowledge to make the best possible decision at the last step [@bubeck_pure_2011].
This is a **pure exploration** problem
since the agent is never penalized
for taking suboptimal actions up to the final decision.
Minimizing the _cumulative_ regret (or maximizing the _cumulative_ reward)
means we must strike a balance between _exploration_ and _exploitation_
throughout the entire decision-making process.

Throughout the chapter,
we will try to upper bound the regret of various algorithms in two different senses:

1.  Upper bound the *expected regret*,
    i.e. show that for some upper bound $M_\Ep$,
    $$
    \E[\regret] \le M_\Ep.
    $$ {#eq-expected-regret-bound}
2.  Find a *high-probability* upper bound on the regret,
    i.e. show that for some small _failure probability_ $\delta > 0$,
    $$
    \pr(\regret \le M_{\Ep, \delta}) \ge 1-\delta
    $$ {#eq-probabilistic-regret-bound}
    for some upper bound $M_{\Ep, \delta}$.

The first approach says that the regret is at most $M_\Ep$ in expectation.
However, the agent might still achieve a higher or lower regret
on a particular randomization.
The second approach says that, with probability at least $1-\delta$,
the agent will achieve regret at most $M_{\Ep, \delta}$.
However, it doesn't say anything about the regret in the remaining $\delta$ fraction of runs,
which could be arbitrarily high.

::: {#exr-markov-expected}
#### Expected regret bounds yield probabilistic regret bounds

Suppose we have an upper bound $M_\Ep$ on the expected regret.
Show that, with probability $1-\delta$,
$M_{\Ep, \delta} := M_\Ep / \delta$ upper bounds the regret.
Note this is a much higher bound!
For example, if $\delta = 0.01$,
the high-probability bound is $100$ times as large.
:::

::: {#rem-pac-bounds}
#### Issues with regret

Is regret the right way to measure performance?
Consider the following two algorithms:

1.  An algorithm which sometimes chooses very bad arms.
2.  An algorithm which often chooses slightly suboptimal arms.

We might care a lot about this distinction in practice,
for example, in healthcare,
where an occasional very bad decision could have extreme consequences.
However, these two algorithms could achieve the exact same expected total regret.

Other performance measures include
**probably approximately correct** (PAC) bounds
and **uniform** high-probability regret bounds.
For a stronger guarantee,
@dann_unifying_2017 introduced the **Uniform-PAC** bound.
We won't discuss these in detail here.
:::

We'd like to achieve **sublinear regret** in expectation:

$$
\E[\regret] = o(\Ep).
$$ {#eq-sublinear-regret}

(See @sec-o-notation if you're unfamiliar with big-oh notation.)
An algorithm with sublinear regret
will eventually do as well as the oracle strategy
as $\Ep \to \infty$.

::: {#rem-sublinear-issues}
#### Interpreting sublinear regret

An algorithm with sublinear regret in expectation
doesn't necessarily always choose the correct arm.
In fact, such an algorithm can still choose a suboptimal arm infinitely many times.
Consider a two-armed Bernoulli bandit with $\mu^0 = 0$ and $\mu^1 = 1$.
An algorithm that chooses arm $0$ $\sqrt{\Ep}$ times out of $\Ep$
still achieves $O(\sqrt{\Ep})$ regret in expectation.

Also, sublinear regret measures the _asymptotic_ performance of the algorithm
as $\Ep \to \infty$.
It doesn't tell us about the _rate_ at which the algorithm approaches optimality.
:::

In the remaining sections,
we'll walk through a series of MAB algorithms
and see how they make different tradeoffs
between exploration and exploitation.

```{python}
def plot_strategy(mab: MAB, agent: Agent):
    plt.figure(figsize=(6, 4))

    # plot reward and cumulative regret
    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label="reward")
    cum_regret = np.cumsum(regret_per_step(mab, agent))
    plt.plot(np.arange(mab.T), cum_regret, label="cumulative regret")

    # draw colored circles for arm choices
    colors = ["red", "green", "blue"]
    color_array = [colors[k] for k in agent.choices]
    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c=color_array)

    # Add legend entries for each arm
    for i, color in enumerate(colors):
        plt.scatter([], [], c=color, label=f'arm {i}')

    # labels and title
    plt.xlabel("Pull index")
    plt.legend()
    plt.show()
```

## Pure exploration {#sec-pure-exploration}

A trivial strategy is to always choose arms at random (i.e. "pure exploration").

::: {#def-pure-exploration}
#### Pure exploration

On the $\ep$th pull,
the arm $a_\ep$ is sampled uniformly at random
from all arms:

$$
a_\ep \sim \text{Unif}([\Arm])
$$ {#eq-pure-exploration}
:::

```{python}
class PureExploration(Agent):
    def choose_arm(self):
        """Choose an arm uniformly at random."""
        return solutions.pure_exploration_choose_arm(self)
```

::: {#thm-pure-exploration-regret}
#### Pure exploration expected regret

The pure exploration strategy achieves regret $\Theta(\Ep)$ in expectation.
:::

::: {.proof}
By choosing arms uniformly at random,

$$
\E_{a_\ep \sim \text{Unif}([\Arm])}[\mu^{a_\ep}]
=
\bar \mu
:=
\frac{1}{\Arm} \sum_{\arm=0}^{\Arm-1} \mu^\arm
$$ {#eq-pure-exploration-1}

so the expected regret is simply

$$
\begin{aligned}
    \E[\regret]
    &= \sum_{\ep = 0}^{\Ep-1} \E[\mu^\star - \mu^{a_\ep}]
    \\
    &= \Ep \cdot (\mu^\star - \bar \mu).
\end{aligned}
$$ {#eq-pure-exploration-2}

This scales as $\Theta(\Ep)$ in expectation,
i.e. *linear* in the number of pulls $\Ep$.
:::

```{python}
#| label: fig-pure-exploration
#| fig-cap: Reward and cumulative regret of the pure exploration algorithm.

agent = PureExploration(mab.K, mab.T)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

Pure exploration doesn't use any information about the environment to improve its strategy.
The distribution over its arm choices always appears "(uniformly) random".

## Pure greedy {#sec-pure-greedy}

How might we improve on pure exploration?
Instead, we could try each arm once,
and then commit to the one with the highest observed reward.
We'll call this the **pure greedy** strategy.

::: {#def-pure-greedy}
#### Pure greedy

On the $\ep$th pull,
the arm $a_\ep$ is chosen according to

$$
a_\ep := \begin{cases}
\ep & \ep < \Arm \\
\arg\max_{\arm \in [\Arm]} r_\arm & \ep \ge \Arm
\end{cases}
$$ {#eq-pure-greedy}

where $r_\ep$ denotes the reward obtained on the $\ep$th pull.
:::

```{python}
class PureGreedy(Agent):
    def choose_arm(self):
        """Choose the arm with the highest observed reward on its first pull."""
        return solutions.pure_greedy_choose_arm(self)
```

How does the expected regret of this strategy
compare to that of pure exploration?
We'll do a more general analysis in the following section.
Now, for intuition, suppose there's just $K=2$ arms with means $\mu^1 > \mu^0$.
If $r_1 > r_0$,
then we repeatedly pull arm $1$ and achieve zero regret per pull.
If $r_0 > r_1$,
then we repeatedly pull arm $0$ and incur $\mu^1 - \mu^0$ regret per pull.
Accounting for the first two pulls
and the case of a tie,
we obtain the following lower bound:

$$
\E[\regret] \ge \pr(r^0 > r^1) \Ep \cdot (\mu^1 - \mu^0)
$$ {#eq-pure-greedy-regret}

This is still $\Theta(\Ep)$,
the same as pure exploration!

```{python}
#| label: fig-pure-greedy
#| fig-cap: Reward and cumulative regret of the pure greedy algorithm.

agent = PureGreedy(mab.K, mab.T)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

The cumulative regret is a straight line
because the regret only depends on the arms chosen and not the actual reward observed.
In fact, we see in @fig-pure-greedy
that the greedy algorithm can get lucky on the first set of pulls
and act entirely optimally for that experiment!
But its _average_ regret is still linear,
since the chance of sticking with a suboptimal arm is nonzero.



## Explore-then-commit {#sec-etc}

We can improve the pure greedy algorithm (@def-pure-greedy) as follows:
let's reduce the variance of the reward estimates
by pulling each arm $N_\text{explore} \ge 1$ times before committing.
This is called the **explore-then-commit** strategy.

::: {#def-etc}
#### Explore-then-commit

The explore-then-commit algorithm first pulls each arm $N_\text{explore}$ times.
Let

$$
\hat \mu^\arm := \frac{1}{N_\text{explore}} \sum_{n=0}^{N_\text{explore}-1} r_{\arm N_\text{explore} + n}
$$ {#eq-etc-average}

denote the empirical mean reward of arm $\arm$ during these pulls.
Afterwards,
on the $\ep$th pull
(for $\ep \ge N_\text{explore} \Arm$),
it chooses

$$
a_\ep := \arg\max_{\arm \in [\Arm]} \hat \mu^\arm.
$$ {#eq-etc}
:::

Note that the “pure greedy” strategy above is just the special case where $N_\text{explore} = 1$.

```{python}
class ExploreThenCommit(Agent):
    def __init__(self, K: int, T: int, N_explore: int):
        super().__init__(K, T)
        self.N_explore = N_explore

    def choose_arm(self):
        return solutions.etc_choose_arm(self)
```

```{python}
#| label: fig-etc
#| fig-cap: Reward and cumulative regret of the explore-then-commit algorithm

agent = ExploreThenCommit(mab.K, mab.T, mab.T // 15)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

This algorithm finds the true optimal arm
more frequently than the pure greedy strategy.
We would expect ETC to then have a lower expected regret.
We will show that ETC achieves the following regret bound:

::: {#thm-etc-regret}
#### ETC high-probability regret bound

The regret of ETC satisfies, with high probability,

$$
\regret = \widetilde{O}(\Ep^{2/3} \Arm^{1/3}),
$$ {#eq-etc-regret}

where the tilde means we ignore logarithmic factors.
:::

:::: {.proof}

Let's analyze the expected regret of the explore-then-commit strategy by splitting it up
into the exploration and exploitation phases.

**Exploration phase**.
This phase takes $N_\text{explore} \Arm$ pulls.
Since at each step we incur at most $1$ regret,
the total regret is at most $N_\text{explore} \Arm$.

**Exploitation phase**.
This will take a bit more effort.
We'll prove that for any total time $\Ep$,
we can choose $N_\text{explore}$ such that with arbitrarily high probability,
the regret is sublinear.

Let $\widehat{\arm}$ denote the arm chosen after the exploration phase.
We know the regret from the exploitation phase is

$$
\Ep_\text{exploit} (\mu^\star - \mu^{\widehat{\arm}}) \qquad \text{where} \qquad \Ep_\text{exploit} := \Ep - N_\text{explore} \Arm.
$$ {#eq-etc-exploit-regret}

So we'd need $\mu^\star - \mu^{\widehat{\arm}} \to 0$
as $\Ep \to \infty$
in order to achieve sublinear regret.
How can we do this?

Let's define $\Delta^\arm := \hat \mu^\arm - \mu^\arm$
to denote how far the mean estimate for arm $\arm$ is from the true mean.
How can we bound this quantity?
We'll use the following useful inequality for i.i.d. bounded random variables:

::: {#thm-hoeffding}
#### Hoeffding's inequality

Let $X_0, \dots, X_{N-1}$ be i.i.d. random variables
with $X_n \in [0, 1]$ almost surely for each $n \in [N]$.
Then for any $\delta > 0$,

$$
\pr\left(
    \left| \frac{1}{N} \sum_{n=1}^N (X_n - \E[X_n]) \right| > \sqrt{\frac{\ln(2/\delta)}{2N}}
\right) \le \delta.
$$ {#eq-hoeffding}
:::

::: {.proof}
The proof of this inequality is beyond the scope of this book.
See @vershynin_high-dimensional_2018 [ch. 2.2].
:::

We can apply this directly to the rewards for a given arm $\arm$,
since the rewards from that arm are i.i.d.:

$$
\pr\left(|\Delta^\arm | > \sqrt{
    \frac{\ln(2/\delta)}{2N_\text{explore}}
} \right) \le \delta.
$$ {#eq-hoeffding-etc}

This can be thought of as a $1-\delta$ confidence interval for the true arm mean $\mu^\arm$.
But we can't apply this to arm $\widehat{\arm}$ directly
since $\widehat{\arm}$ is itself a random variable.
Instead, we need to bound the error across all the arms simultaneously,
so that the resulting bound will apply
*no matter what* $\widehat{\arm}$ “crystallizes” to.
The **union bound** provides a simple way to do this.
(See @thm-union-bound for a review.)

In our case,
we have one event per arm,
stating that the CI for that arm contains the true mean.
The probability that _all_ of the CIs are accurate is then

$$
\begin{aligned}
    \pr\left(
        \forall \arm \in [\Arm] : |\Delta^\arm|
        \le
        \sqrt{\frac{\ln(2/\delta)}{2N_\text{explore}}}
    \right) &\ge 1- \Arm \delta.
\end{aligned}
$$ {#eq-hoeffding-union}

This lower-bounds the probability that the CI for arm $\widehat{\arm}$ is accurate.
Let us return to our original task of bounding $\mu^\star - \mu^{\widehat{\arm}}$.
We apply the useful trick of “adding zero”:

$$
\begin{aligned}
    \mu^{\arm^\star} - \mu^{\widehat{\arm}} &= \mu^{\arm^\star} - \mu^{\widehat{\arm}} + (\hat \mu^{\arm^\star} - \hat \mu^{\arm^\star}) + (\hat \mu^{\widehat{\arm}} - \hat \mu^{\widehat{\arm}}) \\
    &= \Delta^{\widehat{\arm}} - \Delta^{\arm^\star} + \underbrace{(\hat \mu^{\arm^\star} - \hat \mu^{\widehat{\arm}})}_{\le 0 \text{ by definition of } \widehat{\arm}} \\
    &\le 2 \sqrt{\frac{\ln(2 \Arm / \delta')}{2N_\text{explore}}} \text{ with probability at least } 1-\delta'
\end{aligned}
$$ {#eq-hoeffding-trick}

where we've set $\delta' := \Arm \delta$. Putting this all
together, we've shown that, with probability $1 - \delta'$,

$$
\regret \le N_\text{explore} \Arm + \Ep_\text{exploit} \cdot \sqrt{\frac{2\ln(2 \Arm/\delta')}{N_\text{explore}}}.
$$ {#eq-etc-regret-1}

Note that it suffices for $N_\text{explore}$ to be on the order of $\sqrt{\Ep}$ to achieve sublinear regret.
In particular, we can find the optimal $N_\text{explore}$
by setting the derivative of @eq-etc-regret-1 with respect to $N_\text{explore}$ to zero:

$$
\begin{aligned}
    0 &= \Arm - \Ep_\text{exploit} \cdot \frac{1}{2} \sqrt{\frac{2\ln(2 \Arm /\delta')}{N_\text{explore}^3}} \\
    N_\text{explore} &= \left(
        \Ep_\text{exploit} \cdot \frac{\sqrt{\ln(2 \Arm / \delta')/2}}{ \Arm }
    \right)^{2/3}
\end{aligned}
$$ {#eq-etc-regret-2}

Plugging this into the expression for the regret,
we have (still with probability $1-\delta'$):

$$
\begin{aligned}
    \regret &\le 3 \Ep^{2/3} \sqrt[3]{\Arm \ln(2 \Arm /\delta') / 2} \\
    &= \widetilde{O}(\Ep^{2/3} \Arm^{1/3}),
\end{aligned}
$$ {#eq-etc-bound}

satisfying @thm-etc-regret.
::::

The ETC algorithm is rather "abrupt"
in that it switches from exploration to exploitation after a fixed number of pulls.
What if the total number of pulls $\Ep$ isn't known in advance?
We'll need to use a more gradual transition,
which brings us to the *epsilon-greedy* algorithm.



## Epsilon-greedy {#sec-epsilon-greedy}

Instead of doing all of the exploration and then all of the exploitation in separate phases,
which requires knowing the time horizon
beforehand,
we can instead _interleave_ exploration and exploitation by,
at each pull,
choosing a random action with some probability.
We call this the **epsilon-greedy** algorithm.

::: {#def-epsilon-greedy}
#### Epsilon-greedy

Let $N^\arm_\ep$ denote the number of times
that arm $\arm$ is pulled within the first $\ep$ overall pulls,
and let $\hat \mu^\arm_\ep$ denote the sample mean of the corresponding rewards:

$$
\begin{aligned}
    N^\arm_\ep &:= \sum_{\ep' = 0}^{\ep-1} \mathbf{1} \{ a_{\ep'} = \arm \}, \\
    \hat \mu^\arm_\ep &:= \frac{1}{N^\arm_\ep} \sum_{\ep' = 0}^{\ep-1} \mathbf{1} \{ a_{\ep'} = \arm \} r_{\ep'}. \\
\end{aligned}
$$ {#eq-bandits-count}

On the $\ep$th pull,
the arm $a_\ep$ is chosen according to

$$
a_\ep := \begin{cases}
\text{sample from } \text{Unif}([K]) & \text{with probability } \epsilon_\ep \\
\arg\max_{\arm \in [K]} \hat \mu^\arm_\ep & \text{with probability } 1 - \epsilon_\ep,
\end{cases}
$$ {#eq-epsilon-greedy}

where $\epsilon_\ep \in [0, 1]$ is the probability of choosing a random action.
:::

```{python}
class EpsilonGreedy(Agent):
    def __init__(
        self,
        K: int,
        T: int,
        ε_array: Float[Array, " T"],
    ):
        super().__init__(K, T)
        self.ε_array = ε_array

    def choose_arm(self):
        return solutions.epsilon_greedy_choose_arm(self)
```

```{python}
#| label: fig-epsilon-greedy
#| fig-cap: Reward and cumulative regret of the epsilon-greedy algorithm

agent = EpsilonGreedy(mab.K, mab.T, np.full(mab.T, 0.1))
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

We let the exploration probability $\epsilon_\ep$ vary over time.
This lets us gradually *decrease* $\epsilon_\ep$
as we learn more about the reward distributions
and no longer need to explore as much.

::: {#exr-constant-eps}
#### Regret for constant epsilon

What is the asymptotic expected regret of the algorithm if we set $\epsilon$ to be a constant?
Is this different from pure exploration (@def-pure-exploration)?
:::

::: {#thm-epsilon-greedy-regret}
#### Epsilon-greedy expected regret

Let $\epsilon_\ep := \sqrt[3]{\Arm \ln(\ep) / \ep}$.
Then the epsilon-greedy achieves a regret of $\widetilde O(\ep^{2/3} \Arm^{1/3})$ in expectation (ignoring logarithmic factors).
:::

::: {.proof}
We won't prove this here.
See @agarwal_reinforcement_2022 for a proof.
:::

In ETC, we had to set $N_\text{explore}$ based on the total number of pulls $\Ep$.
But the epsilon-greedy algorithm handles the exploration incrementally:
the regret rate holds for *any* $\ep$,
and doesn't depend on the total number of pulls $\Ep$.

But the way epsilon-greedy and ETC explore is rather naive:
they explore *uniformly* across all the arms.
But what if we could be smarter about it,
and explore *more* for arms that we're less certain about?



## Upper Confidence Bound (UCB) {#sec-ucb}

The upper confidence bound algorithm is the first strategy we study
that explores _adaptively_:
it identifies arms it is less certain about,
and explicitly trades off between learning more about these
and exploiting arms that already seem good.

To quantify how *certain* we are about the mean of each arm,
UCB computes statistical *confidence intervals* (CIs) for the arm means,
and then chooses the arm with the highest *upper confidence bound*
(hence its name).
Concretely, after $\ep$ pulls,
we compute upper confidence bounds $M^\arm_\ep$ for each arm $\arm$
such that $\mu^\arm \le M^\arm_\ep$ with high probability,
and select $a_\ep := \arg \max_{\arm \in [\Arm]} M^\arm_\ep$.
This operates on the principle of **the benefit of the doubt (i.e. optimism in the face of uncertainty)**:
we'll choose the arm that we're most _optimistic_ about.

```{python}
#| label: fig-ucb-example
#| fig-cap: Visualization of confidence intervals for the arm means. UCB would choose arm $0$ since it has the highest upper confidence bound.

means = [0.1, 0.8, 0.4]
lower_bounds = [-0.4745434, 0.2002283, 0.04175852]
upper_bounds = [0.9745434, 0.9247717, 0.95824148]

whiskers = np.array([lower_bounds, upper_bounds])

fig, ax = plt.subplots()
for i in range(len(means)):
    ax.plot([i, i], [whiskers[0, i], whiskers[1, i]], color='black')
    ax.plot([i - 0.1, i + 0.1], [whiskers[0, i], whiskers[0, i]], color='black')
    ax.plot([i - 0.1, i + 0.1], [whiskers[1, i], whiskers[1, i]], color='blue')

ax.scatter(range(len(means)), means, color='red', zorder=5, label="$\mu^k$")
ax.set_xticks(range(len(means)))
ax.set_xticklabels([f'Arm {i}' for i in range(3)])
ax.set_ylabel(r'Mean reward')
fig.legend()
plt.show()
```

::: {#thm-ucb-ci}
#### Confidence intervals for arm means

As in our definition of the epsilon-greedy algorithm (@def-epsilon-greedy),
let $N^\arm_\ep$ denote the number of times
we pull arm $\arm$ within the first $\ep$ pulls,
and let $\hat \mu^\arm_\ep$ denote the sample mean of the corresponding rewards.
Then with probability at least $1 - \delta$,
for all $\ep \ge \Arm$,

$$
\mu^\arm \in \left[
    \hat \mu^\arm_\ep - \frac{1}{\sqrt{N^\arm_\ep}} \cdot \sqrt{
    \frac{\ln(2 \ep/\delta)}{2}
    },
    \hat \mu^\arm_\ep + \frac{1}{\sqrt{N^\arm_\ep}} \cdot \sqrt{
    \frac{\ln(2 \ep/\delta)}{2}
    }
\right].
$$ {#eq-ucb-ci}

We assume that during the first $\Arm$ pulls, we pull each arm once.
:::

::: {#def-ucb}
#### Upper confidence bound algorithm [@auer_using_2002]

The UCB algorithm first pulls each arm once.
Then it chooses the $\ep$th pull (for $\ep \ge \Arm$) by taking

$$
a_\ep := \arg\max_{\arm \in [\Arm]} \hat \mu^\arm_\ep + \sqrt{
    \frac{\ln(2 \ep/\delta)}{2 N^\arm_\ep}
},
$$ {#eq-ucb-bound}

where $\delta \in (0, 1)$ is a parameter that controls the width of the confidence interval.
:::

Intuitively, UCB prioritizes arms where:

1.  $\hat \mu^\arm_\ep$ is large, i.e. the arm's corresponding sample mean is high,
    and we'd choose it for *exploitation*, and
2.  $\sqrt{\frac{\ln(2 \ep/\delta)}{2N^\arm_\ep}}$ is large,
    i.e. $N^\arm_\ep$ is small and we're still uncertain about the arm,
    and we'd choose it for *exploration*.

$\delta$ is formally the coverage probability of the confidence interval,
but we can also treat it as a parameter that trades off between exploration and exploitation:

- A smaller $\delta$ would give us a larger interval, emphasizing the exploration term.
- A larger $\delta$ would give a tighter interval, prioritizing the current sample means.

We now prove @thm-ucb-ci.

::: {.proof}
Our proof is similar to that in @thm-etc-regret:
we use Hoeffding's inequality to construct a CI for the mean
of a bounded random variable
(i.e. the rewards from a given arm).
However, we must be careful,
since the number of samples from arm $\arm$ up to time $\ep$,
$N^\arm_\ep$,
is a random variable.

Hoeffding's inequality (@thm-hoeffding) tells us that,
for $N$ i.i.d. samples $\tilde r_0, \dots, \tilde r_{N-1}$ from arm $\arm$,

$$
| \tilde \mu^\arm_N - \mu^\arm | \le \sqrt{
    \frac{\ln( 2 / \delta )}{2 N}
}
$$ {#eq-hoeffding-ucb}

with probability at least $1 - \delta$,
where $\tilde \mu^\arm_N = \frac{1}{N} \sum_{n=0}^{N-1} \tilde r_n$.
The union bound then tells us that with probability at least $1 - \delta'$,
for all $N = 1, \dots, \ep$,

$$
\forall N = 1, \dots, \ep :
| \tilde \mu^\arm_N - \mu^\arm | \le \sqrt{
    \frac{\ln( 2 \ep / \delta' )}{2 N}
}.
$$ {#eq-ucb-union}

Since $N^\arm_\ep \in \{ 1, \dots, \ep \}$
(we assume each arm is pulled once at the start),
@eq-ucb-union implies that

$$
| \tilde \mu^\arm_{N^\arm_\ep} - \mu^\arm | \le \sqrt{
    \frac{\ln( 2 \ep / \delta' )}{N}
}
$$ {#eq-ucb-specific-arm}

with probability at least $1 - \delta'$ as well.
But notice that $\tilde \mu^\arm_{N^\arm_\ep}$ is identically distributed to $\hat \mu^\arm_\ep$:
both refer to the sample mean of the first $N^\arm_\ep$ pulls from arm $\arm$.
This gives the bound in @thm-ucb-ci (after relabelling $\delta' \mapsto \delta$).
:::


```{python}
class UCB(Agent):
    def __init__(self, K: int, T: int, delta: float):
        super().__init__(K, T)
        self.delta = delta

    def choose_arm(self):
        return solutions.ucb_choose_arm(self)
```

```{python}
#| label: fig-ucb
#| fig-cap: Reward and cumulative regret of the upper confidence bound algorithm

agent = UCB(mab.K, mab.T, 0.9)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

As desired, this explores in a smarter, *adaptive* way compared to the
previous algorithms. Does it achieve lower regret?

::: {#thm-ucb-regret}
#### UCB high-probability regret bound

With probability $1-\delta''$,

$$
\regret = \widetilde O(\Arm \sqrt{\Ep})
$$ {#eq-ucb-regret}
:::

::: {.proof}
First we'll bound the regret incurred at each timestep.
Then we'll bound the *total* regret across timesteps.

For the sake of analysis, we'll use a slightly looser bound
that applies across the whole time horizon and across all arms.
We'll omit the derivation since it's very similar to the above
(walk through it yourself for practice).

$$
\begin{aligned}
    \pr\left(\forall \arm \le \Arm, \ep < \Ep : |\hat \mu^\arm_\ep - \mu^\arm | \le B^\arm_\ep \right) &\ge 1-\delta'' \\
    \text{where} \quad B^\arm_\ep &:= \sqrt{\frac{\ln(2 \Ep \Arm/\delta'')}{2 N^\arm_\ep}}.
\end{aligned}
$$ {#eq-ucb-regret-1}

Intuitively, $B^\arm_\ep$ denotes the *width* of the CI for arm $\arm$ at time
$\ep$. Then, assuming the above uniform bound holds (which occurs with
probability $1-\delta''$), we can bound the regret at each timestep as
follows:

$$
\begin{aligned}
    \mu^\star - \mu^{a_\ep} &\le \hat \mu^{\arm^\star}_\ep + B_\ep^{\arm^\star} - \mu^{a_\ep} && \text{applying UCB to arm } \arm^\star \\
    &\le \hat \mu^{a_\ep}_\ep + B^{a_\ep}_\ep - \mu^{a_\ep} && \text{since UCB chooses } a_\ep = \arg \max_{\arm \in [\Arm]} \hat \mu^\arm_\ep + B_\ep^{\arm} \\
    &\le 2 B^{a_\ep}_\ep && \text{since } \hat \mu^{a_\ep}_\ep - \mu^{a_\ep} \le B^{a_\ep}_\ep \text{ by definition of } B^{a_\ep}_\ep \\
\end{aligned}
$$ {#eq-ucb-regret-2}

Summing this across timesteps gives

$$
\begin{aligned}
    \regret &\le \sum_{\ep = 0}^{\Ep-1} 2 B^{a_\ep}_\ep \\
    &= \sqrt{2\ln(2 \Ep \Arm/\delta'')} \sum_{\ep = 0}^{\Ep-1} (N^{a_\ep}_\ep)^{-1/2} \\
    \sum_{\ep = 0}^{\Ep-1} (N^{a_\ep}_\ep)^{-1/2} &= \sum_{\ep = 0}^{\Ep-1} \sum_{\arm=0}^{\Arm-1} \mathbf{1}\{ a_\ep = k \} (N^\arm_\ep)^{-1/2} \\
    &= \sum_{\arm=0}^{\Arm-1} \sum_{n=1}^{N_\Ep^\arm} n^{-1/2} \\
    &\le K \sum_{n=1}^\Ep n^{-1/2} \\
    \sum_{n=1}^\Ep n^{-1/2} &\le 1 + \int_1^\Ep x^{-1/2} \ \mathrm{d}x \\
    &= 1 + (2 \sqrt{x})_1^\Ep \\
    &= 2 \sqrt{\Ep} - 1 \\
    &\le 2 \sqrt{\Ep} \\
\end{aligned}
$$

Putting everything together gives, with probability $1-\delta''$,

$$
\begin{aligned}
    \regret &\le 2 \Arm \sqrt{2 \Ep \ln(2 \Ep \Arm /\delta'')} \\
    &= \widetilde O(\Arm \sqrt{\Ep}),
\end{aligned}
$$ {#eq-ucb-regret-final}

as in @thm-ucb-regret.
:::

In fact, we can do a more sophisticated analysis to trim off a factor of $\sqrt{\Arm}$
and show $\regret = \widetilde O(\sqrt{\Ep \Arm})$.


### Lower bound on regret (intuition)

Is it possible to do better than $\Omega(\sqrt{\Ep})$ in general? In fact,
no! We can show that any algorithm must incur $\Omega(\sqrt{\Ep})$ regret
in the worst case. We won't rigorously prove this here, but the
intuition is as follows.

The Central Limit Theorem tells us that with $\Ep$ i.i.d. samples from
some distribution, we can only learn the mean of the distribution to
within $\Omega(1/\sqrt{\Ep})$ (the standard deviation). Then, since we get
$\Ep$ samples spread out across the arms, we can only learn each arm's
mean to an even looser degree.

That is, if two arms have means that are within about $1/\sqrt{\Ep}$, we
won't be able to confidently tell them apart, and will sample them about
equally. But then we'll incur regret
$$
\Omega((\Ep/2) \cdot (1/\sqrt{\Ep})) = \Omega(\sqrt{\Ep}).
$$ {#eq-regret-lower-bound}



## Thompson sampling and Bayesian bandits {#sec-thompson-sampling}

So far, we've treated the parameters $\mu^0, \dots, \mu^{K-1}$ of the reward distributions as *fixed*.
Instead, we can take a **Bayesian** approach
where we treat them as random variables from some **prior distribution**.
Then, upon pulling an arm and observing a reward,
we can simply *condition* on this observation
to exactly describe the **posterior distribution** over the parameters.
This fully describes the information we gain about the parameters from observing the reward.

From this Bayesian perspective,
the **Thompson sampling** algorithm follows naturally:
just sample from the distribution of the optimal arm,
given the observations!

```{python}
class Distribution:
    def sample(self) -> Float[Array, " K"]:
        """Sample a vector of means for the K arms."""
        ...

    def update(self, arm: int, reward: float):
        """Condition on obtaining `reward` from the given arm."""
        ...
```

```{python}
class ThompsonSampling(Agent):
    def __init__(self, K: int, T: int, prior: Distribution):
        super().__init__(K, T)
        self.distribution = prior

    def choose_arm(self):
        means = self.distribution.sample()
        return random_argmax(means)

    def update_history(self, arm: int, reward: int):
        super().update_history(arm, reward)
        self.distribution.update(arm, reward)
```

In other words, we sample each arm proportionally to how likely we think
it is to be optimal, given the observations so far. This strikes a good
exploration-exploitation tradeoff: we explore more for arms that we're
less certain about, and exploit more for arms that we're more certain
about. Thompson sampling is a simple yet powerful algorithm that
achieves state-of-the-art performance in many settings.

::: {#exm-bayesian-bernoulli}
#### Bayesian Bernoulli bandit

We've been working in the Bernoulli bandit setting,
where arm $\arm$ yields a reward of $1$ with probability $\mu^\arm$ and no reward otherwise.
The vector of success probabilities $\boldsymbol{\mu} = (\mu^0, \dots, \mu^{\Arm-1})$ thus describes the entire MAB.

Under the Bayesian perspective,
we think of $\boldsymbol{\mu}$ as a *random* vector
drawn from some prior distribution $p \in \triangle([0, 1]^\Arm)$.
For example, we might have $p$ be the Uniform distribution over the unit hypercube $[0, 1]^\Arm$, that is,

$$
p(\boldsymbol{\mu}) = \begin{cases}
    1 & \text{if } \boldsymbol{\mu}\in [0, 1]^\Arm \\
    0 & \text{otherwise}
\end{cases}
$$ {#eq-hypercube-uniform}

In this case, upon viewing some reward, we can exactly calculate the **posterior** distribution of $\boldsymbol{\mu}$ using Bayes's rule (i.e. the definition of conditional probability):

$$
\begin{aligned}
    \pr(\boldsymbol{\mu} \mid a_0, r_0) &\propto \pr(r_0 \mid a_0, \boldsymbol{\mu}) \pr(a_0 \mid \boldsymbol{\mu}) \pr(\boldsymbol{\mu}) \\
    &\propto (\mu^{a_0})^{r_0} (1 - \mu^{a_0})^{1-r_0}.
\end{aligned}
$$

This is the PDF of the
$\text{Beta}(1 + r_0, 1 + (1 - r_0))$ distribution, which is a conjugate
prior for the Bernoulli distribution. That is, if we start with a Beta
prior on $\mu^\arm$ (note that $\text{Unif}([0, 1]) = \text{Beta}(1, 1)$),
then the posterior, after conditioning on samples from
$\text{Bern}(\mu^\arm)$, will also be Beta. This is a very convenient
property, since it means we can simply update the parameters of the Beta
distribution upon observing a reward, rather than having to recompute
the entire posterior distribution from scratch.
:::

```{python}
class Beta(Distribution):
    def __init__(self, K: int, alpha: int = 1, beta: int = 1):
        self.alphas = np.full(K, alpha)
        self.betas = np.full(K, beta)

    def sample(self):
        return np.random.beta(self.alphas, self.betas)

    def update(self, arm: int, reward: int):
        self.alphas[arm] += reward
        self.betas[arm] += 1 - reward
```

```{python}
#| label: fig-thompson
#| fig-cap: Reward and cumulative regret of the Thompson sampling algorithm

beta_distribution = Beta(mab.K)
agent = ThompsonSampling(mab.K, mab.T, beta_distribution)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

It turns out that asymptotically,
Thompson sampling is optimal in the following sense.
@lai_asymptotically_1985 prove an *instance-dependent* lower bound that says for *any* bandit algorithm,

$$
\liminf_{\Ep \to \infty} \frac{\E[N_\Ep^\arm]}{\ln(\Ep)} \ge \frac{1}{\kl{\mu^\arm}{\mu^\star}}
$$ {#eq-instance-lower-bound}

where

$$
\kl{\mu^\arm}{\mu^\star} := \mu^\arm \ln \frac{\mu^\arm}{\mu^\star} + (1 - \mu^\arm) \ln \frac{1 - \mu^\arm}{1 - \mu^\star}
$$ {#eq-kl-bayesian}

measures the **Kullback-Leibler divergence**
from the Bernoulli distribution with mean $\mu^\arm$
to the Bernoulli distribution with mean $\mu^\star$.
It turns out that Thompson sampling achieves this lower bound with equality!
That is, not only is the error *rate* optimal, but
the *constant factor* is optimal as well.



## Contextual bandits {#sec-contextual-bandits}

In the multi-armed bandits setting described above,
each pull is exactly the same.
We don't obtain any prior information about the reward distributions.
However, consider the online advertising example (@exm-advertising),
where each arm corresponds to an ad we could show a given user,
and we receive a reward if and only if the user clicks on the ad.
Suppose Alice is interested in sports and politics,
while Bob is interested in news and entertainment.
The ads that Alice is likely to click
differ from those that Bob is likely to click.
That is,
the reward distributions of the arms
depend on some prior _context_.
We can model such environments using **contextual bandits**.


::: {#def-contextual-bandit}
#### Contextual bandit

At each pull $\ep$,
a *context* $x_\ep$ is chosen from the set $\mathcal{X}$ of all possible contexts.
The learner gets to observe the context
_before_ they choose an action.
That is, we treat the action chosen on the $\ep$th pull
as a function of the context: $a_\ep(x_\ep)$.
Then, the learner observes the reward from the chosen arm,
where the reward distribution also depends on the context.
:::

::: {#exm-advertising-contextual}
#### Online advertising as a contextual bandit problem

Suppose you represent an online advertising company
and you are tasked with placing ads on the New York Times website.
Articles are categorized into six sections,
each with its own web page:
U.S., World, Business, Arts, Lifestyle, and Opinion.
You might decide to frame this as a contextual bandits problem
with $|\mathcal{X}| = 6$.
This is because the demographic of readers likely differs across the different pages,
and so the reward distributions differ across pages accordingly.
:::

Suppose our context is *discrete*,
as in @exm-advertising-contextual.
Then we could treat each context as its own MAB,
assigning each context-arm pair to a distinct arm
in an enlarged MAB of $\Arm |\mathcal{X}|$ arms.

::: {#exr-ucb-contextual}
#### Naive UCB for contextual bandits

Write down the UCB algorithm for this enlarged MAB.
That is, write an expression $M^\arm_\ep$ in terms of $\Arm$ and $|\mathcal{X}|$
such that $a_\ep(x_\ep) = \arg\max_{\arm \in [\Arm]} M^\arm_\ep$.
:::

Recall that executing UCB for $\Ep$ timesteps on an MAB with $\Arm$ arms
achieves a regret bound of $\widetilde{O}(\sqrt{\Ep \Arm})$ (@thm-ucb-regret).
So in this problem,
we would achieve regret $\widetilde{O}(\sqrt{\Ep \Arm |\mathcal{X}|})$ in the contextual MAB,
which has a polynomial dependence on $|\mathcal{X}|$.
But in a situation where we have too many possible contexts,
or the context is a continuous value,
this algorithm no longer works.

Note that this "enlarged MAB" treats the different contexts as entirely unrelated to each other,
while in practice, often contexts are *related* to each other in some way.
In the medical trial example (@exm-clinical-trials),
clients with similar health situations may benefit from the same medications.
How can we incorporate this structure into our solution?



### Linear contextual bandits {#sec-lin-ucb}

::: {#rem-sl-contextual-dependence}
#### Supervised learning

The material in this section depends on basic knowledge of supervised learning (@sec-sl).
:::

Suppose now that we observe a vector-valued context,
that is, $\mathcal{X} = \mathbb{R}^D$.
For example, in a medical trial (@exm-clinical-trials),
we might know the patient's height, age, blood pressure, etc.,
each of which would be a separate element of the vector.
The approach of treating each context as its own MAB problem
no longer works out-of-the-box
since the number of contexts is infinite.
Instead, we assume that the mean reward of each arm $\arm$
is a function of the context: $\mu^\arm : \mathcal{X} \to \R$.

Rather than the Bernoulli bandit problem (@def-bernoulli-bandit),
we'll assume that the mean reward is _linear_ in the context.
(This model is easier to work with for our purposes.)
That is, a reward $r^\arm$ from arm $\arm$ is drawn according to

$$
r^\arm = x^\top \theta^\arm + \varepsilon,
$$ {#eq-linear-reward-model}

where $\theta^\arm \in \mathbb{R}^D$ describes a *feature direction* for arm $\arm$
and $\varepsilon$ is some independent noise with mean $0$ and variance $\sigma^2$.
This clearly has

$$
\mu^\arm(x) = x^\top \theta^\arm.
$$ {#eq-contextual-true-mean}

Then, on the $\ep$th pull,
given context $x_\ep$,
we can model the mean reward from arm $\arm$ as

$$
\hat \mu^\arm_\ep(x_\ep) := x_\ep^\top \hat \theta^\arm_\ep,
$$ {#eq-linear-contextual}

where $\hat \theta^\arm_\ep$ is some estimator of $\theta^\arm$
computed from the first $\ep$ pulls.

::: {#rem-bernoulli-linear}
#### Assumptions for linear models

Why did we switch from Bernoulli reward distributions
to the additive model in @eq-linear-reward-model?
The estimator @eq-linear-contextual is a bit odd
for the Bernoulli bandit problem:
since $\mu^\arm$ is the success probability of arm $\arm$,
it would have to lie between $0$ and $1$,
but $\hat \mu^\arm_\ep(x_\ep)$ extends outside this interval.
The question of how to model the unknown parameters of a distribution
is a **supervised learning** problem (@sec-sl).
For instance, in Bernoulli bandit,
we could instead use a _logistic regression_ model for each arm's success probability.
We won't dive into more advanced supervised learning topics here.
:::

Our goal is still to develop an algorithm
that chooses an arm $a_\ep(x_\ep)$ to pull
based on the experience gained during the first $\ep$ trials.
We will use _supervised learning_,
in particular, empirical risk minimization (@sec-erm),
to estimate $\theta^\arm$ for each arm.
That is, we compute $\hat \theta^\arm_\ep$ as the vector
that minimizes squared error from the observed rewards:

$$
\hat \theta_\ep^\arm := \arg\min_{\theta \in \mathbb{R}^D}
    \sum_{\ep' \in \mathcal{I}^\arm_\ep}
    (r_{\ep'} - x_{\ep'}^\top \theta)^2,
$$ {#eq-contextual-bandits-estimator}

where

$$
\mathcal{I}^\arm_\ep := \{ \ep' \in [\ep] \mid a_{\ep'} = \arm \}
$$ {#eq-arm-indices}

denotes the subset of $[\ep]$ at which arm $\arm$ was pulled.
Note that $|\mathcal{I}^\arm_\ep| = N^\arm_\ep$
as defined in @eq-bandits-count.
The ERM problem in @eq-contextual-bandits-estimator
has the closed-form solution known as the **ordinary least squares** (OLS\index{ordinary least squares}) estimator:

$$
\begin{aligned}
    \hat \theta_\ep^\arm &= (\widehat \Sigma_\ep^\arm)^{-1}\left(
        \frac{1}{N^\arm_\ep} \sum_{\ep' \in \mathcal{I}^\arm_\ep} x_{\ep'} r_{\ep'}
    \right) \\
    \text{where} \quad \widehat \Sigma_\ep^\arm &= \frac{1}{N^\arm_\ep} \sum_{\ep' \in \mathcal{I}^\arm_\ep} x_{\ep'} x_{\ep'}^\top.
\end{aligned}
$$ {#eq-ols-bandit}

$\widehat \Sigma_\ep^\arm$ is the (biased) empirical covariance matrix of the contexts
across the trials where arm $\arm$ was pulled (out of the first $\ep$ trials).

::: {#rem-invertible}
#### Invertibility

In @eq-ols-bandit,
we write the expression $(\widehat \Sigma_\ep^\arm)^{-1}$,
assuming that $\widehat \Sigma_\ep^\arm$ is invertible.
This only holds if the context features are linearly independent,
that is,
there does not exist some coordinate $d \in [D]$
that can be written as a linear combination of the other coordinates.
One way to ensure that $\widehat \Sigma_\ep^\arm$ is invertible
is to add a $\lambda I$ regularization term to it.
This is equivalent to solving a *ridge regression* problem instead of the unregularized least squares problem.
:::

Now, given a sequence of $\ep$ contexts, pulls, and rewards
$x_0, a_0, r_0, \dots, x_{\ep-1}, a_{\ep-1}, r_{\ep-1}$,
we can estimate $\mu^\arm(x_\ep)$ for each arm.
This is helpful,
but we haven't yet solved the problem of _deciding_ which arms to pull in each context
to trade off between exploration and exploitation.

The upper confidence bound (UCB) algorithm (@def-ucb)
was a useful strategy.
What would we need to adapt UCB to this new setting?
Recall that at each step,
UCB estimates a _confidence interval_ for each arm mean,
and chooses the arm with the highest upper confidence bound.
We already have an estimator $\hat \mu^\arm_\ep(x_\ep)$ of each arm mean,
so all that is left is to compute the width of the confidence interval itself.

::: {#thm-lin-ucb-ci}
#### LinUCB confidence interval

Suppose we pull each arm once during the first $\Arm$ trials.
Then at each pull $\ep \ge \Arm$,
for all arms $\arm \in [\Arm]$ and all contexts $x \in \mathcal{X}$,
we have that with probability at least $1 - 1/\beta^2$,

$$
\mu^\arm(x) \in \left[
    x^\top \hat \theta^\arm_\ep
    - \beta \frac{\sigma}{\sqrt{N^\arm_\ep}} \cdot \sqrt{x^\top (\widehat \Sigma^\arm_\ep)^{-1} x},
    x^\top \hat \theta^\arm_\ep
    + \beta \frac{\sigma}{\sqrt{N^\arm_\ep}} \cdot \sqrt{x^\top (\widehat \Sigma^\arm_\ep)^{-1} x}
\right],
$$ {#eq-lin-ucb-ci}

where

- $\sigma$ is the standard deviation of the reward (@eq-linear-reward-model),
- $N^\arm_\ep$ is the number of times arm $\arm$ was pulled in the first $\ep$ pulls (@eq-bandits-count),
- $\hat \theta^\arm_\ep$ is the OLS estimator for arm $\arm$ (@eq-ols-bandit),
- and $\widehat \Sigma^\arm_\ep$ is the sample covariance matrix of the contexts given arm $\arm$ (@eq-ols-bandit).
:::

We can then plug this interval into the UCB strategy (@def-ucb)
to obtain what is known as the LinUCB algorithm:

::: {#def-lin-ucb}
#### LinUCB [@li_contextual-bandit_2010]

LinUCB first pulls each arm once.
Then, for pull $\ep \ge \Arm$,
LinUCB chooses an arm according to

$$
a_\ep(x_\ep) := \arg\max_{\arm \in [\Arm]}
    x^\top \hat \theta^\arm_\ep
    +
    \beta \frac{\sigma}{\sqrt{N^\arm_\ep}} \sqrt{x_\ep^\top (\widehat \Sigma^\arm_\ep)^{-1} x_\ep},
$$ {#eq-lin-ucb}

where $\sigma, N^\arm_\ep, \hat \theta^\arm_\ep, \widehat \Sigma^\arm_\ep$ are defined as in @thm-lin-ucb-ci,
and $\beta > 0$ controls the width of the confidence interval
(and thereby the exploration-exploitation tradeoff).
:::

The first term is exactly our predicted mean reward $\hat \mu^\arm_\ep(x_\ep)$.
We can think of it as the term encouraging _exploitation_ of an arm
if its predicted mean reward is high.
The second term is the width of the confidence interval given by @thm-lin-ucb-ci.
This is the _exploration_ term
that encourages an arm when $x_\ep$
is *not aligned* with the data seen so far,
or if arm $\arm$ has not been explored much and so $N_\ep^\arm$ is small.

Now we prove @thm-lin-ucb-ci.

:::: {.proof}
How can we construct the upper confidence bound in @thm-lin-ucb-ci?
Previously, we treated the pulls of an arm as i.i.d. samples
and used Hoeffding's inequality to bound the distance of the sample mean, our estimator, from the true mean.
However, now our estimator is not a sample mean,
but rather the OLS estimator above @eq-ols-bandit.
We will construct the confidence interval using **Chebyshev's inequality**,
which gives a confidence interval for the mean of a distribution based on that distribution's _variance_.

::: {#thm-chebyshev}
#### Chebyshev's inequality

For a random variable $Y$ with mean $\mu$ and variance $\sigma^2$,

$$
|Y - \mu| \le \beta \sigma \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
$$ {#eq-chebyshev}
:::

We want to construct a CI for $x^\top \theta^\arm$,
the mean reward from arm $\arm$.
Our estimator is $x^\top \hat \theta^\arm_\ep$.
Applying Chebyshev's inequality requires us to compute the variance of our estimator.

::: {#thm-variance-ols}
#### Variance of OLS

Given the linear reward model in @eq-linear-reward-model,

$$
\text{Var}(x^\top \hat \theta_\ep^\arm)
=
\frac{\sigma^2}{N^\arm_\ep} x^\top (\widehat \Sigma^\ep_\arm)^{-1} x,
$$ {#eq-variance-ols}

where $\widehat \Sigma^\ep_\arm$ is defined in @eq-ols-bandit.
:::

::: {.proof}
We leave the proof as an exercise.
It follows from applications of the law of iterated expectations.
:::

Now we can substitute @eq-variance-ols into @eq-chebyshev
to obtain the desired CI:

$$
\begin{aligned}
    x_\ep^\top \theta^\arm \le x_\ep^\top \hat \theta_\ep^\arm
    + \beta \sqrt{x_\ep^\top (A_\ep^\arm)^{-1} x_\ep} \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\end{aligned}
$$ {#eq-ols-chebyshev}
::::


```{python}
class LinUCBPseudocode(Agent):
    def __init__(
        self, K: int, T: int, D: int, lam: float, get_c: Callable[[int], float]
    ):
        super().__init__(K, T)
        self.lam = lam
        self.get_c = get_c
        self.contexts = [None for _ in range(K)]
        self.A = np.repeat(lam * np.eye(D)[...], K)  # regularization
        self.targets = np.zeros(K, D)
        self.w = np.zeros(K, D)

    def choose_arm(self, context: Float[Array, " D"]):
        c = self.get_c(self.count)
        scores = self.w @ context + c * np.sqrt(
            context.T @ np.linalg.solve(self.A, context)
        )
        return random_argmax(scores)

    def update_history(self, context: Float[Array, " D"], arm: int, reward: int):
        self.A[arm] += np.outer(context, context)
        self.targets[arm] += context * reward
        self.w[arm] = np.linalg.solve(self.A[arm], self.targets[arm])
```


## Key takeaways

In this chapter,
we explored the **multi-armed bandit** setting for analyzing sequential decision-making in an unknown environment.
An MAB consists of multiple arms,
each with an unknown reward distribution.
The agent's task is to learn about these through interaction,
eventually minimizing the _regret_,
which measures how suboptimal the chosen arms were.

We saw algorithms such as **upper confidence bound** and **Thompson sampling**
that handle the tradeoff between _exploration_ and _exploitation_,
that is,
the tradeoff between choosing arms that the agent is _uncertain_ about
and arms the agent already supposes are be good.

We finally discussed **contextual bandits**,
in which the agent gets to observe some _context_ that affects the reward distributions.
We can approach these problems through **supervised learning** approaches.


## Bibliographic notes

The problem that a bandit algorithm faces after $\ep$ pulls
is to infer differences between the means of a set of distributions
given $\ep$ samples spread out across those distributions.
This is a well-studied problem in statistics.

A common practice in various fields such as psychology
is to compare within-group variances to the variance of the group means.
This popular class of methods is known as **analysis of variance** (ANOVA).
It has been studied at least since Laplace in the 1770s [@stigler_history_2003]
and was further popularized by @fisher_statistical_1925.
William Thompson studied the two-armed mean distinction problem in @thompson_likelihood_1933 and @thompson_theory_1935.
Our focus has been on the decision-making aspect of the bandit problem,
rather than the hypothesis testing aspect.

Adding the sequential decision-making aspect turns the above into the full multi-armed bandit problem.
@wald_statistical_1949 studies the sequential analysis problem in depth.
@robbins_aspects_1952 then builds on Wald's work
and provides a clear exposition of the bandit problem
in the form presented in this chapter.



::: {#rem-etymology-lin-ucb}
#### Disambiguation of LinUCB

The name "LinUCB" is used for various similar algorithms throughout the literature.
Here we use it as defined in @li_contextual-bandit_2010.
This algorithm was also considered in @auer_using_2002,
the paper that introduced UCB,
under the name "Associative Reinforcement Learning with Linear Value Functions".
:::


::: {.content-visible when-profile="thesis"}
## Differences from course

The core material in this chapter is essentially identical to the way it was taught in the course,
with some additions and clarifications.
We progress through a sequence of algorithms
to highlight the exploration-exploitation tradeoff.

One interesting choice is whether to introduce bandits before or after MDPs.
Bandits are simpler due to the lack of state,
and serve as a useful setting to investigate the exploration-exploitation tradeoff,
while MDPs are the primary framework for RL
and serve as the setting for much more diverse and interesting problems.
We summarize this in @tbl-bandits-full to make this picture clear for students
and tie these distinct settings together.

How do other textbooks present this material?
@sutton_reinforcement_2018 begins with bandits and proceeds to finite MDPs.
@mannor_reinforcement_2024 saves bandits until the very last chapter.
In the first iteration of the course in Fall 2022,
we began with bandits;
we changed to beginning with MDPs in Fall 2023 and 2024.
Overall, I hold that as long as the relationship between the settings is presented clearly,
either order makes sense,
and so @sec-bandits and @sec-mdps are essentially independent.

I include some more discussion of how to interpret asymptotic regret bounds:
it's simple to look at a plot and see that the expected regret is sublinear,
but it's important to understand the consequences of this in applications.

I also focus exclusively on Bernoulli bandit for the majority of the chapter
(up until contextual bandits, when I switch to the linear case).
This is arguably the simplest bandit environment
and also enjoys the Beta-Binomial conjugacy in the Thompson sampling algorithm.
:::
