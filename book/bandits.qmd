{{< include macros.qmd >}}

# Multi-Armed Bandits {#sec-bandits}

## Introduction

The **multi-armed bandit** (MAB\index{multi-armed bandit}) problem is a simple setting for studying
the basic challenges of sequential decision-making.
In this setting,
an agent repeatedly chooses from a fixed set of actions,
called **arms**,
each of which has an associated reward distribution.
The agent’s goal is to maximize the total reward it receives over some time period.
Here are a few examples:


::: {#exm-advertising layout-ncol="2"}
#### Online advertising

Let’s suppose you, the agent, are an advertising company.
You have $\arm$ different ads that you can show to users;
For concreteness, let’s suppose there’s just a single user.
You receive $1$ reward if the user clicks the ad, and $0$ otherwise.
Thus, the unknown *reward distribution* associated to each ad
is a Bernoulli distribution defined by the probability that the user clicks on the ad.
Your goal is to maximize the total number of clicks by the user.

![Deciding which ad to put on a billboard can be treated as a MAB problem [@negative_space_photo_2015].](shared/advertisements.jpg)
:::

::: {#exm-clinical-trials layout-ncol="2"}
#### Clinical trials

Suppose you’re a pharmaceutical company, and you’re testing a new drug.
You have $\arm$ different dosages of the drug that you can administer to patients.
You receive $1$ reward if the patient recovers, and $0$ otherwise.
Thus, the unknown *reward distribution* associated to each dosage
is a Bernoulli distribution defined by the probability that the patient recovers.
Your goal is to maximize the total number of patients that recover.

![Optimizing between different medications can be treated as a multi-armed bandit problem [@pixabay_20_2016].](shared/medication.jpg)
:::

The MAB is the simplest setting for exploring the **exploration-exploitation tradeoff**:
should the agent choose new actions to learn more about the environment,
or should it choose actions that it already knows to be good?

In this chapter, we will introduce the multi-armed bandits setting,
and discuss some of the challenges that arise when trying to solve problems in this setting.
We will also introduce some of the key concepts that we will use throughout the book,
such as regret and exploration-exploitation tradeoffs.


```{python}
from jaxtyping import Float, Array
import numpy as np
import latexify
from typing import Callable, Union
import matplotlib.pyplot as plt
import utils

import solutions.bandits as solutions

np.random.seed(184)

def random_argmax(ary: Array) -> int:
    """Take an argmax and randomize between ties."""
    max_idx = np.flatnonzero(ary == ary.max())
    return np.random.choice(max_idx).item()


# used as decorator
latex = latexify.algorithmic(
    trim_prefixes={"mab"},
    id_to_latex={"arm": "a_\ep", "reward": "r", "means": "\mu", "t": r"\ep", "T": r"\Ep"},
    use_math_symbols=True,
)
```

:::: {#rem-multi-armed}
#### Etymology

The name “multi-armed bandits” comes from slot machines in casinos,
which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.
::::

## The multi-armed bandit problem {#sec-bandits-problem}

Let's frame the MAB problem mathematically.
In both @exm-advertising and @exm-clinical-trials,
the outcome of each decision is binary (i.e. $0$ or $1$).
This is the **Bernoulli bandit** problem,
which we'll use throughout the chapter.

::: {#def-bernoulli-bandit}
#### Bernoulli bandit problem\index{Bernoulli bandit}

Let $\Arm$ denote the number of arms.
We’ll label them $0, \dots, \Arm-1$ and use *superscripts* to indicate the arm index.
Since we seldom need to raise a number to a power,
this won’t cause much confusion.

Let $\mu^\arm$ be the mean of arm $\arm$,
such that pulling arm $\arm$ either returns reward $1$ with probability $\mu^\arm$ or $0$ otherwise.

The agent gets $\Ep$ chances to pull an arm.
The task is to determine a strategy for maximizing the total reward.
:::

```{python}
class MAB:
    """The Bernoulli multi-armed bandit environment.

    :param means: the means (success probabilities) of the reward distributions for each arm
    :param T: the time horizon
    """

    def __init__(self, means: Float[Array, " K"], T: int):
        assert all(0 <= p <= 1 for p in means)
        self.means = means
        self.T = T
        self.K = self.means.size
        self.best_arm = random_argmax(self.means)

    def pull(self, k: int) -> int:
        """Pull the `k`-th arm and sample from its (Bernoulli) reward distribution."""
        reward = np.random.rand() < self.means[k].item()
        return +reward
```

```{python}
mab = MAB(means=np.array([0.1, 0.8, 0.4]), T=100)
```

In pseudocode, the agent’s interaction with the MAB environment
can be described by the following process:

```{python}
def mab_loop(mab: MAB, agent: "Agent") -> int:
    for t in range(mab.T):
        arm = agent.choose_arm()  # in 0, ..., K-1
        reward = mab.pull(arm)
        agent.update_history(arm, reward)


latex(mab_loop)
```

```{python}
class Agent:
    """Stores the pull history and uses it to decide which arm to pull next.
        
    Since we are working with Bernoulli bandits,
    we can summarize the pull history concisely in a (K, 2) array.
    """

    def __init__(self, K: int, T: int):
        """The MAB agent that decides how to choose an arm given the past history."""
        self.K = K
        self.T = T
        self.rewards = []  # for plotting
        self.choices = []
        self.history = np.zeros((K, 2), dtype=int)

    def choose_arm(self) -> int:
        """Choose an arm of the MAB. Algorithm-specific."""
        ...

    def count(self) -> int:
        """The number of pulls made. Also the current step index."""
        return len(self.rewards)

    def update_history(self, arm: int, reward: int):
        self.rewards.append(reward)
        self.choices.append(arm)
        self.history[arm, reward] += 1
```

What’s the *optimal* strategy for the agent,
i.e. the one that achieves the highest expected reward?
Convince yourself that the agent should try
to always pull the arm with the highest expected reward:

::: {#def-optimal-arm}
#### Optimal arm

We call the arm whose reward distribution has the highest mean
the **optimal arm**:

$$
\begin{aligned}
\arm^\star &:= \arg\max_{\arm \in [\Arm]} \mu^\arm
\mu^\star &:= \mu^{\arm^\star} = \max_{\arm \in [\Arm]} \mu^\arm.
\end{aligned}
$$ {#eq-optimal-arm}
:::

When analyzing an MAB algorithm,
rather than measuring its performance in terms of the (expected) total reward,
we instead compare it to the "oracle" strategy
that knows the optimal arm in advance.
This gives rise to a measure of performance
known as **regret**:

:::: {#def-regret}
#### Regret\index{regret}

The agent’s **regret** after $\Ep$ timesteps is defined as

$$
\text{Regret}_\Ep := \sum_{\ep=0}^{\Ep-1} \mu^\star - \mu^{a_\ep}.
$$ {#eq-regret}
::::

```{python}
def regret_per_step(mab: MAB, agent: Agent):
    """Get the difference from the average reward of the optimal arm. The sum of these is the regret."""
    return [mab.means[mab.best_arm] - mab.means[arm] for arm in agent.choices]
```

Note that this depends on the *true means* of the pulled arms,
and is independent of the observed rewards.
The regret $\text{Regret}_\Ep$ is a random variable
where the randomness comes from the agent’s strategy
(i.e. the sequence of actions $a_0, \dots, a_{\Arm-1}$).

Also note that we care about the _total_ regret across all decisions,
rather than just the final decision.
If we only cared about the final decision,
the best strategy would be to learn as much as possible about the environment,
and then use that knowledge at the last step [@bubeck_pure_2011].
This is a **pure exploration** problem
since the agent is never explicitly penalized
for any actions taken before the final decision.
Minimizing the _total_ regret (or maximizing the _total_ reward)
means we must strike a balance between _exploration_ and _exploitation_
throughout the decision-making process.

Throughout the chapter,
we will try to upper bound the regret of various algorithms in two different senses:

1.  Upper bound the *expected regret*,
    i.e. show that for some upper bound $M_\Ep$,
    $$
    \E[\text{Regret}_\Ep] \le M_\Ep.
    $$ {#eq-expected-regret-bound}
2.  Find a *high-probability* upper bound on the regret,
    i.e. show that for some small _failure probability_ $\delta > 0$,
    $$
    \pr(\text{Regret}_\Ep \le M_{\Ep, \delta}) \ge 1-\delta
    $$ {#eq-probabilistic-regret-bound}
    for some upper bound $M_{\Ep, \delta}$.

Note that these two approaches say very different things about the regret.
The first approach says that the *average* regret is at most $M_\Ep$.
However, the agent might still achieve a higher regret on a particular randomization.
The second approach says that, *with high probability*,
the agent will achieve regret at most $M_{\Ep, \delta}$.
However, it doesn’t say anything about the regret in the remaining $\delta$ fraction of runs,
which might be arbitrarily high.

::: {#exr-markov-expected}
#### Expected regret bounds yield probabilistic regret bounds

Suppose we have an upper bound $M_\Ep$ on the expected regret.
Show that, with probability $1-\delta$,
$M_{\Ep, \delta} := M_\Ep / \delta$ upper bounds the regret.
:::

::: {#rem-pac-bounds}
#### Issues with regret

Is regret the right way to measure performance?
An algorithm which sometimes chooses very bad arms
can achieve the same regret
as an algorithm which often chooses slightly suboptimal arms.
We might care about this distinction in practice,
for example, in healthcare,
where an occasional very bad decision could have extreme consequences.

Other performance measures are used to evaluate algorithms:
**probably approximately correct** (PAC) bounds
and **uniform** high-probability regret bounds.
For a stronger guarantee,
@dann_unifying_2017 introduces the **Uniform-PAC** bound.
We won't discuss these in detail here.
:::

We’d like to achieve **sublinear regret** in expectation:

$$
\E[\text{Regret}_\Ep] = o(\Ep).
$$ {#eq-sublinear-regret}

(See @sec-o-notation if you're unfamiliar with big-oh notation.)
This means that, as $\Ep \to \infty$,
an algorithm with sublinear regret will eventually do as well as the oracle strategy.

::: {#rem-sublinear-issues}
#### Interpreting sublinear regret

Note that an algorithm with sublinear regret in expectation
doesn't necessarily always choose the correct arm.
In fact, such an algorithm can still choose a suboptimal arm infinitely many times.
Consider a two-armed Bernoulli bandit with $\mu^0 = 0$ and $\mu^1 = 1$.
An algorithm that chooses arm $0$ $\sqrt{\Ep}$ times out of $\Ep$
still achieves $O(\sqrt{\Ep})$ regret in expectation.
:::

The rest of the chapter will walk through a series
of MAB algorithms.
We will see how different algorithms make different tradeoffs
between exploration and exploitation.

```{python}

def plot_strategy(mab: MAB, agent: Agent):
    plt.figure(figsize=(10, 6))

    # plot reward and cumulative regret
    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label="reward")
    cum_regret = np.cumsum(regret_per_step(mab, agent))
    plt.plot(np.arange(mab.T), cum_regret, label="cumulative regret")

    # draw colored circles for arm choices
    colors = ["red", "green", "blue"]
    color_array = [colors[k] for k in agent.choices]
    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c=color_array)

    # Add legend entries for each arm
    for i, color in enumerate(colors):
        plt.scatter([], [], c=color, label=f'arm {i}')

    # labels and title
    plt.xlabel("timestep")
    plt.legend()
    # plt.title(f"{agent.__class__.__name__} reward and regret")
    plt.show()
```

## Pure exploration {#sec-pure-exploration}

A trivial strategy is to always choose arms at random (i.e. "pure exploration").

```{python}
class PureExploration(Agent):
    def choose_arm(self):
        """Choose an arm uniformly at random."""
        return solutions.pure_exploration_choose_arm(self)

latex(solutions.pure_exploration_choose_arm)
```

Note that

$$
\E_{a_\ep \sim \text{Unif}([\Arm])}[\mu^{a_\ep}]
=
\bar \mu
:=
\frac{1}{\Arm} \sum_{\arm=1}^\Arm \mu^\arm
$$

so the expected regret is simply

$$
\begin{aligned}
    \E[\text{Regret}_\Ep]
    &= \sum_{\ep = 0}^{\Ep-1} \E[\mu^\star - \mu^{a_\ep}]
    \\
    &= \Ep (\mu^\star - \bar \mu) > 0.
\end{aligned}
$$

This scales as $\Theta(T)$, i.e. *linear* in the number of timesteps $\Ep$. There’s no learning here: the agent doesn’t use any information about the environment to improve its strategy. You can see that the distribution over its arm choices always appears "(uniformly) random".

```{python}
#| label: fig-pure-exploration
#| fig-cap: Reward and cumulative regret of the pure exploration algorithm

agent = PureExploration(mab.K, mab.T)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

## Pure greedy {#sec-pure-greedy}

How might we improve on pure exploration?
Instead, we could try each arm once,
and then commit to the one with the highest observed reward.
We’ll call this the **pure greedy** strategy.

```{python}
class PureGreedy(Agent):
    def choose_arm(self):
        """Choose the arm with the highest observed reward on its first pull."""
        return solutions.pure_greedy_choose_arm(self)
```

Note we’ve used superscripts $r^\arm$ during the exploration phase to
indicate that we observe exactly one reward for each arm. Then we use
subscripts $r_\ep$ during the exploitation phase to indicate that we
observe a sequence of rewards from the chosen greedy arm $\hat \arm$.

How does the expected regret of this strategy compare to that of pure
exploration? We’ll do a more general analysis in the following section.
Now, for intuition, suppose there’s just $K=2$ arms, with Bernoulli
reward distributions with means $\mu^0 > \mu^1$.

Let’s let $r^0$ be the random reward from the first arm
and $r^1$ be the random reward from the second.
If $r^0 > r^1$, then we achieve zero regret.
Otherwise, we achieve regret $\Ep (\mu^0 - \mu^1)$. Thus, the
expected regret is simply:

$$
\begin{aligned}
    \E[\text{Regret}_\Ep] &= \pr(r^0 < r^1) \cdot \Ep (\mu^0 - \mu^1) + c \\
    &= (1 - \mu^0) \mu^1 \cdot \Ep (\mu^0 - \mu^1) + c
\end{aligned}
$$

Which is still $\Theta(\Ep)$, the same as pure exploration!

```{python}
#| label: fig-pure-greedy
#| fig-cap: Reward and cumulative regret of the pure greedy algorithm

agent = PureGreedy(mab.K, mab.T)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

The cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed.
In fact, if the greedy algorithm happens to get lucky on the first set of pulls,
it may act entirely optimally for that episode!
But its _average_ regret is still linear,
since the chance of sticking with a suboptimal arm is nonzero.



## Explore-then-commit {#sec-etc}

We can improve the pure greedy algorithm as follows:
let’s reduce the variance of the reward estimates
by pulling each arm $\Ep_\text{explore}> 1$ times before committing.
This is called the **explore-then-commit** strategy.
Note that the “pure greedy” strategy above is just the special case where $\Ep_\text{explore} = 1$.

```{python}
class ExploreThenCommit(Agent):
    def __init__(self, K: int, T: int, N_explore: int):
        super().__init__(K, T)
        self.N_explore = N_explore

    def choose_arm(self):
        return solutions.etc_choose_arm(self)
```

```{python}
#| label: fig-etc
#| fig-cap: Reward and cumulative regret of the explore-then-commit algorithm

agent = ExploreThenCommit(mab.K, mab.T, mab.T // 15)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

Notice that now, the graphs are much more consistent,
and the algorithm finds the true optimal arm and sticks with it much more frequently.
We would expect ETC to then have a better (i.e. lower) average regret. Can we prove this?



### ETC regret analysis {#sec-etc-regret-analysis}

Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up
into the exploration and exploitation phases.

#### Exploration phase.

This phase takes $\Ep_\text{explore} \Arm$ timesteps.
Since at each step we incur at most $1$ regret,
the total regret is at most $\Ep_\text{explore} \Arm$.

#### Exploitation phase.

This will take a bit more effort.
We’ll prove that for any total time $\Ep$,
we can choose $\Ep_\text{explore}$ such that with arbitrarily high probability,
the regret is sublinear.

Let $\hat \arm$ denote the arm chosen after the exploration phase.
We know the regret from the exploitation phase is

$$
\Ep_\text{exploit} (\mu^\star - \mu^{\hat \arm}) \qquad \text{where} \qquad \Ep_\text{exploit} := \Ep - \Ep_\text{explore} \Arm.
$$ {#eq-etc-exploit-regret}

So we’d like to bound $\mu^\star - \mu^{\hat \arm} = o(1)$
(as a function of $\Ep$)
in order to achieve sublinear regret.
How can we do this?

Let’s define $\Delta^\arm := \hat \mu^\arm - \mu^\arm$
to denote how far the mean estimate for arm $\arm$ is from the true mean.
How can we bound this quantity?
We’ll use the following useful inequality for i.i.d. bounded random variables:

::: {#thm-hoeffding}
#### Hoeffding’s inequality

Let $X_0, \dots, X_{N-1}$ be i.i.d. random variables
with $X_n \in [0, 1]$ almost surely for each $n \in [N]$.
Then for any $\delta > 0$,

$$
\pr\left(
    \left| \frac{1}{N} \sum_{n=1}^N (X_n - \E[X_n]) \right| > \sqrt{\frac{\ln(2/\delta)}{2N}}
\right) \le \delta.
$$ {#eq-hoeffding}
:::

::: {.proof}
The proof of this inequality is beyond the scope of this book.
See @vershynin_high-dimensional_2018 [ch. 2.2].
:::

We can apply this directly to the rewards for a given arm $\arm$,
since the rewards from that arm are i.i.d.:

$$
\pr\left(|\Delta^\arm | > \sqrt{
    \frac{\ln(2/\delta)}{2\Ep_\text{explore}}
} \right) \le \delta.
$$ {#eq-hoeffding-etc}

But note that we can’t apply this to arm $\hat \arm$ directly
since $\hat \arm$ is itself a random variable.
Instead, we need to bound the error across all the arms simultaneously,
so that the resulting bound will apply
*no matter what* $\hat \arm$ “crystallizes” to.
The **union bound** provides a simple way to do this:

::: {#thm-union-bound}
#### Union bound

Consider a set of events $A_0, \dots, A_{N-1}$. Then

$$
\pr(\exists n \in [N]. A_n) \le \sum_{n=0}^{N-1} \pr(A_n).
$$ {#eq-union-1}

In particular,
if $\pr(A_n) \ge 1 - \delta$ for each $n \in [N]$,
we have

$$
\pr(\forall n \in [N]. A_n) \ge 1 - N \delta.
$$ {#eq-union-2}
:::

::: {#exr-union}
#### Union bound for successes

Prove the second statement above.
:::

Applying the union bound across the arms for the l.h.s. event of @eq-hoeffding-etc, we have

$$
\begin{aligned}
    \pr\left(
        \forall \arm \in [\Arm], |\Delta^\arm|
        \le
        \sqrt{\frac{\ln(2/\delta)}{2\Ep_\text{explore}}}
    \right) &\ge 1- \Arm \delta
\end{aligned}
$$ {#eq-hoeffding-union}

Then to apply this bound to $\hat \arm$ in particular, we
can apply the useful trick of “adding zero”:

$$
\begin{aligned}
    \mu^{\arm^\star} - \mu^{\hat \arm} &= \mu^{\arm^\star} - \mu^{\hat \arm} + (\hat \mu^{\arm^\star} - \hat \mu^{\arm^\star}) + (\hat \mu^{\hat \arm} - \hat \mu^{\hat \arm}) \\
    &= \Delta^{\hat \arm} - \Delta^{\arm^\star} + \underbrace{(\hat \mu^{\arm^\star} - \hat \mu^{\hat \arm})}_{\le 0 \text{ by definition of } \hat \arm} \\
    &\le 2 \sqrt{\frac{\ln(2 \Arm / \delta')}{2\Ep_\text{explore}}} \text{ with probability at least } 1-\delta'
\end{aligned}
$$ {#eq-hoeffding-trick}

where we’ve set $\delta' = \Arm \delta$. Putting this all
together, we’ve shown that, with probability $1 - \delta'$,

$$
\text{Regret}_\Ep \le \Ep_\text{explore} \Arm + \Ep_\text{exploit} \cdot \sqrt{\frac{2\ln(2 \Arm/\delta')}{\Ep_\text{explore}}}.
$$ {#eq-etc-regret-1}

Note that it suffices for $\Ep_\text{explore}$ to be on the order of $\sqrt{\Ep}$ to achieve sublinear regret.
In particular, we can find the optimal $\Ep_\text{explore}$
by setting the derivative of the r.h.s. to zero:

$$
\begin{aligned}
    0 &= \Arm - \Ep_\text{exploit} \cdot \frac{1}{2} \sqrt{\frac{2\ln(2 \Arm /\delta')}{\Ep_\text{explore}^3}} \\
    \Ep_\text{explore} &= \left(
        \Ep_\text{exploit} \cdot \frac{\sqrt{\ln(2 \Arm / \delta')/2}}{ \Arm }
    \right)^{2/3}
\end{aligned}
$$ {#eq-etc-regret-2}

Plugging this into the expression for the regret,
we have (still with probability $1-\delta'$):

$$
\begin{aligned}
    \text{Regret}_\Ep &\le 3 \Ep^{2/3} \sqrt[3]{\Arm \ln(2 \Arm /\delta') / 2} \\
    &= \widetilde{O}(\Ep^{2/3} \Arm^{1/3}).
\end{aligned}
$$ {#eq-etc-bound}

The ETC algorithm is rather "abrupt"
in that it switches from exploration to exploitation after a fixed number of pulls.
What if the total number of pulls $\Ep$ isn't known in advance?
We'll need to use a more gradual transition,
which brings us to the *epsilon-greedy* algorithm.



## Epsilon-greedy {#sec-epsilon-greedy}

Instead of doing all of the exploration and then all of the exploitation
separately – which additionally requires knowing the time horizon
beforehand – we can instead interleave exploration and exploitation by,
at each timestep, choosing a random action with some probability. We
call this the **epsilon-greedy** algorithm.

```{python}
class EpsilonGreedy(Agent):
    def __init__(
        self,
        K: int,
        T: int,
        ε_array: Float[Array, " T"],
    ):
        super().__init__(K, T)
        self.ε_array = ε_array

    def choose_arm(self):
        return solutions.epsilon_greedy_choose_arm(self)
```

```{python}
#| label: fig-epsilon-greedy
#| fig-cap: Reward and cumulative regret of the epsilon-greedy algorithm

agent = EpsilonGreedy(mab.K, mab.T, np.full(mab.T, 0.1))
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

Note that we let $\epsilon$ vary over time. In particular, we might want to gradually *decrease* $\epsilon$ as we learn more about the reward distributions and no longer need to spend time exploring.

::: {#exr-constant-eps}
#### Regret for constant epsilon

What is the asymptotic expected regret of the algorithm if we set $\epsilon$ to be a constant?
:::

It turns out that setting $\epsilon_\ep = \sqrt[3]{\Arm \ln(\ep) / \ep}$
also achieves a regret of $\widetilde O(\ep^{2/3} \Arm^{1/3})$ (ignoring the logarithmic factors).
(We will not prove this here; a proof can be found in @agarwal_reinforcement_2022.)

In ETC, we had to set $\Ep_\text{explore}$ based on the total number of timesteps $\Ep$.
But the epsilon-greedy algorithm actually handles the exploration *automatically*:
the regret rate holds for *any* $\ep$,
and doesn’t depend on the final horizon $\Ep$.

But the way these algorithms explore is rather naive:
we’ve been exploring *uniformly* across all the arms.
But what if we could be smarter about it,
and explore *more* for arms that we’re less certain about?



## Upper Confidence Bound (UCB) {#sec-ucb}

To quantify how *certain* we are about the mean of each arm,
we’ll compute *confidence intervals* for our estimators,
and then choose the arm with the highest *upper confidence bound*.
This operates on the principle of **the benefit of the doubt (i.e. optimism in the face of uncertainty)**:
we’ll choose the arm that we’re most optimistic about.

In particular, for each arm $\arm$ at time $\ep$,
we’d like to compute some upper confidence bound $M^\arm_\ep$
such that $\hat \mu^\arm_\ep \le M^\arm_\ep$ with high probability,
and then choose $a_\ep := \arg \max_{\arm \in [\Arm]} M^\arm_\ep$.
But how should we compute $M^\arm_\ep$?

In @sec-etc-regret-analysis,
we were able to compute this bound using Hoeffding’s inequality,
which assumes that the number of samples is *fixed*.
This was the case in ETC (where we pull each arm $\Ep_\text{explore}$ times),
but in UCB, the number of times we pull each arm depends on the agent’s actions,
which in turn depend on the random rewards and are therefore stochastic.
So we *can’t* use Hoeffding’s inequality directly.

Instead, we’ll apply the same trick we used in the ETC analysis:
we’ll use the **union bound** to compute a *looser* bound
that holds *uniformly* across all timesteps and arms.
Let’s introduce some notation to discuss this.

Let $N^\arm_\ep$ denote the (random) number of times
that arm $\arm$ has been pulled within the first $\ep$ timesteps,
and $\hat \mu^\arm_\ep$ denote the sample average of those pulls.
That is,

$$
\begin{aligned}
    N^\arm_\ep &:= \sum_{\tau=0}^{\ep-1} \mathbf{1} \{ a_\tau = \arm \} \\
    \hat \mu^\arm_\ep &:= \frac{1}{N^\arm_\ep} \sum_{\tau=0}^{\ep-1} \mathbf{1} \{ a_\tau = \arm \} r_\tau.
\end{aligned}
$$ {#eq-bandits-count}

To achieve the "fixed sample size" assumption,
we’ll need to shift our index from *time* to *number of samples from each arm*.
In particular, we’ll define $\widetilde r^\arm_n$ to be the $n$th sample from arm $\arm$,
and $\widetilde \mu^\arm_n$ to be the sample average of the first $n$ samples from arm $\arm$.
Then, for a fixed $n$, this satisfies the "fixed sample size" assumption,
and we can apply Hoeffding’s inequality to get a bound on $\widetilde \mu^\arm_n$.

So how can we extend our bound on $\widetilde\mu^\arm_n$ to $\hat \mu^\arm_\ep$?
Well, we know $N^\arm_\ep \le \ep$
(where equality would be the case if and only if we pulled arm $\arm$ every time).
So we can apply the same trick as last time,
where we uniform-ize across all possible values of $N^\arm_\ep$:

$$
\begin{aligned}
    \pr\left(
        \forall n \le \ep, |\widetilde \mu^\arm_n - \mu^\arm |
        \le
        \sqrt{\frac{\ln(2/\delta)}{2n}}
    \right) &\ge 1- \ep \delta.
\end{aligned}
$$ {#eq-bandits-uniform-count}

In particular, since $N^\arm_\ep \le \ep$,
and $\widetilde \mu^\arm_{N^\arm_\ep} = \hat \mu^\arm_\ep$ by definition,
we have

$$
\begin{aligned}
    \pr\left(
        |\hat \mu^\arm_\ep - \mu^\arm |
        \le
        \sqrt{
        \frac{\ln(2 \ep/\delta')}{ 2N^\arm_\ep}
        }
    \right)
    &\ge
    1-\delta' \text{ where } \delta'
    :=
    \ep \delta.
\end{aligned}
$$ {#eq-hoeffding-bound}

This bound would then suffice for applying the UCB algorithm!
That is, the upper confidence bound for arm $\arm$ would be

$$
M^\arm_\ep := \hat \mu^\arm_\ep + \sqrt{\frac{\ln(2 \ep/\delta')}{2 N^\arm_\ep}},
$$ {#eq-ucb-bound}

where we can choose $\delta'$ depending on how tight we want the interval to be.

- A smaller $\delta'$ would give us a larger and higher-confidence interval, emphasizing the exploration term.
- A larger $\delta'$ would give a tighter and lower-confidence interval, prioritizing the current sample averages.

We can now use this to define the UCB algorithm.

```{python}
class UCB(Agent):
    def __init__(self, K: int, T: int, delta: float):
        super().__init__(K, T)
        self.delta = delta

    def choose_arm(self):
        return solutions.ucb_choose_arm(self)
```

Intuitively, UCB prioritizes arms where:

1.  $\hat \mu^\arm_\ep$ is large, i.e. the arm has a high sample average, and
    we’d choose it for *exploitation*, and
2.  $\sqrt{\frac{\ln(2 \ep/\delta')}{2N^\arm_\ep}}$ is large, i.e. we’re still
    uncertain about the arm, and we’d choose it for *exploration*.

As desired, this explores in a smarter, *adaptive* way compared to the
previous algorithms. Does it achieve lower regret?

```{python}
#| label: fig-ucb
#| fig-cap: Reward and cumulative regret of the upper confidence bound algorithm

agent = UCB(mab.K, mab.T, 0.9)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

### UCB regret analysis

First we’ll bound the regret incurred at each timestep. Then we’ll bound
the *total* regret across timesteps.

For the sake of analysis, we’ll use a slightly looser bound that applies
across the whole time horizon and across all arms. We’ll omit the
derivation since it’s very similar to the above (walk through it
yourself for practice).

$$
\begin{aligned}
    \pr\left(\forall \arm \le \Arm, \ep < \Ep : |\hat \mu^\arm_\ep - \mu^\arm | \le B^\arm_\ep \right) &\ge 1-\delta'' \\
    \text{where} \quad B^\arm_\ep &:= \sqrt{\frac{\ln(2 \Ep \Arm/\delta'')}{2 N^\arm_\ep}}.
\end{aligned}
$$ {#eq-ucb-regret-1}

Intuitively, $B^\arm_\ep$ denotes the *width* of the CI for arm $\arm$ at time
$\ep$. Then, assuming the above uniform bound holds (which occurs with
probability $1-\delta''$), we can bound the regret at each timestep as
follows:

$$
\begin{aligned}
    \mu^\star - \mu^{a_\ep} &\le \hat \mu^{\arm^\star}_\ep + B_\ep^{\arm^\star} - \mu^{a_\ep} && \text{applying UCB to arm } \arm^\star \\
    &\le \hat \mu^{a_\ep}_\ep + B^{a_\ep}_\ep - \mu^{a_\ep} && \text{since UCB chooses } a_\ep = \arg \max_{\arm \in [\Arm]} \hat \mu^\arm_\ep + B_\ep^{\arm} \\
    &\le 2 B^{a_\ep}_\ep && \text{since } \hat \mu^{a_\ep}_\ep - \mu^{a_\ep} \le B^{a_\ep}_\ep \text{ by definition of } B^{a_\ep}_\ep \\
\end{aligned}
$$ {#eq-ucb-regret-2}

Summing this across timesteps gives

$$
\begin{aligned}
    \text{Regret}_\Ep &\le \sum_{\ep = 0}^{\Ep-1} 2 B^{a_\ep}_\ep \\
    &= \sqrt{2\ln(2 \Ep \Arm/\delta'')} \sum_{\ep = 0}^{\Ep-1} (N^{a_\ep}_\ep)^{-1/2} \\
    \sum_{\ep = 0}^{\Ep-1} (N^{a_\ep}_\ep)^{-1/2} &= \sum_{\ep = 0}^{\Ep-1} \sum_{\arm=0}^{\Arm-1} \mathbf{1}\{ a_\ep = k \} (N^\arm_\ep)^{-1/2} \\
    &= \sum_{\arm=0}^{\Arm-1} \sum_{n=1}^{N_\Ep^\arm} n^{-1/2} \\
    &\le K \sum_{n=1}^\Ep n^{-1/2} \\
    \sum_{n=1}^\Ep n^{-1/2} &\le 1 + \int_1^\Ep x^{-1/2} \ \mathrm{d}x \\
    &= 1 + (2 \sqrt{x})_1^\Ep \\
    &= 2 \sqrt{\Ep} - 1 \\
    &\le 2 \sqrt{\Ep} \\
\end{aligned}
$$

Putting everything together gives

$$
\begin{aligned}
    \text{Regret}_\Ep &\le 2 \Arm \sqrt{2 \Ep \ln(2 \Ep \Arm /\delta'')} && \text{with probability } 1-\delta'' \\
    &= \widetilde O(K\sqrt{\Ep})
\end{aligned}
$$ {#eq-ucb-regret}

In fact, we can do a more sophisticated analysis to trim off a factor of $\sqrt{\Arm}$
and show $\text{Regret}_\Ep = \widetilde O(\sqrt{\Ep \Arm})$.



### Lower bound on regret (intuition)

Is it possible to do better than $\Omega(\sqrt{\Ep})$ in general? In fact,
no! We can show that any algorithm must incur $\Omega(\sqrt{\Ep})$ regret
in the worst case. We won’t rigorously prove this here, but the
intuition is as follows.

The Central Limit Theorem tells us that with $\Ep$ i.i.d. samples from
some distribution, we can only learn the mean of the distribution to
within $\Omega(1/\sqrt{\Ep})$ (the standard deviation). Then, since we get
$\Ep$ samples spread out across the arms, we can only learn each arm’s
mean to an even looser degree.

That is, if two arms have means that are within about $1/\sqrt{\Ep}$, we
won’t be able to confidently tell them apart, and will sample them about
equally. But then we’ll incur regret
$$
\Omega((\Ep/2) \cdot (1/\sqrt{\Ep})) = \Omega(\sqrt{\Ep}).
$$ {#eq-regret-lower-bound}



## Thompson sampling and Bayesian bandits {#sec-thompson-sampling}

So far, we’ve treated the parameters $\mu^0, \dots, \mu^{K-1}$ of the reward distributions as *fixed*.
Instead, we can take a **Bayesian** approach
where we treat them as random variables from some **prior distribution**.
Then, upon pulling an arm and observing a reward,
we can simply *condition* on this observation
to exactly describe the **posterior distribution** over the parameters.
This fully describes the information we gain about the parameters from observing the reward.

From this Bayesian perspective,
the **Thompson sampling** algorithm follows naturally:
just sample from the distribution of the optimal arm,
given the observations!

```{python}
class Distribution:
    def sample(self) -> Float[Array, " K"]:
        """Sample a vector of means for the K arms."""
        ...

    def update(self, arm: int, reward: float):
        """Condition on obtaining `reward` from the given arm."""
        ...
```

```{python}
class ThompsonSampling(Agent):
    def __init__(self, K: int, T: int, prior: Distribution):
        super().__init__(K, T)
        self.distribution = prior

    def choose_arm(self):
        means = self.distribution.sample()
        return random_argmax(means)

    def update_history(self, arm: int, reward: int):
        super().update_history(arm, reward)
        self.distribution.update(arm, reward)
```

In other words, we sample each arm proportionally to how likely we think
it is to be optimal, given the observations so far. This strikes a good
exploration-exploitation tradeoff: we explore more for arms that we’re
less certain about, and exploit more for arms that we’re more certain
about. Thompson sampling is a simple yet powerful algorithm that
achieves state-of-the-art performance in many settings.

::: {#exm-bayesian-bernoulli}
#### Bayesian Bernoulli bandit

We’ve been working in the Bernoulli bandit setting,
where arm $\arm$ yields a reward of $1$ with probability $\mu^\arm$ and no reward otherwise.
The vector of success probabilities $\boldsymbol{\mu} = (\mu^0, \dots, \mu^{\Arm-1})$ thus describes the entire MAB.

Under the Bayesian perspective,
we think of $\boldsymbol{\mu}$ as a *random* vector
drawn from some prior distribution $p \in \triangle([0, 1]^\Arm)$.
For example, we might have $p$ be the Uniform distribution over the unit hypercube $[0, 1]^\Arm$, that is,

$$
p(\boldsymbol{\mu}) = \begin{cases}
    1 & \text{if } \boldsymbol{\mu}\in [0, 1]^\Arm \\
    0 & \text{otherwise}
\end{cases}
$$ {#eq-hypercube-uniform}

In this case, upon viewing some reward, we can exactly calculate the **posterior** distribution of $\boldsymbol{\mu}$ using Bayes’s rule (i.e. the definition of conditional probability):

$$
\begin{aligned}
    \pr(\boldsymbol{\mu} \mid a_0, r_0) &\propto \pr(r_0 \mid a_0, \boldsymbol{\mu}) \pr(a_0 \mid \boldsymbol{\mu}) \pr(\boldsymbol{\mu}) \\
    &\propto (\mu^{a_0})^{r_0} (1 - \mu^{a_0})^{1-r_0}.
\end{aligned}
$$

This is the PDF of the
$\text{Beta}(1 + r_0, 1 + (1 - r_0))$ distribution, which is a conjugate
prior for the Bernoulli distribution. That is, if we start with a Beta
prior on $\mu^\arm$ (note that $\text{Unif}([0, 1]) = \text{Beta}(1, 1)$),
then the posterior, after conditioning on samples from
$\text{Bern}(\mu^\arm)$, will also be Beta. This is a very convenient
property, since it means we can simply update the parameters of the Beta
distribution upon observing a reward, rather than having to recompute
the entire posterior distribution from scratch.
:::

```{python}
class Beta(Distribution):
    def __init__(self, K: int, alpha: int = 1, beta: int = 1):
        self.alphas = np.full(K, alpha)
        self.betas = np.full(K, beta)

    def sample(self):
        return np.random.beta(self.alphas, self.betas)

    def update(self, arm: int, reward: int):
        self.alphas[arm] += reward
        self.betas[arm] += 1 - reward
```

```{python}
#| label: fig-thompson
#| fig-cap: Reward and cumulative regret of the Thompson sampling algorithm

beta_distribution = Beta(mab.K)
agent = ThompsonSampling(mab.K, mab.T, beta_distribution)
mab_loop(mab, agent)
plot_strategy(mab, agent)
```

It turns out that asymptotically,
Thompson sampling is optimal in the following sense.
@lai_asymptotically_1985 prove an *instance-dependent* lower bound that says for *any* bandit algorithm,

$$
\liminf_{\Ep \to \infty} \frac{\E[N_\Ep^\arm]}{\ln(\Ep)} \ge \frac{1}{\kl{\mu^\arm}{\mu^\star}}
$$ {#eq-instance-lower-bound}

where

$$
\kl{\mu^\arm}{\mu^\star} := \mu^\arm \ln \frac{\mu^\arm}{\mu^\star} + (1 - \mu^\arm) \ln \frac{1 - \mu^\arm}{1 - \mu^\star}
$$ {#eq-kl-bayesian}

measures the **Kullback-Leibler divergence**
from the Bernoulli distribution with mean $\mu^\arm$
to the Bernoulli distribution with mean $\mu^\star$.
It turns out that Thompson sampling achieves this lower bound with equality!
That is, not only is the error *rate* optimal, but
the *constant factor* is optimal as well.



## Contextual bandits

In the above MAB environment, the reward distributions of the arms
remain constant. However, in many real-world settings, we might receive
additional information that affects these distributions. For example, in
the online advertising case where each arm corresponds to an ad we could
show the user, we might receive information about the user's preferences
that changes how likely they are to click on a given ad. We can model
such environments using **contextual bandits**.

::: {#def-contextual-bandit}
#### Contextual bandit

At each pull $\ep$,
a *context* $x_\ep$ is drawn from some distribution $\nu_{\text{x}}$.
The learner gets to observe the context,
and choose an action $a_\ep$ according to some context-dependent policy $\pi_\ep(x_\ep)$.
Then, the learner observes the reward from the chosen arm $r_\ep \sim \nu^{a_\ep}(x_\ep)$.
The reward distribution also depends on the context.
:::


Assuming our context is *discrete*,
we can just perform the same algorithms,
treating each context-arm pair as its own arm.
This gives us an enlarged MAB of $K |\mathcal{X}|$ arms.

::: {#exr-ucb-contextual}
#### Naive UCB for contextual bandits

Write down the UCB algorithm for this enlarged MAB. That is, write an
expression for $\pi_\ep(x_\ep) = \arg\max_a \dots$.
:::

Recall that running UCB for $\Ep$ timesteps on an MAB with $K$ arms achieves a regret bound of $\widetilde{O}(\sqrt{\Ep \Arm})$.
So in this problem,
we would achieve regret $\widetilde{O}(\sqrt{\Ep \Arm |\mathcal{X}|})$ in the contextual MAB,
which has a polynomial dependence on $|\mathcal{X}|$.
But in a situation where we have large,
or even infinitely many contexts,
e.g. in the case where our context is a continuous value,
this becomes intractable.

Note that this "enlarged MAB" treats the different contexts as entirely unrelated to each other,
while in practice, often contexts are *related* to each other in some way:
for example, we might want to advertise similar products to users with similar preferences.
How can we incorporate this structure into our solution?



### Linear contextual bandits {#sec-lin-ucb}

We want to model the *mean reward* of arm $\arm$ as a function of the context,
i.e. $\mu^\arm(x)$.
One simple model is the *linear* one:

$$
\mu^\arm(x) = x^\top \theta^\arm,
$$ {#eq-linear-contextual}

where $x \in \mathcal{X} = \mathbb{R}^D$
and $\theta^\arm \in \mathbb{R}^D$ describes a *feature direction* for arm $\arm$.
**Supervised learning** (see @sec-sl) gives us a way to estimate a conditional expectation from samples.
We learn a *least squares* estimator from the timesteps where arm $\arm$ was selected:

$$
\hat \theta_\ep^\arm = \arg\min_{\theta \in \mathbb{R}^d} \sum_{\{ i \in [\ep] : a_i = k \}} (r_i - x_i^\top \theta)^2.
$$ {#eq-contextual-bandits-estimator}

This has the closed-form solution known as the *ordinary least squares*
(OLS) estimator:

$$
\begin{aligned}
    \hat \theta_\ep^\arm          & = (A_\ep^\arm)^{-1} \sum_{\{ i \in [\ep] : a_i = \arm \}} x_i r_i \\
    \text{where} \quad A_\ep^\arm & = \sum_{\{ i \in [\ep] : a_i = \arm \}} x_i x_i^\top.
\end{aligned}
$$ {#eq-ols-bandit}

We can now apply the UCB algorithm in this environment in order to
balance *exploration* of new arms and *exploitation* of arms that we
believe to have high reward. But how should we construct the upper
confidence bound? Previously, we treated the pulls of an arm as i.i.d.
samples and used Hoeffding's inequality to bound the distance of the
sample mean, our estimator, from the true mean. However, now our
estimator is not a sample mean, but rather the OLS estimator above @eq-ols-bandit. Instead, we'll use **Chebyshev's
inequality** to construct an upper confidence bound.

::: {#thm-chebyshev}
#### Chebyshev's inequality

For a random variable $Y$ such that
$\E Y = 0$ and $\E Y^2 = \sigma^2$,

$$
|Y| \le \beta \sigma \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
$$ {#eq-chebyshev}
:::

Since the OLS estimator is known to be unbiased (try proving this
yourself), we can apply Chebyshev's inequality to
$x_\ep^\top (\hat \theta_\ep^\arm - \theta^\arm)$:

$$
\begin{aligned}
    x_\ep^\top \theta^\arm \le x_\ep^\top \hat \theta_\ep^\arm + \beta \sqrt{x_\ep^\top (A_\ep^\arm)^{-1} x_\ep} \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\end{aligned}
$$ {#eq-ols-chebyshev}

::: {#exr-variance-ols}
#### Variance of OLS estimator

Show that $x_\ep^\top (A_\ep^\arm)^{-1} x_\ep$ is the variance of $x_\ep^\top \hat \theta_\ep^\arm$.
This result follows from some algebra on the definition of the OLS estimator @eq-ols-bandit.
:::

The first term is exactly our predicted reward $\hat \mu^\arm_\ep(x_\ep)$.
To interpret the second term, note that

$$
x_\ep^\top (A_\ep^\arm)^{-1} x_\ep = \frac{1}{N_\ep^\arm} x_\ep^\top (\Sigma_\ep^\arm)^{-1} x_\ep,
$$ {#eq-contextual-1}

where

$$
\Sigma_\ep^\arm = \frac{1}{N_\ep^\arm} \sum_{\{ i \in [t] : a_i = k \}} x_i x_i^\top
$$ {#eq-contextual-2}

is the empirical covariance matrix of the contexts
(assuming that the context has mean zero).
That is, the learner is encouraged to choose arms
when $x_\ep$ is *not aligned* with the data seen so far,
or if arm $\arm$ has not been explored much
and so $N_\ep^\arm$ is small.

We can now substitute these quantities into UCB to get the **LinUCB** algorithm:

```{python}
class LinUCBPseudocode(Agent):
    def __init__(
        self, K: int, T: int, D: int, lam: float, get_c: Callable[[int], float]
    ):
        super().__init__(K, T)
        self.lam = lam
        self.get_c = get_c
        self.contexts = [None for _ in range(K)]
        self.A = np.repeat(lam * np.eye(D)[...], K)  # regularization
        self.targets = np.zeros(K, D)
        self.w = np.zeros(K, D)

    def choose_arm(self, context: Float[Array, " D"]):
        c = self.get_c(self.count)
        scores = self.w @ context + c * np.sqrt(
            context.T @ np.linalg.solve(self.A, context)
        )
        return random_argmax(scores)

    def update_history(self, context: Float[Array, " D"], arm: int, reward: int):
        self.A[arm] += np.outer(context, context)
        self.targets[arm] += context * reward
        self.w[arm] = np.linalg.solve(self.A[arm], self.targets[arm])
```

::: {#rem-invertible}
#### Invertibility

Note that the matrix $A_\ep^\arm$ above might not be invertible.
When does this occur?
One way to address this is to include a $\lambda I$ regularization term
to ensure that $A_\ep^\arm$ is invertible.
This is equivalent to solving a *ridge regression* problem instead of the unregularized least squares problem.
:::



$c_\ep$ is similar to the $\log (2 \ep/\delta')$ term of UCB:
It controls the width of the confidence interval.
Here, we treat it as a tunable parameter, though in a theoretical analysis,
it would depend on $A_\ep^\arm$
and the probability $\delta$ with which the bound holds.

Using similar tools for UCB,
we can also prove an $\widetilde{O}(\sqrt{\Ep})$ regret bound.
The full details of the analysis can be found in Section 3 of @agarwal_reinforcement_2022.

## Key takeaways

In this chapter,
we explored the **multi-armed bandit** setting for analyzing sequential decision-making in an unknown environment.
An MAB consists of multiple arms,
each with an unknown reward distribution.
The agent's task is to learn about these through interaction,
eventually minimizing the _regret_,
which measures how suboptimal the chosen arms were.

We saw algorithms such as **upper confidence bound** and **Thompson sampling**
that handle the tradeoff between _exploration_ and _exploitation_,
that is,
the tradeoff between choosing arms that the agent is _uncertain_ about
and arms the agent already supposes are be good.

We finally discussed **contextual bandits**,
in which the agent gets to observe some _context_ that affects the reward distributions.
We can approach these problems through **supervised learning** approaches.



::: {.content-visible when-profile="thesis"}
## Differences from course

This chapter remained nearly identical to the way it was taught in the course.
We progress through a sequence of algorithms
to highlight the exploration-exploitation tradeoff.
:::
