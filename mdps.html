<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Markov Decision Processes – An Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./control.html" rel="next">
<link href="./introduction.html" rel="prev">
<link href="./shared/184.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e39e07555a3cad66df478e90649eca5f.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="2&nbsp; Markov Decision Processes – An Introduction to Reinforcement Learning">
<meta property="og:description" content="">
<meta property="og:image" content="https://rlbook.adzc.ai/shared/chess-pieces.jpg">
<meta property="og:site_name" content="An Introduction to Reinforcement Learning">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./mdps.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Introduction to Reinforcement Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/adzcai/rlbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proofs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Proofs</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#sec-mdps-intro" id="toc-sec-mdps-intro" class="nav-link" data-scroll-target="#sec-mdps-intro"><span class="header-section-number">2.2</span> Introduction to Markov decision processes</a></li>
  <li><a href="#sec-finite-horizon-mdps" id="toc-sec-finite-horizon-mdps" class="nav-link" data-scroll-target="#sec-finite-horizon-mdps"><span class="header-section-number">2.3</span> Finite-horizon Markov decision processes</a>
  <ul class="collapse">
  <li><a href="#sec-eval-dp-finite-horizon" id="toc-sec-eval-dp-finite-horizon" class="nav-link" data-scroll-target="#sec-eval-dp-finite-horizon"><span class="header-section-number">2.3.1</span> Policy evaluation</a></li>
  <li><a href="#sec-finite-opt-dp" id="toc-sec-finite-opt-dp" class="nav-link" data-scroll-target="#sec-finite-opt-dp"><span class="header-section-number">2.3.2</span> Optimality</a></li>
  </ul></li>
  <li><a href="#sec-infinite-horizon-mdps" id="toc-sec-infinite-horizon-mdps" class="nav-link" data-scroll-target="#sec-infinite-horizon-mdps"><span class="header-section-number">2.4</span> Infinite-horizon Markov decision processes</a>
  <ul class="collapse">
  <li><a href="#differences-from-the-finite-horizon-setting" id="toc-differences-from-the-finite-horizon-setting" class="nav-link" data-scroll-target="#differences-from-the-finite-horizon-setting"><span class="header-section-number">2.4.1</span> Differences from the finite horizon setting</a></li>
  <li><a href="#contraction-mappings" id="toc-contraction-mappings" class="nav-link" data-scroll-target="#contraction-mappings"><span class="header-section-number">2.4.2</span> Contraction mappings</a></li>
  <li><a href="#sec-infinite-policy-evaluation" id="toc-sec-infinite-policy-evaluation" class="nav-link" data-scroll-target="#sec-infinite-policy-evaluation"><span class="header-section-number">2.4.3</span> Policy evaluation</a></li>
  <li><a href="#sec-infinite-opt" id="toc-sec-infinite-opt" class="nav-link" data-scroll-target="#sec-infinite-opt"><span class="header-section-number">2.4.4</span> Optimality</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">2.5</span> Key takeaways</a></li>
  <li><a href="#sec-mdps-bib" id="toc-sec-mdps-bib" class="nav-link" data-scroll-target="#sec-mdps-bib"><span class="header-section-number">2.6</span> Bibliographic notes and further reading</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/mdps.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/mdps.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-mdps" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p><strong>Machine learning</strong> studies algorithms that learn to solve a task “on their own”, without needing a programmer to implement handwritten “if statements”. <strong>Reinforcement learning (RL)</strong> is a branch of machine learning that focuses on <strong>decision problems</strong> like the following:</p>
<p><span class="theorem-title"><strong>Example 2.1 (Decision problems)</strong></span> &nbsp;</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/chess-pieces.jpg" class="img-fluid figure-img"></p>
<figcaption>Board games and video games, such as a game of chess, where a player character takes actions that update the state of the game <span class="citation" data-cites="guy_chess_2006">(<a href="references.html#ref-guy_chess_2006" role="doc-biblioref">Guy, 2006</a>)</span>.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/cargo-ship.jpg" class="img-fluid figure-img"></p>
<figcaption>Inventory management, where a company must efficiently move resources from producers to consumers <span class="citation" data-cites="frans_berkelaar_container_2009">(<a href="references.html#ref-frans_berkelaar_container_2009" role="doc-biblioref">Frans Berkelaar, 2009</a>)</span>.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/robot-arm.jpg" class="img-fluid figure-img"></p>
<figcaption>Robotic control, where a robot can move and interact with the real world to complete some task <span class="citation" data-cites="gpa_photo_archive_robotic_2017">(<a href="references.html#ref-gpa_photo_archive_robotic_2017" role="doc-biblioref">GPA Photo Archive, 2017</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>All of these tasks involve taking a sequence of <strong>actions</strong> in order to achieve some goal. This <strong>interaction loop</strong> can be summarized in the following diagram:</p>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-rl-interaction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-rl-interaction">graph LR
    Agent --"takes *action*"--&gt; Environment
    Environment --"observes *state*, *reward*"--&gt; Agent
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The RL interaction loop. The agent chooses an <strong>action</strong> that affects the environment. The environment updates according to the <strong>state transitions</strong>. Then the agent observes the updated environment and a <strong>reward signal</strong> that describes progress made towards the goal.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In this chapter, we’ll investigate the most popular mathematical formalization for such tasks: <strong>Markov decision processes (MDPs)</strong>. We will study <strong>dynamic programming (DP)</strong> algorithms for solving tasks when the rules of the environment are totally known. We’ll describe how to <em>evaluate</em> different policies and how to compute (or approximate) the <strong>optimal policy</strong> for a given MDP. We’ll introduce the <strong>Bellman consistency condition</strong>, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.</p>
<div id="rem-complications" class="proof remark">
<p><span class="proof-title"><em>Remark 2.1</em> (Further generalizations). </span>In most real tasks, we <em>don’t</em> explicitly know the rules of the environment, or can’t concisely represent them on a computer. We will deal with this in future chapters; this chapter assumes the environment is known and accessible, so there’s nothing to <em>learn</em> about the environment.</p>
<p>Additionally, in many tasks, only a <em>subset</em> of the state is visible to the observer. Such <em>partially observed</em> environments are out of scope of this textbook; see <a href="#sec-mdps-bib" class="quarto-xref"><span>Section 2.6</span></a> for additional resources.</p>
</div>
<div id="952b9be5" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> NamedTuple, Float, Array, partial, jax, jnp, latexify, latex</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown, display</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-mdps-intro" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-mdps-intro"><span class="header-section-number">2.2</span> Introduction to Markov decision processes</h2>
<p>To gain a better grasp of decision processes and identify the key features that we want to formalize, let us frame the <em>robotic control</em> task in <a href="#exm-decision-problems" class="quarto-xref">ex.&nbsp;<span>2.1</span></a> as a decision problem.</p>
<div id="exm-mdp-robotics" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Robotic control as a decision problem)</strong></span> Suppose the goal is to move a robot to a certain position.</p>
<ul>
<li>The <strong>state</strong> consists of the positions and velocities of the robot’s joints.</li>
<li>The <strong>action</strong> consists of the forces to apply to the robot’s motors.</li>
<li>The <strong>state transitions</strong> are essentially the rules of physics: after applying a certain force to the joints, their positions and velocities would change according to physical law.</li>
<li>We <strong>reward</strong> positions that are closer to the desired position. To be more energy-efficient, we could also deduct reward for applying a lot of force, or add other terms that describe the ideal behaviour.</li>
</ul>
</div>
<div id="exr-decision-problems" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1 (Identifying decision problems)</strong></span> For each of the other examples in <a href="#exm-decision-problems" class="quarto-xref">ex.&nbsp;<span>2.1</span></a>, what information should the <em>state</em> include? What might the valid set of <em>actions</em> be? Describe the <em>state transitions</em> heuristically, and a <em>reward function</em> that describes how the task should be solved.</p>
</div>
<p>In many sequential decision-making problems, the state transitions only depend on the <em>current</em> state and action. For example, in <a href="#exm-mdp-robotics" class="quarto-xref">ex.&nbsp;<span>2.2</span></a>, if we use Newton’s laws of physics to compute the state transitions, then just knowing the current positions and velocities is enough to calculate the next positions and velocities, since Newton’s laws are second-order. We say that such state transitions satisfy the <strong>Markov property</strong>.</p>
<div id="def-markov-informal" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Markov property (informal))</strong></span> An environment’s state transitions satisfy the <strong>Markov property</strong> if “what happens next” only depends on the current state of the environment and not on the history of how we got here.</p>
</div>
<p>We will formally define the Markov property in <a href="#def-markov-property" class="quarto-xref">def.&nbsp;<span>2.5</span></a> after introducing some notation for describing MDPs.</p>
<div id="exr-understanding-mdps" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2 (Checking for the Markov property)</strong></span> Look back at the state transitions you described in <a href="#exr-decision-problems" class="quarto-xref">Exercise&nbsp;<span>2.1</span></a>. Do they satisfy the Markov property? (For chess, consider the threefold repetition rule: if the same position occurs three times during the game, either player may claim a draw.)</p>
</div>
<p>Sequential decision-making problems that satisfy the Markov property are called <strong>Markov decision processes</strong> (MDPs). MDPs are the core setting of modern RL research. We can further classify MDPs based on whether or not the task eventually ends.</p>
<div id="def-episodic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Episodic tasks)</strong></span> Tasks that have a well-defined start and end, such as a game of chess or a football match, are called <strong>episodic</strong>. Each episode starts afresh and ends once the environment reaches a <strong>terminal state</strong>. The number of steps in an episode is called its <strong>horizon</strong>.</p>
</div>
<div id="def-continuing" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Continuing tasks)</strong></span> On the other hand, <strong>continuing tasks</strong>, such as managing a business’s inventory, have no clear start or end point. The agent interacts with the environment in an indefinite loop.</p>
</div>
<p>Consider an episodic task where the horizon is fixed and known in advance. That is, the agent takes up to <span class="math inline">\(H\)</span> actions, where <span class="math inline">\(H\)</span> is some finite positive integer. We model such tasks with a <strong>finite-horizon</strong> MDP. Otherwise, if there’s no limit to how long the episode can continue or if the task is continuing, we model the task with an <strong>infinite-horizon</strong> MDP. We’ll begin with the finite-horizon case in <a href="#sec-finite-horizon-mdps" class="quarto-xref"><span>Section 2.3</span></a> and discuss the infinite-horizon case in <a href="#sec-infinite-horizon-mdps" class="quarto-xref"><span>Section 2.4</span></a>.</p>
</section>
<section id="sec-finite-horizon-mdps" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-finite-horizon-mdps"><span class="header-section-number">2.3</span> Finite-horizon Markov decision processes</h2>
<p>Many real-world tasks, such as most sports games or video games, end after a fixed number of actions from the agent. In each episode:</p>
<ol type="1">
<li>The environment starts in some initial state <span class="math inline">\(s_0\)</span>.</li>
<li>The agent observes the state and takes an action <span class="math inline">\(a_0\)</span>.</li>
<li>The environment updates to state <span class="math inline">\(s_1\)</span> according to the action.</li>
</ol>
<p>Steps 2 and 3 are repeated <span class="math inline">\(H\)</span> times, resulting in a sequence of states and actions <span class="math inline">\(s_0, a_0, \dots, s_{H-1}, a_{H-1}, s_H\)</span>, where <span class="math inline">\(s_H\)</span> is some <strong>terminal state</strong>. Here’s the notation we will use to describe an MDP:</p>
<div id="def-finite-horizon-mdp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Finite-horizon Markov decision process)</strong></span> The components of a finite-horizon Markov decision process are:</p>
<ol type="1">
<li><p>The <strong>state</strong> that the agent interacts with. We use <span class="math inline">\(s_h\)</span> to denote the state at time <span class="math inline">\(h\)</span>. <span class="math inline">\(\mathcal{S}\)</span> denotes the set of possible states, called the <strong>state space</strong>.</p></li>
<li><p>The <strong>actions</strong> that the agent can take. We use <span class="math inline">\(a_h\)</span> to denote the action at time <span class="math inline">\(h\)</span>. <span class="math inline">\(\mathcal{A}\)</span> denotes the set of possible actions, called the <strong>action space</strong>.</p></li>
<li><p>An <strong>initial state distribution</strong> <span class="math inline">\(P_0 \in \triangle(\mathcal{S})\)</span>.</p></li>
<li><p>The <strong>state transitions</strong> (a.k.a. <strong>dynamics</strong>) <span class="math inline">\(P : \mathcal{S} \times \mathcal{A} \to \triangle(\mathcal{S})\)</span> that describe what state the agent transitions to after taking an action. We write <span class="math inline">\(P(s_{h+ 1} \mid s_h, a_h)\)</span> for the probability of transitioning to state <span class="math inline">\(s_{h+1}\)</span> when starting in state <span class="math inline">\(s_h\)</span> and taking action <span class="math inline">\(a_h\)</span>. Note that we use the letter <span class="math inline">\(P\)</span> for the state transition function and the symbol <span class="math inline">\(\mathbb{P}\)</span> more generally to indicate probabilities of events.</p></li>
<li><p>The <strong>reward function</strong>. In this course, we’ll take it to be a deterministic function on state-action pairs, <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>, but many results will extend to a <em>stochastic</em> reward signal. We will use <span class="math inline">\(r_{h} := r(s_h, a_h)\)</span> to denote the reward obtained at time <span class="math inline">\(h\)</span>.</p></li>
<li><p>A <strong>time horizon</strong> <span class="math inline">\(H\in \mathbb{N}\)</span> that specifies the maximum number of interactions in a <strong>trajectory</strong>, that is, a single sequence of states, actions, and rewards:</p></li>
</ol>
<p><span id="eq-trajectory"><span class="math display">\[
(s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1}).
\tag{2.1}\]</span></span></p>
<p>(Some sources omit the action and reward at the final time step.)</p>
<p>Combined together, these objects specify a finite-horizon Markov decision process:</p>
<p><span id="eq-finite-horizon-mdp"><span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P_0, P, r, H).
\tag{2.2}\]</span></span></p>
</div>
<p>When there are <strong>finitely</strong> many states and actions, i.e. <span class="math inline">\(|\mathcal{S}|, |\mathcal{A}| &lt; \infty\)</span>, we can express the relevant quantities as vectors and matrices (i.e.&nbsp;<em>tables</em> of values):</p>
<p><span class="math display">\[
\begin{aligned}
    P_0 &amp;\in [0, 1]^{|\mathcal{S}|} &amp;
    P &amp;\in [0, 1]^{(|\mathcal{S} \times \mathcal{A}|) \times |\mathcal{S}|} &amp;
    r &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}
\end{aligned}
\]</span></p>
<p>(Verify that the types and shapes provided above make sense!)</p>
<div id="6f4fea8d" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MDP(NamedTuple):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A description of a Markov decision process with finitely many states and actions."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    S: <span class="bu">int</span>  <span class="co"># number of states</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    A: <span class="bu">int</span>  <span class="co"># number of actions</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    mu: Float[Array, <span class="st">" S"</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    P: Float[Array, <span class="st">"S A S"</span>]  <span class="co"># "current" state, "current" action, "next" state</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    r: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    H: <span class="bu">int</span>  <span class="co"># the horizon</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    gamma: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># discount factor (used later)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transition(NamedTuple):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single state-action-reward interaction with the environment.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">    A trajectory consists of a sequence of transitions.</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    s: <span class="bu">int</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    a: <span class="bu">int</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    r: <span class="bu">float</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="def-markov-property" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Markov property)</strong></span> A decision process satisfies the <strong>Markov property</strong> if the next state is independent from the past states and actions when conditioned on the current state and action:</p>
<p><span class="math display">\[
\mathbb{P}(s_{h+1} \mid s_0, a_0, \dots, s_h, a_h) = \mathbb{P}(s_{h+1} \mid s_h, a_h)
\]</span></p>
<p>By their definition, Markov decision processes satisfy the Markov property. This is because the state transitions are <em>defined</em> using the function <span class="math inline">\(P\)</span>, which only takes in a single state-action pair to transition from.</p>
</div>
<div id="exm-tidy-mdp" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Tidying MDP)</strong></span> Let’s consider a simple decision problem throughout this chapter: the task of keeping your room tidy.</p>
<p>Your room has the possible states <span class="math inline">\(\mathcal{S} = \{ \text{orderly}, \text{messy} \}.\)</span> You can take either of the actions <span class="math inline">\(\mathcal{A} = \{ \text{ignore}, \text{tidy} \}.\)</span> The room starts off orderly, that is, <span class="math inline">\(P_0(\text{orderly}) = 1\)</span>.</p>
<p>The <strong>state transitions</strong> are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it <em>might</em> become messy, according to the probabilities in <a href="#tbl-tidy" class="quarto-xref">tbl.&nbsp;<span>2.1</span></a>.</p>
<p>The <strong>rewards</strong> are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy your additional time). Tidying a messy room is a chore that gives no reward.</p>
<p>Consider a time horizon of <span class="math inline">\(H= 7\)</span> days (one interaction per day). Let <span class="math inline">\(t = 0\)</span> correspond to Monday and <span class="math inline">\(t = 6\)</span> correspond to Sunday.</p>
</div>
<div class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tidy_mdp <span class="op">=</span> MDP(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    S<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = orderly, 1 = messy</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    A<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = ignore, 1 = tidy</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>]),  <span class="co"># start in orderly state</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    P<span class="op">=</span>jnp.array([</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.7</span>, <span class="fl">0.3</span>],  <span class="co"># orderly, ignore</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># orderly, tidy</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.0</span>, <span class="fl">1.0</span>],  <span class="co"># messy, ignore</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># messy, tidy</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span>jnp.array([</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="fl">1.0</span>,   <span class="co"># orderly, ignore</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># orderly, tidy</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># messy, ignore</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.0</span>,   <span class="co"># messy, tidy</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    H<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(<span class="op">*</span>[</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        [<span class="st">'orderly'</span>, <span class="st">'orderly'</span>, <span class="st">'messy'</span>, <span class="st">'messy'</span>],</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        [<span class="st">'ignore'</span>, <span class="st">'tidy'</span>, <span class="st">'ignore'</span>, <span class="st">'tidy'</span>],</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.P[:, :, <span class="dv">0</span>].flatten(),</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.P[:, :, <span class="dv">1</span>].flatten(),</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.r.flatten(),</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a$"</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$P(\text</span><span class="sc">{orderly}</span><span class="vs"> \mid s, a)$"</span>,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$P(\text</span><span class="sc">{messy}</span><span class="vs"> \mid s, a)$"</span>,</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$r(s, a)$"</span>,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-tidy" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tidy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Description of the MDP in <a href="#exm-tidy-mdp" class="quarto-xref">ex.&nbsp;<span>2.3</span></a>.
</figcaption>
<div aria-describedby="tbl-tidy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="3">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: left;"><span class="math inline">\(a\)</span></th>
<th style="text-align: right;"><span class="math inline">\(P(\text{orderly} \mid s, a)\)</span></th>
<th style="text-align: right;"><span class="math inline">\(P(\text{messy} \mid s, a)\)</span></th>
<th style="text-align: right;"><span class="math inline">\(r(s, a)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">orderly</td>
<td style="text-align: left;">ignore</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">orderly</td>
<td style="text-align: left;">tidy</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">messy</td>
<td style="text-align: left;">ignore</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-1</td>
</tr>
<tr class="even">
<td style="text-align: left;">messy</td>
<td style="text-align: left;">tidy</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We’ll now introduce several core concepts when working with MDPs.</p>
<div id="def-policy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Policies)</strong></span> A <strong>policy</strong> <span class="math inline">\(\pi\)</span> describes the agent’s strategy: which actions it takes in a given situation. A key goal of RL is to find the <strong>optimal policy</strong> that maximizes the total reward on average.</p>
<p>There are three axes along which policies can vary: their outputs, inputs, and time-dependence.</p>
<ol type="1">
<li>Outputs: <strong>Deterministic or stochastic.</strong> A deterministic policy outputs actions while a stochastic policy outputs <em>distributions</em> over actions.</li>
</ol>
<div id="fig-deterministic-stochastic-policy" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deterministic-stochastic-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/deterministic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A deterministic policy, which produces a single action</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/stochastic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A stochastic policy, which produces a distribution over possible actions</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-deterministic-stochastic-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2
</figcaption>
</figure>
</div>
<ol start="2" type="1">
<li><p>Inputs: <strong>history-independent or history-dependent.</strong> A history-independent (a.k.a. “Markovian”) policy only depends on the current state, while a history-dependent policy depends on the sequence of past states, actions, and rewards. We’ll only consider history-independent policies in this course.</p></li>
<li><p><strong>Stationary or time-dependent.</strong> A stationary (a.k.a. time-homogeneous) policy keeps the same strategy at all time steps, while a time-dependent policy can depend on the current timestep. For consistency with states and actions, we will denote the timestep as a subscript, i.e.&nbsp;<span class="math inline">\(\pi = \{ \pi_0, \dots, \pi_{H-1} \}.\)</span></p></li>
</ol>
</div>
<p>Note that for finite state and action spaces, we can represent a randomized mapping <span class="math inline">\(\mathcal{S} \to \triangle(\mathcal{A})\)</span> as a matrix <span class="math inline">\(\pi \in [0, 1]^{\mathcal{S} \times \mathcal{A}}\)</span> where each row describes the policy’s distribution over actions for the corresponding state.</p>
<div id="exm-tidy-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Policies for the tidying MDP)</strong></span> Here are some possible deterministic policies for the tidying MDP <a href="#exm-tidy-mdp" class="quarto-xref">ex.&nbsp;<span>2.3</span></a>:</p>
<ul>
<li>Always tidy: <span class="math inline">\(\pi(s) = \text{tidy}\)</span>.</li>
<li>Only tidy on weekends: <span class="math inline">\(\pi_h(s) = \text{tidy}\)</span> if <span class="math inline">\(h\in \{ 5, 6 \}\)</span> and <span class="math inline">\(\pi_h(s) = \text{ignore}\)</span> otherwise.</li>
<li>Only tidy if the room is messy: <span class="math inline">\(\pi_h(\text{messy}) = \text{tidy}\)</span> and <span class="math inline">\(\pi_h(\text{orderly}) = \text{ignore}\)</span> for all <span class="math inline">\(h\)</span>.</li>
</ul>
</div>
<div id="80de20ba" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrays of shape (H, S, A) represent time-dependent policies</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tidy_policy_always_tidy <span class="op">=</span> (</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    .at[:, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tidy_policy_weekends <span class="op">=</span> (</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">5</span>:<span class="dv">7</span>, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">0</span>:<span class="dv">5</span>, :, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>tidy_policy_messy_only <span class="op">=</span> (</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">1</span>, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">0</span>, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!-- ::: {#rem-jax-immutable}
Array objects in Jax are **immutable,** that is, they cannot be _changed._
This might seem inconvenient, but in larger projects,
immutability makes code easier to reason about.
::: -->
<div id="exr-markov-future" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3 (Conditional independence for future states)</strong></span> Suppose actions are chosen according to a history-independent policy. Use the chain rule of probability to show that, conditioned on the current state and action <span class="math inline">\(s_h, a_h\)</span>, <em>all</em> future states <span class="math inline">\(s_{h'}\)</span> where <span class="math inline">\(h' &gt; h\)</span> are conditionally independent of the past states and actions. That is, for all <span class="math inline">\(h' &gt; h\)</span>,</p>
<p><span class="math display">\[
\mathbb{P}(s_{h'} \mid s_0, a_0, \dots, s_h, a_h) = \mathbb{P}(s_{h'} \mid s_h, a_h).
\]</span></p>
</div>
<div id="def-trajectory-distribution" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 (Trajectory distribution)</strong></span> Once we’ve chosen a policy, we can sample trajectories by repeatedly choosing actions according to the policy and observing the rewards and updated states returned by the environment. This generative process induces a distribution <span class="math inline">\(\rho^{\pi}\)</span> over trajectories. (We assume that <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P\)</span> are clear from context.)</p>
</div>
<div id="exm-tidy-traj" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Trajectories in the tidying environment)</strong></span> Here is a possible trajectory for the tidying example:</p>
<div class="{table}">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(h\)</span></th>
<th style="text-align: center;"><span class="math inline">\(0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(4\)</span></th>
<th style="text-align: center;"><span class="math inline">\(5\)</span></th>
<th style="text-align: center;"><span class="math inline">\(6\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(s\)</span></td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(a\)</span></td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(r\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Could any of the policies in <a href="#exm-tidy-policy" class="quarto-xref">ex.&nbsp;<span>2.4</span></a> have generated this trajectory?</p>
</div>
<p>Note that for a history-independent policy, using the Markov property (<a href="#def-markov-property" class="quarto-xref">def.&nbsp;<span>2.5</span></a>), we can write down the likelihood function of this probability distribution in an <strong>autoregressive</strong> way (i.e.&nbsp;one timestep at a time):</p>
<div id="thm-autoregressive-trajectories" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Trajectory distributions of history-independent policies)</strong></span> Let <span class="math inline">\(\pi\)</span> be a history-independent policy. Then its trajectory distribution <span class="math inline">\(\rho^\pi\)</span> (<a href="#def-trajectory-distribution" class="quarto-xref">def.&nbsp;<span>2.7</span></a>) over sequences of actions and states <span class="math inline">\((s_0, a_0, \dots, s_{H-1}, a_{H-1})\)</span> can be written as</p>
<p><span id="eq-autoregressive-trajectories"><span class="math display">\[
\rho^{\pi}(\tau) := P_0(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots
P(s_{H-1} \mid s_{H-2}, a_{H-2}) \pi_{H-1}(a_{H-1} \mid s_{H-1}).
\tag{2.3}\]</span></span></p>
<p>Note that we use a deterministic reward function, so we only consider randomness over the states and actions.</p>
</div>
<div id="exr-stochastic-reward-trajectory-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.4 (Trajectory distribution for stochastic reward)</strong></span> Modify <a href="#eq-autoregressive-trajectories" class="quarto-xref">eq.&nbsp;<span>2.3</span></a> for the case when the reward function is stochastic, that is, <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \triangle(\mathbb{R})\)</span>.</p>
</div>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-trajectory-diagram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trajectory-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-trajectory-diagram">graph LR
    S0($$s_0$$) -- $$\pi_0$$ --&gt; A0{{$$a_0$$}}
    S0 &amp; A0 --&gt; R0[$$r_0$$]
    A0 &amp; S0 --&gt; S1($$s_1$$)
    S1 -- $$\pi_1$$ --&gt; A1{{$$a_1$$}}
    S1 &amp; A1 --&gt; R1[$$r_1$$]
    A1 &amp; S1 --&gt; S2($$s_2$$)
    S2 -- $$\pi_2$$ --&gt; A2{{$$a_2$$}}
    S2 &amp; A2 --&gt; R2[$$r_2$$]
    A2 &amp; S2 --&gt; S3($$...$$)

    class S0,S1,S2,S3 state
    class A0,A1,A2 action
    class R0,R1,R2 reward

    classDef state fill: lightgreen;
    classDef action fill: lightyellow;
    classDef reward fill: lightblue;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trajectory-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: A trajectory is generated by taking a sequence of actions according to the history-independent policy.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="a829d725" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trajectory_log_likelihood(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    traj: <span class="bu">list</span>[Transition],</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    pi: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute the log-likelihood of a trajectory under a given MDP and policy."""</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initial distribution and action</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> jnp.log(mdp.mu[traj[<span class="dv">0</span>].s])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    total <span class="op">+=</span> jnp.log(pi[traj[<span class="dv">0</span>].s, traj[<span class="dv">0</span>].a])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remaining state transitions and actions</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, mdp.H):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(mdp.P[traj[i <span class="op">-</span> <span class="dv">1</span>].s, traj[i <span class="op">-</span> <span class="dv">1</span>].a, traj[i].s])</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(pi[traj[i].s, traj[i].a])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>For a deterministic policy <span class="math inline">\(\pi\)</span>, we have that <span class="math inline">\(\pi_h(a \mid s) = \mathbf{1}\left\{a = \pi_h(s)\right\}\)</span>; that is, the probability of taking an action is <span class="math inline">\(1\)</span> if it’s the unique action prescribed by the policy for that state and <span class="math inline">\(0\)</span> otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution <span class="math inline">\(P_0\)</span> and the state transitions <span class="math inline">\(P\)</span>.</p>
<section id="sec-eval-dp-finite-horizon" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="sec-eval-dp-finite-horizon"><span class="header-section-number">2.3.1</span> Policy evaluation</h3>
<p>Recall that the core goal of an RL algorithm is to find a policy that maximizes the expected total reward</p>
<p><span id="eq-expected-total-reward"><span class="math display">\[
\mathop{\mathbb{E}}[r_0 + \cdots + r_{H-1}],
\tag{2.4}\]</span></span></p>
<p>where <span class="math inline">\(r_h= r(s_h, a_h)\)</span>. Note that the quantity <span class="math inline">\(r_0 + \cdots + r_{H-1}\)</span> is a random variable whose distribution depends on the policy’s trajectory distribution <span class="math inline">\(\rho^\pi\)</span> (<a href="#def-trajectory-distribution" class="quarto-xref">def.&nbsp;<span>2.7</span></a>). We need tools for describing the expected total reward achieved by a given policy, starting in specific states and actions. This will allow us to compare the quality of different policies and compute the optimal policy.</p>
<div id="def-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 (Value function)</strong></span> Let <span class="math inline">\(\pi\)</span> be a history-independent policy. Its <strong>value function</strong> at time <span class="math inline">\(h\)</span> returns the expected remaining reward when starting in a specific state:</p>
<p><span id="eq-v"><span class="math display">\[
V_h^\pi(s) := \mathop{\mathbb{E}}_{\tau \sim \rho^\pi} [r_h+ \cdots + r_{H-1} \mid s_h= s]
\tag{2.5}\]</span></span></p>
</div>
<div id="def-q" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9 (Action-value function)</strong></span> Similarly, a history-independent policy <span class="math inline">\(\pi\)</span>’s <strong>action-value function</strong> (aka Q function) at time <span class="math inline">\(h\)</span> returns its expected remaining reward, starting in a specific state <em>and</em> taking a specific action:</p>
<p><span id="eq-q"><span class="math display">\[
Q_h^\pi(s, a) := \mathop{\mathbb{E}}_{\tau \sim \rho^\pi} [r_h+ \cdots + r_{H-1} \mid s_h= s, a_h= a].
\tag{2.6}\]</span></span></p>
</div>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-value-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-value-trajectory">graph LR
    S0($$s_h = s$$) -- $$\pi_h$$ --&gt; A0{{$$a_h$$}}
    S0 &amp; A0 --&gt; R0[$$r_h$$]
    A0 &amp; S0 --&gt; S1("$$s_{h+1}$$")
    S1 -- "$$\pi_{h+1}$$" --&gt; A1{{"$$a_{h+1}$$"}}
    S1 &amp; A1 --&gt; R1["$$r_{h+1}$$"]
    A1 &amp; S1 --&gt; S2("$$s_{h+2}$$")
    S2 -- "$$\pi_{h+2}$$" --&gt; A2{{"$$a_{h+2}$$"}}
    S2 &amp; A2 --&gt; R2["$$r_{h+2}$$"]
    A2 &amp; S2 --&gt; S3("...")

    class S0,R0,R1,R2 thick

    classDef thick stroke-width: 4px;

    class S0,S1,S2,S3 state
    class A0,A1,A2 action
    class R0,R1,R2 reward

    classDef state fill: lightgreen;
    classDef action fill: lightyellow;
    classDef reward fill: lightblue;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: The policy starts in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span>. Then the expected remaining reward is computed.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-action-value-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-action-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-action-value-trajectory">graph LR
    S0($$s_h = s$$) ~~~ A0{{$$a_h = a$$}}
    S0 &amp; A0 --&gt; R0[$$r_h$$]
    A0 &amp; S0 --&gt; S1("$$s_{h+1}$$")
    S1 -- "$$\pi_{h+1}$$" --&gt; A1{{"$$a_{h+1}$$"}}
    S1 &amp; A1 --&gt; R1["$$r_{h+1}$$"]
    A1 &amp; S1 --&gt; S2("$$s_{h+2}$$")
    S2 -- "$$\pi_{h+2}$$" --&gt; A2{{"$$a_{h+2}$$"}}
    S2 &amp; A2 --&gt; R2["$$r_{h+2}$$"]
    A2 &amp; S2 --&gt; S3("...")

    class S0,A0,R0,R1,R2 thick;

    classDef thick stroke-width: 4px;

    class S0,S1,S2,S3 state
    class A0,A1,A2 action
    class R0,R1,R2 reward

    classDef state fill: lightgreen;
    classDef action fill: lightyellow;
    classDef reward fill: lightblue;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-action-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: The policy starts in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span> and takes action <span class="math inline">\(a\)</span>. Then the expected remaining reward is computed.
</figcaption>
</figure>
</div>
</div>
</div>
<p>These two functions define each other in the following way:</p>
<div id="thm-relating-v-q" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2 (Relating <span class="math inline">\(V^\pi\)</span> and <span class="math inline">\(Q^\pi\)</span>)</strong></span> Let <span class="math inline">\(\pi\)</span> be a history-independent policy. The value of a state is the expected action-value in that state over actions drawn from the policy:</p>
<p><span id="eq-relating-v-q"><span class="math display">\[
V_h^\pi(s) = \mathop{\mathbb{E}}_{a \sim \pi_h(\cdot \mid s)} [Q_h^\pi(s, a)].
\tag{2.7}\]</span></span></p>
<p>The value of a state-action pair is the sum of the immediate reward and the expected value of the following state:</p>
<p><span id="eq-relating-q-v"><span class="math display">\[
Q_h^\pi(s, a) = r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} [V_{h+1}^\pi(s')].
\tag{2.8}\]</span></span></p>
</div>
<div id="exr-proof-consistency" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.5 (Proof of relationship between <span class="math inline">\(V^\pi\)</span> and <span class="math inline">\(Q^\pi\)</span>)</strong></span> Use the law of iterated expectations to show <a href="#eq-relating-v-q" class="quarto-xref">eq.&nbsp;<span>2.7</span></a>. Now apply linearity of expectation to show <a href="#eq-relating-q-v" class="quarto-xref">eq.&nbsp;<span>2.8</span></a>. Where did you use the assumption that <span class="math inline">\(\pi\)</span> doesn’t depend on the past states and actions?</p>
</div>
<div id="rem-q-v-asymmetry" class="proof remark">
<p><span class="proof-title"><em>Remark 2.2</em> (Computing <span class="math inline">\(Q^\pi\)</span> from <span class="math inline">\(V^\pi\)</span> requires environment). </span>Note that all you need to compute <span class="math inline">\(V_h^\pi\)</span> from <span class="math inline">\(Q_h^\pi\)</span> (<a href="#eq-relating-v-q" class="quarto-xref">eq.&nbsp;<span>2.7</span></a>) is knowledge of the policy <span class="math inline">\(\pi\)</span>. On the other hand, to compute <span class="math inline">\(Q_h^\pi\)</span> from <span class="math inline">\(V_{h+1}^\pi\)</span> (<a href="#eq-relating-q-v" class="quarto-xref">eq.&nbsp;<span>2.8</span></a>), you need knowledge of the reward function and state transitions to look ahead one step. In later chapters, we’ll study problems where the environment is <em>unknown</em>, making it more useful to approximate <span class="math inline">\(Q_h^\pi\)</span> than <span class="math inline">\(V_h^\pi\)</span>.</p>
</div>
<div id="83227caf" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_v(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    q: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the value function for a given policy in a known finite MDP</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from its action-value function.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.average(q, weights<span class="op">=</span>policy, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_q(</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    v_next: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the action-value function in a known finite MDP</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from the corresponding value function.</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the discount factor is relevant later</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v_next</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># convert a list of v functions to a list of q functions</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>v_ary_to_q_ary <span class="op">=</span> jax.vmap(v_to_q, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Putting <a href="#eq-relating-v-q" class="quarto-xref">eq.&nbsp;<span>2.7</span></a> and <a href="#eq-relating-q-v" class="quarto-xref">eq.&nbsp;<span>2.8</span></a> together reveals the <em>Bellman consistency equations</em>. By simply considering the cumulative reward as the sum of the <em>immediate</em> reward and the <em>remaining</em> reward, we can describe the value function in terms of itself. The resulting system of equations is named after <strong>Richard Bellman</strong> (1920–1984), who is credited with introducing dynamic programming in 1953.</p>
<div id="thm-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3 (Bellman consistency equations)</strong></span> For a history-independent policy <span class="math inline">\(\pi\)</span>,</p>
<p><span id="eq-consistency"><span class="math display">\[
V_h^\pi(s) = \mathop{\mathbb{E}}_{\substack{a \sim \pi_h(s) \\ s' \sim P(s, a)}} [r(s, a) + V_{h+1}^\pi(s')].
\tag{2.9}\]</span></span></p>
<p>The proof is left as <a href="#exr-proof-consistency" class="quarto-xref">Exercise&nbsp;<span>2.5</span></a>. This is a system of <span class="math inline">\(H |\mathcal{S}|\)</span> equations (one equation per state per timestep) in <span class="math inline">\(H |\mathcal{S}|\)</span> unknowns (the values <span class="math inline">\(V_h^\pi(s)\)</span>), so <span class="math inline">\(V^\pi\)</span> is the unique solution, as long as no two states are exactly identical to each other.</p>
</div>
<div id="126c60fe" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_bellman_consistency_v(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"H S A"</span>],</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    v_ary: Float[Array, <span class="st">"H S"</span>],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Check that the given (time-dependent) "value function"</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    satisfies the Bellman consistency equation.</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        jnp.allclose(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># lhs</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>            v_ary[h],</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rhs</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            jnp.<span class="bu">sum</span>(policy[h] <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v_ary[h <span class="op">+</span> <span class="dv">1</span>]), axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-bellman-for-deterministic" class="proof remark">
<p><span class="proof-title"><em>Remark 2.3</em> (The Bellman consistency equation for deterministic policies). </span>Note that for history-independent deterministic policies, the Bellman consistency equations simplify to</p>
<p><span id="eq-bellman-deterministic"><span class="math display">\[
V_h^\pi(s) = r(s, \pi_h(s)) + \mathop{\mathbb{E}}_{s' \sim P(s, \pi_h(s))} [V_{h+1}^\pi(s')].
\tag{2.10}\]</span></span></p>
</div>
<p>How can we actually <em>evaluate</em> a given policy, that is, compute its value function?</p>
<p>Suppose we start with some guess <span class="math inline">\(v_h: \mathcal{S} \to \mathbb{R}\)</span> for the state values at time <span class="math inline">\(h= 0, \dots, H-1\)</span>. We write <span class="math inline">\(v\)</span> in lowercase to indicate that it might not be an actual value function of a policy. How might we improve this guess?</p>
<p>Recall that the Bellman consistency equations (<a href="#eq-consistency" class="quarto-xref">eq.&nbsp;<span>2.9</span></a>) hold for the value functions of history-independent policies. So if <span class="math inline">\(v\)</span> is close to the target <span class="math inline">\(V^\pi\)</span>, it should nearly satisfy the system of equations associated with <span class="math inline">\(\pi\)</span>. With this in mind, suppose we replace <span class="math inline">\(V^\pi_{h+1}\)</span> with <span class="math inline">\(v_{h+1}\)</span> on the right-hand side. Then the new right-hand side quantity,</p>
<p><span id="eq-updated-bellman"><span class="math display">\[
\mathop{\mathbb{E}}_{\substack{a \sim \pi_h(s) \\ s' \sim P(s, a)}} [r(s, a) + v_{h+1}(s')],
\tag{2.11}\]</span></span></p>
<p>can be thought of as follows: we take one action according to <span class="math inline">\(\pi\)</span>, observe the immediate reward, and evaluate the next state using <span class="math inline">\(v\)</span>. This is illustrated in <a href="#fig-updated-bellman" class="quarto-xref">fig.&nbsp;<span>2.6</span></a> below. This operation gives us an <em>updated</em> estimate of the value of <span class="math inline">\(V^\pi_h(s)\)</span> that is at least as accurate as applying <span class="math inline">\(v(s)\)</span> directly.</p>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-updated-bellman" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-updated-bellman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-updated-bellman">graph LR
    S0($$s$$) -- $$\pi_h$$ --&gt; A0{{"$$a$$"}}
    S0 &amp; A0 --&gt; R0["$$r_h = r(s, a)$$"]
    A0 &amp; S0 --&gt; S1("$$s'$$")
    S1 -- "$$v_{h+1}$$" --&gt; R1["$$r_{h+1} + \cdots + r_{H-1} \approx v_{h+1}(s')$$"]

    class R0,R1 thick

    classDef thick stroke-width: 4px;

    class S0,S1 state
    class A0 action
    class R0,R1 reward

    classDef state fill: lightgreen;
    classDef action fill: lightyellow;
    classDef reward fill: lightblue;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-updated-bellman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: We evaluate the next state using <span class="math inline">\(v_{h+1}\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can treat this operation of taking <span class="math inline">\(v_h\)</span> to its improved version in <a href="#eq-updated-bellman" class="quarto-xref">eq.&nbsp;<span>2.11</span></a> as a <em>higher-order function</em> known as the Bellman operator.</p>
<div id="def-bellman-operator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.10 (Bellman operator)</strong></span> Let <span class="math inline">\(\pi : \mathcal{S} \to \triangle(\mathcal{A})\)</span>. the Bellman operator <span class="math inline">\(\mathcal{J}^{\pi} : (\mathcal{S} \to \mathbb{R}) \to (\mathcal{S} \to \mathbb{R})\)</span> is the higher-order function that takes in a function <span class="math inline">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman equation with <span class="math inline">\(v\)</span> substituted in:</p>
<p><span id="eq-bellman-operator"><span class="math display">\[
\mathcal{J}^{\pi}(v) := \left(
    s \mapsto \mathop{\mathbb{E}}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v(s')]
\right).
\tag{2.12}\]</span></span></p>
</div>
<p>The Bellman operator is a crucial tool for reasoning about MDPs. Intuitively, it answers the following question: if we evaluate the <em>next</em> state using <span class="math inline">\(v\)</span>, how good is the <em>current</em> state, if we take a single action from the given policy?</p>
<div id="cell-fig-bellman-pseudocode" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator_looping(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Looping definition of the Bellman operator.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Concise version is below</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    v_new <span class="op">=</span> jnp.zeros((mdp.S,))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(mdp.A):</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> s_next <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                v_new[s] <span class="op">+=</span> (</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                    policy[s, a]</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> mdp.P[s, a, s_next]</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> (mdp.r[s, a] <span class="op">+</span> mdp.gamma <span class="op">*</span> v[s_next])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v_new</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we can concisely implement this using the `q_to_v` and `v_to_q` utilities from above:</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator(</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""For a known finite MDP, the Bellman operator can be exactly evaluated."""</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_v(policy, v_to_q(mdp, v))  <span class="co"># equivalent</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(policy <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>latex(</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    bellman_operator_looping,</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"v_new"</span>: <span class="vs">r"v^\text</span><span class="sc">{lhs}</span><span class="vs">"</span>, <span class="st">"policy"</span>: <span class="vs">r"\pi"</span>, <span class="st">"s_next"</span>: <span class="vs">r"s'"</span>},</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    trim_prefixes<span class="op">=</span>{<span class="st">"mdp"</span>},</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    subscript_type<span class="op">=</span><span class="st">"call"</span>,</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-bellman-pseudocode" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="8">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bellman-pseudocode-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{bellman\_operator\_looping}(\mathrm{mdp}: \mathrm{MDP}, \pi: \mathbb{R}^{S \times A}, v: \mathbb{R}^{S}) \\ \hspace{1em} \textrm{"
    Looping definition of the Bellman operator.
    Concise version is below
    "} \\ \hspace{1em} v^\text{lhs} \gets \mathbf{0}^{S} \\ \hspace{1em} \mathbf{for} \ s \in \mathrm{range} \mathopen{}\left( S \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathbf{for} \ a \in \mathrm{range} \mathopen{}\left( A \mathclose{}\right) \ \mathbf{do} \\ \hspace{3em} \mathbf{for} \ s' \in \mathrm{range} \mathopen{}\left( S \mathclose{}\right) \ \mathbf{do} \\ \hspace{4em} v^\text{lhs}(s) \gets v^\text{lhs}(s) + \pi(s, a) P(s, a, s') \cdot \mathopen{}\left( r(s, a) + \gamma v(s') \mathclose{}\right) \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{2em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ v^\text{lhs} \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bellman-pseudocode-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: An algorithm for computing the Bellman operator for a finite state and action space.
</figcaption>
</figure>
</div>
</div>
<p>The Bellman operator also gives us a concise way to express the Bellman consistency equations (<a href="#eq-consistency" class="quarto-xref">eq.&nbsp;<span>2.9</span></a>) for the value function:</p>
<p><span id="eq-bellman-operator-consistency"><span class="math display">\[
V_h^\pi = \mathcal{J}^{\pi_h}(V_{h+1}^\pi)
\tag{2.13}\]</span></span></p>
<p>The Bellman consistency equations (<a href="#eq-consistency" class="quarto-xref">eq.&nbsp;<span>2.9</span></a>) give us a convenient algorithm for evaluating stationary policies: it expresses the value function at timestep <span class="math inline">\(h\)</span> as a function of the value function at timestep <span class="math inline">\(h+1\)</span>. This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman operator to compute the value function at each time step.</p>
<div id="cell-fig-dp-eval-finite" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dp_eval_finite(mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"H S"</span>]:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a policy using dynamic programming."""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((mdp.H <span class="op">+</span> <span class="dv">1</span>, mdp.S))  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        V[h] <span class="op">=</span> bellman_operator(mdp, policy[h], V[h <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>latex(</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    dp_eval_finite,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"bellman_operator"</span>: <span class="vs">r"\mathcal</span><span class="sc">{J}</span><span class="vs">"</span>, <span class="st">"policy"</span>: <span class="vs">r"\pi"</span>, <span class="st">"V"</span>: <span class="st">"V^\pi"</span>},</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-dp-eval-finite" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="9">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dp-eval-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{dp\_eval\_finite}(\mathrm{mdp}: \mathrm{MDP}, \pi: \mathbb{R}^{S \times A}) \\ \hspace{1em} \textrm{"Evaluate a policy using dynamic programming."} \\ \hspace{1em} V^\pi \gets \mathbf{0}^{\mathrm{mdp}.H + 1 \times \mathrm{mdp}.S} \\ \hspace{1em} \mathbf{for} \ h \in \mathrm{range} \mathopen{}\left( \mathrm{mdp}.H - 1, -1, -1 \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} V^\pi_{h} \gets \mathcal{J} \mathopen{}\left( \mathrm{mdp}, \pi_{h}, V^\pi_{h + 1} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ V^\pi_{:-1} \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dp-eval-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: A dynamic programming algorithm for evaluating a policy in a finite-horizon MDP.
</figcaption>
</figure>
</div>
</div>
<p>This runs in time <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>. Note that the implementation of the Bellman operator in <a href="#fig-bellman-pseudocode" class="quarto-xref">fig.&nbsp;<span>2.7</span></a> can be easily modified to compute <span class="math inline">\(Q^\pi\)</span> as an intermediate step. Do you see how?</p>
<div id="exm-tidy-eval-finite" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Tidying policy evaluation)</strong></span> Let’s evaluate the policy from <a href="#exm-tidy-policy" class="quarto-xref">ex.&nbsp;<span>2.4</span></a> in the tidying MDP that tidies if and only if the room is messy. We’ll use the Bellman consistency equation to compute the value function at each time step.</p>
<p><span class="math display">\[
\begin{aligned}
V_{H-1}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) \\
&amp;= 1 \\
V_{H-1}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) \\
&amp;= 0 \\
V_{H-2}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \mathop{\mathbb{E}}_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-1}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1 + 0.3 \cdot 0 \\
&amp;= 1.7 \\
V_{H-2}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \mathop{\mathbb{E}}_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-1}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 \\
V_{H-3}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \mathop{\mathbb{E}}_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-2}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1.7 + 0.3 \cdot 1 \\
&amp;= 2.49 \\
V_{H-3}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \mathop{\mathbb{E}}_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-2}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1.7
\end{aligned}
\]</span></p>
<p>etc. You may wish to repeat this computation for the other policies to get a better sense of this algorithm.</p>
</div>
<div id="f9d584f1" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>V_messy <span class="op">=</span> dp_eval_finite(tidy_mdp, tidy_policy_messy_only)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(<span class="op">*</span>[</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"orderly"</span>, <span class="st">"messy"</span>],</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>V_messy,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"$s$"</span>] <span class="op">+</span> [<span class="ss">f"$h=</span><span class="sc">{</span>h<span class="sc">}</span><span class="ss">$"</span> <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(V_messy))]</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="10">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=3\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=4\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=5\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=6\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">orderly</td>
<td style="text-align: right;">5.56217</td>
<td style="text-align: right;">4.79277</td>
<td style="text-align: right;">4.0241</td>
<td style="text-align: right;">3.253</td>
<td style="text-align: right;">2.49</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">messy</td>
<td style="text-align: right;">4.79277</td>
<td style="text-align: right;">4.0241</td>
<td style="text-align: right;">3.253</td>
<td style="text-align: right;">2.49</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="sec-finite-opt-dp" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-finite-opt-dp"><span class="header-section-number">2.3.2</span> Optimality</h3>
<p>We’ve just seen how to <em>evaluate</em> a given policy. But how can we find the <em>best</em> policy for a given environment? We must first define what it means for a policy to be optimal. We’ll investigate optimality for <em>history-independent</em> policies. In <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>2.6</span></a>, we’ll see that constraining ourselves to history-independent policies isn’t actually a constraint: the best history-independent policy performs at least as well as the best history-dependent policy, in all scenarios!</p>
<div id="rem-optimal-intuition" class="proof remark">
<p><span class="proof-title"><em>Remark 2.4</em> (Intuition). </span>How is this possible? Recall the Markov property (<a href="#def-markov-property" class="quarto-xref">def.&nbsp;<span>2.5</span></a>), which says that once we know the current state, the next state becomes independent from the past history. This means that history-dependent policies, intuitively speaking, don’t benefit over simply history-independent ones in MDPs.</p>
</div>
<div id="def-optimal-policy-finite" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.11 (Optimal policies)</strong></span> Let <span class="math inline">\(\pi^\star\)</span> be a history-independent policy. We say that <span class="math inline">\(\pi^\star\)</span> is <strong>optimal</strong> if <span class="math inline">\(V_h^{\pi^\star}(s) \ge V_h^{\pi}(s)\)</span> for any history-independent policy <span class="math inline">\(\pi\)</span> at any time <span class="math inline">\(h\in [H]\)</span>. In other words, an optimal history-independent policy achieves the highest possible expected remaining reward in every state at every time.</p>
</div>
<div id="exm-optimal-tidy" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Optimal policy in the tidying MDP)</strong></span> For the tidying MDP (<a href="#exm-tidy-mdp" class="quarto-xref">ex.&nbsp;<span>2.3</span></a>), you might guess that the optimal strategy is</p>
<p><span id="eq-tidy-optimal"><span class="math display">\[
\pi(s) = \begin{cases}
\text{tidy} &amp; s = \text{messy} \\
\text{ignore} &amp; \text{otherwise}
\end{cases}
\tag{2.14}\]</span></span></p>
<p>since this keeps the room in the “orderly” state in which reward can be obtained.</p>
</div>
<div id="def-optimal-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.12 (Optimal value function)</strong></span> Given a state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span>, the optimal value <span class="math inline">\(V^\star_h(s)\)</span> is the <em>maximum</em> expected remaining reward achievable by any history-independent policy <span class="math inline">\(\pi\)</span>:</p>
<p><span id="eq-optimal-value"><span class="math display">\[
V^\star_h(s) := \max_{\pi} V^{\pi}_h(s).
\tag{2.15}\]</span></span></p>
<p>The optimal action-value function is defined analogously:</p>
<p><span id="eq-optimal-action-value"><span class="math display">\[
Q^\star_h(s, a) := \max_{\pi} Q^{\pi}_h(s, a).
\tag{2.16}\]</span></span></p>
</div>
<p>These satisfy the following relationship (cf <a href="#thm-relating-v-q" class="quarto-xref">Theorem&nbsp;<span>2.2</span></a>):</p>
<div id="thm-relating-optimal-v-q" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.4 (Relating <span class="math inline">\(V^\star\)</span> and <span class="math inline">\(Q^\star\)</span>)</strong></span> The optimal value of a state is the maximum value across actions from that state:</p>
<p><span id="eq-optimal-v-q"><span class="math display">\[
V_h^\star(s) = \max_{a \in \mathcal{A}} Q^\star_h(s, a).
\tag{2.17}\]</span></span></p>
<p>The optimal value of an action is the immediate reward plus the expected value from the next state:</p>
<p><span id="eq-optimal-q-v"><span class="math display">\[
Q^\star_h(s, a) = r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} [V^\star_{h+1}(s')].
\tag{2.18}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first prove <a href="#eq-optimal-v-q" class="quarto-xref">eq.&nbsp;<span>2.17</span></a>. We begin by expanding the definition of the optimal value function using <a href="#eq-relating-v-q" class="quarto-xref">eq.&nbsp;<span>2.7</span></a>:</p>
<p><span id="eq-step-v-q"><span class="math display">\[
\begin{aligned}
V_h^\star(s)
&amp;= \max_\pi V^\pi_h(s) \\
&amp;= \max_\pi \mathop{\mathbb{E}}_{a \sim \pi_h(s)} [
    Q^\pi_h(s, a)
].
\end{aligned}
\tag{2.19}\]</span></span></p>
<p>Now note that <span class="math inline">\(Q^\pi_h\)</span> doesn’t depend on <span class="math inline">\(\pi_h\)</span>, so we can split the maximization over <span class="math inline">\(\pi = (\pi_0, \dots, \pi_{H-1})\)</span> into an <em>outer</em> optimization over the immediate action <span class="math inline">\(\pi_h\)</span> and an <em>inner</em> optimization over the remaining actions <span class="math inline">\(\pi_{h+ 1}, \dots, \pi_{H-1}\)</span>:</p>
<p><span id="eq-step-split-optimization"><span class="math display">\[
\max_\pi \mathop{\mathbb{E}}_{a \sim \pi_h(s)} \Big[
    Q^\pi_h(s, a)
]
= \max_{\pi_h} \mathop{\mathbb{E}}_{a \sim \pi_h(s)} \Big[
    \max_{\pi_{h+1}, \dots, \pi_{H-1}}
    Q^\pi_h(s, a)
\Big].
\tag{2.20}\]</span></span></p>
<p>But now the inner quantity is exactly <span class="math inline">\(Q^\star_h(s, a)\)</span> as defined in <a href="#eq-optimal-action-value" class="quarto-xref">eq.&nbsp;<span>2.16</span></a>. Now note that maximizing over <span class="math inline">\(\pi_h\)</span> reduces to just maximizing over the action taken:</p>
<p><span id="eq-max-mean"><span class="math display">\[
\max_{\pi_h} \mathop{\mathbb{E}}_{a \sim \pi_h(s)} [\cdots] = \max_{a \in \mathcal{A}} [\cdots].
\tag{2.21}\]</span></span></p>
<p>This proves <a href="#eq-optimal-v-q" class="quarto-xref">eq.&nbsp;<span>2.17</span></a>.</p>
<p>We now prove <a href="#eq-optimal-q-v" class="quarto-xref">eq.&nbsp;<span>2.18</span></a>. We begin by using <a href="#eq-relating-q-v" class="quarto-xref">eq.&nbsp;<span>2.8</span></a>:</p>
<p><span class="math display">\[
\begin{aligned}
Q^\star_h(s, a) &amp;= \max_\pi Q^\pi_h(s, a) \\
&amp;= \max_\pi \Big[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)}[ V^\pi_{h+1}(s') ] \Big]
\end{aligned}
\]</span></p>
<p>We can move the maximization over <span class="math inline">\(\pi\)</span> under the expectation since <span class="math inline">\(s, a\)</span>, and <span class="math inline">\(s'\)</span> are all independent of <span class="math inline">\(\pi\)</span>. Substituting the definition of <span class="math inline">\(V^\star\)</span> concludes the proof.</p>
</div>
<p>As in the Bellman consistency equations (<a href="#thm-consistency" class="quarto-xref">Theorem&nbsp;<span>2.3</span></a>), combining <a href="#eq-optimal-v-q" class="quarto-xref">eq.&nbsp;<span>2.17</span></a> and <a href="#eq-optimal-q-v" class="quarto-xref">eq.&nbsp;<span>2.18</span></a> gives the <em>Bellman optimality equations</em>.</p>
<div id="thm-bellman-opt" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.5 (Bellman optimality equations)</strong></span> The optimal value function function (<a href="#def-optimal-value" class="quarto-xref">def.&nbsp;<span>2.12</span></a>) satisfies, for all timesteps <span class="math inline">\(h\in [H-1]\)</span>,</p>
<p><span id="eq-bellman-opt"><span class="math display">\[
V_h^\star(s) = \max_a r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} [V_{h+1}^\star(s')].
\tag{2.22}\]</span></span></p>
<p>Like the Bellman consistency equations (<a href="#eq-consistency" class="quarto-xref">eq.&nbsp;<span>2.9</span></a>), This is a system of <span class="math inline">\(H |\mathcal{S}|\)</span> equations in <span class="math inline">\(H |\mathcal{S}|\)</span> unknowns, so <span class="math inline">\(V^\star\)</span> is the unique solution (assuming all the states are distinguishable from each other).</p>
<p>Note that the letter <span class="math inline">\(\pi\)</span> doesn’t appear. This will prove useful when we discuss <strong>off-policy algorithms</strong> later in the course.</p>
</div>
<p>We now construct a deterministic history-independent optimal policy by acting <em>greedily</em> with respect to the optimal action-value function:</p>
<div id="def-greedy-policy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.13 (Greedy policies)</strong></span> For any sequence of functions <span class="math inline">\(q_h : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> for <span class="math inline">\(h = 0, \dots, H-1\)</span>, we define the <strong>greedy policy</strong> <span class="math inline">\(\pi^q\)</span> to be the deterministic policy that selects the action with the highest value according to <span class="math inline">\(q\)</span> at each state:</p>
<p><span id="eq-def-greedy"><span class="math display">\[
\pi_h^q(s) := \arg\max_{a \in \mathcal{A}} q_h(s, a).
\tag{2.23}\]</span></span></p>
<p>Note that it is <em>not</em> true in general that <span class="math inline">\(Q^{\pi^q} = q\)</span> for a greedy policy! For one, <span class="math inline">\(q\)</span> might not even be a consistent Q function.</p>
</div>
<div id="26e9f201" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_greedy(q: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the (deterministic) greedy policy with respect to an action-value function.</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> q.shape[<span class="dv">1</span>]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    a_ary <span class="op">=</span> jnp.argmax(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.eye(A)[a_ary]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_greedy(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the (deterministic) greedy policy with respect to a value function."""</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_greedy(v_to_q(mdp, v))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="thm-optimal-greedy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.6 (A greedy optimal policy)</strong></span> The greedy policy <span class="math inline">\(\pi^{Q^\star}\)</span>, where <span class="math inline">\(Q^\star\)</span> is the optimal action-value function (<a href="#eq-optimal-action-value" class="quarto-xref">eq.&nbsp;<span>2.16</span></a>), is an optimal policy (<a href="#def-optimal-policy-finite" class="quarto-xref">def.&nbsp;<span>2.11</span></a>). Furthermore, <span class="math inline">\(\pi^{Q^\star}\)</span> is optimal across <em>all</em> policies <span class="math inline">\(\pi^\text{any}\)</span>, including history-dependent ones, in the sense that for any partial trajectory</p>
<p><span class="math display">\[
\tau_h= (s_0, a_0, r_0, \dots, s_{h-1}, a_{h-1}, r_{h-1}, s_h),
\]</span></p>
<p>it achieves a higher expected remaining reward:</p>
<p><span id="eq-optimal-all"><span class="math display">\[
\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^{Q^\star}}} [
    r_h+ \cdots + r_{H-1}
    \mid \tau_h
]
\ge
\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^\text{any}}} [
    r_h+ \cdots + r_{H-1}
    \mid \tau_h
].
\tag{2.24}\]</span></span></p>
<p>By conditioning on <span class="math inline">\(\tau_h\)</span>, we mean to condition on the event that <span class="math inline">\(s_{h'}, a_{h'}\)</span> are the state and action visited at time <span class="math inline">\(h' &lt; h\)</span>, and <span class="math inline">\(s_h\)</span> is the state at time <span class="math inline">\(h\)</span>. (If the reward function were stochastic, we would condition on the observed rewards as well.)</p>
</div>
<p>We will now prove by induction that <span class="math inline">\(\pi^{Q^\star}\)</span> is optimal. Essentially, we begin at the <em>end</em> of the trajectory, where the optimal action is simply the one that yields highest immediate reward. Once the optimal values at time <span class="math inline">\(H-1\)</span> are known, we use these to compute the optimal values at time <span class="math inline">\(H-2\)</span> using the Bellman optimality equations (<a href="#eq-bellman-opt" class="quarto-xref">eq.&nbsp;<span>2.38</span></a>), and proceed backward through the trajectory.</p>
<div class="cell" data-fig-width="6" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-dp-optimal-finite" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dp-optimal-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-dp-optimal-finite">graph RL
    Q0["$$Q^\star_{H-1} = r$$"] -- "$$\max_{a \in \mathcal{A}}$$" --&gt; V0
    Q0 -- "$$\arg\max_{a \in \mathcal{A}}$$" --&gt; P0["$$\pi^\star_{H-1}$$"]
    V0["$$V^\star_{H-1}$$"] -- "Bellman equations" --&gt; Q1
    
    Q1["$$Q^\star_{H-2}$$"] -- "$$\max_{a \in \mathcal{A}}$$" --&gt; V1
    Q1 -- "$$\arg\max_{a \in \mathcal{A}}$$" --&gt; P1["$$\pi^\star_{H-2}$$"]
    V1["$$V^\star_{H-2}$$"] -- "Bellman equations" --&gt; Q2

    Q2["..."]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dp-optimal-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: Illustrating a dynamic programming algorithm for computing the optimal policy in a finite-horizon MDP.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>At the end of the trajectory (time step <span class="math inline">\(H-1\)</span>), we can’t take any more actions, so the <span class="math inline">\(Q\)</span>-function simply returns the immediate reward:</p>
<p><span id="eq-q-star-end"><span class="math display">\[
Q^\star_{H-1}(s, a) = r(s, a).
\tag{2.25}\]</span></span></p>
<p><span class="math inline">\(\pi^{Q^\star}\)</span> is then optimal across <em>all</em> policies at time <span class="math inline">\(H-1\)</span>, since the policy only determines a single action, the one which maximizes (expected) remaining reward:</p>
<p><span id="eq-end-optimal"><span class="math display">\[
\mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^{Q^\star}}}[ r_{H-1} \mid s_{H- 1} = s ]
= \max_{a \in \mathcal{A}} r(s, a).
\tag{2.26}\]</span></span></p>
<p>For the inductive step, suppose <span class="math inline">\(\pi^{Q^\star}\)</span> is optimal across <em>all</em> policies at time <span class="math inline">\(h+1\)</span>. Note that this implies <span class="math inline">\(V^{\pi^{Q^\star}} = V^\star\)</span>. Then for any other policy <span class="math inline">\(\pi^\text{any}\)</span> (which could be history-dependent), and for any partial trajectory <span class="math inline">\(\tau_{h}\)</span>,</p>
<p><span id="eq-pi-star-proof"><span class="math display">\[
\begin{aligned}
&amp;{} \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^\text{any}}} [
    r_h+ \cdots + r_{H-1} \mid \tau_{h}
] \\
={}&amp;
\mathop{\mathbb{E}}_{\substack{a \sim \pi^\text{any}_h(\tau_h) \\ s' \sim P(s_h, a)}} \left[
    r_h+ \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^\text{any}}} [
        r_{h+1} + \cdots + r_{H-1}
        \mid
        \tau_{h},
        a_h= a,
        s_{h+1} = s'
    ]
\right] \\
\le{}&amp;
\mathop{\mathbb{E}}_{\substack{a \sim \pi^\text{any}_h(\tau_h) \\ s' \sim P(s_h, a)}} \left[
    r_h+ V^\star_{h+1}(s')
\right] \\
\le{}&amp;
\max_{a \in \mathcal{A}}
r(s_h, a)
+
\mathop{\mathbb{E}}_{s' \sim P(s_h, a)} \left[
    V^\star_{h+1}(s')
\right] \\
={}&amp; V^\star_h(s_h).
\end{aligned}
\tag{2.27}\]</span></span></p>
<p>This completes the inductive step, which shows that <span class="math inline">\(\pi^{Q^\star}\)</span> is optimal across <em>all</em> policies at every time <span class="math inline">\(h\in [H]\)</span>.</p>
</div>
<div id="cell-fig-optimal-policy" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co"> fix latexify bugs</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_policy(mdp: MDP):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> np.zeros((mdp.H, mdp.S, mdp.A))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> np.zeros((mdp.H, mdp.S, mdp.A))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((mdp.H<span class="op">+</span><span class="dv">1</span>, mdp.S))  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        Q[h] <span class="op">=</span> mdp.r <span class="op">+</span> mdp.P <span class="op">@</span> V[h <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        policy[h] <span class="op">=</span> jnp.eye(mdp.S)[jnp.argmax(Q[h], axis<span class="op">=</span><span class="dv">1</span>)]  <span class="co"># one-hot</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        V[h] <span class="op">=</span> jnp.<span class="bu">max</span>(Q[h], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> jnp.stack(Q)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> jnp.stack(policy)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> jnp.stack(V[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> policy, V, Q</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>latex(find_optimal_policy, id_to_latex<span class="op">=</span>{<span class="st">"policy"</span>: <span class="vs">r"\pi"</span>}, trim_prefixes<span class="op">=</span>{<span class="st">"mdp"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-optimal-policy" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="12">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{find\_optimal\_policy}(\mathrm{mdp}: \mathrm{MDP}) \\ \hspace{1em} Q \gets \mathbf{0}^{H \times S \times A} \\ \hspace{1em} \pi \gets \mathbf{0}^{H \times S \times A} \\ \hspace{1em} V \gets \mathbf{0}^{H + 1 \times S} \\ \hspace{1em} \mathbf{for} \ h \in \mathrm{range} \mathopen{}\left( H - 1, -1, -1 \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} Q_{h} \gets r + P V_{h + 1} \\ \hspace{2em} \pi_{h} \gets \mathrm{jnp}.\mathrm{eye} \mathopen{}\left( S \mathclose{}\right)_{\mathrm{jnp}.\mathrm{argmax} \mathopen{}\left( Q_{h} \mathclose{}\right)} \\ \hspace{2em} V_{h} \gets \mathrm{jnp}.\mathrm{max} \mathopen{}\left( Q_{h} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} Q \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( Q \mathclose{}\right) \\ \hspace{1em} \pi \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( \pi \mathclose{}\right) \\ \hspace{1em} V \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( V_{:-1} \mathclose{}\right) \\ \hspace{1em} \mathbf{return} \ \mathopen{}\left( \pi, V, Q \mathclose{}\right) \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: Pseudocode for the dynamic programming algorithm.
</figcaption>
</figure>
</div>
</div>
<p>At each of the <span class="math inline">\(H\)</span> timesteps, we must compute <span class="math inline">\(Q^{\star}_h\)</span> for each of the <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> state-action pairs. Each computation takes <span class="math inline">\(|\mathcal{S}|\)</span> operations to evaluate the average value over <span class="math inline">\(s'\)</span>. Computing <span class="math inline">\(\pi^\star_h\)</span> and <span class="math inline">\(V^\star_h\)</span> then requires computing the value-maximizing action in each state, which is an additional <span class="math inline">\(O(|\mathcal{S}| \mathcal{A}|)\)</span> comparisons. This is dominated by the earlier term, resulting in a total computation time of <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>.</p>
<p>Note that this algorithm is identical to the policy evaluation algorithm (<a href="#fig-dp-eval-finite" class="quarto-xref">fig.&nbsp;<span>2.8</span></a>), but instead of <em>averaging</em> over the actions chosen by a policy, we instead take a <em>maximum</em> over the action-values. We’ll see this relationship between <strong>policy evaluation</strong> and <strong>optimal policy computation</strong> show up again in the infinite-horizon setting.</p>
<div id="aab25238" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pi_opt, V_opt, Q_opt <span class="op">=</span> find_optimal_policy(tidy_mdp)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(pi_opt, tidy_policy_messy_only)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(V_opt, V_messy)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(Q_opt[:<span class="op">-</span><span class="dv">1</span>], v_ary_to_q_ary(tidy_mdp, V_messy)[<span class="dv">1</span>:])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">"Assertions passed (the 'tidy when messy' policy is optimal)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>"Assertions passed (the 'tidy when messy' policy is optimal)"</code></pre>
</div>
</div>
<p>Let us review some equivalent definitions of an optimal policy:</p>
<div id="rem-equivalent-optimal" class="proof remark">
<p><span class="proof-title"><em>Remark 2.5</em> (Equivalent definitions of an optimal policy). </span>Let <span class="math inline">\(\pi\)</span> be a stationary policy. Then the following are all equivalent:</p>
<ol type="1">
<li><span class="math inline">\(\pi\)</span> is optimal across history-independent policies (<a href="#def-optimal-policy-finite" class="quarto-xref">def.&nbsp;<span>2.11</span></a>).</li>
<li><span class="math inline">\(\pi\)</span> is optimal across all policies (<a href="#eq-optimal-all" class="quarto-xref">eq.&nbsp;<span>2.24</span></a>).</li>
<li><span class="math inline">\(V^\pi = V^\star\)</span>.</li>
<li><span class="math inline">\(Q^\pi = Q^\star\)</span>.</li>
<li><span class="math inline">\(V^\pi\)</span> satisfies the Bellman optimality equations (<a href="#eq-bellman-opt" class="quarto-xref">eq.&nbsp;<span>2.38</span></a>).</li>
</ol>
<p>1 and 3 are the same by definition. 3 and 4 are the same since <span class="math inline">\(V^\star\)</span> and <span class="math inline">\(Q^\star\)</span> uniquely define each other by <a href="#eq-optimal-v-q" class="quarto-xref">eq.&nbsp;<span>2.17</span></a> and <span class="math inline">\(V^\pi\)</span> and <span class="math inline">\(Q^\pi\)</span> uniquely define each other by <a href="#eq-relating-v-q" class="quarto-xref">eq.&nbsp;<span>2.7</span></a> and <a href="#eq-relating-q-v" class="quarto-xref">eq.&nbsp;<span>2.8</span></a>. 3 and 5 are the same by the Bellman optimality equations (<a href="#thm-bellman-opt" class="quarto-xref">Theorem&nbsp;<span>2.5</span></a>). 1 and 2 are the same as shown in the proof of <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>2.6</span></a>.</p>
</div>
<div id="rem-why-stochastic" class="proof remark">
<p><span class="proof-title"><em>Remark 2.6</em> (Why stochastic policies). </span>Given that there exists an optimal deterministic policy, why do we try ever try to learn stochastic policies? We will see a partial answer to this when we discuss the <em>exploration-exploitation</em> tradeoff in <a href="bandits.html" class="quarto-xref"><span>Chapter 4</span></a>. So far, we’ve assumed that the environment is totally known. If it isn’t, however, we need some way to explore different possible actions, and stochastic policies provide a natural way to do this.</p>
</div>
</section>
</section>
<section id="sec-infinite-horizon-mdps" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-infinite-horizon-mdps"><span class="header-section-number">2.4</span> Infinite-horizon Markov decision processes</h2>
<p>What if we allow trajectories to continue for an infinite amount of time, that is, set the time horizon <span class="math inline">\(H\)</span> to infinity? This might seem impractical, since in the real world, most of the trajectories we care about terminate after some fixed number of steps. However, we will see that <strong>infinite-horizon</strong> MDPs are simpler in certain regards and can serve as good approximations to finite-horizon tasks. A crucial result is that Bellman operators (<a href="#def-bellman-operator" class="quarto-xref">def.&nbsp;<span>2.10</span></a>) in this setting are <strong>contraction mappings</strong> (<a href="#thm-bellman-contraction" class="quarto-xref">Theorem&nbsp;<span>2.8</span></a>), which yield simple fixed-point iteration algorithms for policy evaluation (<a href="#sec-iterative-pe" class="quarto-xref"><span>Section 2.4.3.2</span></a>) and computing the optimal value function (<a href="#sec-vi" class="quarto-xref"><span>Section 2.4.4.1</span></a>). Finally, we’ll present the policy iteration algorithm (<a href="#sec-pi" class="quarto-xref"><span>Section 2.4.4.2</span></a>). In addition to being an effective tool in its own right, we will find that policy iteration serves as a useful <em>framework</em> for designing and analyzing later algorithms.</p>
<section id="differences-from-the-finite-horizon-setting" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="differences-from-the-finite-horizon-setting"><span class="header-section-number">2.4.1</span> Differences from the finite horizon setting</h3>
<div id="rem-discount" class="proof remark">
<p><span class="proof-title"><em>Remark 2.7</em> (Discounted rewards). </span>First of all, note that the total reward <span class="math inline">\(r_0 + r_{1} + \cdots\)</span> might blow up to infinity. To ensure that we work with bounded quantities, we insert a <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1)\)</span> such that rewards become less valuable the further into the future they are:</p>
<p><span id="eq-discounted-rewards"><span class="math display">\[
r_0 + \gamma r_{1} + \gamma^2 r_{2} + \cdots
=
\sum_{h=0}^\infty \gamma^hr_{h}
\le R \frac{1}{1 - \gamma},
\tag{2.28}\]</span></span></p>
<p>where <span class="math inline">\(R\)</span> is the maximum possible reward. We can think of <span class="math inline">\(\gamma\)</span> as measuring how much we care about the future: if it’s close to <span class="math inline">\(0\)</span>, we only care about the near-term rewards; if it’s close to <span class="math inline">\(1\)</span>, we put more weight into future rewards.</p>
<p>You can also analyze <span class="math inline">\(\gamma\)</span> as the <em>probability of continuing</em> the trajectory at each time step. (This is equivalent to <span class="math inline">\(H\)</span> being distributed by a First Success distribution with success probability <span class="math inline">\(\gamma\)</span>.) This accords with the above interpretation: if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(0\)</span>, the trajectory will likely be very short, while if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(1\)</span>, the trajectory will likely continue for a long time.</p>
</div>
<p>The other components of the MDP remain from the finite-horizon setting (<a href="#def-finite-horizon-mdp" class="quarto-xref">def.&nbsp;<span>2.4</span></a>):</p>
<p><span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P_0, P, r, \gamma).
\]</span></p>
<div id="0cb7ace2" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code-wise, we can reuse the `MDP` class from before @def-finite-horizon-mdp and set `mdp.H = float('inf')`.</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>tidy_mdp_inf <span class="op">=</span> tidy_mdp._replace(H<span class="op">=</span><span class="bu">float</span>(<span class="st">"inf"</span>), gamma<span class="op">=</span><span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-stationary-policies" class="proof remark">
<p><span class="proof-title"><em>Remark 2.8</em> (Stationary policies). </span>Time-dependent policies become difficult to handle in the infinite-horizon case. Not only would they be computationally impossible to store, but also, many of the DP approaches we saw required us to start at the end of the trajectory, which is no longer possible. We’ll shift to <strong>stationary</strong> policies <span class="math inline">\(\pi : \mathcal{S} \to \mathcal{A}\)</span> (deterministic) or <span class="math inline">\(\triangle(\mathcal{A})\)</span> (stochastic), which don’t explicitly depend on the timestep.</p>
</div>
<div id="exr-tidy-stationary" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.6 (Stationary policy examples)</strong></span> Which of the policies in <a href="#exm-tidy-policy" class="quarto-xref">ex.&nbsp;<span>2.4</span></a> (policies in the tidying MDP) are stationary?</p>
</div>
<div id="def-value-infinite" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.14 (Value functions)</strong></span> We’ll also consider stationary value functions <span class="math inline">\(V^\pi : \mathcal{S} \to \mathbb{R}\)</span> and <span class="math inline">\(Q^\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>. These now represent the expected total <em>discounted</em> reward:</p>
<p><span id="eq-consistency-infinite"><span class="math display">\[
\begin{aligned}
    V^\pi(s) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^\pi} [r_0 + \gamma r_{1} + \gamma^2 r_{2} + \cdots \mid s_0 = s] \\
    Q^\pi(s, a) &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^\pi} [r_0 + \gamma r_{1} + \gamma^2 r_{2} + \cdots \mid s_0 = s, a_0 = a] \\
\end{aligned}
\tag{2.29}\]</span></span></p>
</div>
<div id="exr-infinite-no-time-step" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.7 (Time-independent)</strong></span> Note that we add up the rewards starting from time <span class="math inline">\(0\)</span>, whereas in the finite-horizon case, we needed to explicitly add the rewards from time <span class="math inline">\(h\)</span> onwards. Heuristically speaking, why does it no longer matter which time step we start on when defining the value function? Refer back to the Markov property (<a href="#def-markov-property" class="quarto-xref">def.&nbsp;<span>2.5</span></a>).</p>
</div>
<div id="thm-infinite-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.7 (Bellman consistency equations)</strong></span> The Bellman consistency equations play the same role they did in the finite-horizon setting (<a href="#thm-consistency" class="quarto-xref">Theorem&nbsp;<span>2.3</span></a>). We need to insert a factor of <span class="math inline">\(\gamma\)</span> to account for the discounting, and the <span class="math inline">\(h\)</span> subscript is gone since <span class="math inline">\(V^\pi\)</span> is stationary:</p>
<p><span id="eq-infinite-consistency"><span class="math display">\[
V^\pi(s) = \mathop{\mathbb{E}}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma V^\pi(s')].
\tag{2.30}\]</span></span></p>
<p>The value function and action-value function are still related in the same way:</p>
<p><span id="eq-infinite-v-and-q"><span class="math display">\[
\begin{aligned}
V^\pi(s) &amp;= \mathop{\mathbb{E}}_{a \sim \pi(s)} [Q(s, a)] \\
Q^\pi(s, a) &amp;= r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} [V^\pi(s')].
\end{aligned}
\tag{2.31}\]</span></span></p>
</div>
</section>
<section id="contraction-mappings" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="contraction-mappings"><span class="header-section-number">2.4.2</span> Contraction mappings</h3>
<p>Recall that a policy’s <em>Bellman operator</em> takes a guess for the policy’s value function and returns an <em>improved</em> guess using one step of the policy (<a href="#def-bellman-operator" class="quarto-xref">def.&nbsp;<span>2.10</span></a>). In the infinite-horizon setting, this has the same form:</p>
<div id="def-infinite-bellman" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.15 (Bellman operator)</strong></span> Let <span class="math inline">\(\pi : \mathcal{S} \to \triangle(\mathcal{A})\)</span> be a stationary policy. Its <strong>Bellman operator</strong> is defined</p>
<p><span id="eq-infinite-bellman-operator"><span class="math display">\[
\mathcal{J}^{\pi}(v) := \Big(
    s \mapsto \mathop{\mathbb{E}}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma v(s')]
\Big).
\tag{2.32}\]</span></span></p>
</div>
<p>The crucial property of the Bellman operator is that it is a <strong>contraction mapping</strong> for any policy. Intuitively, if we start with two guesses <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>, if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate.</p>
<div id="def-contraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.16 (Contraction mapping)</strong></span> Let <span class="math inline">\(X\)</span> be a set with a norm <span class="math inline">\(\|\cdot\|\)</span>. We call an operator <span class="math inline">\(f: X \to X\)</span> a <strong>contraction mapping</strong> if for any <span class="math inline">\(x, y \in X\)</span>,</p>
<p><span class="math display">\[
\|f(x) - f(y)\| \le \gamma \|x - y\|
\]</span></p>
<p>for some fixed <span class="math inline">\(\gamma \in (0, 1)\)</span>.</p>
</div>
<div id="exr-contraction-shrinks" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.8 (Contraction mappings pull points together)</strong></span> Show that for a contraction mapping <span class="math inline">\(f\)</span> with coefficient <span class="math inline">\(\gamma\)</span>, for all <span class="math inline">\(t \in \mathbb{N}\)</span>,</p>
<p><span id="eq-contraction-shrinks"><span class="math display">\[
\|f^{(t)}(x) - f^{(t)}(y)\| \le \gamma^t \|x - y\|,
\tag{2.33}\]</span></span></p>
<p>i.e.&nbsp;any two points will be pushed closer by at least a factor of <span class="math inline">\(\gamma\)</span> at each iteration.</p>
</div>
<p>It is a powerful fact, known as the <strong>Banach fixed-point theorem</strong>, that every contraction mapping has a unique <strong>fixed point</strong> <span class="math inline">\(x^\star\)</span> such that <span class="math inline">\(f(x^\star) = x^\star\)</span>. This means that if we repeatedly apply <span class="math inline">\(f\)</span> to any starting point, we will eventually converge to <span class="math inline">\(x^\star\)</span>:</p>
<p><span id="eq-contraction-convergence"><span class="math display">\[
\|f^{(t)}(x) - x^\star\| \le \gamma^t \|x - x^\star\|.
\tag{2.34}\]</span></span></p>
<p>Let’s return to the RL setting and apply this result to the Bellman operator. How can we measure the distance between two functions <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>? We’ll take the <strong>supremum norm</strong> as our distance metric:</p>
<p><span class="math display">\[
\| v - u \|_{\infty} := \sup_{s \in \mathcal{S}} |v(s) - u(s)|,
\]</span></p>
<p>i.e.&nbsp;we compare the functions on the state that causes the biggest gap between them. The Bellman consistency equations (<a href="#thm-infinite-consistency" class="quarto-xref">Theorem&nbsp;<span>2.7</span></a>) state that <span class="math inline">\(V^\pi\)</span> is the fixed point of <span class="math inline">\(\mathcal{J}^\pi\)</span>. Then <a href="#eq-contraction-convergence" class="quarto-xref">eq.&nbsp;<span>2.34</span></a> implies that if we repeatedly apply <span class="math inline">\(\mathcal{J}^\pi\)</span> to any starting guess, we will eventually converge to <span class="math inline">\(V^\pi\)</span>:</p>
<p><span id="eq-bellman-convergence"><span class="math display">\[
\|(\mathcal{J}^\pi)^{(t)}(v) - V^\pi \|_{\infty} \le \gamma^{t} \| v - V^\pi\|_{\infty}.
\tag{2.35}\]</span></span></p>
<p>We’ll use this useful fact to prove the convergence of several algorithms later on.</p>
<div id="thm-bellman-contraction" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.8 (The Bellman operator is a contraction mapping)</strong></span> Let <span class="math inline">\(\pi\)</span> be a stationary policy. There exists <span class="math inline">\(\gamma \in (0, 1)\)</span> such that for any two functions <span class="math inline">\(u, v : \mathcal{S} \to \mathbb{R}\)</span>,</p>
<p><span class="math display">\[
\|\mathcal{J}^{\pi} (v) - \mathcal{J}^{\pi} (u) \|_{\infty} \le \gamma \|v - u \|_{\infty}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For all states <span class="math inline">\(s \in \mathcal{S}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
|[\mathcal{J}^{\pi} (v)](s) - [\mathcal{J}^{\pi} (u)](s)|&amp;= \Big| \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right] \\
&amp;\qquad - \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} u(s') \right] \Big| \\
&amp;= \gamma \left|\mathop{\mathbb{E}}_{s' \sim P(s, a)} [v(s') - u(s')] \right| \\
&amp;\le \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)}|v(s') - u(s')| \qquad \text{(Jensen's inequality)} \\
&amp;\le \gamma \max_{s'} |v(s') - u(s')| \\
&amp;= \gamma \|v - u \|_{\infty}.
\end{aligned}
\]</span></p>
</div>
</section>
<section id="sec-infinite-policy-evaluation" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="sec-infinite-policy-evaluation"><span class="header-section-number">2.4.3</span> Policy evaluation</h3>
<p>The backwards DP technique we used in the finite-horizon case (<a href="#sec-eval-dp-finite-horizon" class="quarto-xref"><span>Section 2.3.1</span></a>) no longer works since there is no “final timestep” to start from. We’ll need another approach to policy evaluation.</p>
<p>The Bellman consistency equations (<a href="#thm-infinite-consistency" class="quarto-xref">Theorem&nbsp;<span>2.7</span></a>) yield a system of <span class="math inline">\(|\mathcal{S}|\)</span> equations we can solve to evaluate a deterministic policy <em>exactly</em>. For a faster approximate solution, we can iterate the policy’s Bellman operator, since we know that it has a unique fixed point at the true value function.</p>
<section id="matrix-inversion-for-deterministic-policies" class="level4" data-number="2.4.3.1">
<h4 data-number="2.4.3.1" class="anchored" data-anchor-id="matrix-inversion-for-deterministic-policies"><span class="header-section-number">2.4.3.1</span> Matrix inversion for deterministic policies</h4>
<p>Note that when the policy <span class="math inline">\(\pi\)</span> is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:</p>
<p><span class="math display">\[
\begin{aligned}
    r^{\pi} &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; P^{\pi} &amp;\in [0, 1]^{|\mathcal{S}| \times |\mathcal{S}|} &amp; P_0 &amp;\in [0, 1]^{|\mathcal{S}|} \\
    \pi &amp;\in \mathcal{A}^{|\mathcal{S}|} &amp; V^\pi &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; Q^\pi &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}.
\end{aligned}
\]</span></p>
<p>For <span class="math inline">\(P^\pi\)</span>, we’ll treat the rows as the states and the columns as the next states. Then <span class="math inline">\(P^\pi_{s, s'}\)</span> is the probability of transitioning from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span> under policy <span class="math inline">\(\pi\)</span>.</p>
<div id="exm-tidy-tabular-evaluation" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Tidying MDP)</strong></span> The tabular MDP from <a href="#exm-tidy-mdp" class="quarto-xref">ex.&nbsp;<span>2.3</span></a> has <span class="math inline">\(|\mathcal{S}| = 2\)</span> and <span class="math inline">\(|\mathcal{A}| = 2\)</span>. Let’s write down the quantities for the policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy:</p>
<p><span class="math display">\[
r^{\pi} = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
\quad
P^{\pi} = \begin{bmatrix} 0.7 &amp; 0.3 \\ 1 &amp; 0 \end{bmatrix},
\quad
P_0 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\]</span></p>
<p>We’ll see how to evaluate this policy in the next section.</p>
</div>
<p>The Bellman consistency equation for a deterministic policy can be written in tabular notation as</p>
<p><span class="math display">\[
V^\pi = r^\pi + \gamma P^\pi V^\pi.
\]</span></p>
<p>(Unfortunately, this notation doesn’t simplify the expression for <span class="math inline">\(Q^\pi\)</span>.) This system of equations can be solved with a matrix inversion:</p>
<p><span id="eq-matrix-inversion-pe"><span class="math display">\[
V^\pi = (I - \gamma P^\pi)^{-1} r^\pi.
\tag{2.36}\]</span></span></p>
<div id="exr-invertible-dynamics" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.9 (Matrix invertibility)</strong></span> Note we’ve assumed that <span class="math inline">\(I - \gamma P^\pi\)</span> is invertible. Can you see why this is the case? (Recall that a linear operator, i.e.&nbsp;a square matrix, is invertible if and only if its null space is trivial; that is, it doesn’t map any nonzero vector to zero. Show that <span class="math inline">\(I - \gamma P^\pi\)</span> maps any nonzero vector to another nonzero vector.)</p>
</div>
<div id="11d1dc8d" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_deterministic_infinite(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> jnp.argmax(policy, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># un-one-hot</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    P_pi <span class="op">=</span> mdp.P[jnp.arange(mdp.S), pi]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    r_pi <span class="op">=</span> mdp.r[jnp.arange(mdp.S), pi]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.linalg.solve(jnp.eye(mdp.S) <span class="op">-</span> mdp.gamma <span class="op">*</span> P_pi, r_pi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exm-tidy-eval-infinite" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Tidying policy evaluation)</strong></span> Let’s use the same policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy. Setting <span class="math inline">\(\gamma = 0.95\)</span>, we must invert</p>
<p><span class="math display">\[
I - \gamma P^{\pi} = \begin{bmatrix} 1 - 0.95 \times 0.7 &amp; - 0.95 \times 0.3 \\ - 0.95 \times 1 &amp; 1 - 0.95 \times 0 \end{bmatrix} = \begin{bmatrix} 0.335 &amp; -0.285 \\ -0.95 &amp; 1 \end{bmatrix}.
\]</span></p>
<p>The inverse to two decimal points is</p>
<p><span class="math display">\[
(I - \gamma P^{\pi})^{-1} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix}.
\]</span></p>
<p>Thus the value function is</p>
<p><span class="math display">\[
V^{\pi} = (I - \gamma P^{\pi})^{-1} r^{\pi} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 15.56 \\ 14.79 \end{bmatrix}.
\]</span></p>
<p>Let’s sanity-check this result. Since rewards are at most <span class="math inline">\(1\)</span>, the maximum cumulative return of a trajectory is at most <span class="math inline">\(1/(1-\gamma) = 20\)</span>. We see that the value function is indeed slightly lower than this.</p>
</div>
<div id="a005d351" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>eval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>Array([15.56419, 14.78598], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="sec-iterative-pe" class="level4" data-number="2.4.3.2">
<h4 data-number="2.4.3.2" class="anchored" data-anchor-id="sec-iterative-pe"><span class="header-section-number">2.4.3.2</span> Iterative policy evaluation</h4>
<p>The matrix inversion above takes roughly <span class="math inline">\(O(|\mathcal{S}|^3)\)</span> time. It also only works for deterministic policies. Can we trade off the requirement of finding the <em>exact</em> value function for a faster <em>approximate</em> algorithm that will also extend to stochastic policies?</p>
<p>Let’s use the Bellman operator to define an iterative algorithm for computing the value function. We’ll start with an initial guess <span class="math inline">\(v^{(0)}\)</span> with elements in <span class="math inline">\([0, 1/(1-\gamma)]\)</span> and then iterate the Bellman operator:</p>
<p><span class="math display">\[
v^{(t+1)} = \mathcal{J}^{\pi}(v^{(t)}),
\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(v^{(t)} = (\mathcal{J}^{\pi})^{(t)} (v^{(0)})\)</span>. Note that each iteration takes <span class="math inline">\(O(|\mathcal{S}|^2)\)</span> time for the matrix-vector multiplication.</p>
<div id="07e2959f" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> supremum_norm(v):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(jnp.<span class="bu">abs</span>(v))  <span class="co"># same as jnp.linalg.norm(v, jnp.inf)</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loop_until_convergence(op, v, ε<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Repeatedly apply op to v until convergence (in supremum norm)."""</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        v_new <span class="op">=</span> op(v)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> supremum_norm(v_new <span class="op">-</span> v) <span class="op">&lt;</span> ε:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> v_new</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v_new</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> iterative_evaluation(mdp: MDP, pi: Float[Array, <span class="st">"S A"</span>], ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_operator, mdp, pi)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Then, as we showed in <a href="#eq-bellman-convergence" class="quarto-xref">eq.&nbsp;<span>2.35</span></a>, by the Banach fixed-point theorem:</p>
<p><span class="math display">\[
\|v^{(t)} - V^\pi \|_{\infty} \le \gamma^{t} \| v^{(0)} - V^\pi\|_{\infty}.
\]</span></p>
<div id="ab3bae8e" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>iterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<div id="rem-iterations-vi" class="proof remark">
<p><span class="proof-title"><em>Remark 2.9</em> (Convergence of iterative policy evaluation). </span>How many iterations do we need for an <span class="math inline">\(\epsilon\)</span>-accurate estimate? We can work backwards to solve for <span class="math inline">\(t\)</span> (note that <span class="math inline">\(\log \gamma &lt; 0\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
    \gamma^t \|v^{(0)} - V^\pi\|_{\infty} &amp;\le \epsilon \\
    t &amp;\ge \frac{\log (\epsilon / \|v^{(0)} - V^\pi\|_{\infty})}{\log \gamma} \\
    &amp;= \frac{\log (\|v^{(0)} - V^\pi\|_{\infty} / \epsilon)}{\log (1 / \gamma)},
\end{aligned}
\]</span></p>
<p>and so the number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
<p>Note that we’ve applied the inequalities <span class="math inline">\(\|v^{(0)} - V^\pi\|_{\infty} \le 1/(1-\gamma)\)</span> and <span class="math inline">\(\log (1/x) \ge 1-x\)</span>.</p>
</div>
</section>
</section>
<section id="sec-infinite-opt" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="sec-infinite-opt"><span class="header-section-number">2.4.4</span> Optimality</h3>
<p>Now let’s move on to solving for an optimal policy in the infinite-horizon case. As in <a href="#def-optimal-policy-finite" class="quarto-xref">def.&nbsp;<span>2.11</span></a>, an <strong>optimal policy</strong> <span class="math inline">\(\pi^\star\)</span> is one that does at least as well as any other policy in all situations. That is, for all policies <span class="math inline">\(\pi\)</span>, times <span class="math inline">\(h\in \mathbb{N}\)</span>, and initial trajectories <span class="math inline">\(\tau_{\le h} = (s_0, a_0, r_0, \dots, s_h)\)</span>,</p>
<p><span id="eq-optimal-policy-infinite"><span class="math display">\[
\begin{aligned}
    &amp;
    \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi^{\star}}}[
        r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} + \cdots \mid \tau_{\le h}] \\
    \ge{}
    &amp;
    \mathop{\mathbb{E}}_{\tau \sim \rho^{\pi}}[
        r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} + \cdots \mid \tau_{\le h}]
\end{aligned}
\tag{2.37}\]</span></span></p>
<p>Once again, all optimal policies share the same <strong>optimal value function</strong> <span class="math inline">\(V^\star\)</span>, and the greedy policy with respect to this value function is optimal.</p>
<div id="exr-optimal-greedy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.10 (The greedy optimal policy)</strong></span> Verify this by modifying the proof <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>2.6</span></a> from the finite-horizon case.</p>
</div>
<p>So how can we compute such an optimal policy? We can’t use the backwards DP approach from the finite-horizon case <a href="#fig-optimal-policy" class="quarto-xref">fig.&nbsp;<span>2.10</span></a> since there’s no “final timestep” to start from. Instead, we’ll exploit the fact that the Bellman consistency equation <a href="#eq-consistency-infinite" class="quarto-xref">eq.&nbsp;<span>2.29</span></a> for the optimal value function doesn’t depend on any policy:</p>
<p><span id="eq-bellman-opt"><span class="math display">\[
V^\star(s) = \max_a \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} V^\star(s'). \right]
\tag{2.38}\]</span></span></p>
<p>As before, thinking of the r.h.s. of <a href="#eq-bellman-opt" class="quarto-xref">eq.&nbsp;<span>2.38</span></a> as an operator on value functions gives the <strong>Bellman optimality operator</strong></p>
<p><span id="eq-bellman-opt-operator"><span class="math display">\[
[\mathcal{J}^{\star}(v)](s) = \max_a \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right]
\tag{2.39}\]</span></span></p>
<div id="707fb5af" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_optimality_operator(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_optimal(v: Float[Array, <span class="st">" S"</span>], mdp: MDP):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.allclose(v, bellman_optimality_operator(v, mdp))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="sec-vi" class="level4" data-number="2.4.4.1">
<h4 data-number="2.4.4.1" class="anchored" data-anchor-id="sec-vi"><span class="header-section-number">2.4.4.1</span> Value iteration</h4>
<p>Since the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as <strong>value iteration</strong>.</p>
<div id="892612c6" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> value_iteration(mdp: MDP, ε: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iterate the Bellman optimality operator until convergence."""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_optimality_operator, mdp)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="492a48d3" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>value_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<p>Note that the runtime analysis for an <span class="math inline">\(\epsilon\)</span>-optimal value function is exactly the same as <a href="#sec-iterative-pe" class="quarto-xref"><span>Section 2.4.3.2</span></a>! This is because value iteration is simply the special case of applying iterative policy evaluation to the <em>optimal</em> value function.</p>
<p>As the final step of the algorithm, to return an actual policy <span class="math inline">\(\hat \pi\)</span>, we can simply act greedily with respect to the final iteration <span class="math inline">\(v^{(T)}\)</span> of our above algorithm:</p>
<p><span id="eq-value-iteration-extract-policy"><span class="math display">\[
\hat \pi(s) = \arg\max_a \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v^{(T)}(s') \right].
\tag{2.40}\]</span></span></p>
<p>We must be careful, though: the value function of this greedy policy, <span class="math inline">\(V^{\hat \pi}\)</span>, is <em>not</em> the same as <span class="math inline">\(v^{(T)}\)</span>, which need not even be a well-defined value function for some policy!</p>
<p>The bound on the policy’s quality is actually quite loose: if <span class="math inline">\(\|v^{(T)} - V^\star\|_{\infty} \le \epsilon\)</span>, then the greedy policy <span class="math inline">\(\hat \pi\)</span> satisfies <span class="math inline">\(\|V^{\hat \pi} - V^\star\|_{\infty} \le \frac{2\gamma}{1-\gamma} \epsilon\)</span>, which might potentially be very large.</p>
<div id="thm-greedy-worsen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.9 (Greedy policy value worsening)</strong></span> <span class="math display">\[
\|V^{\hat \pi} - V^\star \|_{\infty} \le \frac{2 \gamma}{1-\gamma} \|v - V^\star\|_{\infty}
\]</span></p>
<p>where <span class="math inline">\(\hat \pi(s) = \arg\max_a q(s, a)\)</span> is the greedy policy with respect to</p>
<p><span class="math display">\[
q(s, a) = r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s').
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first have</p>
<p><span class="math display">\[
\begin{aligned}
        V^{\star}(s) - V^{\hat \pi}(s) &amp;= Q^{\star}(s,\pi^\star(s)) - Q^{\hat \pi}(s, \hat \pi(s))\\
        &amp;= [Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s))] + [Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))].
\end{aligned}
\]</span></p>
<p>Let us bound these two quantities separately.</p>
<p>For the first quantity, note that by the definition of <span class="math inline">\(\hat \pi\)</span>, we have</p>
<p><span class="math display">\[
q(s, \hat \pi(s)) \ge q(s,\pi^\star(s)).
\]</span></p>
<p>Let’s add <span class="math inline">\(q(s, \hat \pi(s)) - q(s,\pi^\star(s)) \ge 0\)</span> to the first term to get</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s)) &amp;\le [Q^{\star}(s,\pi^\star(s))- q(s,\pi^\star(s))] + [q(s, \hat \pi(s)) - Q^{\star}(s, \hat \pi(s))] \\
        &amp;= \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \pi^{\star}(s))} [ V^{\star}(s') - v(s') ] + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \hat \pi(s))} [ v(s') - V^{\star}(s') ] \\
        &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty}.
\end{aligned}
\]</span></p>
<p>The second quantity is bounded by</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))
        &amp;=
        \gamma \mathop{\mathbb{E}}_{s'\sim P(s, \hat \pi(s))}\left[ V^\star(s') - V^{\hat \pi}(s') \right] \\
        &amp; \leq
        \gamma \|V^{\star} - V^{\hat \pi}\|_\infty
\end{aligned}
\]</span></p>
<p>and thus</p>
<p><span class="math display">\[
\begin{aligned}
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty} + \gamma \|V^{\star} - V^{\hat \pi}\|_\infty \\
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le \frac{2 \gamma \|v - V^{\star}\|_{\infty}}{1-\gamma}.
\end{aligned}
\]</span></p>
</div>
<p>So in order to compensate and achieve <span class="math inline">\(\|V^{\hat \pi} - V^{\star}\| \le \epsilon\)</span>, we must have</p>
<p><span class="math display">\[
\|v^{(T)} - V^\star\|_{\infty} \le \frac{1-\gamma}{2 \gamma} \epsilon.
\]</span></p>
<p>This means, using <a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>2.9</span></a>, we need to run value iteration for</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{\gamma}{\epsilon (1-\gamma)^2}\right) \right)
\]</span></p>
<p>iterations to achieve an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function.</p>
</section>
<section id="sec-pi" class="level4" data-number="2.4.4.2">
<h4 data-number="2.4.4.2" class="anchored" data-anchor-id="sec-pi"><span class="header-section-number">2.4.4.2</span> Policy iteration</h4>
<p>Can we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function <em>together</em>? This is the idea behind <strong>policy iteration</strong>. In each step, we simply set the policy to act greedily with respect to its own value function.</p>
<div id="d9f6804e" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_iteration(mdp: MDP, ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iteratively improve the policy and value function."""</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> op(pi):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    pi_init <span class="op">=</span> jnp.ones((mdp.S, mdp.A)) <span class="op">/</span> mdp.A  <span class="co"># uniform random policy</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, pi_init, ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="ed7f7e33" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>policy_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>Array([[1., 0.],
       [0., 1.]], dtype=float32)</code></pre>
</div>
</div>
<p>Although PI appears more complex than VI, we’ll use <a href="#thm-bellman-contraction" class="quarto-xref">Theorem&nbsp;<span>2.8</span></a> to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an <span class="math inline">\(\epsilon\)</span>-optimal value function (<a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>2.9</span></a>), although in practice, PI often converges in fewer iterations. Why so? Intuitively, by iterating through actual policies, we can “skip over” different functions that represent the same policy, which value iteration might iterate through. For a concrete example, suppose we have an MDP with three states <span class="math inline">\(\mathcal{S} = \{\text A, \text B, \text C\}\)</span>. Compare the two functions below:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>display(Markdown(tabulate(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>: [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>],</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 0$"</span>: [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>],</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 1$"</span>: [<span class="fl">0.2</span>, <span class="fl">0.9</span>, <span class="fl">0.5</span>],</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"keys"</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>)))</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>display(Markdown(tabulate(</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>: [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>],</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 0$"</span>: [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>],</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 1$"</span>: [<span class="fl">0.3</span>, <span class="fl">1.0</span>, <span class="fl">0.6</span>],</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"keys"</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-multiple-q-same-policy" class="quarto-layout-panel anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-multiple-q-same-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.2: Two action-value functions that result in the same greedy policy
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display cell-output-markdown quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-multiple-q-same-policy" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-multiple-q-same-policy-1" class="do-not-create-environment quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-multiple-q-same-policy-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) One Q-function
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="tbl-multiple-q-same-policy-1" class="do-not-create-environment caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.5</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display cell-output-markdown quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-multiple-q-same-policy" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-multiple-q-same-policy-2" class="do-not-create-environment quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-multiple-q-same-policy-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Another Q-function
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="tbl-multiple-q-same-policy-2" class="do-not-create-environment caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.6</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
</div>
</figure>
</div>
<p>These both map to the same greedy policy, so policy iteration would never iterate through both of these functions, while value iteration might.</p>
<div id="thm-pi-iter-analysis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.10 (Policy Iteration runtime and convergence)</strong></span> The number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>This bound follows from the contraction property <a href="#eq-bellman-convergence" class="quarto-xref">eq.&nbsp;<span>2.35</span></a>:</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
<p>We’ll prove that the iterates of PI respect the contraction property by showing that the policies improve monotonically:</p>
<p><span class="math display">\[
V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s).
\]</span></p>
<p>Then we’ll use this to show <span class="math inline">\(V^{\pi^{t+1}}(s) \ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)\)</span>. Note that</p>
<p><span class="math display">\[
\begin{aligned}
(s) &amp;= \max_a \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} V^{\pi^{t}}(s') \right] \\
    &amp;= r(s, \pi^{t+1}(s)) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \pi^{t+1}(s))} V^{\pi^{t}}(s')
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\([\mathcal{J}^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s)\)</span>, we then have</p>
<p><span id="eq-pi-iter-proof"><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) \\
    &amp;= \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right].
\end{aligned}
\tag{2.41}\]</span></span></p>
<p>But note that the expression being averaged is the same as the expression on the l.h.s. with <span class="math inline">\(s\)</span> replaced by <span class="math inline">\(s'\)</span>. So we can apply the same inequality recursively to get</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge  \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge \gamma^2 \mathop{\mathbb{E}}_{\substack{s' \sim P(s, \pi^{t+1}(s)) \\ s'' \sim P(s', \pi^{t+1}(s'))}} \left[V^{\pi^{t+1}}(s'') -  V^{\pi^{t}}(s'') \right]\\
    &amp;\ge \cdots
\end{aligned}
\]</span></p>
<p>which implies that <span class="math inline">\(V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)\)</span> for all <span class="math inline">\(s\)</span> (since the r.h.s. converges to zero). We can then plug this back into <a href="#eq-pi-iter-proof" class="quarto-xref">eq.&nbsp;<span>2.41</span></a> to get the desired result:</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) &amp;= \gamma \mathop{\mathbb{E}}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge 0 \\
    V^{\pi^{t+1}}(s) &amp;\ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)
\end{aligned}
\]</span></p>
<p>This means we can now apply the Bellman convergence result <a href="#eq-bellman-convergence" class="quarto-xref">eq.&nbsp;<span>2.35</span></a> to get</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \|\mathcal{J}^{\star} (V^{\pi^{t}}) - V^{\star}\|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
</div>
</section>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">2.5</span> Key takeaways</h2>
<p>Markov decision processes (MDPs) are a framework for sequential decision making under uncertainty. They consist of a state space <span class="math inline">\(\mathcal{S}\)</span>, an action space <span class="math inline">\(\mathcal{A}\)</span>, an initial state distribution <span class="math inline">\(P_0 \in \triangle(\mathcal{S})\)</span>, a transition function <span class="math inline">\(P(s' \mid s, a)\)</span>, and a reward function <span class="math inline">\(r(s, a)\)</span>. They can be finite-horizon (ends after <span class="math inline">\(H\)</span> timesteps) or infinite-horizon (where rewards scale by <span class="math inline">\(\gamma \in (0, 1)\)</span> at each timestep). Our goal is to find a policy <span class="math inline">\(\pi\)</span> that maximizes expected total reward. Policies can be <strong>deterministic</strong> or <strong>stochastic</strong>, <strong>history-dependent</strong> or <strong>history-independent</strong>, <strong>stationary</strong> or <strong>time-dependent</strong>. A policy induces a distribution over <strong>trajectories</strong>.</p>
<p>We can evaluate a policy by computing its <strong>value function</strong> <span class="math inline">\(V^\pi(s)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span>. We can also compute the <strong>state-action value function</strong> <span class="math inline">\(Q^\pi(s, a)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(\pi\)</span>. In the finite-horizon setting, these also depend on the timestep <span class="math inline">\(h\)</span>.</p>
<p>The <strong>Bellman consistency equation</strong> is an equation that the value function must satisfy. It can be used to solve for the value functions exactly. Thinking of the r.h.s. of this equation as an operator on value functions gives the <strong>Bellman operator</strong>.</p>
<p>In the finite-horizon setting, we can compute the optimal policy using <strong>dynamic programming</strong>. In the infinite-horizon setting, we can compute the optimal policy using <strong>value iteration</strong> or <strong>policy iteration</strong>.</p>
</section>
<section id="sec-mdps-bib" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-mdps-bib"><span class="header-section-number">2.6</span> Bibliographic notes and further reading</h2>
<p>The MDP framework can be traced to <span class="citation" data-cites="puterman_markov_1994">Puterman (<a href="references.html#ref-puterman_markov_1994" role="doc-biblioref">1994</a>)</span>. The proof of <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>2.6</span></a> can be found in <span class="citation" data-cites="agarwal_reinforcement_2022">Agarwal et al. (<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>MDPs are the most common framework used throughout RL. See <a href="introduction.html#sec-intro-bib" class="quarto-xref"><span>Section 1.4</span></a> for a list of popular textbooks on RL.</p>
<p>In most real-world problems, we don’t observe the entire state of the environment. Instead, we only get access to what our senses can perceive. This is what makes games like hide-and-seek enjoyable. We can model such environment as <strong>partially observed MDPs</strong> (POMDPs). <span class="citation" data-cites="powell_reinforcement_2022">Powell (<a href="references.html#ref-powell_reinforcement_2022" role="doc-biblioref">2022</a>)</span> presents a unified framework for sequential decision problems.</p>
<p>Another important bandit algorithm is <strong>Gittins indices</strong> <span class="citation" data-cites="gittins_multi-armed_2011">(<a href="references.html#ref-gittins_multi-armed_2011" role="doc-biblioref">Gittins, 2011</a>)</span>. The derivation of this algorithm is out of scope of this course.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-agarwal_reinforcement_2022" class="csl-entry" role="listitem">
Agarwal, A., Jiang, N., Kakade, S. M., &amp; Sun, W. (2022). <em>Reinforcement learning: <span>Theory</span> and algorithms</em>. <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</a>
</div>
<div id="ref-frans_berkelaar_container_2009" class="csl-entry" role="listitem">
Frans Berkelaar. (2009). <em>Container ship <span>MSC</span> davos - westerschelde - zeeland</em> [Graphic]. <a href="https://www.flickr.com/photos/28169156@N03/52957948820/">https://www.flickr.com/photos/28169156@N03/52957948820/</a>
</div>
<div id="ref-gittins_multi-armed_2011" class="csl-entry" role="listitem">
Gittins, J. C. (2011). <em>Multi-armed bandit allocation indices</em> (2nd ed). Wiley.
</div>
<div id="ref-gpa_photo_archive_robotic_2017" class="csl-entry" role="listitem">
GPA Photo Archive. (2017). <em>Robotic arm</em> [Graphic]. <a href="https://www.flickr.com/photos/iip-photo-archive/36123310136/">https://www.flickr.com/photos/iip-photo-archive/36123310136/</a>
</div>
<div id="ref-guy_chess_2006" class="csl-entry" role="listitem">
Guy, R. (2006). <em>Chess</em> [Graphic]. <a href="https://www.flickr.com/photos/romainguy/230416692/">https://www.flickr.com/photos/romainguy/230416692/</a>
</div>
<div id="ref-powell_reinforcement_2022" class="csl-entry" role="listitem">
Powell, W. B. (2022). <em>Reinforcement learning and stochastic optimization: A unified framework for sequential decisions</em>. Wiley.
</div>
<div id="ref-puterman_markov_1994" class="csl-entry" role="listitem">
Puterman, M. L. (1994). <em>Markov decision processes: Discrete stochastic dynamic programming</em>. Wiley.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/rlbook\.adzc\.ai");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introduction.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./control.html" class="pagination-link" aria-label="Linear Quadratic Regulators">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025 Alexander Cai</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/mdps.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/mdps.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/adzcai/rlbook" aria-current="page">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>