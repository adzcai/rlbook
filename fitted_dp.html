
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5. Fitted Dynamic Programming Algorithms &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'fitted_dp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Policy Optimization" href="pg.html" />
    <link rel="prev" title="4. Supervised learning" href="supervised_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mdps.html">1. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">2. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">3. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised_learning.html">4. Supervised learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="pg.html">6. Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="imitation_learning.html">7. Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="planning.html">8. Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">9. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="contextual_bandits.html">10. Contextual bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">11. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="background.html">12. Appendix: Background</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/fitted_dp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fitted Dynamic Programming Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">5.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">5.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">5.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="fitted-dynamic-programming-algorithms">
<span id="fitted-dp"></span><h1><span class="section-number">5. </span>Fitted Dynamic Programming Algorithms<a class="headerlink" href="#fitted-dynamic-programming-algorithms" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#empirical-risk-minimization" id="id2">Empirical risk minimization</a></p></li>
<li><p><a class="reference internal" href="#fitted-value-iteration" id="id3">Fitted value iteration</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id4">Summary</a></p></li>
</ul>
</nav>
<p>We borrow these definitions from the <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">rand</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">184</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Transition</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">a</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">r</span><span class="p">:</span> <span class="nb">float</span>


<span class="n">Trajectory</span> <span class="o">=</span> <span class="nb">list</span><span class="p">[</span><span class="n">Transition</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the number of actions in the dataset. Assumes actions range from 0 to A-1.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">a</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">œÑ</span><span class="p">)</span> <span class="k">for</span> <span class="n">œÑ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>


<span class="n">State</span> <span class="o">=</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">]</span>  <span class="c1"># arbitrary shape</span>

<span class="c1"># assume finite `A` actions and f outputs an array of Q-values</span>
<span class="c1"># i.e. Q(s, a, h) is implemented as f(s, h)[a]</span>
<span class="n">QFunction</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; A&quot;</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">Q_zero</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Q-function that always returns zero.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>


<span class="c1"># a deterministic time-dependent policy</span>
<span class="n">Policy</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">q_to_greedy</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">QFunction</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the greedy policy for the given state-action value function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="introduction">
<h2><a class="toc-backref" href="#id1" role="doc-backlink"><span class="section-number">5.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>The <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter discussed the case of <strong>finite</strong> MDPs, where the state and action spaces <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> were finite.
This gave us a closed-form expression for computing the r.h.s. of <a class="reference internal" href="mdps.html#bellman_consistency">the Bellman one-step consistency equation</a>.
In this chapter, we consider the case of <strong>large</strong> or <strong>continuous</strong> state spaces, where the state space is too large to be enumerated.
In this case, we need to <em>approximate</em> the value function and Q-function using methods from <strong>supervised learning</strong>.</p>
<p>We will first take a quick detour to introduce the <em>empirical risk minimization</em> framework for function approximation.
We will then see its application to <em>fitted</em> RL algorithms,
which attempt to learn the optimal value function (and the optimal policy) from a dataset of trajectories.</p>
</section>
<section id="empirical-risk-minimization">
<span id="erm"></span><h2><a class="toc-backref" href="#id2" role="doc-backlink"><span class="section-number">5.2. </span>Empirical risk minimization</a><a class="headerlink" href="#empirical-risk-minimization" title="Link to this heading">#</a></h2>
<p>The <strong>supervised learning</strong> task is as follows:
We seek to learn the relationship between some input variables <span class="math notranslate nohighlight">\(x\)</span> and some output variable <span class="math notranslate nohighlight">\(y\)</span>
(drawn from their joint distribution).
Precisely, we want to find a function <span class="math notranslate nohighlight">\(\hat f : x \mapsto y\)</span> that minimizes the
<em>squared error</em> of the prediction:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f} \E[(y - f(x))^2]
\]</div>
<p>An equivalent framing is that we seek to approximate the <em>conditional expectation</em> of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="proof theorem admonition" id="conditional_expectation_minimizes_mse">
<p class="admonition-title"><span class="caption-number">Theorem 5.1 </span> (Conditional expectation minimizes mean squared error)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[
\arg\min_{f} \E[(y - f(x))^2] = (x \mapsto \E[y \mid x])
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We can decompose the mean squared error as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\E[(y - f(x))^2] &amp;= \E[ (y - \E[y \mid x] + \E[y \mid x] - f(x))^2 ] \\
&amp;= \E[ (y - \E[y \mid x])^2 ] + \E[ (\E[y \mid x] - f(x))^2 ] + 2 \E[ (y - \E[y \mid x])(\E[y \mid x] - f(x)) ] \\
\end{aligned}
\end{split}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Use the law of iterated expectations to show that the last term is zero.</p>
</div>
<p>The first term is the irreducible error, and the second term is the error due to the approximation,
which is minimized at <span class="math notranslate nohighlight">\(0\)</span> when <span class="math notranslate nohighlight">\(f(x) = \E[y \mid x]\)</span>.</p>
</div>
<p>In most applications, the joint distribution of <span class="math notranslate nohighlight">\(x, y\)</span> is unknown or extremely complex, and so we can‚Äôt
analytically evaluate <span class="math notranslate nohighlight">\(\E [y \mid x]\)</span>.
Instead, our strategy is to draw <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> from the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>,
and then use the <em>sample average</em> <span class="math notranslate nohighlight">\(\sum_{i=1}^N (y_i - f(x_i))^2 / N\)</span> to approximate the mean squared error.
Then we use a <em>fitting method</em> to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that minimizes this objective
and thus approximates the conditional expectation.
This approach is called <strong>empirical risk minimization</strong>.</p>
<div class="proof definition admonition" id="empirical_risk_minimization">
<p class="admonition-title"><span class="caption-number">Definition 5.1 </span> (Empirical risk minimization)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset of samples <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_N, y_N)\)</span>, empirical risk minimization seeks to find a function <span class="math notranslate nohighlight">\(f\)</span> (from some class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>) that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2
\]</div>
<p>We will cover the details of the minimization process in <a class="reference internal" href="supervised_learning.html#supervised-learning"><span class="std std-ref">the next section</span></a>.</p>
</section>
</div><div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Why is it important that we constrain our search to a class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>?</p>
<p>Hint: Consider the function <span class="math notranslate nohighlight">\(f(x) = \sum_{i=1}^N y_i \mathbb{1}_{\{ x = x_i \}}\)</span>. What is the empirical risk of this function? Would you consider it a good approximation of the conditional expectation?</p>
</div>
</section>
<section id="fitted-value-iteration">
<h2><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">5.3. </span>Fitted value iteration</a><a class="headerlink" href="#fitted-value-iteration" title="Link to this heading">#</a></h2>
<p>Let us apply ERM to the RL problem of computing the optimal policy / value function.</p>
<p>How did we compute the optimal value function in MDPs with <em>finite</em> state and action spaces?</p>
<ul class="simple">
<li><p>In a <a class="reference internal" href="mdps.html#id2"><span class="std std-ref">finite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#pi_star_dp">dynamic programming</a>, working backwards from the end of the time horizon, to compute the optimal value function exactly.</p></li>
<li><p>In an <a class="reference internal" href="mdps.html#infinite-horizon-mdps"><span class="std std-ref">infinite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#value-iteration"><span class="std std-ref">value iteration</span></a>, which iterates the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(1.7)</a> to approximately compute the optimal value function.</p></li>
</ul>
<p>Our existing approaches represent the value function, and the MDP itself,
in matrix notation.
But what happens if the state space is extremely large, or even infinite (e.g. real-valued)?
Then computing a weighted sum over all possible next states, which is required to compute the Bellman operator,
becomes intractable.</p>
<p>Instead, we will need to use <em>function approximation</em> methods from supervised learning to solve for the value function in an alternative way.</p>
<p>In particular, suppose we have a dataset of <span class="math notranslate nohighlight">\(N\)</span> trajectories <span class="math notranslate nohighlight">\(\tau_1, \dots, \tau_N \sim \rho_{\pi}\)</span> from some policy <span class="math notranslate nohighlight">\(\pi\)</span> (called the <strong>data collection policy</strong>) acting in the MDP of interest.
Let us indicate the trajectory index in the superscript, so that</p>
<div class="math notranslate nohighlight">
\[
\tau_i = \{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \dots, s_{\hor-1}^i, a_{\hor-1}^i, r_{\hor-1}^i \}.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">collect_data</span><span class="p">(</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span> <span class="n">œÄ</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collect a dataset of trajectories from the given policy (or a random one).&quot;&quot;&quot;</span>
    <span class="n">trajectories</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="n">rand</span><span class="o">.</span><span class="n">bits</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">rand</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
        <span class="n">œÑ</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
            <span class="c1"># sample from a random policy</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">œÄ</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="n">œÄ</span> <span class="k">else</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">s_next</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">œÑ</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Transition</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">s_next</span>
        <span class="n">trajectories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">œÑ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trajectories</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">)</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">collect_data</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># show first five transitions from first trajectory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                                                                                | 0/100 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                                                                  | 11/100 [00:00&lt;00:00, 105.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                                              | 22/100 [00:00&lt;00:01, 77.61it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                            | 32/100 [00:00&lt;00:00, 83.42it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                      | 44/100 [00:00&lt;00:00, 93.45it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                              | 57/100 [00:00&lt;00:00, 102.16it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 71/100 [00:00&lt;00:00, 112.76it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                             | 84/100 [00:00&lt;00:00, 115.60it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 98/100 [00:00&lt;00:00, 122.12it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00&lt;00:00, 108.07it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Transition(s=array([-0.00767412,  1.4020356 , -0.77731264, -0.39489663,  0.00889908,
         0.17607279,  0.        ,  0.        ], dtype=float32), a=2, r=0.4116697539428856),
 Transition(s=array([-0.015343  ,  1.3934507 , -0.7757339 , -0.3816226 ,  0.0176519 ,
         0.17507371,  0.        ,  0.        ], dtype=float32), a=2, r=0.521110008516058),
 Transition(s=array([-0.0229064 ,  1.3848708 , -0.7657348 , -0.38147786,  0.02694122,
         0.18580344,  0.        ,  0.        ], dtype=float32), a=0, r=-1.2505839552063946),
 Transition(s=array([-0.03047018,  1.3756915 , -0.7657622 , -0.40815678,  0.03622892,
         0.18577108,  0.        ,  0.        ], dtype=float32), a=3, r=-0.14553357267450792),
 Transition(s=array([-0.03795023,  1.3659201 , -0.7552518 , -0.4344905 ,  0.0433989 ,
         0.14341286,  0.        ,  0.        ], dtype=float32), a=1, r=-2.0988040694638515)]
</pre></div>
</div>
</div>
</div>
<p>Can we view the dataset of trajectories as a ‚Äúlabelled dataset‚Äù in order to apply supervised learning to approximate the optimal Q-function? Yes!
Recall that we can characterize the optimal Q-function using the <a class="reference internal" href="mdps.html#bellman_consistency_optimal">Bellman optimality equations</a>,
which don‚Äôt depend on an actual policy:</p>
<div class="math notranslate nohighlight">
\[
Q_\hi^\star(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [\max_{a'} Q_{\hi+1}^\star(s', a')]
\]</div>
<p>We can think of the arguments to the Q-function ‚Äì i.e. the current state, action, and timestep <span class="math notranslate nohighlight">\(\hi\)</span> ‚Äì
as the inputs <span class="math notranslate nohighlight">\(x\)</span>, and the r.h.s. of the above equation as the label <span class="math notranslate nohighlight">\(f(x)\)</span>. Note that the r.h.s. can also be expressed as a <strong>conditional expectation</strong>:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \E [y \mid x] \quad \text{where} \quad y = r(s_\hi, a_\hi) + \max_{a'} Q^\star_{\hi + 1}(s', a').
\]</div>
<p>Approximating the conditional expectation is precisely the task that <a class="reference internal" href="#erm"><span class="std std-ref">empirical risk minimization</span></a> is suited for!</p>
<p>Our above dataset would give us <span class="math notranslate nohighlight">\(N \cdot \hor\)</span> samples in the dataset:</p>
<div class="math notranslate nohighlight">
\[
x_{i \hi} = (s_\hi^i, a_\hi^i, \hi) \qquad y_{i \hi} = r(s_\hi^i, a_\hi^i) + \max_{a'} Q^\star_{\hi + 1}(s_{\hi + 1}^i, a')
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We pass the state and timestep as input to the Q-function</span>
<span class="sd">    and return an array of Q-values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[(</span><span class="n">œÑ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">œÑ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">œÑ</span> <span class="ow">in</span> <span class="n">trajectories</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">œÑ</span><span class="p">))]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span> <span class="k">for</span> <span class="n">ary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rows</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">get_y</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">œÄ</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the dataset of trajectories into a dataset for supervised learning.</span>
<span class="sd">    If `œÄ` is None, instead estimates the optimal Q function.</span>
<span class="sd">    Otherwise, estimates the Q function of œÄ.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">œÑ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">œÑ</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">œÑ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
            <span class="n">Q_values</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">œÄ</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="k">if</span> <span class="n">œÄ</span> <span class="k">else</span> <span class="n">Q_values</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">œÑ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;states:&quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;actions:&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;timesteps:&quot;</span><span class="p">,</span> <span class="n">h</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>states: [[-0.00767412  1.4020356  -0.77731264 -0.39489663  0.00889908  0.17607279
   0.          0.        ]
 [-0.015343    1.3934507  -0.7757339  -0.3816226   0.0176519   0.17507371
   0.          0.        ]
 [-0.0229064   1.3848708  -0.7657348  -0.38147786  0.02694122  0.18580344
   0.          0.        ]
 [-0.03047018  1.3756915  -0.7657622  -0.40815678  0.03622892  0.18577108
   0.          0.        ]
 [-0.03795023  1.3659201  -0.7552518  -0.4344905   0.0433989   0.14341286
   0.          0.        ]]
actions: [2 2 0 3 1]
timesteps: [0 1 2 3 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([ 0.41166976,  0.52111   , -1.250584  , -0.14553358, -2.098804  ],      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Then we can use empirical risk minimization to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that approximates the optimal Q-function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will see some examples of fitting methods in the next section</span>
<span class="n">FittingMethod</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;N D&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; N&quot;</span><span class="p">]],</span> <span class="n">QFunction</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>But notice that the definition of <span class="math notranslate nohighlight">\(y_{i \hi}\)</span> depends on the Q-function itself!
How can we resolve this circular dependency?
Recall that we faced the same issue <a class="reference internal" href="mdps.html#iterative-pe"><span class="std std-ref">when evaluating a policy in an infinite-horizon MDP</span></a>. There, we iterated the <a class="reference internal" href="mdps.html#bellman_operator">Bellman operator</a> since we knew that the policy‚Äôs value function was a fixed point of the policy‚Äôs Bellman operator.
We can apply the same strategy here, using the <span class="math notranslate nohighlight">\(\hat f\)</span> from the previous iteration to compute the labels <span class="math notranslate nohighlight">\(y_{i \hi}\)</span>,
and then using this new dataset to fit the next iterate.</p>
<div class="proof algorithm admonition" id="fitted_q_iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 5.1 </span> (Fitted Q-function iteration)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(1.7)</a></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_q_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted Q-function iteration using the given dataset.</span>
<span class="sd">    Returns an estimate of the optimal Q-function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use this fixed-point interation to <em>evaluate</em> a policy using the dataset (not necessarily the one used to generate the trajectories):</p>
<div class="proof algorithm admonition" id="fitted_evaluation">
<p class="admonition-title"><span class="caption-number">Algorithm 5.2 </span> (Fitted policy evaluation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy <span class="math notranslate nohighlight">\(\pi : \mathcal{S} \times [H] \to \Delta(\mathcal{A})\)</span> to be evaluated.</p>
<p><strong>Output:</strong> An approximation of the value function <span class="math notranslate nohighlight">\(Q^\pi\)</span> of the policy.</p>
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the <a class="reference internal" href="mdps.html#bellman_consistency">Bellman consistency equation</a> for the given policy.</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_evaluation</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">œÄ</span><span class="p">:</span> <span class="n">Policy</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted policy evaluation using the given dataset.</span>
<span class="sd">    Returns an estimate of the Q-function of the given policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">,</span> <span class="n">œÄ</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Spot the difference between <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> and <code class="docutils literal notranslate"><span class="pre">fitted_q_iteration</span></code>. (See the definition of <code class="docutils literal notranslate"><span class="pre">get_y</span></code>.)
How would you modify this algorithm to evaluate the data collection policy?</p>
</div>
<p>We can use this policy evaluation algorithm to adapt the <a class="reference internal" href="mdps.html#policy-iteration"><span class="std std-ref">policy iteration algorithm</span></a> to this new setting. The algorithm remains exactly the same ‚Äì repeatedly make the policy greedy w.r.t. its own value function ‚Äì except now we must evaluate the policy (i.e. compute its value function) using the iterative <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_policy_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">evaluation_epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">œÄ_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># constant zero policy</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run fitted policy iteration using the given dataset.&quot;&quot;&quot;</span>
    <span class="n">œÄ</span> <span class="o">=</span> <span class="n">œÄ_init</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fitted_evaluation</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">œÄ</span><span class="p">,</span> <span class="n">evaluation_epochs</span><span class="p">)</span>
        <span class="n">œÄ</span> <span class="o">=</span> <span class="n">q_to_greedy</span><span class="p">(</span><span class="n">Q_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">œÄ</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">5.4. </span>Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Supervised learning</p>
      </div>
    </a>
    <a class="right-next"
       href="pg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Policy Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">5.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">5.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">5.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>