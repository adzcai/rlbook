<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Supervised learning – An Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./fitted_dp.html" rel="next">
<link href="./bandits.html" rel="prev">
<link href="./shared/184.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e39e07555a3cad66df478e90649eca5f.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="5&nbsp; Supervised learning – An Introduction to Reinforcement Learning">
<meta property="og:description" content="">
<meta property="og:image" content="https://rlbook.adzc.ai/supervised_learning_files/figure-html/fig-mnist-output-1.png">
<meta property="og:site_name" content="An Introduction to Reinforcement Learning">
<meta property="og:image:height" content="174">
<meta property="og:image:width" content="174">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./supervised_learning.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Introduction to Reinforcement Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/adzcai/rlbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proofs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Proofs</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-sl-intro" id="toc-sec-sl-intro" class="nav-link active" data-scroll-target="#sec-sl-intro"><span class="header-section-number">5.1</span> Introduction</a></li>
  <li><a href="#sec-sl-task" id="toc-sec-sl-task" class="nav-link" data-scroll-target="#sec-sl-task"><span class="header-section-number">5.2</span> The supervised learning task</a>
  <ul class="collapse">
  <li><a href="#sec-loss" id="toc-sec-loss" class="nav-link" data-scroll-target="#sec-loss"><span class="header-section-number">5.2.1</span> Loss functions</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">5.2.2</span> Model selection</a></li>
  </ul></li>
  <li><a href="#sec-erm" id="toc-sec-erm" class="nav-link" data-scroll-target="#sec-erm"><span class="header-section-number">5.3</span> Empirical risk minimization</a>
  <ul class="collapse">
  <li><a href="#function-classes" id="toc-function-classes" class="nav-link" data-scroll-target="#function-classes"><span class="header-section-number">5.3.1</span> Function classes</a></li>
  <li><a href="#sec-sl-parameterized" id="toc-sec-sl-parameterized" class="nav-link" data-scroll-target="#sec-sl-parameterized"><span class="header-section-number">5.3.2</span> Parameterized function classes</a></li>
  <li><a href="#sec-sl-gd" id="toc-sec-sl-gd" class="nav-link" data-scroll-target="#sec-sl-gd"><span class="header-section-number">5.3.3</span> Gradient descent</a></li>
  </ul></li>
  <li><a href="#examples-of-parameterized-function-classes" id="toc-examples-of-parameterized-function-classes" class="nav-link" data-scroll-target="#examples-of-parameterized-function-classes"><span class="header-section-number">5.4</span> Examples of parameterized function classes</a>
  <ul class="collapse">
  <li><a href="#sec-sl-linear" id="toc-sec-sl-linear" class="nav-link" data-scroll-target="#sec-sl-linear"><span class="header-section-number">5.4.1</span> Linear regression</a></li>
  <li><a href="#sec-sl-nn" id="toc-sec-sl-nn" class="nav-link" data-scroll-target="#sec-sl-nn"><span class="header-section-number">5.4.2</span> Neural networks</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">5.5</span> Key takeaways</a></li>
  <li><a href="#sec-sl-bib" id="toc-sec-sl-bib" class="nav-link" data-scroll-target="#sec-sl-bib"><span class="header-section-number">5.6</span> Bibliographic notes and further reading</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/supervised_learning.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/supervised_learning.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-sl" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="sec-sl-intro" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-sl-intro"><span class="header-section-number">5.1</span> Introduction</h2>
<div id="53ad9ad6" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> Float, Array, Callable, plt, np, latex</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets <span class="im">import</span> MNIST</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> config <span class="im">import</span> MNIST_PATH</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Supervised learning</strong> (SL) is a core subfield of machine learning alongside RL and unsupervised learning. The typical SL task is to approximate an <em>unknown function</em> given a dataset of input-output examples from that function.</p>
<div id="exm-image-classification" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 (Image classification)</strong></span> One of the most common examples of an SL problem is the task of image classification: Given a dataset of images and their respective labels, construct a function that takes an image and outputs the correct label.</p>
<p><a href="#fig-mnist" class="quarto-xref">fig.&nbsp;<span>5.1</span></a> illustrates two samples (that is, input-output pairs) from the MNIST database of handwritten digits <span class="citation" data-cites="deng_mnist_2012">(<a href="references.html#ref-deng_mnist_2012" role="doc-biblioref">Deng, 2012</a>)</span>. This is a task that most humans can easily accomplish. By providing many samples of digits and their labels to a machine, SL algorithms can learn to solve this task as well.</p>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> MNIST(MNIST_PATH, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(data.data[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.title(f"Label: {data.targets[0]}")</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.imshow(data.data[<span class="dv">1</span>], cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.title(f"Label: {data.targets[1]}")</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-mnist" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mnist" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mnist-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mnist-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-mnist-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-mnist">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mnist-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) The handwritten digit <span class="math inline">\(5\)</span>.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-mnist" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-mnist-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mnist-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-mnist-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-mnist">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mnist-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) The handwritten digit <span class="math inline">\(0\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: The MNIST image classification dataset of <span class="math inline">\(28   imes 28\)</span> handwritten digits.
</figcaption>
</figure>
</div>
<p>When might function approximation be useful in RL? Recall that the definition of an MDP includes the state transitions <span class="math inline">\(P : \mathcal{S} \times \mathcal{A} \to \triangle(\mathcal{S})\)</span> and a reward function <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> (<a href="mdps.html#def-finite-horizon-mdp" class="quarto-xref">def.&nbsp;<span>2.4</span></a>). If we <em>know</em> both of these functions, in the sense that we are able to query them on arbitrary inputs, we can use powerful planning algorithms to solve for the optimal policy exactly (see <a href="mdps.html#sec-finite-opt-dp" class="quarto-xref"><span>Section 2.3.2</span></a>). Thus, if either (or both) of these is not known, we can use an SL algorithm to <strong>model</strong> the environment and then solve the modeled environment using dynamic programming. This approach is called <strong>fitted DP</strong> and will be covered in <a href="fitted_dp.html" class="quarto-xref"><span>Chapter 6</span></a>.</p>
<p>We do not seek to comprehensively discuss supervised learning; see the bibiographic notes at the end of the chapter (<a href="#sec-sl-bib" class="quarto-xref"><span>Section 5.6</span></a>) for further resources. We hope to to leave you with an understanding of what types of problems SL can solve and how SL algorithms can be applied to RL.</p>
</section>
<section id="sec-sl-task" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-sl-task"><span class="header-section-number">5.2</span> The supervised learning task</h2>
<p>In SL, we are given a dataset of labelled samples <span class="math inline">\((x_1, y_1), \dots, (x_N, y_N)\)</span> that are independently sampled from some <strong>data generating process</strong>. Mathematically, we describe this data generating process as a joint distribution <span class="math inline">\(p \in \triangle(\mathcal{X} \times \mathcal{Y})\)</span>, where <span class="math inline">\(\mathcal{X}\)</span> is the space of possible inputs and <span class="math inline">\(\mathcal{Y}\)</span> is the space of possible outputs. Note that, by the chain rule of probability, this can be factored as <span class="math inline">\(p(x, y) = p(y \mid x) p(x)\)</span>.</p>
<div id="exm-sl-distributions" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 (Joint distributions for image classification)</strong></span> For example, in <a href="#exm-image-classification" class="quarto-xref">ex.&nbsp;<span>5.1</span></a>, the marginal distribution over <span class="math inline">\(x\)</span> is assumed to be the distribution of handwritten digits by humans, scanned as <span class="math inline">\(28 \times 28\)</span> grayscale images. The conditional distribution <span class="math inline">\(y \mid x\)</span> is assumed to be the distribution over <span class="math inline">\(\{ 0, \dots, 9 \}\)</span> that a human would assign to the image <span class="math inline">\(x\)</span>.</p>
</div>
<p>Our task is to compute a “good” <strong>prediction rule</strong> <span class="math inline">\(\hat f : \mathcal{X} \to \mathcal{Y}\)</span> that takes an input and tries to predict the corresponding output.</p>
<section id="sec-loss" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="sec-loss"><span class="header-section-number">5.2.1</span> Loss functions</h3>
<p>How can we measure how “good” a prediction rule is? The most common way is to use a <strong>loss function</strong> <span class="math inline">\(\ell : \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}\)</span> that compares the guess <span class="math inline">\(\hat y := \hat f(x)\)</span> with the true output <span class="math inline">\(y\)</span>. <span class="math inline">\(\ell(\hat y, y)\)</span> should be low if the prediction rule accurately guessed the output, and high if the prediction was incorrect.</p>
<div id="exm-zero-one-loss" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (Zero-one loss)</strong></span> In the image classification task <a href="#exm-image-classification" class="quarto-xref">ex.&nbsp;<span>5.1</span></a>, we have <span class="math inline">\(X = [0, 1]^{28 \times 28}\)</span> (the space of <span class="math inline">\(28\)</span>-by-<span class="math inline">\(28\)</span> grayscale images) and <span class="math inline">\(Y = \{ 0, \dots, 9 \}\)</span> (the image’s label). We could use the zero-one loss function,</p>
<p><span id="eq-zero-one"><span class="math display">\[
\ell(\hat y, y) = \begin{cases}
0 &amp; \hat y = y \\
1 &amp; \hat y \ne y
\end{cases}
\tag{5.1}\]</span></span></p>
<p>to measure the accuracy of the prediction rule. That is, if the prediction rule assigns the wrong label to an image, it incurs a <em>loss</em> of one for that sample.</p>
</div>
<div id="exm-square-loss" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 (Square loss)</strong></span> For a continuous output (i.e.&nbsp;<span class="math inline">\(\mathcal{Y} \subseteq \mathbb{R}\)</span>), we typically use the <strong>squared difference</strong> as the loss function:</p>
<p><span id="eq-square"><span class="math display">\[
\ell(\hat y, y) = (\hat y - y)^2
\tag{5.2}\]</span></span></p>
<p>The square loss is nice to work with analytically since its derivative with respect to <span class="math inline">\(\hat y\)</span> is simply <span class="math inline">\(2 (\hat y - y)\)</span>. (Sometimes authors define the square loss as <em>half</em> of the above value to cancel the factor of <span class="math inline">\(2\)</span> in the derivative. Generally speaking, scaling the loss by some constant scalar has no practical effect.)</p>
</div>
<div id="cell-fig-squared-loss" class="cell" data-fig-width="3" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">20</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\hat y - y$"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\hat y, y)$"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-squared-loss" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-squared-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-squared-loss-output-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-squared-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Square loss.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="model-selection" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">5.2.2</span> Model selection</h3>
<p>Ultimately, we want a prediction rule that does well on new, unseen samples from the data generating process. We can thus ask, how much loss does the prediction rule incur <em>in expectation</em>? This is called the prediction rule’s <strong>generalization error</strong> or <strong>test error</strong>.</p>
<div id="def-test-err" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 (Generalization error)</strong></span> Given a loss function <span class="math inline">\(\ell\)</span> and a prediction rule <span class="math inline">\(\hat f\)</span>, the <em>generalization error</em> of the prediction rule is defined as the expected loss over the data generating process.</p>
<p><span id="eq-test-err"><span class="math display">\[
\text{error}_{\text{g}}(\hat f) := \mathop{\mathbb{E}}_{(x, y) \sim p} [ \ell(\hat f(x), y) ]
\tag{5.3}\]</span></span></p>
<p>Suppose we sample a new input and output from the data generating process, make a guess according to our prediction rule, and use the loss function to compare our guess to the true output. If we repeat this many times, the average loss would approach the generalization error.</p>
</div>
<p>The goal of SL is then to find the prediction rule that minimizes the test error. For certain loss functions, the theoretical optimum can be analytically computed, such as for squared error.</p>
<div id="thm-conditional-expectation-minimizes-mse" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (The conditional expectation minimizes mean squared error)</strong></span> An important result is that, under the square loss, the optimal prediction rule is the <strong>conditional expectation</strong>:</p>
<p><span id="eq-opt-cond"><span class="math display">\[
\arg\min_{f} \mathop{\mathbb{E}}[(y - f(x))^2] = (x \mapsto \mathop{\mathbb{E}}[y \mid x])
\tag{5.4}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We can decompose the mean squared error as</p>
<p><span id="eq-decompose-mse"><span class="math display">\[
\begin{aligned}
\mathop{\mathbb{E}}[(y - f(x))^2] &amp;= \mathop{\mathbb{E}}[ (y - \mathop{\mathbb{E}}[y \mid x] + \mathop{\mathbb{E}}[y \mid x] - f(x))^2 ] \\
&amp;= \mathop{\mathbb{E}}[ (y - \mathop{\mathbb{E}}[y \mid x])^2 ] + \mathop{\mathbb{E}}[ (\mathop{\mathbb{E}}[y \mid x] - f(x))^2 ] \\
&amp;\quad {} + 2 \mathop{\mathbb{E}}[ (y - \mathop{\mathbb{E}}[y \mid x])(\mathop{\mathbb{E}}[y \mid x] - f(x)) ] \\
\end{aligned}
\tag{5.5}\]</span></span></p>
<p>We leave it as an exercise to show that the last term is zero. (Hint: use the law of iterated expectations.) The first term is the <strong>noise</strong>, or irreducible error, that doesn’t depend on <span class="math inline">\(f\)</span>, and the second term is the error due to the approximation, which is minimized at <span class="math inline">\(0\)</span> when <span class="math inline">\(f(x) = \mathop{\mathbb{E}}[y \mid x]\)</span>.</p>
</div>
<p>In most applications, such as in <a href="#exm-sl-distributions" class="quarto-xref">ex.&nbsp;<span>5.2</span></a>, we can’t integrate over the joint distribution of <span class="math inline">\(x, y\)</span>, and so we can’t evaluate <span class="math inline">\(\mathop{\mathbb{E}}[y \mid x]\)</span> analytically. Instead, all we get are <span class="math inline">\(N\)</span> samples from the joint distribution of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. How might we use these to <em>approximate</em> the generalization error?</p>
</section>
</section>
<section id="sec-erm" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-erm"><span class="header-section-number">5.3</span> Empirical risk minimization</h2>
<p>To estimate the generalization error, we could simply take the <em>sample mean</em> of the loss over the training data. This is called the <strong>training loss</strong> or <strong>empirical risk</strong>:</p>
<div id="def-empirical-risk" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 (Empirical risk)</strong></span> Given a dataset <span class="math inline">\((x_1, y_1), \dots, (x_N, y_N)\)</span> sampled i.i.d. from the data generating process, and a loss function <span class="math inline">\(\ell\)</span>, the empirical risk of the prediction rule <span class="math inline">\(\hat f\)</span> is the average loss across the dataset:</p>
<p><span id="eq-training-loss"><span class="math display">\[
\text{training loss}(\hat f) := \frac 1 N \sum_{n=1}^N \ell(\hat f(x_n), y_n).
\tag{5.6}\]</span></span></p>
</div>
<p>By the law of large numbers, as <span class="math inline">\(N\)</span> grows to infinity, the training loss converges to the generalization error (<a href="#def-test-err" class="quarto-xref">def.&nbsp;<span>5.1</span></a>).</p>
<p>The <strong>empirical risk minimization</strong> (ERM) approach is to find a prediction rule that minimizes the empirical risk.</p>
<div id="def-erm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 (Empirical risk minimization)</strong></span> An ERM algorithm requires two ingredients to be chosen based on our <strong>domain knowledge</strong> about the DGP:</p>
<ol type="1">
<li>A <strong>function class</strong> <span class="math inline">\(\mathcal{F}\)</span>, that is, the space of functions to consider.</li>
<li>A <strong>fitting method</strong> that uses the dataset to find the element of <span class="math inline">\(\mathcal{F}\)</span> that minimizes the training loss.</li>
</ol>
<p>This allows us to compute the empirical risk minimizer:</p>
<p><span id="eq-erm"><span class="math display">\[
\begin{aligned}
\hat f_\text{ERM} &amp;:= \arg\min_{f \in \mathcal{F}} \text{training loss}(f) \\
&amp;= \arg\min_{f \in \mathcal{F}}\frac 1 N \sum_{n=1}^N \ell(f(x_n), y_n).
\end{aligned}
\tag{5.7}\]</span></span></p>
</div>
<section id="function-classes" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="function-classes"><span class="header-section-number">5.3.1</span> Function classes</h3>
<p>How should we choose the correct function class? In fact, why do we need to constrain our search at all?</p>
<div id="exr-overfitting" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.1 (Overfitting)</strong></span> Suppose we are trying to approximate a relationship between real-valued inputs and outputs using square loss as our loss function. Consider the prediction rule (visualized in <a href="#fig-pathological-predictor" class="quarto-xref">fig.&nbsp;<span>5.3</span></a>)</p>
<p><span id="eq-overfitting"><span class="math display">\[
\hat f(x) = \sum_{n=1}^N y_n \mathbf{1}\left\{x = x_n\right\}.
\tag{5.8}\]</span></span></p>
<p>What is the empirical risk of this function? How well does it perform on newly generated samples?</p>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x_axis <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>, n)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    x_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>, <span class="dv">10</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> np.sin(np.pi <span class="op">*</span> x_train)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> np.where(np.isclose(x_axis[:, <span class="va">None</span>], x_train, atol<span class="op">=</span><span class="dv">2</span><span class="op">/</span>n), y_train, <span class="dv">0</span>).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_axis, y_hat, label<span class="op">=</span><span class="vs">r'$\hat f(x)$'</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    plt.scatter(x_train, y_train, color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    plt.gcf().set_size_inches(<span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-pathological-predictor" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pathological-predictor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pathological-predictor" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pathological-predictor-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pathological-predictor-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-pathological-predictor-output-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-pathological-predictor">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pathological-predictor-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) One training dataset.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pathological-predictor" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-pathological-predictor-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pathological-predictor-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-pathological-predictor-output-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-pathological-predictor">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pathological-predictor-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Another training dataset.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pathological-predictor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: A pathological prediction rule.
</figcaption>
</figure>
</div>
<p>The choice of <span class="math inline">\(\mathcal{F}\)</span> depends on our <strong>domain knowledge</strong> about the task. On one hand, <span class="math inline">\(\mathcal{F}\)</span> should be large enough to contain the true relationship, but on the other, it shouldn’t be <em>too</em> expressive; otherwise, it will <strong>overfit</strong> to random noise in the labels. The larger and more complex the function class, the more accurately we will be able to approximate any particular training dataset (i.e.&nbsp;smaller <strong>bias</strong>), but the more drastically the function will vary for <em>different</em> training datasets (i.e.&nbsp;larger <strong>variance</strong>). For most loss functions, including the square loss, it is possible to express the generalization error (<a href="#def-test-err" class="quarto-xref">def.&nbsp;<span>5.1</span></a>) as a sum of a bias term and a variance term. The mathematical details of this so-called <strong>bias-variance tradeoff</strong> can be found, for example, in <span class="citation" data-cites="hastie_elements_2013">Hastie et al. (<a href="references.html#ref-hastie_elements_2013" role="doc-biblioref">2013, Chapter 2.9</a>)</span>.</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>degrees <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">50</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>x_axis <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data(sigma<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    x_train <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>, n_samples)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> np.sin(np.pi <span class="op">*</span> x_train) <span class="op">+</span> sigma <span class="op">*</span> np.random.normal(size<span class="op">=</span>n_samples)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_train, y_train</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transform(x: Float[Array, <span class="st">" N"</span>], d: <span class="bu">int</span>):</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.column_stack([</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">**</span> d_</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> d_ <span class="kw">in</span> <span class="bu">range</span>(d <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> degrees:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        x_train, y_train <span class="op">=</span> generate_data()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        x_features <span class="op">=</span> transform(x_train, d)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> np.linalg.lstsq(x_features, y_train)[<span class="dv">0</span>]</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        y_hat <span class="op">=</span> transform(x_axis, d) <span class="op">@</span> w</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> <span class="st">'blue'</span> <span class="cf">if</span> _ <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'red'</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        plt.scatter(x_train, y_train, color<span class="op">=</span>color, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        plt.plot(x_axis, y_hat, color<span class="op">=</span>color)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    plt.xlim(<span class="op">-</span><span class="dv">1</span>, <span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    plt.ylim(<span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1.2</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    plt.gcf().set_size_inches(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-bias-variance" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bias-variance" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-bias-variance-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bias-variance-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-bias-variance-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-bias-variance">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bias-variance-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Degree <span class="math inline">\(2\)</span> polynomials
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bias-variance" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-bias-variance-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bias-variance-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-bias-variance-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-bias-variance">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bias-variance-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Degree <span class="math inline">\(5\)</span> polynomials
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bias-variance" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-bias-variance-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bias-variance-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="supervised_learning_files/figure-html/fig-bias-variance-output-3.png" class="img-fluid figure-img" data-ref-parent="fig-bias-variance">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bias-variance-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Degree <span class="math inline">\(50\)</span> polynomials
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Demonstrating the bias-variance tradeoff through polynomial regression. Increasing the degree increases the complexity of the polynomial function class.
</figcaption>
</figure>
</div>
<p>We must also consider <em>practical</em> constraints on the function class. We need an efficient algorithm to actually compute the function in the class that minimizes the training error. This point should not be underestimated! The success of modern deep learning, for example, is in large part due to hardware developments that have made certain operations more efficient than others.</p>
</section>
<section id="sec-sl-parameterized" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="sec-sl-parameterized"><span class="header-section-number">5.3.2</span> Parameterized function classes</h3>
<p>Both of the function classes we will consider, linear maps and neural networks, are <strong>finite-dimensional</strong>, a.k.a. <strong>parameterized</strong>. The notion of a parameterized function class is best illustrated by example:</p>
<div id="exm-quadratics" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 (Quadratic functions)</strong></span> consider the class of <strong>quadratic functions</strong>, i.e.&nbsp;polynomials of degree <span class="math inline">\(2\)</span>. This is a three-dimensional function space (<span class="math inline">\(D = 3\)</span>), since we can describe any quadratic <span class="math inline">\(p\)</span> as</p>
<p><span id="eq-quadratic-1"><span class="math display">\[
p(x) = a x^2 + b x + c,
\tag{5.9}\]</span></span></p>
<p>where <span class="math inline">\(a, b, c\)</span> are the three parameters. We could also use a different parameterization:</p>
<p><span id="eq-quadratic-2"><span class="math display">\[
p(x) = a' (x - b')^2 + c'.
\tag{5.10}\]</span></span></p>
</div>
<div id="def-parameter" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 (Parameters)</strong></span> Let <span class="math inline">\(\mathcal{F}\)</span> be a class of functions mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathcal{Y}\)</span>. We say that <span class="math inline">\(\mathcal{F}\)</span> is a <strong>parameterized</strong> function class if each <span class="math inline">\(f \in \mathcal{F}\)</span> can be identified using <span class="math inline">\(D\)</span> <strong>parameters</strong>.</p>
</div>
<div id="exr-parameterization-matters" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.2 (Parameterization matters)</strong></span> Note that the choice of parameterization can impact the performance of the chosen fitting method. What is the derivative of <a href="#eq-quadratic-1" class="quarto-xref">eq.&nbsp;<span>5.9</span></a> with respect to <span class="math inline">\(a, b, c\)</span>? Compare this to the derivative of <a href="#eq-quadratic-2" class="quarto-xref">eq.&nbsp;<span>5.10</span></a> with respect to <span class="math inline">\(a', b', c'\)</span>. This shows that gradient-based fitting methods may change their behaviour depending on the parameterization.</p>
</div>
<p>Using a parameterized function class allows us to reframe the ERM problem <a href="#def-erm" class="quarto-xref">def.&nbsp;<span>5.3</span></a> in terms of optimizing over the <em>parameters</em> instead of over the functions they represent:</p>
<p><span id="eq-erm-parameterized"><span class="math display">\[
\begin{aligned}
\hat \theta_\text{ERM} &amp;:= \arg\min_{\theta \in \mathbb{R}^D} \text{training loss}(f_\theta) \\
&amp;= \frac{1}{N} \sum_{n=1}^N (y_n - f_\theta(x_n))^2
\end{aligned}
\tag{5.11}\]</span></span></p>
<p>In general, optimizing over a <em>finite-dimensional</em> space is much, much easier than optimizing over an <em>infinite-dimensional</em> space.</p>
</section>
<section id="sec-sl-gd" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="sec-sl-gd"><span class="header-section-number">5.3.3</span> Gradient descent</h3>
<p>One widely applicable fitting method for parameterized function classes is <strong>gradient descent</strong>.</p>
<p>Let <span class="math inline">\(L(\theta) = \text{training loss}(f_\theta)\)</span> denote the empirical risk in terms of the parameters. The <strong>gradient descent</strong> algorithm iteratively updates the parameters according to the rule</p>
<p><span class="math display">\[
\theta^{t+1} = \theta^t - \eta \nabla_\theta L(\theta^t)
\]</span></p>
<p>where <span class="math inline">\(\eta &gt; 0\)</span> is the <strong>learning rate</strong> and <span class="math inline">\(\nabla_\theta L(\theta^t)\)</span> indicates the <strong>gradient</strong> of <span class="math inline">\(L\)</span> at the point <span class="math inline">\(\theta^t\)</span>. Recall that the gradient of a function at a point is a vector in the direction that increases the function’s value the most within a <em>neighborhood</em>. So by taking small steps in the oppposite direction, we obtain a solution that achieves a slightly lower loss than the current one.</p>
<div id="fig-gd" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="6">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Params <span class="op">=</span> Float[Array, <span class="st">" D"</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    loss: Callable[[Params], <span class="bu">float</span>],</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    θ_init: Params,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    η: <span class="bu">float</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    epochs: <span class="bu">int</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Run gradient descent to minimize the given loss function</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">    (expressed in terms of the parameters).</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    θ <span class="op">=</span> θ_init</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        θ <span class="op">=</span> θ <span class="op">-</span> η <span class="op">*</span> grad(loss)(θ)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> θ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-gd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5
</figcaption>
</figure>
</div>
<p>In <a href="pg.html#sec-computing-derivatives" class="quarto-xref"><span>Section 7.3.1</span></a>, we will discuss methods for implementing the <code>grad</code> function above, which takes in a function and returns its gradient, which can then be evaluated at a point.</p>
<p>Why do we need to scale down the step size by <span class="math inline">\(\eta\)</span>? The key word above is “neighborhood”. The gradient only describes the function within a local region around the point, whose size depends on the function’s smoothness. If we take a step that’s too large, we might end up with a <em>worse</em> solution by overshooting the region where the gradient is accurate. Note that, as a result, we can’t guarantee finding a <em>global</em> optimum of the function; we can only find <em>local</em> optima that are the best parameters within some neighborhood.</p>
<p>Another issue is that it’s often expensive to compute <span class="math inline">\(\nabla_\theta L\)</span> when <span class="math inline">\(N\)</span> is very large. Instead of calculating the gradient for every point in the dataset and averaging these, we can simply draw a <strong>batch</strong> of samples from the dataset and average the gradient across just these samples. Note that this is an unbiased random <em>estimator</em> of the true gradient. This algorithm is known as <strong>stochastic gradient descent</strong>. The added noise sometimes helps to jump to better solutions with a lower overall empirical risk.</p>
<p>Stepping for a moment back into the world of RL, you might wonder, why can’t we simply apply gradient descent to the total reward? It turns out that the gradient of the total reward with respect to the policy parameters known as the <strong>policy gradient</strong>, is challenging but possible to approximate. In <a href="pg.html" class="quarto-xref"><span>Chapter 7</span></a>, we will do exactly this.</p>
</section>
</section>
<section id="examples-of-parameterized-function-classes" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="examples-of-parameterized-function-classes"><span class="header-section-number">5.4</span> Examples of parameterized function classes</h2>
<section id="sec-sl-linear" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="sec-sl-linear"><span class="header-section-number">5.4.1</span> Linear regression</h3>
<p>In linear regression, we assume that the function <span class="math inline">\(f\)</span> is linear in the parameters:</p>
<p><span class="math display">\[
\mathcal{F} = \{ x \mapsto \theta^\top x \mid \theta \in \mathbb{R}^D \}
\]</span></p>
<p>You may already be familiar with linear regression from an introductory statistics course. This function class is extremely simple and only contains linear functions, whose graphs look like “lines of best fit” through the training data. It turns out that, when minimizing the squared error, the empirical risk minimizer has a closed-form solution, known as the <strong>ordinary least squares</strong> estimator. Let us write <span class="math inline">\(Y = (y_1, \dots, y_n)^\top \in \mathbb{R}^N\)</span> and <span class="math inline">\(X = (x_1, \dots, x_N)^\top \in \mathbb{R}^{N \times D}\)</span>. Then we can write</p>
<p><span id="eq-erm-linear-regression"><span class="math display">\[
\begin{aligned}
\hat \theta &amp;= \arg\min_{\theta \in \mathbb{R}^D} \frac{1}{2} \sum_{n=1}^N (y_n - \theta^\top x_n)^2 \\
&amp;= \arg\min_{\theta \in \mathbb{R}^D} \frac 1 2 \|Y - X \theta \|^2 \\
&amp;= (X^\top X)^{-1} X^\top Y,
\end{aligned}
\tag{5.12}\]</span></span></p>
<p>where we have assumed that the columns of <span class="math inline">\(X\)</span> are linearly independent so that the matrix <span class="math inline">\(X^\top X\)</span> is invertible.</p>
<p>What happens if the columns aren’t linearly independent? In this case, out of the possible solutions with the minimum empirical risk, we typically choose the one with the <em>smallest norm</em>.</p>
<div id="exr-smallest-norm" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.3 (Gradient descent finds the minimum norm solution)</strong></span> Gradient descent on the ERM problem (<a href="#eq-erm-linear-regression" class="quarto-xref">eq.&nbsp;<span>5.12</span></a>), initialized at the origin and using a small enough step size, eventually finds the parameters with the smallest norm. In practice, since the squared error gradient is convenient to compute, running gradient descent can be faster than explicitly computing the inverse (or pseudoinverse) of a matrix.</p>
<p>Assume that <span class="math inline">\(N &lt; D\)</span> and that the data points are linearly independent.</p>
<ol type="1">
<li><p>Let <span class="math inline">\(\hat{\theta}\)</span> be the solution found by gradient descent. Show that <span class="math inline">\(\hat{\theta}\)</span> is a linear combination of the data points, that is, <span class="math inline">\(\hat{\theta} = X^\top a\)</span>, where <span class="math inline">\(a \in \mathbb{R}^N\)</span>.</p></li>
<li><p>Let <span class="math inline">\(w \in \mathbb{R}^D\)</span> be another empirical risk minimizer i.e.&nbsp;<span class="math inline">\(X w = y\)</span>. Show that <span class="math inline">\(\hat{\theta}^\top (w - \hat{\theta}) = 0\)</span>.</p></li>
<li><p>Use this to show that <span class="math inline">\(\|\hat{\theta}\| \le \|w\|\)</span>, showing that the gradient descent solution has the smallest norm out of all solutions that fit the data. (No need for algebra; there is a nice geometric solution!)</p></li>
</ol>
</div>
<p>Though linear regression may appear trivially simple, it is a very powerful tool for more complex models to build upon. For instance, to expand the expressiveness of linear models, we can first <em>transform</em> the input <span class="math inline">\(x\)</span> using some feature mapping <span class="math inline">\(\phi\)</span>, i.e.&nbsp;<span class="math inline">\(\widetilde x = \phi(x)\)</span>, and then fit a linear model in the transformed space instead. By using domain knowledge to choose a useful feature mapping, we can obtain a powerful SL method for a particular task.</p>
</section>
<section id="sec-sl-nn" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="sec-sl-nn"><span class="header-section-number">5.4.2</span> Neural networks</h3>
<p>In neural networks, we assume that the function <span class="math inline">\(f\)</span> is a composition of linear functions (represented by matrices <span class="math inline">\(W_i\)</span>) and non-linear activation functions (denoted by <span class="math inline">\(\sigma\)</span>):</p>
<p><span class="math display">\[
\mathcal{F} = \{ x \mapsto \sigma(W_L \sigma(W_{L-1} \dots \sigma(W_1 x + b_1) \dots + b_{L-1}) + b_L) \}
\]</span></p>
<p>where <span class="math inline">\(W_\ell \in \mathbb{R}^{D_{\ell+1} \times D_\ell}\)</span> and <span class="math inline">\(b_\ell \in \mathbb{R}^{D_{\ell+1}}\)</span> are the parameters of the <span class="math inline">\(i\)</span>-th layer, and <span class="math inline">\(\sigma\)</span> is the activation function.</p>
<p>This function class is highly expressive and allows for more parameters. This makes it more susceptible to overfitting on smaller datasets, but also allows it to represent more complex functions. In practice, however, neural networks exhibit interesting phenomena during training, and are often able to generalize well even with many parameters.</p>
<p>Another reason for their popularity is the efficient <strong>backpropagation</strong> algorithm for computing the gradient of the output with respect to the parameters. Essentially, the hierarchical structure of the neural network, i.e.&nbsp;computing the output of the network as a composition of functions, allows us to use the chain rule to compute the gradient of the output with respect to the parameters of each layer.</p>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">5.5</span> Key takeaways</h2>
<p><strong>Supervised learning</strong> is a field of machine learning that seeks to approximate some unknown function given a dataset of example input-output pairs from that function. In particular, we typically seek to compute a <strong>prediction rule</strong> that takes in an input value and returns a good guess for the corresponding output. We score prediction rules using a <strong>loss function</strong> that measures how incorrectly it guesses. We want to find a prediction rule that achieves low loss on unseen data points. We do this by searching over a class of functions to find one that minimizes the <strong>empirical risk</strong> over the training dataset. We finally saw two popular examples of <strong>parameterized</strong> function classes: linear regression and neural networks.</p>
</section>
<section id="sec-sl-bib" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="sec-sl-bib"><span class="header-section-number">5.6</span> Bibliographic notes and further reading</h2>
<p>Supervised learning is the largest subfield of machine learning; we do not attempt to comprehensively survey recent progress here. Rather, here are some textbooks for interested students to read further.</p>
<p><span class="citation" data-cites="james_introduction_2023">James et al. (<a href="references.html#ref-james_introduction_2023" role="doc-biblioref">2023</a>)</span> provides an accessible introduction to supervised learning. <span class="citation" data-cites="hastie_elements_2013">Hastie et al. (<a href="references.html#ref-hastie_elements_2013" role="doc-biblioref">2013</a>)</span> examines the subject in even further depth and covers many relevant supervised learning methods. <span class="citation" data-cites="nielsen_neural_2015">Nielsen (<a href="references.html#ref-nielsen_neural_2015" role="doc-biblioref">2015</a>)</span> provides a comprehensive introduction to neural networks and backpropagation. <span class="citation" data-cites="vapnik_nature_2000">Vapnik (<a href="references.html#ref-vapnik_nature_2000" role="doc-biblioref">2000</a>)</span> is another prominent textbook on classical statistical learning from before the “deep learning era”. <span class="citation" data-cites="bishop_pattern_2006">Bishop (<a href="references.html#ref-bishop_pattern_2006" role="doc-biblioref">2006</a>)</span> focuses on the Bayesian perspective.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-bishop_pattern_2006" class="csl-entry" role="listitem">
Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer.
</div>
<div id="ref-deng_mnist_2012" class="csl-entry" role="listitem">
Deng, L. (2012). The <span>MNIST</span> database of handwritten digit images for machine learning research. <em>IEEE Signal Processing Magazine</em>, <em>29</em>(6), 141–142. <span>IEEE Signal Processing Magazine</span>. <a href="https://doi.org/10.1109/MSP.2012.2211477">https://doi.org/10.1109/MSP.2012.2211477</a>
</div>
<div id="ref-hastie_elements_2013" class="csl-entry" role="listitem">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2013). <em>The elements of statistical learning: <span>Data</span> mining, inference, and prediction</em>. Springer Science &amp; Business Media. <a href="https://books.google.com?id=yPfZBwAAQBAJ">https://books.google.com?id=yPfZBwAAQBAJ</a>
</div>
<div id="ref-james_introduction_2023" class="csl-entry" role="listitem">
James, G., Witten, D., Hastie, T., Tibshirani, R., &amp; Taylor, J. (2023). <em>An introduction to statistical learning: With applications in python</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-38747-0">https://doi.org/10.1007/978-3-031-38747-0</a>
</div>
<div id="ref-nielsen_neural_2015" class="csl-entry" role="listitem">
Nielsen, M. A. (2015). <em>Neural networks and deep learning</em>. Determination Press. <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>
</div>
<div id="ref-vapnik_nature_2000" class="csl-entry" role="listitem">
Vapnik, V. N. (2000). <em>The nature of statistical learning theory</em>. Springer. <a href="https://doi.org/10.1007/978-1-4757-3264-1">https://doi.org/10.1007/978-1-4757-3264-1</a>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/rlbook\.adzc\.ai");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./bandits.html" class="pagination-link" aria-label="Multi-Armed Bandits">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./fitted_dp.html" class="pagination-link" aria-label="Fitted Dynamic Programming Algorithms">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025 Alexander Cai</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/supervised_learning.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/supervised_learning.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/adzcai/rlbook" aria-current="page">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>