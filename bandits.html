<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Multi-Armed Bandits – An Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./supervised_learning.html" rel="next">
<link href="./control.html" rel="prev">
<link href="./shared/184.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e39e07555a3cad66df478e90649eca5f.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="4&nbsp; Multi-Armed Bandits – An Introduction to Reinforcement Learning">
<meta property="og:description" content="">
<meta property="og:image" content="https://rlbook.adzc.ai/shared/advertisements.jpg">
<meta property="og:site_name" content="An Introduction to Reinforcement Learning">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bandits.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Introduction to Reinforcement Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/adzcai/rlbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.pdf">
              <i class="bi bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./An-Introduction-to-Reinforcement-Learning.epub">
              <i class="bi bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proofs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Proofs</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
  <li><a href="#sec-bandit-problem" id="toc-sec-bandit-problem" class="nav-link" data-scroll-target="#sec-bandit-problem"><span class="header-section-number">4.2</span> The multi-armed bandit problem</a>
  <ul class="collapse">
  <li><a href="#regret" id="toc-regret" class="nav-link" data-scroll-target="#regret"><span class="header-section-number">4.2.1</span> Regret</a></li>
  </ul></li>
  <li><a href="#sec-pure-exploration" id="toc-sec-pure-exploration" class="nav-link" data-scroll-target="#sec-pure-exploration"><span class="header-section-number">4.3</span> Pure exploration</a></li>
  <li><a href="#sec-pure-greedy" id="toc-sec-pure-greedy" class="nav-link" data-scroll-target="#sec-pure-greedy"><span class="header-section-number">4.4</span> Pure greedy</a></li>
  <li><a href="#sec-etc" id="toc-sec-etc" class="nav-link" data-scroll-target="#sec-etc"><span class="header-section-number">4.5</span> Explore-then-commit</a></li>
  <li><a href="#sec-epsilon-greedy" id="toc-sec-epsilon-greedy" class="nav-link" data-scroll-target="#sec-epsilon-greedy"><span class="header-section-number">4.6</span> Epsilon-greedy</a></li>
  <li><a href="#sec-ucb" id="toc-sec-ucb" class="nav-link" data-scroll-target="#sec-ucb"><span class="header-section-number">4.7</span> Upper Confidence Bound (UCB)</a>
  <ul class="collapse">
  <li><a href="#lower-bound-on-regret-intuition" id="toc-lower-bound-on-regret-intuition" class="nav-link" data-scroll-target="#lower-bound-on-regret-intuition"><span class="header-section-number">4.7.1</span> Lower bound on regret (intuition)</a></li>
  </ul></li>
  <li><a href="#sec-thompson-sampling" id="toc-sec-thompson-sampling" class="nav-link" data-scroll-target="#sec-thompson-sampling"><span class="header-section-number">4.8</span> Thompson sampling and Bayesian bandits</a></li>
  <li><a href="#sec-contextual-bandits" id="toc-sec-contextual-bandits" class="nav-link" data-scroll-target="#sec-contextual-bandits"><span class="header-section-number">4.9</span> Contextual bandits</a>
  <ul class="collapse">
  <li><a href="#sec-lin-ucb" id="toc-sec-lin-ucb" class="nav-link" data-scroll-target="#sec-lin-ucb"><span class="header-section-number">4.9.1</span> Linear contextual bandits</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">4.10</span> Key takeaways</a></li>
  <li><a href="#bibliographic-notes-and-further-reading" id="toc-bibliographic-notes-and-further-reading" class="nav-link" data-scroll-target="#bibliographic-notes-and-further-reading"><span class="header-section-number">4.11</span> Bibliographic notes and further reading</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/bandits.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/bandits.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-bandits" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>The <strong>multi-armed bandit</strong> (MAB) problem is a simple setting for studying the basic challenges of sequential decision-making. In this setting, an agent repeatedly chooses from a fixed set of actions, called <strong>arms</strong>, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period. Here are a few examples:</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="exm-advertising" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 (Online advertising)</strong></span> Let’s suppose you, the agent, are an advertising company. You have <span class="math inline">\(K\)</span> different ads that you can show to users. Let’s suppose you are targeting a single user. You receive <span class="math inline">\(1\)</span> reward if the user clicks the ad, and <span class="math inline">\(0\)</span> otherwise. Thus, the unknown <em>reward distribution</em> associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.</p>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/advertisements.jpg" class="img-fluid figure-img"></p>
<figcaption>Deciding which ad to put on a billboard can be treated as a MAB problem. Image from <span class="citation" data-cites="negative_space_photo_2015">Negative Space (<a href="references.html#ref-negative_space_photo_2015" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="exm-clinical-trials" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 (Clinical trials)</strong></span> Suppose you’re a pharmaceutical company, and you’re testing a new drug. You have <span class="math inline">\(K\)</span> different dosages of the drug that you can administer to patients. You receive <span class="math inline">\(1\)</span> reward if the patient recovers, and <span class="math inline">\(0\)</span> otherwise. Thus, the unknown reward distribution associated with each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.</p>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/medication.jpg" class="img-fluid figure-img"></p>
<figcaption>Optimizing between different medications can be treated as a MAB problem. Image from <span class="citation" data-cites="pixabay_20_2016">Pixabay (<a href="references.html#ref-pixabay_20_2016" role="doc-biblioref">2016a</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The MAB is the simplest setting for exploring the <strong>exploration-exploitation tradeoff</strong>: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?</p>
<p>In this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.</p>
<div id="7a92f106" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jaxtyping <span class="im">import</span> Float, Array</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> latexify</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable, Union</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> utils</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> solutions.bandits <span class="im">as</span> solutions</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">184</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_argmax(ary: Array) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Take an argmax and randomize between ties."""</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    max_idx <span class="op">=</span> np.flatnonzero(ary <span class="op">==</span> ary.<span class="bu">max</span>())</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.choice(max_idx).item()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># used as decorator</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>latex <span class="op">=</span> latexify.algorithmic(</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    trim_prefixes<span class="op">=</span>{<span class="st">"mab"</span>},</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"arm"</span>: <span class="st">"a_\ep"</span>, <span class="st">"reward"</span>: <span class="st">"r"</span>, <span class="st">"means"</span>: <span class="st">"\mu"</span>, <span class="st">"t"</span>: <span class="vs">r"\ep"</span>, <span class="st">"T"</span>: <span class="vs">r"\Ep"</span>},</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    use_math_symbols<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-multi-armed" class="proof remark">
<p><span class="proof-title"><em>Remark 4.1</em> (Etymology). </span>The name “multi-armed bandit” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.</p>
</div>
<div id="tbl-bandits-full" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bandits-full-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: How bandits leads into the full RL problem.
</figcaption>
<div aria-describedby="tbl-bandits-full-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 41%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Environment is…</th>
<th>known (planning/optimal control)</th>
<th>unknown (learning from experience)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>stateless/invariant (<span class="math inline">\(\vert\mathcal{S}\vert = 1\)</span>)</td>
<td>(trivial)</td>
<td>multi-armed bandits</td>
</tr>
<tr class="even">
<td>stateful/dynamic</td>
<td>MDP planning (dynamic programming)</td>
<td>“full” RL</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-bandit-problem" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-bandit-problem"><span class="header-section-number">4.2</span> The multi-armed bandit problem</h2>
<p>Let’s frame the MAB problem mathematically. In both <a href="#exm-advertising" class="quarto-xref">ex.&nbsp;<span>4.1</span></a> and <a href="#exm-clinical-trials" class="quarto-xref">ex.&nbsp;<span>4.2</span></a>, the outcome of each decision is binary (i.e.&nbsp;<span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>). This is the <strong>Bernoulli bandit</strong> problem, which we’ll use throughout the chapter.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="def-bernoulli-bandit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1 (Bernoulli bandit problem)</strong></span> Let <span class="math inline">\(K\)</span> denote the number of arms. We’ll label them <span class="math inline">\(0, \dots, K-1\)</span> and use <em>superscripts</em> to indicate the arm index <span class="math inline">\(k\)</span>. Since we seldom need to raise a number to a power, this won’t cause much confusion.</p>
<p>Let <span class="math inline">\(\mu^k\)</span> be the mean of arm <span class="math inline">\(k\)</span>, such that pulling arm <span class="math inline">\(k\)</span> either returns reward <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\mu^k\)</span> or <span class="math inline">\(0\)</span> otherwise.</p>
<p>The agent gets <span class="math inline">\(T\)</span> chances to pull an arm. The task is to determine a strategy for maximizing the total reward.</p>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/flipping-coins.jpg" class="img-fluid figure-img"></p>
<figcaption>Each arm <span class="math inline">\(k\)</span> in a Bernoulli bandit is a biased coin with probability <span class="math inline">\(\mu^k\)</span> of showing heads. Each “pull” corresponds to flipping a coin. The goal is to maximize the number of heads given <span class="math inline">\(T\)</span> flips. Image from <span class="citation" data-cites="pixabay_coins_2016">Pixabay (<a href="references.html#ref-pixabay_coins_2016" role="doc-biblioref">2016b</a>)</span>.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="7e5f945c" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MAB:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The Bernoulli multi-armed bandit environment."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, means: Float[Array, <span class="st">" K"</span>], T: <span class="bu">int</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Constructor.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        :param means: the means (success probabilities) of the reward distributions for each arm</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">        :param T: the time horizon</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">all</span>(<span class="dv">0</span> <span class="op">&lt;=</span> p <span class="op">&lt;=</span> <span class="dv">1</span> <span class="cf">for</span> p <span class="kw">in</span> means)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> means</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> <span class="va">self</span>.means.size</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_arm <span class="op">=</span> random_argmax(<span class="va">self</span>.means)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pull(<span class="va">self</span>, k: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Pull the `k`-th arm and sample from its (Bernoulli) reward distribution."""</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> np.random.rand() <span class="op">&lt;</span> <span class="va">self</span>.means[k].item()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">+</span>reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="b5e9fabc" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mab <span class="op">=</span> MAB(means<span class="op">=</span>np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>]), T<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-mab-mdp-connection" class="proof remark">
<p><span class="proof-title"><em>Remark 4.2</em> (Connection between MAB and MDP settings). </span>A multi-armed bandit is a special case of a finite-horizon Markov decision process with a single state (<span class="math inline">\(|\mathcal{S}| = 1\)</span>) and a horizon of <span class="math inline">\(H= 1\)</span>. (Conceptually speaking, the environment is “stateless”, but an MDP with “no states” doesn’t make sense.) As such, Each arm is an action: <span class="math inline">\(|\mathcal{A}| = K\)</span>. the expected value of pulling arm <span class="math inline">\(k\)</span>, which we denote <span class="math inline">\(\mu^k\)</span> in this chapter, is exactly the Q-function (<a href="mdps.html#def-q" class="quarto-xref">def.&nbsp;<span>2.9</span></a>):</p>
<p><span id="eq-q-mean"><span class="math display">\[
\mu^k= Q(k)
\tag{4.1}\]</span></span></p>
<p>Note that we omit the arguments for <span class="math inline">\(\pi, h\)</span>, and <span class="math inline">\(s\)</span>, which are meaningless in the MAB setting.</p>
</div>
<p>In pseudocode, the agent’s interaction with the MAB environment can be described by the following process:</p>
<div id="cell-fig-mab-interaction" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mab_loop(mab: MAB, agent: <span class="st">"Agent"</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(mab.T):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        arm <span class="op">=</span> agent.choose_arm()  <span class="co"># in 0, ..., K-1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> mab.pull(arm)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        agent.update_history(arm, reward)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>latex(mab_loop)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-mab-interaction" class="cell-output cell-output-display cell-output-markdown quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mab-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{mab\_loop}(\mathrm{mab}: \mathrm{MAB}, \mathrm{agent}: \textrm{"Agent"}) \\ \hspace{1em} \mathbf{for} \ t\in \mathrm{range} \mathopen{}\left( T\mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} a_t\gets \mathrm{agent}.\mathrm{choose\_arm} \mathopen{}\left( \mathclose{}\right) \\ \hspace{2em} r \gets \mathrm{pull} \mathopen{}\left( a_t\mathclose{}\right) \\ \hspace{2em} \mathrm{agent}.\mathrm{update\_history} \mathopen{}\left( a_t, r \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mab-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: The MAB interaction loop. <span class="math inline">\(a_t\in [K]\)</span> denotes the <span class="math inline">\(t\)</span>th arm pulled.
</figcaption>
</figure>
</div>
</div>
<div id="c9a04a42" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Agent:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stores the pull history and uses it to decide which arm to pull next.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Since we are working with Bernoulli bandit,</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    we can summarize the pull history concisely in a (K, 2) array.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The MAB agent that decides how to choose an arm given the past history."""</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> K</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards <span class="op">=</span> []  <span class="co"># for plotting</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.choices <span class="op">=</span> []</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.history <span class="op">=</span> np.zeros((K, <span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose an arm of the MAB. Algorithm-specific."""</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> count(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The number of pulls made. Also the current step index."""</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.rewards)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards.append(reward)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.choices.append(arm)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.history[arm, reward] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>What’s the <em>optimal</em> strategy for the agent, i.e.&nbsp;the one that achieves the highest expected reward? Convince yourself that the agent should try to always pull the arm with the highest expected reward:</p>
<div id="def-optimal-arm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 (Optimal arm)</strong></span> We call the arm whose reward distribution has the highest mean the <strong>optimal arm</strong>:</p>
<p><span id="eq-optimal-arm"><span class="math display">\[
\begin{aligned}
k^\star &amp;:= \arg\max_{k\in [K]} \mu^k\\
\mu^\star &amp;:= \mu^{k^\star} = \max_{k\in [K]} \mu^k.
\end{aligned}
\tag{4.2}\]</span></span></p>
</div>
<section id="regret" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="regret"><span class="header-section-number">4.2.1</span> Regret</h3>
<p>When analyzing an MAB algorithm, rather than measuring its performance in terms of the (expected) total reward, we instead compare it to the “oracle” strategy that knows the optimal arm in advance. This gives rise to a measure of performance known as <strong>regret</strong>, which answers, how much better <em>could</em> you have done had you known the optimal arm from the beginning? This provides a more meaningful baseline than the expected total reward, whose magnitude may vary widely depending on the bandit problem.</p>
<div id="exm-regret" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 (Regret provides a more meaningful baseline)</strong></span> Consider the problem of grade inflation or deflation. If you went to a different school or university, you might have gotten a very different grade, independently of your actual ability. This is like comparing bandit algorithms based on the expected total reward: the baseline varies depending on the problem (i.e.&nbsp;school), making it hard to compare the algorithms (i.e.&nbsp;students) themselves. Instead, it makes more sense to measure you relative to the (theoretical) best student from your school. This is analogous to measuring algorithm performance using regret.</p>
</div>
<div id="def-regret" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3 (Regret)</strong></span> The agent’s (cumulative) <strong>regret</strong> after <span class="math inline">\(T\)</span> pulls is defined as</p>
<p><span id="eq-regret"><span class="math display">\[
\text{Regret}_T:= \sum_{t=0}^{T-1} \mu^\star - \mu^{a_t},
\tag{4.3}\]</span></span></p>
<p>where <span class="math inline">\(a_t\in [K]\)</span> is the arm chosen on the <span class="math inline">\(t\)</span>th pull.</p>
</div>
<div id="4a9253a4" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> regret_per_step(mab: MAB, agent: Agent):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the difference from the average reward of the optimal arm. The sum of these is the regret."""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [mab.means[mab.best_arm] <span class="op">-</span> mab.means[arm] <span class="cf">for</span> arm <span class="kw">in</span> agent.choices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This depends on the <em>true means</em> of the pulled arms, and is independent of the observed rewards. The regret <span class="math inline">\(\text{Regret}_T\)</span> is a random variable where the randomness comes from the agent’s strategy (i.e.&nbsp;the sequence of actions <span class="math inline">\(a_0, \dots, a_{K-1}\)</span>).</p>
<p>Also note that we care about the <em>total</em> regret across all decisions, rather than just the final decision. If we only cared about the quality of the final decision, the best strategy would be to learn as much as possible about the environment, and then use that knowledge to make the best possible decision at the last step. This is a <strong>pure exploration</strong> problem since the agent is never penalized for taking suboptimal actions up to the final decision. Minimizing the <em>cumulative</em> regret (or maximizing the <em>cumulative</em> reward) means we must strike a balance between <em>exploration</em> and <em>exploitation</em> throughout the entire decision-making process.</p>
<p>Throughout the chapter, we will try to upper bound the regret of various algorithms in two different senses:</p>
<ol type="1">
<li>Upper bound the <em>expected regret</em>, i.e.&nbsp;show that for some upper bound <span class="math inline">\(M_T\)</span>, <span id="eq-expected-regret-bound"><span class="math display">\[
\mathop{\mathbb{E}}[\text{Regret}_T] \le M_T.
\tag{4.4}\]</span></span></li>
<li>Find a <em>high-probability</em> upper bound on the regret, i.e.&nbsp;show that for some small <em>failure probability</em> <span class="math inline">\(\delta &gt; 0\)</span>, <span id="eq-probabilistic-regret-bound"><span class="math display">\[
\mathbb{P}(\text{Regret}_T\le M_{T, \delta}) \ge 1-\delta
\tag{4.5}\]</span></span> for some upper bound <span class="math inline">\(M_{T, \delta}\)</span>.</li>
</ol>
<p>The first approach says that the regret is at most <span class="math inline">\(M_T\)</span> in expectation. However, the agent might still achieve a higher or lower regret on a particular randomization. The second approach says that, with probability at least <span class="math inline">\(1-\delta\)</span>, the agent will achieve regret at most <span class="math inline">\(M_{T, \delta}\)</span>. However, it doesn’t say anything about the regret in the remaining <span class="math inline">\(\delta\)</span> fraction of runs, which could be arbitrarily high.</p>
<div id="exr-markov-expected" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.1 (Expected regret bounds yield probabilistic regret bounds)</strong></span> Suppose we have an upper bound <span class="math inline">\(M_T\)</span> on the expected regret. Show that, with probability <span class="math inline">\(1-\delta\)</span>, <span class="math inline">\(M_{T, \delta} := M_T/ \delta\)</span> upper bounds the regret. Note this is a much higher bound! For example, if <span class="math inline">\(\delta = 0.01\)</span>, the high-probability bound is <span class="math inline">\(100\)</span> times as large.</p>
</div>
<div id="rem-pac-bounds" class="proof remark">
<p><span class="proof-title"><em>Remark 4.3</em> (Issues with regret). </span>Is regret the right way to measure performance? Consider the following two algorithms:</p>
<ol type="1">
<li>An algorithm which sometimes chooses very bad arms.</li>
<li>An algorithm which often chooses slightly suboptimal arms.</li>
</ol>
<p>We might care a lot about this distinction in practice, for example, in healthcare, where an occasional very bad decision could have extreme consequences. However, these two algorithms could achieve the exact same expected total regret.</p>
<p>Other performance measures include <strong>probably approximately correct</strong> (PAC) bounds and <strong>uniform</strong> high-probability regret bounds. There is also a <strong>Uniform-PAC</strong> bound. We won’t discuss these in detail here; see the bibliographic notes for more details.</p>
</div>
<p>We’d like to achieve <strong>sublinear regret</strong> in expectation:</p>
<p><span id="eq-sublinear-regret"><span class="math display">\[
\mathop{\mathbb{E}}[\text{Regret}_T] = o(T).
\tag{4.6}\]</span></span></p>
<p>(See <a href="background.html#sec-bg-o-notation" class="quarto-xref"><span>Section A.1</span></a> if you’re unfamiliar with big-oh notation.) An algorithm with sublinear regret will eventually do as well as the oracle strategy as <span class="math inline">\(T\to \infty\)</span>.</p>
<div id="rem-sublinear-issues" class="proof remark">
<p><span class="proof-title"><em>Remark 4.4</em> (Interpreting sublinear regret). </span>An algorithm with sublinear regret in expectation doesn’t necessarily always choose the correct arm. In fact, such an algorithm can still choose a suboptimal arm infinitely many times. Consider a two-armed Bernoulli bandit with <span class="math inline">\(\mu^0 = 0\)</span> and <span class="math inline">\(\mu^1 = 1\)</span>. An algorithm that chooses arm <span class="math inline">\(0\)</span> <span class="math inline">\(\sqrt{T}\)</span> times out of <span class="math inline">\(T\)</span> still achieves <span class="math inline">\(O(\sqrt{T})\)</span> regret in expectation.</p>
<p>Also, sublinear regret measures the <em>asymptotic</em> performance of the algorithm as <span class="math inline">\(T\to \infty\)</span>. It doesn’t tell us about the <em>rate</em> at which the algorithm approaches optimality.</p>
</div>
<p>In the remaining sections, we’ll walk through a series of MAB algorithms and see how they make different tradeoffs between exploration and exploitation.</p>
<div id="dd9819e6" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_strategy(mab: MAB, agent: Agent):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">2</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot reward and cumulative regret</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label<span class="op">=</span><span class="st">"reward"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    cum_regret <span class="op">=</span> np.cumsum(regret_per_step(mab, agent))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(mab.T), cum_regret, label<span class="op">=</span><span class="st">"cumulative regret"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># draw colored circles for arm choices</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    color_array <span class="op">=</span> [colors[k] <span class="cf">for</span> k <span class="kw">in</span> agent.choices]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c<span class="op">=</span>color_array)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add legend entries for each arm</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, color <span class="kw">in</span> <span class="bu">enumerate</span>(colors):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        plt.scatter([], [], c<span class="op">=</span>color, label<span class="op">=</span><span class="ss">f'arm </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels and title</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Pull index"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="sec-pure-exploration" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-pure-exploration"><span class="header-section-number">4.3</span> Pure exploration</h2>
<p>A trivial strategy is to always choose arms at random (i.e.&nbsp;“pure exploration”).</p>
<div id="def-pure-exploration" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4 (Pure exploration)</strong></span> On the <span class="math inline">\(t\)</span>th pull, the arm <span class="math inline">\(a_t\)</span> is sampled uniformly at random from all arms:</p>
<p><span id="eq-pure-exploration"><span class="math display">\[
a_t\sim \text{Unif}([K])
\tag{4.7}\]</span></span></p>
</div>
<div id="79911aae" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PureExploration(Agent):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose an arm uniformly at random."""</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.pure_exploration_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="thm-pure-exploration-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 (Pure exploration expected regret)</strong></span> The pure exploration strategy achieves regret <span class="math inline">\(\Theta(T)\)</span> in expectation.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By choosing arms uniformly at random,</p>
<p><span id="eq-pure-exploration-1"><span class="math display">\[
\mathop{\mathbb{E}}_{a_t\sim \text{Unif}([K])}[\mu^{a_t}]
=
\bar \mu
:=
\frac{1}{K} \sum_{k=0}^{K-1} \mu^k
\tag{4.8}\]</span></span></p>
<p>so the expected regret is simply</p>
<p><span id="eq-pure-exploration-2"><span class="math display">\[
\begin{aligned}
    \mathop{\mathbb{E}}[\text{Regret}_T]
    &amp;= \sum_{t= 0}^{T-1} \mathop{\mathbb{E}}[\mu^\star - \mu^{a_t}]
    \\
    &amp;= T\cdot (\mu^\star - \bar \mu).
\end{aligned}
\tag{4.9}\]</span></span></p>
<p>This scales as <span class="math inline">\(\Theta(T)\)</span> in expectation, i.e.&nbsp;<em>linear</em> in the number of pulls <span class="math inline">\(T\)</span>.</p>
</div>
<div id="cell-fig-pure-exploration" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> PureExploration(mab.K, mab.T)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pure-exploration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pure-exploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-pure-exploration-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pure-exploration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Reward and cumulative regret of the pure exploration algorithm.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Pure exploration doesn’t use any information about the environment to improve its strategy. The distribution over its arm choices always appears “(uniformly) random”.</p>
</section>
<section id="sec-pure-greedy" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-pure-greedy"><span class="header-section-number">4.4</span> Pure greedy</h2>
<p>How might we improve on pure exploration? Instead, we could try each arm once, and then commit to the one with the highest observed reward. We’ll call this the <strong>pure greedy</strong> strategy.</p>
<div id="def-pure-greedy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.5 (Pure greedy)</strong></span> On the <span class="math inline">\(t\)</span>th pull, the arm <span class="math inline">\(a_t\)</span> is chosen according to</p>
<p><span id="eq-pure-greedy"><span class="math display">\[
a_t:= \begin{cases}
t&amp; t&lt; K\\
\arg\max_{k\in [K]} r_k&amp; t\ge K
\end{cases}
\tag{4.10}\]</span></span></p>
<p>where <span class="math inline">\(r_t\)</span> denotes the reward obtained on the <span class="math inline">\(t\)</span>th pull.</p>
</div>
<div id="e1a363a1" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PureGreedy(Agent):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose the arm with the highest observed reward on its first pull."""</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.pure_greedy_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>How does the expected regret of this strategy compare to that of pure exploration? We’ll do a more general analysis in the following section. Now, for intuition, suppose there’s just <span class="math inline">\(K=2\)</span> arms with means <span class="math inline">\(\mu^1 &gt; \mu^0\)</span>. If <span class="math inline">\(r_1 &gt; r_0\)</span>, then we repeatedly pull arm <span class="math inline">\(1\)</span> and achieve zero regret per pull. If <span class="math inline">\(r_0 &gt; r_1\)</span>, then we repeatedly pull arm <span class="math inline">\(0\)</span> and incur <span class="math inline">\(\mu^1 - \mu^0\)</span> regret per pull. Accounting for the first two pulls and the case of a tie, we obtain the following lower bound:</p>
<p><span id="eq-pure-greedy-regret"><span class="math display">\[
\mathop{\mathbb{E}}[\text{Regret}_T] \ge \mathbb{P}(r^0 &gt; r^1) T\cdot (\mu^1 - \mu^0)
\tag{4.11}\]</span></span></p>
<p>This is still <span class="math inline">\(\Theta(T)\)</span>, the same as pure exploration!</p>
<div id="cell-fig-pure-greedy" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> PureGreedy(mab.K, mab.T)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pure-greedy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pure-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-pure-greedy-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pure-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Reward and cumulative regret of the pure greedy algorithm.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, we see in <a href="#fig-pure-greedy" class="quarto-xref">fig.&nbsp;<span>4.3</span></a> that the greedy algorithm can get lucky on the first set of pulls and act entirely optimally for that experiment! But its <em>average</em> regret is still linear, since the chance of sticking with a suboptimal arm is nonzero.</p>
</section>
<section id="sec-etc" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-etc"><span class="header-section-number">4.5</span> Explore-then-commit</h2>
<p>We can improve the pure greedy algorithm (<a href="#def-pure-greedy" class="quarto-xref">def.&nbsp;<span>4.5</span></a>) as follows: let’s reduce the variance of the reward estimates by pulling each arm <span class="math inline">\(N_\text{explore} \ge 1\)</span> times before committing. This is called the <strong>explore-then-commit</strong> strategy.</p>
<div id="def-etc" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.6 (Explore-then-commit)</strong></span> The explore-then-commit algorithm first pulls each arm <span class="math inline">\(N_\text{explore}\)</span> times. Let</p>
<p><span id="eq-etc-average"><span class="math display">\[
\hat \mu^k:= \frac{1}{N_\text{explore}} \sum_{n=0}^{N_\text{explore}-1} r_{kN_\text{explore} + n}
\tag{4.12}\]</span></span></p>
<p>denote the empirical mean reward of arm <span class="math inline">\(k\)</span> during these pulls. Afterwards, on the <span class="math inline">\(t\)</span>th pull (for <span class="math inline">\(t\ge N_\text{explore} K\)</span>), it chooses</p>
<p><span id="eq-etc"><span class="math display">\[
a_t:= \arg\max_{k\in [K]} \hat \mu^k.
\tag{4.13}\]</span></span></p>
</div>
<p>Note that the “pure greedy” strategy above is just the special case where <span class="math inline">\(N_\text{explore} = 1\)</span>.</p>
<div id="6994205b" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExploreThenCommit(Agent):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, N_explore: <span class="bu">int</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N_explore <span class="op">=</span> N_explore</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.etc_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-etc" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ExploreThenCommit(mab.K, mab.T, mab.T <span class="op">//</span> <span class="dv">15</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-etc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-etc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-etc-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-etc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Reward and cumulative regret of the explore-then-commit algorithm
</figcaption>
</figure>
</div>
</div>
</div>
<p>This algorithm finds the true optimal arm more frequently than the pure greedy strategy. We would expect ETC to then have a lower expected regret. We will show that ETC achieves the following regret bound:</p>
<div id="thm-etc-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 (ETC high-probability regret bound)</strong></span> The regret of ETC satisfies, with high probability,</p>
<p><span id="eq-etc-regret"><span class="math display">\[
\text{Regret}_T= \widetilde{O}(T^{2/3} K^{1/3}),
\tag{4.14}\]</span></span></p>
<p>where the tilde means we ignore logarithmic factors.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up into the exploration and exploitation phases.</p>
<p><strong>Exploration phase</strong>. This phase takes <span class="math inline">\(N_\text{explore} K\)</span> pulls. Since at each step we incur at most <span class="math inline">\(1\)</span> regret, the total regret is at most <span class="math inline">\(N_\text{explore} K\)</span>.</p>
<p><strong>Exploitation phase</strong>. This will take a bit more effort. We’ll prove that for any total time <span class="math inline">\(T\)</span>, we can choose <span class="math inline">\(N_\text{explore}\)</span> such that with arbitrarily high probability, the regret is sublinear.</p>
<p>Let <span class="math inline">\(\widehat{k}\)</span> denote the arm chosen after the exploration phase. We know the regret from the exploitation phase is</p>
<p><span id="eq-etc-exploit-regret"><span class="math display">\[
T_\text{exploit} (\mu^\star - \mu^{\widehat{k}}) \qquad \text{where} \qquad T_\text{exploit} := T- N_\text{explore} K.
\tag{4.15}\]</span></span></p>
<p>So we’d need <span class="math inline">\(\mu^\star - \mu^{\widehat{k}} \to 0\)</span> as <span class="math inline">\(T\to \infty\)</span> in order to achieve sublinear regret. How can we do this?</p>
<p>Let’s define <span class="math inline">\(\Delta^k:= \hat \mu^k- \mu^k\)</span> to denote how far the mean estimate for arm <span class="math inline">\(k\)</span> is from the true mean. How can we bound this quantity? We’ll use the following useful inequality for i.i.d. bounded random variables:</p>
<div id="thm-hoeffding" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.3 (Hoeffding’s inequality)</strong></span> Let <span class="math inline">\(X_0, \dots, X_{N-1}\)</span> be i.i.d. random variables with <span class="math inline">\(X_n \in [0, 1]\)</span> almost surely for each <span class="math inline">\(n \in [N]\)</span>. Then for any <span class="math inline">\(\delta &gt; 0\)</span>,</p>
<p><span id="eq-hoeffding"><span class="math display">\[
\mathbb{P}\left(
    \left| \frac{1}{N} \sum_{n=1}^N (X_n - \mathop{\mathbb{E}}[X_n]) \right| &gt; \sqrt{\frac{\ln(2/\delta)}{2N}}
\right) \le \delta.
\tag{4.16}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of this inequality is beyond the scope of this book. See <span class="citation" data-cites="vershynin_high-dimensional_2018">Vershynin (<a href="references.html#ref-vershynin_high-dimensional_2018" role="doc-biblioref">2018</a>, ch.&nbsp;2.2)</span>.</p>
</div>
<p>We can apply this directly to the rewards for a given arm <span class="math inline">\(k\)</span>, since the rewards from that arm are i.i.d.:</p>
<p><span id="eq-hoeffding-etc"><span class="math display">\[
\mathbb{P}\left(|\Delta^k| &gt; \sqrt{
    \frac{\ln(2/\delta)}{2N_\text{explore}}
} \right) \le \delta.
\tag{4.17}\]</span></span></p>
<p>This can be thought of as a <span class="math inline">\(1-\delta\)</span> confidence interval for the true arm mean <span class="math inline">\(\mu^k\)</span>. But we can’t apply this to arm <span class="math inline">\(\widehat{k}\)</span> directly since <span class="math inline">\(\widehat{k}\)</span> is itself a random variable. Instead, we need to bound the error across all the arms simultaneously, so that the resulting bound will apply <em>no matter what</em> <span class="math inline">\(\widehat{k}\)</span> “crystallizes” to. The <strong>union bound</strong> provides a simple way to do this. (See <a href="background.html#thm-union-bound" class="quarto-xref">Theorem&nbsp;<span>A.1</span></a> for a review.)</p>
<p>In our case, we have one event per arm, stating that the CI for that arm contains the true mean. The probability that <em>all</em> of the CIs are accurate is then</p>
<p><span id="eq-hoeffding-union"><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left(
        \forall k\in [K] : |\Delta^k|
        \le
        \sqrt{\frac{\ln(2/\delta)}{2N_\text{explore}}}
    \right) &amp;\ge 1- K\delta.
\end{aligned}
\tag{4.18}\]</span></span></p>
<p>This lower-bounds the probability that the CI for arm <span class="math inline">\(\widehat{k}\)</span> is accurate. Let us return to our original task of bounding <span class="math inline">\(\mu^\star - \mu^{\widehat{k}}\)</span>. We apply the useful trick of “adding zero”:</p>
<p><span id="eq-hoeffding-trick"><span class="math display">\[
\begin{aligned}
    \mu^{k^\star} - \mu^{\widehat{k}} &amp;= \mu^{k^\star} - \mu^{\widehat{k}} + (\hat \mu^{k^\star} - \hat \mu^{k^\star}) + (\hat \mu^{\widehat{k}} - \hat \mu^{\widehat{k}}) \\
    &amp;= \Delta^{\widehat{k}} - \Delta^{k^\star} + \underbrace{(\hat \mu^{k^\star} - \hat \mu^{\widehat{k}})}_{\le 0 \text{ by definition of } \widehat{k}} \\
    &amp;\le 2 \sqrt{\frac{\ln(2 K/ \delta')}{2N_\text{explore}}} \text{ with probability at least } 1-\delta'
\end{aligned}
\tag{4.19}\]</span></span></p>
<p>where we’ve set <span class="math inline">\(\delta' := K\delta\)</span>. Putting this all together, we’ve shown that, with probability <span class="math inline">\(1 - \delta'\)</span>,</p>
<p><span id="eq-etc-regret-1"><span class="math display">\[
\text{Regret}_T\le N_\text{explore} K+ T_\text{exploit} \cdot \sqrt{\frac{2\ln(2 K/\delta')}{N_\text{explore}}}.
\tag{4.20}\]</span></span></p>
<p>Note that it suffices for <span class="math inline">\(N_\text{explore}\)</span> to be on the order of <span class="math inline">\(\sqrt{T}\)</span> to achieve sublinear regret. In particular, we can find the optimal <span class="math inline">\(N_\text{explore}\)</span> by setting the derivative of <a href="#eq-etc-regret-1" class="quarto-xref">eq.&nbsp;<span>4.20</span></a> with respect to <span class="math inline">\(N_\text{explore}\)</span> to zero:</p>
<p><span id="eq-etc-regret-2"><span class="math display">\[
\begin{aligned}
    0 &amp;= K- T_\text{exploit} \cdot \frac{1}{2} \sqrt{\frac{2\ln(2 K/\delta')}{N_\text{explore}^3}} \\
    N_\text{explore} &amp;= \left(
        T_\text{exploit} \cdot \frac{\sqrt{\ln(2 K/ \delta')/2}}{ K}
    \right)^{2/3}
\end{aligned}
\tag{4.21}\]</span></span></p>
<p>Plugging this into the expression for the regret, we have (still with probability <span class="math inline">\(1-\delta'\)</span>):</p>
<p><span id="eq-etc-bound"><span class="math display">\[
\begin{aligned}
    \text{Regret}_T&amp;\le 3 T^{2/3} \sqrt[3]{K\ln(2 K/\delta') / 2} \\
    &amp;= \widetilde{O}(T^{2/3} K^{1/3}),
\end{aligned}
\tag{4.22}\]</span></span></p>
<p>satisfying <a href="#thm-etc-regret" class="quarto-xref">Theorem&nbsp;<span>4.2</span></a>.</p>
</div>
<p>The ETC algorithm is rather “abrupt” in that it switches from exploration to exploitation after a fixed number of pulls. What if the total number of pulls <span class="math inline">\(T\)</span> isn’t known in advance? We’ll need to use a more gradual transition, which brings us to the <em>epsilon-greedy</em> algorithm.</p>
</section>
<section id="sec-epsilon-greedy" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="sec-epsilon-greedy"><span class="header-section-number">4.6</span> Epsilon-greedy</h2>
<p>Instead of doing all of the exploration and then all of the exploitation in separate phases, which requires knowing the time horizon beforehand, we can instead <em>interleave</em> exploration and exploitation by, at each pull, choosing a random action with some probability. We call this the <strong>epsilon-greedy</strong> algorithm.</p>
<div id="def-epsilon-greedy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.7 (Epsilon-greedy)</strong></span> Let <span class="math inline">\(N^k_t\)</span> denote the number of times that arm <span class="math inline">\(k\)</span> is pulled within the first <span class="math inline">\(t\)</span> overall pulls, and let <span class="math inline">\(\hat \mu^k_t\)</span> denote the sample mean of the corresponding rewards:</p>
<p><span id="eq-bandits-count"><span class="math display">\[
\begin{aligned}
    N^k_t&amp;:= \sum_{t' = 0}^{t-1} \mathbf{1} \{ a_{t'} = k\}, \\
    \hat \mu^k_t&amp;:= \frac{1}{N^k_t} \sum_{t' = 0}^{t-1} \mathbf{1} \{ a_{t'} = k\} r_{t'}. \\
\end{aligned}
\tag{4.23}\]</span></span></p>
<p>On the <span class="math inline">\(t\)</span>th pull, the arm <span class="math inline">\(a_t\)</span> is chosen according to</p>
<p><span id="eq-epsilon-greedy"><span class="math display">\[
a_t:= \begin{cases}
\text{sample from } \text{Unif}([K]) &amp; \text{with probability } \epsilon_t\\
\arg\max_{k\in [K]} \hat \mu^k_t&amp; \text{with probability } 1 - \epsilon_t,
\end{cases}
\tag{4.24}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon_t\in [0, 1]\)</span> is the probability of choosing a random action.</p>
</div>
<div id="e14c59d4" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EpsilonGreedy(Agent):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        K: <span class="bu">int</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        T: <span class="bu">int</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        ε_array: Float[Array, <span class="st">" T"</span>],</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ε_array <span class="op">=</span> ε_array</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.epsilon_greedy_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-epsilon-greedy" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> EpsilonGreedy(mab.K, mab.T, np.full(mab.T, <span class="fl">0.1</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-epsilon-greedy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-epsilon-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-epsilon-greedy-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-epsilon-greedy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Reward and cumulative regret of the epsilon-greedy algorithm
</figcaption>
</figure>
</div>
</div>
</div>
<p>We let the exploration probability <span class="math inline">\(\epsilon_t\)</span> vary over time. This lets us gradually <em>decrease</em> <span class="math inline">\(\epsilon_t\)</span> as we learn more about the reward distributions and no longer need to explore as much.</p>
<div id="exr-constant-eps" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.2 (Regret for constant epsilon)</strong></span> What is the asymptotic expected regret of the algorithm if we set <span class="math inline">\(\epsilon\)</span> to be a constant? Is this different from pure exploration (<a href="#def-pure-exploration" class="quarto-xref">def.&nbsp;<span>4.4</span></a>)?</p>
</div>
<div id="thm-epsilon-greedy-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.4 (Epsilon-greedy expected regret)</strong></span> Let <span class="math inline">\(\epsilon_t:= \sqrt[3]{K\ln(t) / t}\)</span>. Then the epsilon-greedy achieves a regret of <span class="math inline">\(\widetilde O(t^{2/3} K^{1/3})\)</span> in expectation (ignoring logarithmic factors).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We won’t prove this here. See <span class="citation" data-cites="agarwal_reinforcement_2022">Agarwal et al. (<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">2022</a>)</span> for a proof.</p>
</div>
<p>In ETC, we had to set <span class="math inline">\(N_\text{explore}\)</span> based on the total number of pulls <span class="math inline">\(T\)</span>. But the epsilon-greedy algorithm handles the exploration incrementally: the regret rate holds for <em>any</em> <span class="math inline">\(t\)</span>, and doesn’t depend on the total number of pulls <span class="math inline">\(T\)</span>.</p>
<p>But the way epsilon-greedy and ETC explore is rather naive: they explore <em>uniformly</em> across all the arms. But what if we could be smarter about it, and explore <em>more</em> for arms that we’re less certain about?</p>
</section>
<section id="sec-ucb" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="sec-ucb"><span class="header-section-number">4.7</span> Upper Confidence Bound (UCB)</h2>
<p>The upper confidence bound algorithm is the first strategy we study that explores <em>adaptively</em>: it identifies arms it is less certain about, and explicitly trades off between learning more about these and exploiting arms that already seem good.</p>
<p>To quantify how <em>certain</em> we are about the mean of each arm, UCB computes statistical <em>confidence intervals</em> (CIs) for the arm means, and then chooses the arm with the highest <em>upper confidence bound</em> (hence its name). Concretely, after <span class="math inline">\(t\)</span> pulls, we compute upper confidence bounds <span class="math inline">\(M^k_t\)</span> for each arm <span class="math inline">\(k\)</span> such that <span class="math inline">\(\mu^k\le M^k_t\)</span> with high probability, and select <span class="math inline">\(a_t:= \arg \max_{k\in [K]} M^k_t\)</span>. This operates on the principle of <strong>the benefit of the doubt (i.e.&nbsp;optimism in the face of uncertainty)</strong>: we’ll choose the arm that we’re most <em>optimistic</em> about.</p>
<div id="cell-fig-ucb-example" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>lower_bounds <span class="op">=</span> [<span class="op">-</span><span class="fl">0.4745434</span>, <span class="fl">0.2002283</span>, <span class="fl">0.04175852</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>upper_bounds <span class="op">=</span> [<span class="fl">0.9745434</span>, <span class="fl">0.9247717</span>, <span class="fl">0.95824148</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>whiskers <span class="op">=</span> np.array([lower_bounds, upper_bounds])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(means)):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    ax.plot([i, i], [whiskers[<span class="dv">0</span>, i], whiskers[<span class="dv">1</span>, i]], color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    ax.plot([i <span class="op">-</span> <span class="fl">0.1</span>, i <span class="op">+</span> <span class="fl">0.1</span>], [whiskers[<span class="dv">0</span>, i], whiskers[<span class="dv">0</span>, i]], color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    ax.plot([i <span class="op">-</span> <span class="fl">0.1</span>, i <span class="op">+</span> <span class="fl">0.1</span>], [whiskers[<span class="dv">1</span>, i], whiskers[<span class="dv">1</span>, i]], color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="bu">range</span>(<span class="bu">len</span>(means)), means, color<span class="op">=</span><span class="st">'red'</span>, zorder<span class="op">=</span><span class="dv">5</span>, label<span class="op">=</span><span class="st">"$\mu^k$"</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(<span class="bu">range</span>(<span class="bu">len</span>(means)))</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>ax.set_xticklabels([<span class="ss">f'Arm </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)])</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r'Mean reward'</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>fig.legend()</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-ucb-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ucb-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-ucb-example-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ucb-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Visualization of confidence intervals for the arm means. UCB would choose arm <span class="math inline">\(0\)</span> since it has the highest upper confidence bound.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="thm-ucb-ci" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.5 (Confidence intervals for arm means)</strong></span> As in our definition of the epsilon-greedy algorithm (<a href="#def-epsilon-greedy" class="quarto-xref">def.&nbsp;<span>4.7</span></a>), let <span class="math inline">\(N^k_t\)</span> denote the number of times we pull arm <span class="math inline">\(k\)</span> within the first <span class="math inline">\(t\)</span> pulls, and let <span class="math inline">\(\hat \mu^k_t\)</span> denote the sample mean of the corresponding rewards. Then with probability at least <span class="math inline">\(1 - \delta\)</span>, for all <span class="math inline">\(t\ge K\)</span>,</p>
<p><span id="eq-ucb-ci"><span class="math display">\[
\mu^k\in \left[
    \hat \mu^k_t- \frac{1}{\sqrt{N^k_t}} \cdot \sqrt{
    \frac{\ln(2 t/\delta)}{2}
    },
    \hat \mu^k_t+ \frac{1}{\sqrt{N^k_t}} \cdot \sqrt{
    \frac{\ln(2 t/\delta)}{2}
    }
\right].
\tag{4.25}\]</span></span></p>
<p>We assume that during the first <span class="math inline">\(K\)</span> pulls, we pull each arm once.</p>
</div>
<div id="def-ucb" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.8 (Upper confidence bound algorithm <span class="citation" data-cites="auer_using_2002">(<a href="references.html#ref-auer_using_2002" role="doc-biblioref">Auer, 2002</a>)</span>)</strong></span> The UCB algorithm first pulls each arm once. Then it chooses the <span class="math inline">\(t\)</span>th pull (for <span class="math inline">\(t\ge K\)</span>) by taking</p>
<p><span id="eq-ucb-bound"><span class="math display">\[
a_t:= \arg\max_{k\in [K]} \hat \mu^k_t+ \sqrt{
    \frac{\ln(2 t/\delta)}{2 N^k_t}
},
\tag{4.26}\]</span></span></p>
<p>where <span class="math inline">\(\delta \in (0, 1)\)</span> is a parameter that controls the width of the confidence interval.</p>
</div>
<p>Intuitively, UCB prioritizes arms where:</p>
<ol type="1">
<li><span class="math inline">\(\hat \mu^k_t\)</span> is large, i.e.&nbsp;the arm’s corresponding sample mean is high, and we’d choose it for <em>exploitation</em>, and</li>
<li><span class="math inline">\(\sqrt{\frac{\ln(2 t/\delta)}{2N^k_t}}\)</span> is large, i.e.&nbsp;<span class="math inline">\(N^k_t\)</span> is small and&nbsp;we’re still uncertain about the arm, and we’d choose it for <em>exploration</em>.</li>
</ol>
<p><span class="math inline">\(\delta\)</span> is formally the coverage probability of the confidence interval, but we can also treat it as a parameter that trades off between exploration and exploitation:</p>
<ul>
<li>A smaller <span class="math inline">\(\delta\)</span> would give us a larger interval, emphasizing the exploration term.</li>
<li>A larger <span class="math inline">\(\delta\)</span> would give a tighter interval, prioritizing the current sample means.</li>
</ul>
<p>We now prove <a href="#thm-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.5</span></a>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Our proof is similar to that in <a href="#thm-etc-regret" class="quarto-xref">Theorem&nbsp;<span>4.2</span></a>: we use Hoeffding’s inequality to construct a CI for the mean of a bounded random variable (i.e.&nbsp;the rewards from a given arm). However, we must be careful, since the number of samples from arm <span class="math inline">\(k\)</span> up to time <span class="math inline">\(t\)</span>, <span class="math inline">\(N^k_t\)</span>, is a random variable.</p>
<p>Hoeffding’s inequality (<a href="#thm-hoeffding" class="quarto-xref">Theorem&nbsp;<span>4.3</span></a>) tells us that, for <span class="math inline">\(N\)</span> i.i.d. samples <span class="math inline">\(\widetilde r_0, \dots, \widetilde r_{N-1}\)</span> from arm <span class="math inline">\(k\)</span>,</p>
<p><span id="eq-hoeffding-ucb"><span class="math display">\[
| \widetilde \mu^k_N - \mu^k| \le \sqrt{
    \frac{\ln( 2 / \delta )}{2 N}
}
\tag{4.27}\]</span></span></p>
<p>with probability at least <span class="math inline">\(1 - \delta\)</span>, where <span class="math inline">\(\widetilde \mu^k_N = \frac{1}{N} \sum_{n=0}^{N-1} \widetilde r_n\)</span>. The union bound then tells us that with probability at least <span class="math inline">\(1 - \delta'\)</span>, for all <span class="math inline">\(N = 1, \dots, t\)</span>,</p>
<p><span id="eq-ucb-union"><span class="math display">\[
\forall N = 1, \dots, t:
| \widetilde \mu^k_N - \mu^k| \le \sqrt{
    \frac{\ln( 2 t/ \delta' )}{2 N}
}.
\tag{4.28}\]</span></span></p>
<p>Since <span class="math inline">\(N^k_t\in \{ 1, \dots, t\}\)</span> (we assume each arm is pulled once at the start), <a href="#eq-ucb-union" class="quarto-xref">eq.&nbsp;<span>4.28</span></a> implies that</p>
<p><span id="eq-ucb-specific-arm"><span class="math display">\[
| \widetilde \mu^k_{N^k_t} - \mu^k| \le \sqrt{
    \frac{\ln( 2 t/ \delta' )}{N}
}
\tag{4.29}\]</span></span></p>
<p>with probability at least <span class="math inline">\(1 - \delta'\)</span> as well. But notice that <span class="math inline">\(\widetilde \mu^k_{N^k_t}\)</span> is identically distributed to <span class="math inline">\(\hat \mu^k_t\)</span>: both refer to the sample mean of the first <span class="math inline">\(N^k_t\)</span> pulls from arm <span class="math inline">\(k\)</span>. This gives the bound in <a href="#thm-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.5</span></a> (after relabelling <span class="math inline">\(\delta' \mapsto \delta\)</span>).</p>
</div>
<div id="c66ac5b2" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UCB(Agent):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, delta: <span class="bu">float</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta <span class="op">=</span> delta</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.ucb_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-ucb" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> UCB(mab.K, mab.T, <span class="fl">0.9</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-ucb" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ucb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-ucb-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ucb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.7: Reward and cumulative regret of the upper confidence bound algorithm
</figcaption>
</figure>
</div>
</div>
</div>
<p>As desired, this explores in a smarter, <em>adaptive</em> way compared to the previous algorithms. Does it achieve lower regret?</p>
<div id="thm-ucb-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.6 (UCB high-probability regret bound)</strong></span> With probability <span class="math inline">\(1-\delta''\)</span>,</p>
<p><span id="eq-ucb-regret"><span class="math display">\[
\text{Regret}_T= \widetilde O(K\sqrt{T})
\tag{4.30}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>First we’ll bound the regret incurred at each timestep. Then we’ll bound the <em>total</em> regret across timesteps.</p>
<p>For the sake of analysis, we’ll use a slightly looser bound that applies across the whole time horizon and across all arms. We’ll omit the derivation since it’s very similar to the above (walk through it yourself for practice).</p>
<p><span id="eq-ucb-regret-1"><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left(\forall k\le K, t&lt; T: |\hat \mu^k_t- \mu^k| \le B^k_t\right) &amp;\ge 1-\delta'' \\
    \text{where} \quad B^k_t&amp;:= \sqrt{\frac{\ln(2 TK/\delta'')}{2 N^k_t}}.
\end{aligned}
\tag{4.31}\]</span></span></p>
<p>Intuitively, <span class="math inline">\(B^k_t\)</span> denotes the <em>width</em> of the CI for arm <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>. Then, assuming the above uniform bound holds (which occurs with probability <span class="math inline">\(1-\delta''\)</span>), we can bound the regret at each timestep as follows:</p>
<p><span id="eq-ucb-regret-2"><span class="math display">\[
\begin{aligned}
    \mu^\star - \mu^{a_t} &amp;\le \hat \mu^{k^\star}_t+ B_t^{k^\star} - \mu^{a_t} &amp;&amp; \text{applying UCB to arm } k^\star \\
    &amp;\le \hat \mu^{a_t}_t+ B^{a_t}_t- \mu^{a_t} &amp;&amp; \text{since UCB chooses } a_t= \arg \max_{k\in [K]} \hat \mu^k_t+ B_t^{k} \\
    &amp;\le 2 B^{a_t}_t&amp;&amp; \text{since } \hat \mu^{a_t}_t- \mu^{a_t} \le B^{a_t}_t\text{ by definition of } B^{a_t}_t\\
\end{aligned}
\tag{4.32}\]</span></span></p>
<p>Summing this across timesteps gives</p>
<p><span class="math display">\[
\begin{aligned}
    \text{Regret}_T&amp;\le \sum_{t= 0}^{T-1} 2 B^{a_t}_t\\
    &amp;= \sqrt{2\ln(2 TK/\delta'')} \sum_{t= 0}^{T-1} (N^{a_t}_t)^{-1/2} \\
    \sum_{t= 0}^{T-1} (N^{a_t}_t)^{-1/2} &amp;= \sum_{t= 0}^{T-1} \sum_{k=0}^{K-1} \mathbf{1}\{ a_t= k \} (N^k_t)^{-1/2} \\
    &amp;= \sum_{k=0}^{K-1} \sum_{n=1}^{N_T^k} n^{-1/2} \\
    &amp;\le K \sum_{n=1}^Tn^{-1/2} \\
    \sum_{n=1}^Tn^{-1/2} &amp;\le 1 + \int_1^Tx^{-1/2} \ \mathrm{d}x \\
    &amp;= 1 + (2 \sqrt{x})_1^T\\
    &amp;= 2 \sqrt{T} - 1 \\
    &amp;\le 2 \sqrt{T} \\
\end{aligned}
\]</span></p>
<p>Putting everything together gives, with probability <span class="math inline">\(1-\delta''\)</span>,</p>
<p><span id="eq-ucb-regret-final"><span class="math display">\[
\begin{aligned}
    \text{Regret}_T&amp;\le 2 K\sqrt{2 T\ln(2 TK/\delta'')} \\
    &amp;= \widetilde O(K\sqrt{T}),
\end{aligned}
\tag{4.33}\]</span></span></p>
<p>as in <a href="#thm-ucb-regret" class="quarto-xref">Theorem&nbsp;<span>4.6</span></a>.</p>
</div>
<p>In fact, we can do a more sophisticated analysis to trim off a factor of <span class="math inline">\(\sqrt{K}\)</span> and show <span class="math inline">\(\text{Regret}_T= \widetilde O(\sqrt{TK})\)</span>.</p>
<section id="lower-bound-on-regret-intuition" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="lower-bound-on-regret-intuition"><span class="header-section-number">4.7.1</span> Lower bound on regret (intuition)</h3>
<p>Is it possible to do better than <span class="math inline">\(\Omega(\sqrt{T})\)</span> in general? In fact, no! We can show that any algorithm must incur <span class="math inline">\(\Omega(\sqrt{T})\)</span> regret in the worst case. We won’t rigorously prove this here, but the intuition is as follows.</p>
<p>The Central Limit Theorem tells us that with <span class="math inline">\(T\)</span> i.i.d. samples from some distribution, we can only learn the mean of the distribution to within <span class="math inline">\(\Omega(1/\sqrt{T})\)</span> (the standard deviation). Then, since we get <span class="math inline">\(T\)</span> samples spread out across the arms, we can only learn each arm’s mean to an even looser degree.</p>
<p>That is, if two arms have means that are within about <span class="math inline">\(1/\sqrt{T}\)</span>, we won’t be able to confidently tell them apart, and will sample them about equally. But then we’ll incur regret <span id="eq-regret-lower-bound"><span class="math display">\[
\Omega((T/2) \cdot (1/\sqrt{T})) = \Omega(\sqrt{T}).
\tag{4.34}\]</span></span></p>
</section>
</section>
<section id="sec-thompson-sampling" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="sec-thompson-sampling"><span class="header-section-number">4.8</span> Thompson sampling and Bayesian bandits</h2>
<p>So far, we’ve treated the parameters <span class="math inline">\(\mu^0, \dots, \mu^{K-1}\)</span> of the reward distributions as <em>fixed</em>. Instead, we can take a <strong>Bayesian</strong> approach where we treat them as random variables from some <strong>prior distribution</strong>. Then, upon pulling an arm and observing a reward, we can simply <em>condition</em> on this observation to exactly describe the <strong>posterior distribution</strong> over the parameters. This fully describes the information we gain about the parameters from observing the reward.</p>
<p>From this Bayesian perspective, the <strong>Thompson sampling</strong> algorithm follows naturally: just sample from the distribution of the optimal arm, given the observations!</p>
<div id="0c3b70f3" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Distribution:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" K"</span>]:</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample a vector of means for the K arms."""</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">float</span>):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Condition on obtaining `reward` from the given arm."""</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="5af5d3c9" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ThompsonSampling(Agent):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, prior: Distribution):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distribution <span class="op">=</span> prior</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        means <span class="op">=</span> <span class="va">self</span>.distribution.sample()</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random_argmax(means)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().update_history(arm, reward)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distribution.update(arm, reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In other words, we sample each arm proportionally to how likely we think it is to be optimal, given the observations so far. This strikes a good exploration-exploitation tradeoff: we explore more for arms that we’re less certain about, and exploit more for arms that we’re more certain about. Thompson sampling is a simple yet powerful algorithm that achieves state-of-the-art performance in many settings.</p>
<div id="exm-bayesian-bernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4 (Bayesian Bernoulli bandit)</strong></span> We’ve been working in the Bernoulli bandit setting, where arm <span class="math inline">\(k\)</span> yields a reward of <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\mu^k\)</span> and no reward otherwise. The vector of success probabilities <span class="math inline">\(\boldsymbol{\mu} = (\mu^0, \dots, \mu^{K-1})\)</span> thus describes the entire MAB.</p>
<p>Under the Bayesian perspective, we think of <span class="math inline">\(\boldsymbol{\mu}\)</span> as a <em>random</em> vector drawn from some prior distribution <span class="math inline">\(p \in \triangle([0, 1]^K)\)</span>. For example, we might have <span class="math inline">\(p\)</span> be the Uniform distribution over the unit hypercube <span class="math inline">\([0, 1]^K\)</span>, that is,</p>
<p><span id="eq-hypercube-uniform"><span class="math display">\[
p(\boldsymbol{\mu}) = \begin{cases}
    1 &amp; \text{if } \boldsymbol{\mu}\in [0, 1]^K\\
    0 &amp; \text{otherwise}
\end{cases}
\tag{4.35}\]</span></span></p>
<p>In this case, upon viewing some reward, we can exactly calculate the <strong>posterior</strong> distribution of <span class="math inline">\(\boldsymbol{\mu}\)</span> using Bayes’s rule (i.e.&nbsp;the definition of conditional probability):</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}(\boldsymbol{\mu} \mid a_0, r_0) &amp;\propto \mathbb{P}(r_0 \mid a_0, \boldsymbol{\mu}) \mathbb{P}(a_0 \mid \boldsymbol{\mu}) \mathbb{P}(\boldsymbol{\mu}) \\
    &amp;\propto (\mu^{a_0})^{r_0} (1 - \mu^{a_0})^{1-r_0}.
\end{aligned}
\]</span></p>
<p>This is the PDF of the <span class="math inline">\(\text{Beta}(1 + r_0, 1 + (1 - r_0))\)</span> distribution, which is a conjugate prior for the Bernoulli distribution. That is, if we start with a Beta prior on <span class="math inline">\(\mu^k\)</span> (note that <span class="math inline">\(\text{Unif}([0, 1]) = \text{Beta}(1, 1)\)</span>), then the posterior, after conditioning on samples from <span class="math inline">\(\text{Bern}(\mu^k)\)</span>, will also be Beta. This is a very convenient property, since it means we can simply update the parameters of the Beta distribution upon observing a reward, rather than having to recompute the entire posterior distribution from scratch.</p>
</div>
<div id="9c63a660" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Beta(Distribution):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, alpha: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>, beta: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> np.full(K, alpha)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> np.full(K, beta)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.beta(<span class="va">self</span>.alphas, <span class="va">self</span>.betas)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas[arm] <span class="op">+=</span> reward</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas[arm] <span class="op">+=</span> <span class="dv">1</span> <span class="op">-</span> reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-thompson" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>beta_distribution <span class="op">=</span> Beta(mab.K)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ThompsonSampling(mab.K, mab.T, beta_distribution)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-thompson" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-thompson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bandits_files/figure-html/fig-thompson-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-thompson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.8: Reward and cumulative regret of the Thompson sampling algorithm
</figcaption>
</figure>
</div>
</div>
</div>
<p>It turns out that asymptotically, Thompson sampling is optimal in the following sense. <span class="citation" data-cites="lai_asymptotically_1985">Lai &amp; Robbins (<a href="references.html#ref-lai_asymptotically_1985" role="doc-biblioref">1985</a>)</span> prove an <em>instance-dependent</em> lower bound that says for <em>any</em> bandit algorithm,</p>
<p><span id="eq-instance-lower-bound"><span class="math display">\[
\liminf_{T\to \infty} \frac{\mathop{\mathbb{E}}[N_T^k]}{\ln(T)} \ge \frac{1}{\mathrm{KL}\left(\mu^k\parallel\mu^\star\right)}
\tag{4.36}\]</span></span></p>
<p>where</p>
<p><span id="eq-kl-bayesian"><span class="math display">\[
\mathrm{KL}\left(\mu^k\parallel\mu^\star\right) := \mu^k\ln \frac{\mu^k}{\mu^\star} + (1 - \mu^k) \ln \frac{1 - \mu^k}{1 - \mu^\star}
\tag{4.37}\]</span></span></p>
<p>measures the <strong>Kullback-Leibler divergence</strong> from the Bernoulli distribution with mean <span class="math inline">\(\mu^k\)</span> to the Bernoulli distribution with mean <span class="math inline">\(\mu^\star\)</span>. It turns out that Thompson sampling achieves this lower bound with equality! That is, not only is the error <em>rate</em> optimal, but the <em>constant factor</em> is optimal as well.</p>
</section>
<section id="sec-contextual-bandits" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="sec-contextual-bandits"><span class="header-section-number">4.9</span> Contextual bandits</h2>
<p>In the multi-armed bandits setting described above, each pull is exactly the same. We don’t obtain any prior information about the reward distributions. However, consider the online advertising example (<a href="#exm-advertising" class="quarto-xref">ex.&nbsp;<span>4.1</span></a>), where each arm corresponds to an ad we could show a given user, and we receive a reward if and only if the user clicks on the ad. Suppose Alice is interested in sports and politics, while Bob is interested in news and entertainment. The ads that Alice is likely to click differ from those that Bob is likely to click. That is, the reward distributions of the arms depend on some prior <em>context</em>. We can model such environments using <strong>contextual bandits</strong>.</p>
<div id="def-contextual-bandit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.9 (Contextual bandit)</strong></span> At each pull <span class="math inline">\(t\)</span>, a <em>context</em> <span class="math inline">\(x_t\)</span> is chosen from the set <span class="math inline">\(\mathcal{X}\)</span> of all possible contexts. The learner gets to observe the context <em>before</em> they choose an action. That is, we treat the action chosen on the <span class="math inline">\(t\)</span>th pull as a function of the context: <span class="math inline">\(a_t(x_t)\)</span>. Then, the learner observes the reward from the chosen arm, where the reward distribution also depends on the context.</p>
</div>
<div id="exm-advertising-contextual" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5 (Online advertising as a contextual bandit problem)</strong></span> Suppose you represent an online advertising company and you are tasked with placing ads on the New York Times website. Articles are categorized into six sections, each with its own web page: U.S., World, Business, Arts, Lifestyle, and Opinion. You might decide to frame this as a contextual bandits problem with <span class="math inline">\(|\mathcal{X}| = 6\)</span>. This is because the demographic of readers likely differs across the different pages, and so the reward distributions differ across pages accordingly.</p>
</div>
<p>Suppose our context is <em>discrete</em>, as in <a href="#exm-advertising-contextual" class="quarto-xref">ex.&nbsp;<span>4.5</span></a>. Then we could treat each context as its own MAB, assigning each context-arm pair to a distinct arm in an enlarged MAB of <span class="math inline">\(K|\mathcal{X}|\)</span> arms.</p>
<div id="exr-ucb-contextual" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 4.3 (Naive UCB for contextual bandits)</strong></span> Write down the UCB algorithm for this enlarged MAB. That is, write an expression <span class="math inline">\(M^k_t\)</span> in terms of <span class="math inline">\(K\)</span> and <span class="math inline">\(|\mathcal{X}|\)</span> such that <span class="math inline">\(a_t(x_t) = \arg\max_{k\in [K]} M^k_t\)</span>.</p>
</div>
<p>Recall that executing UCB for <span class="math inline">\(T\)</span> timesteps on an MAB with <span class="math inline">\(K\)</span> arms achieves a regret bound of <span class="math inline">\(\widetilde{O}(\sqrt{TK})\)</span> (<a href="#thm-ucb-regret" class="quarto-xref">Theorem&nbsp;<span>4.6</span></a>). So in this problem, we would achieve regret <span class="math inline">\(\widetilde{O}(\sqrt{TK|\mathcal{X}|})\)</span> in the contextual MAB, which has a polynomial dependence on <span class="math inline">\(|\mathcal{X}|\)</span>. But in a situation where we have too many possible contexts, or the context is a continuous value, this algorithm no longer works.</p>
<p>Note that this “enlarged MAB” treats the different contexts as entirely unrelated to each other, while in practice, often contexts are <em>related</em> to each other in some way. In the medical trial example (<a href="#exm-clinical-trials" class="quarto-xref">ex.&nbsp;<span>4.2</span></a>), clients with similar health situations may benefit from the same medications. How can we incorporate this structure into our solution?</p>
<section id="sec-lin-ucb" class="level3" data-number="4.9.1">
<h3 data-number="4.9.1" class="anchored" data-anchor-id="sec-lin-ucb"><span class="header-section-number">4.9.1</span> Linear contextual bandits</h3>
<div id="rem-sl-contextual-dependence" class="proof remark">
<p><span class="proof-title"><em>Remark 4.5</em> (Supervised learning). </span>The material in this section depends on basic knowledge of supervised learning (<a href="supervised_learning.html" class="quarto-xref"><span>Chapter 5</span></a>).</p>
</div>
<p>Suppose now that we observe a vector-valued context, that is, <span class="math inline">\(\mathcal{X} = \mathbb{R}^D\)</span>. For example, in a medical trial (<a href="#exm-clinical-trials" class="quarto-xref">ex.&nbsp;<span>4.2</span></a>), we might know the patient’s height, age, blood pressure, etc., each of which would be a separate element of the vector. The approach of treating each context as its own MAB problem no longer works out-of-the-box since the number of contexts is infinite. Instead, we assume that the mean reward of each arm <span class="math inline">\(k\)</span> is a function of the context: <span class="math inline">\(\mu^k: \mathcal{X} \to \mathbb{R}\)</span>.</p>
<p>Rather than the Bernoulli bandit problem (<a href="#def-bernoulli-bandit" class="quarto-xref">def.&nbsp;<span>4.1</span></a>), we’ll assume that the mean reward is <em>linear</em> in the context. (This model is easier to work with for our purposes.) That is, a reward <span class="math inline">\(r^k\)</span> from arm <span class="math inline">\(k\)</span> is drawn according to</p>
<p><span id="eq-linear-reward-model"><span class="math display">\[
r^k= x^\top \theta^k+ \varepsilon,
\tag{4.38}\]</span></span></p>
<p>where <span class="math inline">\(\theta^k\in \mathbb{R}^D\)</span> describes a <em>feature direction</em> for arm <span class="math inline">\(k\)</span> and <span class="math inline">\(\varepsilon\)</span> is some independent noise with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This clearly has</p>
<p><span id="eq-contextual-true-mean"><span class="math display">\[
\mu^k(x) = x^\top \theta^k.
\tag{4.39}\]</span></span></p>
<p>Then, on the <span class="math inline">\(t\)</span>th pull, given context <span class="math inline">\(x_t\)</span>, we can model the mean reward from arm <span class="math inline">\(k\)</span> as</p>
<p><span id="eq-linear-contextual"><span class="math display">\[
\hat \mu^k_t(x_t) := x_t^\top \hat \theta^k_t,
\tag{4.40}\]</span></span></p>
<p>where <span class="math inline">\(\hat \theta^k_t\)</span> is some estimator of <span class="math inline">\(\theta^k\)</span> computed from the first <span class="math inline">\(t\)</span> pulls.</p>
<div id="rem-bernoulli-linear" class="proof remark">
<p><span class="proof-title"><em>Remark 4.6</em> (Assumptions for linear models). </span>Why did we switch from Bernoulli reward distributions to the additive model in <a href="#eq-linear-reward-model" class="quarto-xref">eq.&nbsp;<span>4.38</span></a>? The estimator <a href="#eq-linear-contextual" class="quarto-xref">eq.&nbsp;<span>4.40</span></a> is a bit odd for the Bernoulli bandit problem: since <span class="math inline">\(\mu^k\)</span> is the success probability of arm <span class="math inline">\(k\)</span>, it would have to lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, but <span class="math inline">\(\hat \mu^k_t(x_t)\)</span> extends outside this interval. The question of how to model the unknown parameters of a distribution is a <strong>supervised learning</strong> problem (<a href="supervised_learning.html" class="quarto-xref"><span>Chapter 5</span></a>). For instance, in Bernoulli bandit, we could instead use a <em>logistic regression</em> model for each arm’s success probability. We won’t dive into more advanced supervised learning topics here.</p>
</div>
<p>Our goal is still to develop an algorithm that chooses an arm <span class="math inline">\(a_t(x_t)\)</span> to pull based on the experience gained during the first <span class="math inline">\(t\)</span> trials. We will use <em>supervised learning</em>, in particular, empirical risk minimization (<a href="supervised_learning.html#sec-erm" class="quarto-xref"><span>Section 5.3</span></a>), to estimate <span class="math inline">\(\theta^k\)</span> for each arm. That is, we compute <span class="math inline">\(\hat \theta^k_t\)</span> as the vector that minimizes squared error from the observed rewards:</p>
<p><span id="eq-contextual-bandits-estimator"><span class="math display">\[
\hat \theta_t^k:= \arg\min_{\theta \in \mathbb{R}^D}
    \sum_{t' \in \mathcal{I}^k_t}
    (r_{t'} - x_{t'}^\top \theta)^2,
\tag{4.41}\]</span></span></p>
<p>where</p>
<p><span id="eq-arm-indices"><span class="math display">\[
\mathcal{I}^k_t:= \{ t' \in [t] \mid a_{t'} = k\}
\tag{4.42}\]</span></span></p>
<p>denotes the subset of <span class="math inline">\([t]\)</span> at which arm <span class="math inline">\(k\)</span> was pulled. Note that <span class="math inline">\(|\mathcal{I}^k_t| = N^k_t\)</span> as defined in <a href="#eq-bandits-count" class="quarto-xref">eq.&nbsp;<span>4.23</span></a>. The ERM problem in <a href="#eq-contextual-bandits-estimator" class="quarto-xref">eq.&nbsp;<span>4.41</span></a> has the closed-form solution known as the <strong>ordinary least squares</strong> (OLS) estimator:</p>
<p><span id="eq-ols-bandit"><span class="math display">\[
\begin{aligned}
    \hat \theta_t^k&amp;= (\widehat \Sigma_t^k)^{-1}\left(
        \frac{1}{N^k_t} \sum_{t' \in \mathcal{I}^k_t} x_{t'} r_{t'}
    \right) \\
    \text{where} \quad \widehat \Sigma_t^k&amp;= \frac{1}{N^k_t} \sum_{t' \in \mathcal{I}^k_t} x_{t'} x_{t'}^\top.
\end{aligned}
\tag{4.43}\]</span></span></p>
<p><span class="math inline">\(\widehat \Sigma_t^k\)</span> is the (biased) empirical covariance matrix of the contexts across the trials where arm <span class="math inline">\(k\)</span> was pulled (out of the first <span class="math inline">\(t\)</span> trials).</p>
<div id="rem-invertible" class="proof remark">
<p><span class="proof-title"><em>Remark 4.7</em> (Invertibility). </span>In <a href="#eq-ols-bandit" class="quarto-xref">eq.&nbsp;<span>4.43</span></a>, we write the expression <span class="math inline">\((\widehat \Sigma_t^k)^{-1}\)</span>, assuming that <span class="math inline">\(\widehat \Sigma_t^k\)</span> is invertible. This only holds if the context features are linearly independent, that is, there does not exist some coordinate <span class="math inline">\(d \in [D]\)</span> that can be written as a linear combination of the other coordinates. One way to ensure that <span class="math inline">\(\widehat \Sigma_t^k\)</span> is invertible is to add a <span class="math inline">\(\lambda I\)</span> regularization term to it. This is equivalent to solving a <em>ridge regression</em> problem instead of the unregularized least squares problem.</p>
</div>
<p>Now, given a sequence of <span class="math inline">\(t\)</span> contexts, pulls, and rewards <span class="math inline">\(x_0, a_0, r_0, \dots, x_{t-1}, a_{t-1}, r_{t-1}\)</span>, we can estimate <span class="math inline">\(\mu^k(x_t)\)</span> for each arm. This is helpful, but we haven’t yet solved the problem of <em>deciding</em> which arms to pull in each context to trade off between exploration and exploitation.</p>
<p>The upper confidence bound (UCB) algorithm (<a href="#def-ucb" class="quarto-xref">def.&nbsp;<span>4.8</span></a>) was a useful strategy. What would we need to adapt UCB to this new setting? Recall that at each step, UCB estimates a <em>confidence interval</em> for each arm mean, and chooses the arm with the highest upper confidence bound. We already have an estimator <span class="math inline">\(\hat \mu^k_t(x_t)\)</span> of each arm mean, so all that is left is to compute the width of the confidence interval itself.</p>
<div id="thm-lin-ucb-ci" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.7 (LinUCB confidence interval)</strong></span> Suppose we pull each arm once during the first <span class="math inline">\(K\)</span> trials. Then at each pull <span class="math inline">\(t\ge K\)</span>, for all arms <span class="math inline">\(k\in [K]\)</span> and all contexts <span class="math inline">\(x \in \mathcal{X}\)</span>, we have that with probability at least <span class="math inline">\(1 - 1/\beta^2\)</span>,</p>
<p><span id="eq-lin-ucb-ci"><span class="math display">\[
\mu^k(x) \in \left[
    x^\top \hat \theta^k_t
    - \beta \frac{\sigma}{\sqrt{N^k_t}} \cdot \sqrt{x^\top (\widehat \Sigma^k_t)^{-1} x},
    x^\top \hat \theta^k_t
    + \beta \frac{\sigma}{\sqrt{N^k_t}} \cdot \sqrt{x^\top (\widehat \Sigma^k_t)^{-1} x}
\right],
\tag{4.44}\]</span></span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\sigma\)</span> is the standard deviation of the reward (<a href="#eq-linear-reward-model" class="quarto-xref">eq.&nbsp;<span>4.38</span></a>),</li>
<li><span class="math inline">\(N^k_t\)</span> is the number of times arm <span class="math inline">\(k\)</span> was pulled in the first <span class="math inline">\(t\)</span> pulls (<a href="#eq-bandits-count" class="quarto-xref">eq.&nbsp;<span>4.23</span></a>),</li>
<li><span class="math inline">\(\hat \theta^k_t\)</span> is the OLS estimator for arm <span class="math inline">\(k\)</span> (<a href="#eq-ols-bandit" class="quarto-xref">eq.&nbsp;<span>4.43</span></a>),</li>
<li>and <span class="math inline">\(\widehat \Sigma^k_t\)</span> is the sample covariance matrix of the contexts given arm <span class="math inline">\(k\)</span> (<a href="#eq-ols-bandit" class="quarto-xref">eq.&nbsp;<span>4.43</span></a>).</li>
</ul>
</div>
<p>We can then plug this interval into the UCB strategy (<a href="#def-ucb" class="quarto-xref">def.&nbsp;<span>4.8</span></a>) to obtain what is known as the LinUCB algorithm:</p>
<div id="def-lin-ucb" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.10 (LinUCB <span class="citation" data-cites="li_contextual-bandit_2010">(<a href="references.html#ref-li_contextual-bandit_2010" role="doc-biblioref">Li et al., 2010</a>)</span>)</strong></span> LinUCB first pulls each arm once. Then, for pull <span class="math inline">\(t\ge K\)</span>, LinUCB chooses an arm according to</p>
<p><span id="eq-lin-ucb"><span class="math display">\[
a_t(x_t) := \arg\max_{k\in [K]}
    x^\top \hat \theta^k_t
    +
    \beta \frac{\sigma}{\sqrt{N^k_t}} \sqrt{x_t^\top (\widehat \Sigma^k_t)^{-1} x_t},
\tag{4.45}\]</span></span></p>
<p>where <span class="math inline">\(\sigma, N^k_t, \hat \theta^k_t, \widehat \Sigma^k_t\)</span> are defined as in <a href="#thm-lin-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.7</span></a>, and <span class="math inline">\(\beta &gt; 0\)</span> controls the width of the confidence interval (and thereby the exploration-exploitation tradeoff).</p>
</div>
<p>The first term is exactly our predicted mean reward <span class="math inline">\(\hat \mu^k_t(x_t)\)</span>. We can think of it as the term encouraging <em>exploitation</em> of an arm if its predicted mean reward is high. The second term is the width of the confidence interval given by <a href="#thm-lin-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.7</span></a>. This is the <em>exploration</em> term that encourages an arm when <span class="math inline">\(x_t\)</span> is <em>not aligned</em> with the data seen so far, or if arm <span class="math inline">\(k\)</span> has not been explored much and so <span class="math inline">\(N_t^k\)</span> is small.</p>
<p>Now we prove <a href="#thm-lin-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.7</span></a>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>How can we construct the upper confidence bound in <a href="#thm-lin-ucb-ci" class="quarto-xref">Theorem&nbsp;<span>4.7</span></a>? Previously, we treated the pulls of an arm as i.i.d. samples and used Hoeffding’s inequality to bound the distance of the sample mean, our estimator, from the true mean. However, now our estimator is not a sample mean, but rather the OLS estimator above <a href="#eq-ols-bandit" class="quarto-xref">eq.&nbsp;<span>4.43</span></a>. We will construct the confidence interval using <strong>Chebyshev’s inequality</strong>, which gives a confidence interval for the mean of a distribution based on that distribution’s <em>variance</em>.</p>
<div id="thm-chebyshev" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.8 (Chebyshev’s inequality)</strong></span> For a random variable <span class="math inline">\(Y\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>,</p>
<p><span id="eq-chebyshev"><span class="math display">\[
|Y - \mu| \le \beta \sigma \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\tag{4.46}\]</span></span></p>
</div>
<p>We want to construct a CI for <span class="math inline">\(x^\top \theta^k\)</span>, the mean reward from arm <span class="math inline">\(k\)</span>. Our estimator is <span class="math inline">\(x^\top \hat \theta^k_t\)</span>. Applying Chebyshev’s inequality requires us to compute the variance of our estimator.</p>
<div id="thm-variance-ols" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.9 (Variance of OLS)</strong></span> Given the linear reward model in <a href="#eq-linear-reward-model" class="quarto-xref">eq.&nbsp;<span>4.38</span></a>,</p>
<p><span id="eq-variance-ols"><span class="math display">\[
\text{Var}(x^\top \hat \theta_t^k)
=
\frac{\sigma^2}{N^k_t} x^\top (\widehat \Sigma^t_k)^{-1} x,
\tag{4.47}\]</span></span></p>
<p>where <span class="math inline">\(\widehat \Sigma^t_k\)</span> is defined in <a href="#eq-ols-bandit" class="quarto-xref">eq.&nbsp;<span>4.43</span></a>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We leave the proof as an exercise. It follows from applications of the law of iterated expectations.</p>
</div>
<p>Now we can substitute <a href="#eq-variance-ols" class="quarto-xref">eq.&nbsp;<span>4.47</span></a> into <a href="#eq-chebyshev" class="quarto-xref">eq.&nbsp;<span>4.46</span></a> to obtain the desired CI:</p>
<p><span id="eq-ols-chebyshev"><span class="math display">\[
\begin{aligned}
    x_t^\top \theta^k\le x_t^\top \hat \theta_t^k
    + \beta \sqrt{x_t^\top (A_t^k)^{-1} x_t} \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\end{aligned}
\tag{4.48}\]</span></span></p>
</div>
<div id="07051c54" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinUCBPseudocode(Agent):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, D: <span class="bu">int</span>, lam: <span class="bu">float</span>, get_c: Callable[[<span class="bu">int</span>], <span class="bu">float</span>]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lam <span class="op">=</span> lam</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.get_c <span class="op">=</span> get_c</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.contexts <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(K)]</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> np.repeat(lam <span class="op">*</span> np.eye(D)[...], K)  <span class="co"># regularization</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> np.zeros(K, D)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> np.zeros(K, D)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>, context: Float[Array, <span class="st">" D"</span>]):</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> <span class="va">self</span>.get_c(<span class="va">self</span>.count)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> <span class="va">self</span>.w <span class="op">@</span> context <span class="op">+</span> c <span class="op">*</span> np.sqrt(</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            context.T <span class="op">@</span> np.linalg.solve(<span class="va">self</span>.A, context)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random_argmax(scores)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, context: Float[Array, <span class="st">" D"</span>], arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A[arm] <span class="op">+=</span> np.outer(context, context)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets[arm] <span class="op">+=</span> context <span class="op">*</span> reward</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w[arm] <span class="op">=</span> np.linalg.solve(<span class="va">self</span>.A[arm], <span class="va">self</span>.targets[arm])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="key-takeaways" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">4.10</span> Key takeaways</h2>
<p>In this chapter, we explored the <strong>multi-armed bandit</strong> setting for analyzing sequential decision-making in an unknown environment. An MAB consists of multiple arms, each with an unknown reward distribution. The agent’s task is to learn about these through interaction, eventually minimizing the <em>regret</em>, which measures how suboptimal the chosen arms were.</p>
<p>We saw algorithms such as <strong>upper confidence bound</strong> and <strong>Thompson sampling</strong> that handle the tradeoff between <em>exploration</em> and <em>exploitation</em>, that is, the tradeoff between choosing arms that the agent is <em>uncertain</em> about and arms the agent already supposes are be good.</p>
<p>We finally discussed <strong>contextual bandits</strong>, in which the agent gets to observe some <em>context</em> that affects the reward distributions. We can approach these problems through <strong>supervised learning</strong> approaches.</p>
</section>
<section id="bibliographic-notes-and-further-reading" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="bibliographic-notes-and-further-reading"><span class="header-section-number">4.11</span> Bibliographic notes and further reading</h2>
<p>The problem that a bandit algorithm faces after <span class="math inline">\(t\)</span> pulls is to infer differences between the means of a set of distributions given <span class="math inline">\(t\)</span> samples spread out across those distributions. This is a well-studied problem in statistics.</p>
<p>A common practice in various fields such as psychology is to compare within-group variances to the variance of the group means. This popular class of methods is known as <strong>analysis of variance</strong> (ANOVA). It has been studied at least since Laplace in the 1770s <span class="citation" data-cites="stigler_history_2003">(<a href="references.html#ref-stigler_history_2003" role="doc-biblioref">Stigler, 2003</a>)</span> and was further popularized by <span class="citation" data-cites="fisher_statistical_1925">Fisher (<a href="references.html#ref-fisher_statistical_1925" role="doc-biblioref">1925</a>)</span>. William Thompson studied the two-armed mean distinction problem in <span class="citation" data-cites="thompson_likelihood_1933">Thompson (<a href="references.html#ref-thompson_likelihood_1933" role="doc-biblioref">1933</a>)</span> and <span class="citation" data-cites="thompson_theory_1935">Thompson (<a href="references.html#ref-thompson_theory_1935" role="doc-biblioref">1935</a>)</span>. Our focus has been on the decision-making aspect of the bandit problem, rather than the hypothesis testing aspect.</p>
<p>Adding the sequential decision-making aspect turns the above into the full multi-armed bandit problem. <span class="citation" data-cites="wald_statistical_1949">Wald (<a href="references.html#ref-wald_statistical_1949" role="doc-biblioref">1949</a>)</span> studies the sequential analysis problem in depth. <span class="citation" data-cites="robbins_aspects_1952">Robbins (<a href="references.html#ref-robbins_aspects_1952" role="doc-biblioref">1952</a>)</span> then builds on Wald’s work and provides a clear exposition of the bandit problem in the form presented in this chapter. <span class="citation" data-cites="berry_bandit_1985">Berry &amp; Fristedt (<a href="references.html#ref-berry_bandit_1985" role="doc-biblioref">1985</a>)</span> is a comprehensive treatment of bandit problems from a statistical perspective.</p>
<div id="rem-etymology-lin-ucb" class="proof remark">
<p><span class="proof-title"><em>Remark 4.8</em> (Disambiguation of LinUCB). </span>The name “LinUCB” is used for various similar algorithms throughout the literature. Here we use it as defined in <span class="citation" data-cites="li_contextual-bandit_2010">Li et al. (<a href="references.html#ref-li_contextual-bandit_2010" role="doc-biblioref">2010</a>)</span>. This algorithm was also considered in <span class="citation" data-cites="auer_using_2002">Auer (<a href="references.html#ref-auer_using_2002" role="doc-biblioref">2002</a>)</span>, the paper that introduced UCB, under the name “Associative Reinforcement Learning with Linear Value Functions”.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-agarwal_reinforcement_2022" class="csl-entry" role="listitem">
Agarwal, A., Jiang, N., Kakade, S. M., &amp; Sun, W. (2022). <em>Reinforcement learning: <span>Theory</span> and algorithms</em>. <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</a>
</div>
<div id="ref-auer_using_2002" class="csl-entry" role="listitem">
Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. <em>Journal of Machine Learning Research</em>, <em>3</em>, 397–422. <a href="https://www.jmlr.org/papers/v3/auer02a.html">https://www.jmlr.org/papers/v3/auer02a.html</a>
</div>
<div id="ref-berry_bandit_1985" class="csl-entry" role="listitem">
Berry, D. A., &amp; Fristedt, B. (1985). <em>Bandit problems</em>. Springer Netherlands. <a href="https://doi.org/10.1007/978-94-015-3711-7">https://doi.org/10.1007/978-94-015-3711-7</a>
</div>
<div id="ref-fisher_statistical_1925" class="csl-entry" role="listitem">
Fisher, R. A. (1925). <em>Statistical methods for research workers, 11th ed. rev</em>. Edinburgh.
</div>
<div id="ref-lai_asymptotically_1985" class="csl-entry" role="listitem">
Lai, T. L., &amp; Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. <em>Advances in Applied Mathematics</em>, <em>6</em>(1), 4–22. <a href="https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>
</div>
<div id="ref-li_contextual-bandit_2010" class="csl-entry" role="listitem">
Li, L., Chu, W., Langford, J., &amp; Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. <em>Proceedings of the 19th International Conference on <span>World</span> Wide Web</em>, 661–670. <a href="https://doi.org/10.1145/1772690.1772758">https://doi.org/10.1145/1772690.1772758</a>
</div>
<div id="ref-negative_space_photo_2015" class="csl-entry" role="listitem">
Negative Space. (2015). <em>Photo of commercial district during dawn</em> [Graphic]. <a href="https://www.pexels.com/photo/photo-of-commercial-district-during-dawn-34639/">https://www.pexels.com/photo/photo-of-commercial-district-during-dawn-34639/</a>
</div>
<div id="ref-pixabay_20_2016" class="csl-entry" role="listitem">
Pixabay. (2016a). <em>20 mg label blister pack</em> [Graphic]. <a href="https://www.pexels.com/photo/20-mg-label-blister-pack-208512/">https://www.pexels.com/photo/20-mg-label-blister-pack-208512/</a>
</div>
<div id="ref-pixabay_coins_2016" class="csl-entry" role="listitem">
Pixabay. (2016b). <em>Coins on brown wood</em> [Graphic]. <a href="https://www.pexels.com/photo/coins-on-brown-wood-210600/">https://www.pexels.com/photo/coins-on-brown-wood-210600/</a>
</div>
<div id="ref-robbins_aspects_1952" class="csl-entry" role="listitem">
Robbins, H. (1952). Some aspects of the sequential design of experiments. <em>Bulletin of the American Mathematical Society</em>, <em>58</em>(5), 527–535. <a href="https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full">https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full</a>
</div>
<div id="ref-stigler_history_2003" class="csl-entry" role="listitem">
Stigler, S. M. (2003). <em>The history of statistics: The measurement of uncertainty before 1900</em> (9. print). Belknap Pr. of Harvard Univ. Pr.
</div>
<div id="ref-thompson_likelihood_1933" class="csl-entry" role="listitem">
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>Biometrika</em>, <em>25</em>(3/4), 285–294. <a href="https://doi.org/10.2307/2332286">https://doi.org/10.2307/2332286</a>
</div>
<div id="ref-thompson_theory_1935" class="csl-entry" role="listitem">
Thompson, W. R. (1935). On the theory of apportionment. <em>American Journal of Mathematics</em>, <em>57</em>(2), 450–456. <a href="https://doi.org/10.2307/2371219">https://doi.org/10.2307/2371219</a>
</div>
<div id="ref-vershynin_high-dimensional_2018" class="csl-entry" role="listitem">
Vershynin, R. (2018). <em>High-dimensional probability: <span>An</span> introduction with applications in data science</em>. Cambridge University Press. <a href="https://books.google.com?id=NDdqDwAAQBAJ">https://books.google.com?id=NDdqDwAAQBAJ</a>
</div>
<div id="ref-wald_statistical_1949" class="csl-entry" role="listitem">
Wald, A. (1949). Statistical decision functions. <em>The Annals of Mathematical Statistics</em>, <em>20</em>(2), 165–205. <a href="https://doi.org/10.1214/aoms/1177730030">https://doi.org/10.1214/aoms/1177730030</a>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/rlbook\.adzc\.ai");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./control.html" class="pagination-link" aria-label="Linear Quadratic Regulators">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./supervised_learning.html" class="pagination-link" aria-label="Supervised learning">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025 Alexander Cai</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/adzcai/rlbook/edit/main/bandits.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/adzcai/rlbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/adzcai/rlbook/blob/main/bandits.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/adzcai/rlbook" aria-current="page">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>