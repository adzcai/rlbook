[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Reinforcement Learning",
    "section": "",
    "text": "Preface\nWelcome to the study of reinforcement learning! This textbook accompanies the undergraduate course CS 1840/STAT 184 taught at Harvard. It is intended to be an approachable yet rigorous introduction to this active subfield of machine learning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "An Introduction to Reinforcement Learning",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis book assumes the same prerequisites as the course: You should be familiar with multivariable calculus, linear algebra, and probability. For Harvard undergraduates, this is fulfilled by Math 21a, Math 21b, and Stat 110, or their equivalents. Stat 111 is strongly recommended but not required. Specifically, we will assume that you know the following topics. The italicized terms have brief re-introductions in the text:\n\nLinear Algebra: Vectors and matrices, matrix multiplication, matrix inversion, eigenvalues and eigenvectors.\nMultivariable Calculus: Partial derivatives, the chain rule, Taylor series, gradients, directional derivatives, Lagrange multipliers.\nProbability: Random variables, probability distributions, expectation and variance, the law of iterated expectations (Adam’s rule), covariance, conditional probability, Bayes’s rule, and the law of total probability.\n\nYou should also be familiar with basic programming concepts such as variables, functions, loops, etc. Pseudocode listings will be provided for certain algorithms.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "An Introduction to Reinforcement Learning",
    "section": "Course overview",
    "text": "Course overview\nThe course will progress through the following units:\n1  Introduction presents motivation for the RL problem and compares RL to other fields of machine learning.\n2  Markov Decision Processes introduces Markov Decision Processes, the core mathematical framework for describing a large class of interactive environments.\n3  Linear Quadratic Regulators is a standalone chapter on the linear quadratic regulator (LQR), an important tool for continuous control, in which the state and action spaces are no longer finite but rather continuous. This has widespread applications in robotics.\n4  Multi-Armed Bandits introduces the multi-armed bandit (MAB) model for stateless sequential decision-making tasks. In exploring a number of algorithms, we will see how each of them strikes a different balance between exploring new options and exploiting known options. This exploration-exploitation tradeoff is a core consideration in RL algorithm design.\n5  Supervised learning is a standalone crash course on some tools from supervised learning that we will use in later chapters.\n6  Fitted Dynamic Programming Algorithms introduces fitted dynamic programming (fitted DP) algorithms for solving MDPs. These algorithms use supervised learning to approximately evaluate policies when they cannot be evaluated exactly.\n7  Policy Gradient Methods explores an important class of algorithms based on iteratively improving a policy. We will also encounter the use of deep neural networks to express nonlinear policies and approximate nonlinear functions with many inputs and outputs.\n8  Imitation Learning attempts to learn a good policy from expert demonstrations. At its most basic, this is an application of supervised learning to RL tasks.\n9  Tree Search Methods looks at ways to explicitly plan ahead when the environment’s dynamics are known. We will study the Monte Carlo Tree Search heuristic, which has been used to great success in the famous AlphaGo algorithm and its successors.\n10  Exploration in MDPs continues to investigate the exploration-exploitation tradeoff. We will extend ideas from multi-armed bandits to the MDP setting.\nApp. A: Background contains an overview of selected background mathematical content and programming content.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notation",
    "section": "",
    "text": "It’s worth it to spend a few words discussing the notation used for reinforcement learning. RL notation can appear quite complicated, since we often need to index across algorithm iterations, trajectories, and timesteps, so that certain values can have two or three indices attached to them. It’s important to see beyond the formal notation and interpret the quantities being symbolized.\nWe will use the following notation throughout the book. This notation is inspired by Sutton & Barto (2018) and Agarwal et al. (2022). We use \\([N]\\) as shorthand for the set \\(\\{ 0, 1, \\dots, N-1 \\}\\). We try to use each lowercase letter to represent an element of the set denoted by the corresponding (stylized) uppercase letter. We use \\(\\triangle(\\mathcal{X})\\) to denote a distribution supported on some subset of \\(\\mathcal{X}\\). (The triangle is used to evoke the probability simplex.)\n\n\n\nTable 1: Table of integer indices.\n\n\n\n\n\n\n\n\n\n\nIndex\nRange\nDefinition (of index)\n\n\n\n\n\\(h\\)\n\\([H]\\)\nTime horizon index of an MDP (subscript).\n\n\n\\(k\\)\n\\([K]\\)\nArm index of a multi-armed bandit (superscript).\n\n\n\\(t\\)\n\\([T]\\)\nNumber of episodes.\n\n\n\\(i\\)\n\\([I]\\)\nIteration index of an algorithm (subscript or superscript).\n\n\n\n\n\n\n\n\n\nTable 2: Table of common notation.\n\n\n\n\n\n\n\n\n\n\nElement\nSpace\nDefinition (of element)\n\n\n\n\n\\(s\\)\n\\(\\mathcal{S}\\)\nA state.\n\n\n\\(a\\)\n\\(\\mathcal{A}\\)\nAn action.\n\n\n\\(r\\)\n\\(\\mathbb{R}\\)\nA reward.\n\n\n\\(\\gamma\\)\n\\([0, 1]\\)\nA discount factor.\n\n\n\\(\\tau\\)\n\\(\\mathcal{T}\\)\nA trajectory \\((s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1})\\)\n\n\n\\(\\pi\\)\n\\(\\Pi\\)\nA policy.\n\n\n\\(V^\\pi\\)\n\\(\\mathcal{S} \\to \\mathbb{R}\\)\nThe value function of policy \\(\\pi\\).\n\n\n\\(Q^\\pi\\)\n\\(\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\)\nThe action-value function (a.k.a. Q-function) of policy \\(\\pi\\).\n\n\n\\(A^\\pi\\)\n\\(\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\)\nThe advantage function of policy \\(\\pi\\).\n\n\n\\(v\\)\n\\(\\mathcal{S} \\to \\mathbb{R}\\)\nAn approximation to the value function.\n\n\n\\(q\\)\n\\(\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\)\nAn approximation to the Q function.\n\n\n\\(\\theta\\)\n\\(\\Theta\\)\nA parameter vector.\n\n\n\n\n\n\nNote that throughout the text, certain symbols will stand for either random variables or fixed values. We aim to clarify in ambiguous settings.\n\n\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022). Reinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf",
    "crumbs": [
      "Notation"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Core tasks of reinforcement learning\nReinforcement learning (RL) is a branch of machine learning that studies sequential decision-making in unknown environments. An RL algorithm finds a strategy, called a policy, that maximizes the reward it obtains from the environment.\nRL provides a powerful framework for attacking a wide variety of problems, including robotic control, video games and board games, resource management, language modeling, and more. It also provides an interdisciplinary paradigm for studying animal and human behaviour. Many of the most stunning results in machine learning, ranging from AlphaGo (Silver et al., 2016) to ChatGPT (OpenAI, 2022), are built using RL algorithms.\nHow does RL compare to the other two core machine learning paradigms, supervised learning and unsupervised learning?\nThe key difference is that RL algorithms don’t learn from some existing dataset; rather, they must go out and interact with the environment to collect their own data in an online way. This means RL algorithms face a distinct set of challenges from other kinds of machine learning. We’ll discuss these more concretely in Section 1.2.\nWhat tasks, exactly, are important for RL? Typically,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#core-tasks-of-reinforcement-learning",
    "href": "introduction.html#core-tasks-of-reinforcement-learning",
    "title": "1  Introduction",
    "section": "",
    "text": "Policy evaluation (prediction): How ‘good’ is a specific state, or state-action pair (under a given policy)? That is, how much reward does it lead to in the long run? This is also called the task of value estimation.\nPolicy optimization (control): Suppose we fully understand how the environment behaves. What is the best action to take in every scenario?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-challenges",
    "href": "introduction.html#sec-intro-challenges",
    "title": "1  Introduction",
    "section": "1.2 Challenges of reinforcement learning",
    "text": "1.2 Challenges of reinforcement learning\nRecursion (bootstrapping): how can we “reuse” our current predictions to generate new information?\nExploration-exploitation tradeoff: should we try new actions, or capitalize on actions that we currently believe to be good?\nCredit assignment: Consider this example: some mornings, you may wake up well rested, while other mornings, you may wake up drowsy and tired, even if the amount of time you spent asleep stays the same. What could be the cause? You take so many actions throughout the day, and any one of them could be the reason for your good or poor sleep. Was it a skipped meal? A lack of exercise? When these consequences interact with each other, it can be challenging to properly assign credit to the actions that cause the observed effects.\nReproducibility: the high variance inherent in interacting with the environment means that the results of RL experiments can be challenging to reproduce. Even when averaging across multiple random seeds, the same algorithm can achieve drastically different-seeming results (R. Agarwal et al., 2021).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-programming",
    "href": "introduction.html#sec-programming",
    "title": "1  Introduction",
    "section": "1.3 Programming",
    "text": "1.3 Programming\nWhy include code in a textbook? We believe that implementing an algorithm is a strong test of your understanding of it; mathematical notation can often abstract away details, while a computer must be given every single instruction. We have sought to write readable Python code that is self-contained within each file. This approach is inspired by Sussman et al. (2013). There are some ways in which the code style differs from typical software projects:\n\nWe keep use of language features to a minimum, even if it leads to code that could otherwise be more concisely or idiomatically expressed.\nThe variable names used in the code match those used in the main text. For example, the variable s will be used instead of the more explicit state.\n\nWe also make extensive use of Python type annotations to explicitly specify variable types, including shapes of vectors and matrices using the jaxtyping library.\nThis is an interactive book built with Quarto (Allaire et al., 2024). It uses Python 3.11. It uses the JAX library for numerical computing. JAX was chosen for the clarity of its functional style and due to its mature RL ecosystem, sustained in large part by the Google DeepMind research group and a large body of open-source contributors. We use the standard Gymnasium library for interfacing with RL environments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-bib",
    "href": "introduction.html#sec-intro-bib",
    "title": "1  Introduction",
    "section": "1.4 Bibliographic notes and further reading",
    "text": "1.4 Bibliographic notes and further reading\nInterest has surged in RL in the past decades, especially since AlphaGo’s groundbreaking success (Silver et al., 2016). There are a number of recent textbooks that cover the field of RL:\nSchultz et al. (1997) highlights RL as a normative theory for neuroscientific behaviour. Thorndike (1911) puts RL forward as a learning framework for animal behaviours.\nSutton & Barto (2018) set the framework for much of modern RL. Plaat (2022) is a graduate-level textbook on deep reinforcement learning. A. Agarwal et al. (2022) is a useful reference for theoretical guarantees of RL algorithms. S. E. Li (2023) highlights the connections between RL and optimal control. Mannor et al. (2024) is another advanced undergraduate course textbook. Bertsekas & Tsitsiklis (1996) introduced many of the core concepts of RL. Szepesvári (2010) is an invaluable resource on much of the theory underlying the methods in this book and elsewhere. Kochenderfer et al. (2022) provides a more probabilistic perspective alongside Julia code for various RL algorithms.\nThere are also a number of review articles that summarize recent advances. Murphy (2025) gives an overview of the past decade of advancements in RL. Ivanov & D’yakonov (2019) lists many popular algorithms.\nOther textbooks focus on specific RL techniques or on applications of RL to specific fields. Albrecht et al. (2023) discusses multi-agent reinforcement learning. Rao & Jelvis (2022) focuses on applications of RL to finance. Y. Li (2018) surveys applications of RL to various fields.\n\n\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022). Reinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 34, 29304–29320. https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html\n\n\nAlbrecht, S. V., Christianos, F., & Schäfer, L. (2023). Multi-agent reinforcement learning: Foundations and modern approaches. MIT Press. https://www.marl-book.com\n\n\nAllaire, J. J., Teague, C., Scheidegger, C., Xie, Y., & Dervieux, C. (2024). Quarto (Version 1.4) [Computer software]. https://doi.org/10.5281/zenodo.5960048\n\n\nBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena scientific.\n\n\nIvanov, S., & D’yakonov, A. (2019, July 6). Modern deep reinforcement learning algorithms. https://doi.org/10.48550/arXiv.1906.10025\n\n\nKochenderfer, M. J., Wheeler, T. A., & Wray, K. H. (2022). Algorithms for decision making. https://mitpress.mit.edu/9780262047012/algorithms-for-decision-making/\n\n\nLi, S. E. (2023). Reinforcement learning for sequential decision and optimal control. Springer Nature. https://doi.org/10.1007/978-981-19-7784-8\n\n\nLi, Y. (2018). Deep reinforcement learning. https://doi.org/10.48550/arXiv.1810.06339\n\n\nMannor, S., Mansour, Y., & Tamar, A. (2024). Reinforcement learning: foundations. https://sites.google.com/view/rlfoundations/home\n\n\nMurphy, K. (2025, March 24). Reinforcement learning: A comprehensive overview. https://doi.org/10.48550/arXiv.2412.05265\n\n\nOpenAI. (2022, November 30). Introducing ChatGPT. OpenAI News. https://openai.com/index/chatgpt/\n\n\nPlaat, A. (2022). Deep reinforcement learning. Springer Nature. https://doi.org/10.1007/978-981-19-0638-1\n\n\nRao, A., & Jelvis, T. (2022). Foundations of reinforcement learning with applications in finance. Chapman and Hall/CRC. https://doi.org/10.1201/9781003229193\n\n\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A neural substrate of prediction and reward. Science, 275(5306), 1593–1599. https://doi.org/10.1126/science.275.5306.1593\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). Mastering the game of go with deep neural networks and tree search. Nature, 529(7587, 7587), 484–489. https://doi.org/10.1038/nature16961\n\n\nSilver, D., Singh, S., Precup, D., & Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299, 103535. https://doi.org/10.1016/j.artint.2021.103535\n\n\nSussman, G. J., Wisdom, J., & Farr, W. (2013). Functional differential geometry. The MIT Press.\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf\n\n\nSzepesvári, C. (2010). Algorithms for reinforcement learning. Springer International Publishing. https://doi.org/10.1007/978-3-031-01551-9\n\n\nThorndike, E. L. (1911). Animal intelligence: Experimental studies (pp. viii, 297). Macmillan Press. https://doi.org/10.5962/bhl.title.55072\n\n\nTuring, A. (1948). Intelligent machinery. National Physical Laboratory. https://weightagnostic.github.io/papers/turing1948.pdf",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "mdps.html",
    "href": "mdps.html",
    "title": "2  Markov Decision Processes",
    "section": "",
    "text": "2.1 Introduction\nMachine learning studies algorithms that learn to solve a task “on their own”, without needing a programmer to implement handwritten “if statements”. Reinforcement learning (RL) is a branch of machine learning that focuses on decision problems like the following:\nExample 2.1 (Decision problems)\nAll of these tasks involve taking a sequence of actions in order to achieve some goal. This interaction loop can be summarized in the following diagram:\ngraph LR\n    Agent --\"takes *action*\"--&gt; Environment\n    Environment --\"observes *state*, *reward*\"--&gt; Agent\n\n\n\n\nFigure 2.1: The RL interaction loop. The agent chooses an action that affects the environment. The environment updates according to the state transitions. Then the agent observes the updated environment and a reward signal that describes progress made towards the goal.\nIn this chapter, we’ll investigate the most popular mathematical formalization for such tasks: Markov decision processes (MDPs). We will study dynamic programming (DP) algorithms for solving tasks when the rules of the environment are totally known. We’ll describe how to evaluate different policies and how to compute (or approximate) the optimal policy for a given MDP. We’ll introduce the Bellman consistency condition, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.\nCode\nfrom utils import NamedTuple, Float, Array, partial, jax, jnp, latexify, latex\nfrom tabulate import tabulate\nimport numpy as np\nfrom IPython.display import Markdown, display",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#introduction",
    "href": "mdps.html#introduction",
    "title": "2  Markov Decision Processes",
    "section": "",
    "text": "Board games and video games, such as a game of chess, where a player character takes actions that update the state of the game (Guy, 2006).\n\n\n\n\n\n\n\nInventory management, where a company must efficiently move resources from producers to consumers (Frans Berkelaar, 2009).\n\n\n\n\n\n\n\nRobotic control, where a robot can move and interact with the real world to complete some task (GPA Photo Archive, 2017).\n\n\n\n\n\n\n\n\n\nRemark 2.1 (Further generalizations). In most real tasks, we don’t explicitly know the rules of the environment, or can’t concisely represent them on a computer. We will deal with this in future chapters; this chapter assumes the environment is known and accessible, so there’s nothing to learn about the environment.\nAdditionally, in many tasks, only a subset of the state is visible to the observer. Such partially observed environments are out of scope of this textbook; see Section 2.6 for additional resources.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-mdps-intro",
    "href": "mdps.html#sec-mdps-intro",
    "title": "2  Markov Decision Processes",
    "section": "2.2 Introduction to Markov decision processes",
    "text": "2.2 Introduction to Markov decision processes\nTo gain a better grasp of decision processes and identify the key features that we want to formalize, let us frame the robotic control task in ex. 2.1 as a decision problem.\n\nExample 2.2 (Robotic control as a decision problem) Suppose the goal is to move a robot to a certain position.\n\nThe state consists of the positions and velocities of the robot’s joints.\nThe action consists of the forces to apply to the robot’s motors.\nThe state transitions are essentially the rules of physics: after applying a certain force to the joints, their positions and velocities would change according to physical law.\nWe reward positions that are closer to the desired position. To be more energy-efficient, we could also deduct reward for applying a lot of force, or add other terms that describe the ideal behaviour.\n\n\n\nExercise 2.1 (Identifying decision problems) For each of the other examples in ex. 2.1, what information should the state include? What might the valid set of actions be? Describe the state transitions heuristically, and a reward function that describes how the task should be solved.\n\nIn many sequential decision-making problems, the state transitions only depend on the current state and action. For example, in ex. 2.2, if we use Newton’s laws of physics to compute the state transitions, then just knowing the current positions and velocities is enough to calculate the next positions and velocities, since Newton’s laws are second-order. We say that such state transitions satisfy the Markov property.\n\nDefinition 2.1 (Markov property (informal)) An environment’s state transitions satisfy the Markov property if “what happens next” only depends on the current state of the environment and not on the history of how we got here.\n\nWe will formally define the Markov property in def. 2.5 after introducing some notation for describing MDPs.\n\nExercise 2.2 (Checking for the Markov property) Look back at the state transitions you described in Exercise 2.1. Do they satisfy the Markov property? (For chess, consider the threefold repetition rule: if the same position occurs three times during the game, either player may claim a draw.)\n\nSequential decision-making problems that satisfy the Markov property are called Markov decision processes (MDPs). MDPs are the core setting of modern RL research. We can further classify MDPs based on whether or not the task eventually ends.\n\nDefinition 2.2 (Episodic tasks) Tasks that have a well-defined start and end, such as a game of chess or a football match, are called episodic. Each episode starts afresh and ends once the environment reaches a terminal state. The number of steps in an episode is called its horizon.\n\n\nDefinition 2.3 (Continuing tasks) On the other hand, continuing tasks, such as managing a business’s inventory, have no clear start or end point. The agent interacts with the environment in an indefinite loop.\n\nConsider an episodic task where the horizon is fixed and known in advance. That is, the agent takes up to \\(H\\) actions, where \\(H\\) is some finite positive integer. We model such tasks with a finite-horizon MDP. Otherwise, if there’s no limit to how long the episode can continue or if the task is continuing, we model the task with an infinite-horizon MDP. We’ll begin with the finite-horizon case in Section 2.3 and discuss the infinite-horizon case in Section 2.4.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-finite-horizon-mdps",
    "href": "mdps.html#sec-finite-horizon-mdps",
    "title": "2  Markov Decision Processes",
    "section": "2.3 Finite-horizon Markov decision processes",
    "text": "2.3 Finite-horizon Markov decision processes\nMany real-world tasks, such as most sports games or video games, end after a fixed number of actions from the agent. In each episode:\n\nThe environment starts in some initial state \\(s_0\\).\nThe agent observes the state and takes an action \\(a_0\\).\nThe environment updates to state \\(s_1\\) according to the action.\n\nSteps 2 and 3 are repeated \\(H\\) times, resulting in a sequence of states and actions \\(s_0, a_0, \\dots, s_{H-1}, a_{H-1}, s_H\\), where \\(s_H\\) is some terminal state. Here’s the notation we will use to describe an MDP:\n\nDefinition 2.4 (Finite-horizon Markov decision process) The components of a finite-horizon Markov decision process are:\n\nThe state that the agent interacts with. We use \\(s_h\\) to denote the state at time \\(h\\). \\(\\mathcal{S}\\) denotes the set of possible states, called the state space.\nThe actions that the agent can take. We use \\(a_h\\) to denote the action at time \\(h\\). \\(\\mathcal{A}\\) denotes the set of possible actions, called the action space.\nAn initial state distribution \\(P_0 \\in \\triangle(\\mathcal{S})\\).\nThe state transitions (a.k.a. dynamics) \\(P : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S})\\) that describe what state the agent transitions to after taking an action. We write \\(P(s_{h+ 1} \\mid s_h, a_h)\\) for the probability of transitioning to state \\(s_{h+1}\\) when starting in state \\(s_h\\) and taking action \\(a_h\\). Note that we use the letter \\(P\\) for the state transition function and the symbol \\(\\mathbb{P}\\) more generally to indicate probabilities of events.\nThe reward function. In this course, we’ll take it to be a deterministic function on state-action pairs, \\(r : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\), but many results will extend to a stochastic reward signal. We will use \\(r_{h} := r(s_h, a_h)\\) to denote the reward obtained at time \\(h\\).\nA time horizon \\(H\\in \\mathbb{N}\\) that specifies the maximum number of interactions in a trajectory, that is, a single sequence of states, actions, and rewards:\n\n\\[\n(s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1}).\n\\tag{2.1}\\]\n(Some sources omit the action and reward at the final time step.)\nCombined together, these objects specify a finite-horizon Markov decision process:\n\\[\n\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_0, P, r, H).\n\\tag{2.2}\\]\n\nWhen there are finitely many states and actions, i.e. \\(|\\mathcal{S}|, |\\mathcal{A}| &lt; \\infty\\), we can express the relevant quantities as vectors and matrices (i.e. tables of values):\n\\[\n\\begin{aligned}\n    P_0 &\\in [0, 1]^{|\\mathcal{S}|} &\n    P &\\in [0, 1]^{(|\\mathcal{S} \\times \\mathcal{A}|) \\times |\\mathcal{S}|} &\n    r &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}\n\\end{aligned}\n\\]\n(Verify that the types and shapes provided above make sense!)\n\n\nCode\nclass MDP(NamedTuple):\n    \"\"\"A description of a Markov decision process with finitely many states and actions.\"\"\"\n    S: int  # number of states\n    A: int  # number of actions\n    mu: Float[Array, \" S\"]\n    P: Float[Array, \"S A S\"]  # \"current\" state, \"current\" action, \"next\" state\n    r: Float[Array, \"S A\"]\n    H: int  # the horizon\n    gamma: float = 1.0  # discount factor (used later)\n\nclass Transition(NamedTuple):\n    \"\"\"A single state-action-reward interaction with the environment.\n\n    A trajectory consists of a sequence of transitions.\n    \"\"\"\n    s: int\n    a: int\n    r: float\n\n\n\nDefinition 2.5 (Markov property) A decision process satisfies the Markov property if the next state is independent from the past states and actions when conditioned on the current state and action:\n\\[\n\\mathbb{P}(s_{h+1} \\mid s_0, a_0, \\dots, s_h, a_h) = \\mathbb{P}(s_{h+1} \\mid s_h, a_h)\n\\]\nBy their definition, Markov decision processes satisfy the Markov property. This is because the state transitions are defined using the function \\(P\\), which only takes in a single state-action pair to transition from.\n\n\nExample 2.3 (Tidying MDP) Let’s consider a simple decision problem throughout this chapter: the task of keeping your room tidy.\nYour room has the possible states \\(\\mathcal{S} = \\{ \\text{orderly}, \\text{messy} \\}.\\) You can take either of the actions \\(\\mathcal{A} = \\{ \\text{ignore}, \\text{tidy} \\}.\\) The room starts off orderly, that is, \\(P_0(\\text{orderly}) = 1\\).\nThe state transitions are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it might become messy, according to the probabilities in tbl. 2.1.\nThe rewards are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy your additional time). Tidying a messy room is a chore that gives no reward.\nConsider a time horizon of \\(H= 7\\) days (one interaction per day). Let \\(t = 0\\) correspond to Monday and \\(t = 6\\) correspond to Sunday.\n\n\n\nCode\ntidy_mdp = MDP(\n    S=2,  # 0 = orderly, 1 = messy\n    A=2,  # 0 = ignore, 1 = tidy\n    mu=jnp.array([1.0, 0.0]),  # start in orderly state\n    P=jnp.array([\n        [\n            [0.7, 0.3],  # orderly, ignore\n            [1.0, 0.0],  # orderly, tidy\n        ],\n        [\n            [0.0, 1.0],  # messy, ignore\n            [1.0, 0.0],  # messy, tidy\n        ],\n    ]),\n    r=jnp.array([\n        [\n            1.0,   # orderly, ignore\n            -1.0,  # orderly, tidy\n        ],\n        [\n            -1.0,  # messy, ignore\n            0.0,   # messy, tidy\n        ]\n    ]),\n    H=7,\n)\n\nMarkdown(tabulate(\n    zip(*[\n        ['orderly', 'orderly', 'messy', 'messy'],\n        ['ignore', 'tidy', 'ignore', 'tidy'],\n        tidy_mdp.P[:, :, 0].flatten(),\n        tidy_mdp.P[:, :, 1].flatten(),\n        tidy_mdp.r.flatten(),\n    ]),\n    [\n        r\"$s$\",\n        r\"$a$\",\n        r\"$P(\\text{orderly} \\mid s, a)$\",\n        r\"$P(\\text{messy} \\mid s, a)$\",\n        r\"$r(s, a)$\",\n    ]\n))\n\n\n\n\nTable 2.1: Description of the MDP in ex. 2.3.\n\n\n\n\n\n\n\\(s\\)\n\\(a\\)\n\\(P(\\text{orderly} \\mid s, a)\\)\n\\(P(\\text{messy} \\mid s, a)\\)\n\\(r(s, a)\\)\n\n\n\n\norderly\nignore\n0.7\n0.3\n1\n\n\norderly\ntidy\n1\n0\n-1\n\n\nmessy\nignore\n0\n1\n-1\n\n\nmessy\ntidy\n1\n0\n0\n\n\n\n\n\n\n\n\nWe’ll now introduce several core concepts when working with MDPs.\n\nDefinition 2.6 (Policies) A policy \\(\\pi\\) describes the agent’s strategy: which actions it takes in a given situation. A key goal of RL is to find the optimal policy that maximizes the total reward on average.\nThere are three axes along which policies can vary: their outputs, inputs, and time-dependence.\n\nOutputs: Deterministic or stochastic. A deterministic policy outputs actions while a stochastic policy outputs distributions over actions.\n\n\n\n\n\n\n\n\n\nA deterministic policy, which produces a single action\n\n\n\n\n\n\n\nA stochastic policy, which produces a distribution over possible actions\n\n\n\n\n\n\nFigure 2.2\n\n\n\n\nInputs: history-independent or history-dependent. A history-independent (a.k.a. “Markovian”) policy only depends on the current state, while a history-dependent policy depends on the sequence of past states, actions, and rewards. We’ll only consider history-independent policies in this course.\nStationary or time-dependent. A stationary (a.k.a. time-homogeneous) policy keeps the same strategy at all time steps, while a time-dependent policy can depend on the current timestep. For consistency with states and actions, we will denote the timestep as a subscript, i.e. \\(\\pi = \\{ \\pi_0, \\dots, \\pi_{H-1} \\}.\\)\n\n\nNote that for finite state and action spaces, we can represent a randomized mapping \\(\\mathcal{S} \\to \\triangle(\\mathcal{A})\\) as a matrix \\(\\pi \\in [0, 1]^{\\mathcal{S} \\times \\mathcal{A}}\\) where each row describes the policy’s distribution over actions for the corresponding state.\n\nExample 2.4 (Policies for the tidying MDP) Here are some possible deterministic policies for the tidying MDP ex. 2.3:\n\nAlways tidy: \\(\\pi(s) = \\text{tidy}\\).\nOnly tidy on weekends: \\(\\pi_h(s) = \\text{tidy}\\) if \\(h\\in \\{ 5, 6 \\}\\) and \\(\\pi_h(s) = \\text{ignore}\\) otherwise.\nOnly tidy if the room is messy: \\(\\pi_h(\\text{messy}) = \\text{tidy}\\) and \\(\\pi_h(\\text{orderly}) = \\text{ignore}\\) for all \\(h\\).\n\n\n\n\nCode\n# arrays of shape (H, S, A) represent time-dependent policies\ntidy_policy_always_tidy = (\n    jnp.zeros((7, 2, 2))\n    .at[:, :, 1].set(1.0)\n)\ntidy_policy_weekends = (\n    jnp.zeros((7, 2, 2))\n    .at[5:7, :, 1].set(1.0)\n    .at[0:5, :, 0].set(1.0)\n)\ntidy_policy_messy_only = (\n    jnp.zeros((7, 2, 2))\n    .at[:, 1, 1].set(1.0)\n    .at[:, 0, 0].set(1.0)\n)\n\n\n\n\nExercise 2.3 (Conditional independence for future states) Suppose actions are chosen according to a history-independent policy. Use the chain rule of probability to show that, conditioned on the current state and action \\(s_h, a_h\\), all future states \\(s_{h'}\\) where \\(h' &gt; h\\) are conditionally independent of the past states and actions. That is, for all \\(h' &gt; h\\),\n\\[\n\\mathbb{P}(s_{h'} \\mid s_0, a_0, \\dots, s_h, a_h) = \\mathbb{P}(s_{h'} \\mid s_h, a_h).\n\\]\n\n\nDefinition 2.7 (Trajectory distribution) Once we’ve chosen a policy, we can sample trajectories by repeatedly choosing actions according to the policy and observing the rewards and updated states returned by the environment. This generative process induces a distribution \\(\\rho^{\\pi}\\) over trajectories. (We assume that \\(P_0\\) and \\(P\\) are clear from context.)\n\n\nExample 2.5 (Trajectories in the tidying environment) Here is a possible trajectory for the tidying example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(h\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\n\n\n\n\\(s\\)\norderly\norderly\norderly\nmessy\nmessy\norderly\norderly\n\n\n\\(a\\)\ntidy\nignore\nignore\nignore\ntidy\nignore\nignore\n\n\n\\(r\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(0\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\nCould any of the policies in ex. 2.4 have generated this trajectory?\n\nNote that for a history-independent policy, using the Markov property (def. 2.5), we can write down the likelihood function of this probability distribution in an autoregressive way (i.e. one timestep at a time):\n\nTheorem 2.1 (Trajectory distributions of history-independent policies) Let \\(\\pi\\) be a history-independent policy. Then its trajectory distribution \\(\\rho^\\pi\\) (def. 2.7) over sequences of actions and states \\((s_0, a_0, \\dots, s_{H-1}, a_{H-1})\\) can be written as\n\\[\n\\rho^{\\pi}(\\tau) := P_0(s_0) \\pi_0(a_0 \\mid s_0) P(s_1 \\mid s_0, a_0) \\cdots\nP(s_{H-1} \\mid s_{H-2}, a_{H-2}) \\pi_{H-1}(a_{H-1} \\mid s_{H-1}).\n\\tag{2.3}\\]\nNote that we use a deterministic reward function, so we only consider randomness over the states and actions.\n\n\nExercise 2.4 (Trajectory distribution for stochastic reward) Modify eq. 2.3 for the case when the reward function is stochastic, that is, \\(r : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathbb{R})\\).\n\n\n\n\n\n\n\ngraph LR\n    S0($$s_0$$) -- $$\\pi_0$$ --&gt; A0{{$$a_0$$}}\n    S0 & A0 --&gt; R0[$$r_0$$]\n    A0 & S0 --&gt; S1($$s_1$$)\n    S1 -- $$\\pi_1$$ --&gt; A1{{$$a_1$$}}\n    S1 & A1 --&gt; R1[$$r_1$$]\n    A1 & S1 --&gt; S2($$s_2$$)\n    S2 -- $$\\pi_2$$ --&gt; A2{{$$a_2$$}}\n    S2 & A2 --&gt; R2[$$r_2$$]\n    A2 & S2 --&gt; S3($$...$$)\n\n    class S0,S1,S2,S3 state\n    class A0,A1,A2 action\n    class R0,R1,R2 reward\n\n    classDef state fill: lightgreen;\n    classDef action fill: lightyellow;\n    classDef reward fill: lightblue;\n\n\n\n\nFigure 2.3: A trajectory is generated by taking a sequence of actions according to the history-independent policy.\n\n\n\n\n\n\n\nCode\ndef trajectory_log_likelihood(\n    mdp: MDP,\n    traj: list[Transition],\n    pi: Float[Array, \"S A\"],\n) -&gt; float:\n    \"\"\"Compute the log-likelihood of a trajectory under a given MDP and policy.\"\"\"\n\n    # initial distribution and action\n    total = jnp.log(mdp.mu[traj[0].s])\n    total += jnp.log(pi[traj[0].s, traj[0].a])\n\n    # remaining state transitions and actions\n    for i in range(1, mdp.H):\n        total += jnp.log(mdp.P[traj[i - 1].s, traj[i - 1].a, traj[i].s])\n        total += jnp.log(pi[traj[i].s, traj[i].a])\n\n    return total\n\n\nFor a deterministic policy \\(\\pi\\), we have that \\(\\pi_h(a \\mid s) = \\mathbf{1}\\left\\{a = \\pi_h(s)\\right\\}\\); that is, the probability of taking an action is \\(1\\) if it’s the unique action prescribed by the policy for that state and \\(0\\) otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution \\(P_0\\) and the state transitions \\(P\\).\n\n2.3.1 Policy evaluation\nRecall that the core goal of an RL algorithm is to find a policy that maximizes the expected total reward\n\\[\n\\mathop{\\mathbb{E}}[r_0 + \\cdots + r_{H-1}],\n\\tag{2.4}\\]\nwhere \\(r_h= r(s_h, a_h)\\). Note that the quantity \\(r_0 + \\cdots + r_{H-1}\\) is a random variable whose distribution depends on the policy’s trajectory distribution \\(\\rho^\\pi\\) (def. 2.7). We need tools for describing the expected total reward achieved by a given policy, starting in specific states and actions. This will allow us to compare the quality of different policies and compute the optimal policy.\n\nDefinition 2.8 (Value function) Let \\(\\pi\\) be a history-independent policy. Its value function at time \\(h\\) returns the expected remaining reward when starting in a specific state:\n\\[\nV_h^\\pi(s) := \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} [r_h+ \\cdots + r_{H-1} \\mid s_h= s]\n\\tag{2.5}\\]\n\n\nDefinition 2.9 (Action-value function) Similarly, a history-independent policy \\(\\pi\\)’s action-value function (aka Q function) at time \\(h\\) returns its expected remaining reward, starting in a specific state and taking a specific action:\n\\[\nQ_h^\\pi(s, a) := \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} [r_h+ \\cdots + r_{H-1} \\mid s_h= s, a_h= a].\n\\tag{2.6}\\]\n\n\n\n\n\n\n\ngraph LR\n    S0($$s_h = s$$) -- $$\\pi_h$$ --&gt; A0{{$$a_h$$}}\n    S0 & A0 --&gt; R0[$$r_h$$]\n    A0 & S0 --&gt; S1(\"$$s_{h+1}$$\")\n    S1 -- \"$$\\pi_{h+1}$$\" --&gt; A1{{\"$$a_{h+1}$$\"}}\n    S1 & A1 --&gt; R1[\"$$r_{h+1}$$\"]\n    A1 & S1 --&gt; S2(\"$$s_{h+2}$$\")\n    S2 -- \"$$\\pi_{h+2}$$\" --&gt; A2{{\"$$a_{h+2}$$\"}}\n    S2 & A2 --&gt; R2[\"$$r_{h+2}$$\"]\n    A2 & S2 --&gt; S3(\"...\")\n\n    class S0,R0,R1,R2 thick\n\n    classDef thick stroke-width: 4px;\n\n    class S0,S1,S2,S3 state\n    class A0,A1,A2 action\n    class R0,R1,R2 reward\n\n    classDef state fill: lightgreen;\n    classDef action fill: lightyellow;\n    classDef reward fill: lightblue;\n\n\n\n\nFigure 2.4: The policy starts in state \\(s\\) at time \\(h\\). Then the expected remaining reward is computed.\n\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    S0($$s_h = s$$) ~~~ A0{{$$a_h = a$$}}\n    S0 & A0 --&gt; R0[$$r_h$$]\n    A0 & S0 --&gt; S1(\"$$s_{h+1}$$\")\n    S1 -- \"$$\\pi_{h+1}$$\" --&gt; A1{{\"$$a_{h+1}$$\"}}\n    S1 & A1 --&gt; R1[\"$$r_{h+1}$$\"]\n    A1 & S1 --&gt; S2(\"$$s_{h+2}$$\")\n    S2 -- \"$$\\pi_{h+2}$$\" --&gt; A2{{\"$$a_{h+2}$$\"}}\n    S2 & A2 --&gt; R2[\"$$r_{h+2}$$\"]\n    A2 & S2 --&gt; S3(\"...\")\n\n    class S0,A0,R0,R1,R2 thick;\n\n    classDef thick stroke-width: 4px;\n\n    class S0,S1,S2,S3 state\n    class A0,A1,A2 action\n    class R0,R1,R2 reward\n\n    classDef state fill: lightgreen;\n    classDef action fill: lightyellow;\n    classDef reward fill: lightblue;\n\n\n\n\nFigure 2.5: The policy starts in state \\(s\\) at time \\(h\\) and takes action \\(a\\). Then the expected remaining reward is computed.\n\n\n\n\n\nThese two functions define each other in the following way:\n\nTheorem 2.2 (Relating \\(V^\\pi\\) and \\(Q^\\pi\\)) Let \\(\\pi\\) be a history-independent policy. The value of a state is the expected action-value in that state over actions drawn from the policy:\n\\[\nV_h^\\pi(s) = \\mathop{\\mathbb{E}}_{a \\sim \\pi_h(\\cdot \\mid s)} [Q_h^\\pi(s, a)].\n\\tag{2.7}\\]\nThe value of a state-action pair is the sum of the immediate reward and the expected value of the following state:\n\\[\nQ_h^\\pi(s, a) = r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [V_{h+1}^\\pi(s')].\n\\tag{2.8}\\]\n\n\nExercise 2.5 (Proof of relationship between \\(V^\\pi\\) and \\(Q^\\pi\\)) Use the law of iterated expectations to show eq. 2.7. Now apply linearity of expectation to show eq. 2.8. Where did you use the assumption that \\(\\pi\\) doesn’t depend on the past states and actions?\n\n\nRemark 2.2 (Computing \\(Q^\\pi\\) from \\(V^\\pi\\) requires environment). Note that all you need to compute \\(V_h^\\pi\\) from \\(Q_h^\\pi\\) (eq. 2.7) is knowledge of the policy \\(\\pi\\). On the other hand, to compute \\(Q_h^\\pi\\) from \\(V_{h+1}^\\pi\\) (eq. 2.8), you need knowledge of the reward function and state transitions to look ahead one step. In later chapters, we’ll study problems where the environment is unknown, making it more useful to approximate \\(Q_h^\\pi\\) than \\(V_h^\\pi\\).\n\n\n\nCode\ndef q_to_v(\n    policy: Float[Array, \"S A\"],\n    q: Float[Array, \"S A\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"\n    Compute the value function for a given policy in a known finite MDP\n    at a single timestep from its action-value function.\n    \"\"\"\n    return jnp.average(q, weights=policy, axis=1)\n\ndef v_to_q(\n    mdp: MDP,\n    v_next: Float[Array, \" S\"],\n) -&gt; Float[Array, \"S A\"]:\n    \"\"\"\n    Compute the action-value function in a known finite MDP\n    at a single timestep from the corresponding value function.\n    \"\"\"\n    # the discount factor is relevant later\n    return mdp.r + mdp.gamma * mdp.P @ v_next\n\n\n# convert a list of v functions to a list of q functions\nv_ary_to_q_ary = jax.vmap(v_to_q, in_axes=(None, 0))\n\n\nPutting eq. 2.7 and eq. 2.8 together reveals the Bellman consistency equations. By simply considering the cumulative reward as the sum of the immediate reward and the remaining reward, we can describe the value function in terms of itself. The resulting system of equations is named after Richard Bellman (1920–1984), who is credited with introducing dynamic programming in 1953.\n\nTheorem 2.3 (Bellman consistency equations) For a history-independent policy \\(\\pi\\),\n\\[\nV_h^\\pi(s) = \\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi_h(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + V_{h+1}^\\pi(s')].\n\\tag{2.9}\\]\nThe proof is left as Exercise 2.5. This is a system of \\(H |\\mathcal{S}|\\) equations (one equation per state per timestep) in \\(H |\\mathcal{S}|\\) unknowns (the values \\(V_h^\\pi(s)\\)), so \\(V^\\pi\\) is the unique solution, as long as no two states are exactly identical to each other.\n\n\n\nCode\ndef check_bellman_consistency_v(\n    mdp: MDP,\n    policy: Float[Array, \"H S A\"],\n    v_ary: Float[Array, \"H S\"],\n) -&gt; bool:\n    \"\"\"\n    Check that the given (time-dependent) \"value function\"\n    satisfies the Bellman consistency equation.\n    \"\"\"\n    return all(\n        jnp.allclose(\n            # lhs\n            v_ary[h],\n            # rhs\n            jnp.sum(policy[h] * (mdp.r + mdp.gamma * mdp.P @ v_ary[h + 1]), axis=1),\n        )\n        for h in range(mdp.H - 1)\n    )\n\n\n\nRemark 2.3 (The Bellman consistency equation for deterministic policies). Note that for history-independent deterministic policies, the Bellman consistency equations simplify to\n\\[\nV_h^\\pi(s) = r(s, \\pi_h(s)) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi_h(s))} [V_{h+1}^\\pi(s')].\n\\tag{2.10}\\]\n\nHow can we actually evaluate a given policy, that is, compute its value function?\nSuppose we start with some guess \\(v_h: \\mathcal{S} \\to \\mathbb{R}\\) for the state values at time \\(h= 0, \\dots, H-1\\). We write \\(v\\) in lowercase to indicate that it might not be an actual value function of a policy. How might we improve this guess?\nRecall that the Bellman consistency equations (eq. 2.9) hold for the value functions of history-independent policies. So if \\(v\\) is close to the target \\(V^\\pi\\), it should nearly satisfy the system of equations associated with \\(\\pi\\). With this in mind, suppose we replace \\(V^\\pi_{h+1}\\) with \\(v_{h+1}\\) on the right-hand side. Then the new right-hand side quantity,\n\\[\n\\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi_h(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + v_{h+1}(s')],\n\\tag{2.11}\\]\ncan be thought of as follows: we take one action according to \\(\\pi\\), observe the immediate reward, and evaluate the next state using \\(v\\). This is illustrated in fig. 2.6 below. This operation gives us an updated estimate of the value of \\(V^\\pi_h(s)\\) that is at least as accurate as applying \\(v(s)\\) directly.\n\n\n\n\n\n\ngraph LR\n    S0($$s$$) -- $$\\pi_h$$ --&gt; A0{{\"$$a$$\"}}\n    S0 & A0 --&gt; R0[\"$$r_h = r(s, a)$$\"]\n    A0 & S0 --&gt; S1(\"$$s'$$\")\n    S1 -- \"$$v_{h+1}$$\" --&gt; R1[\"$$r_{h+1} + \\cdots + r_{H-1} \\approx v_{h+1}(s')$$\"]\n\n    class R0,R1 thick\n\n    classDef thick stroke-width: 4px;\n\n    class S0,S1 state\n    class A0 action\n    class R0,R1 reward\n\n    classDef state fill: lightgreen;\n    classDef action fill: lightyellow;\n    classDef reward fill: lightblue;\n\n\n\n\nFigure 2.6: We evaluate the next state using \\(v_{h+1}\\).\n\n\n\n\n\nWe can treat this operation of taking \\(v_h\\) to its improved version in eq. 2.11 as a higher-order function known as the Bellman operator.\n\nDefinition 2.10 (Bellman operator) Let \\(\\pi : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\). the Bellman operator \\(\\mathcal{J}^{\\pi} : (\\mathcal{S} \\to \\mathbb{R}) \\to (\\mathcal{S} \\to \\mathbb{R})\\) is the higher-order function that takes in a function \\(v : \\mathcal{S} \\to \\mathbb{R}\\) and returns the r.h.s. of the Bellman equation with \\(v\\) substituted in:\n\\[\n\\mathcal{J}^{\\pi}(v) := \\left(\n    s \\mapsto \\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + v(s')]\n\\right).\n\\tag{2.12}\\]\n\nThe Bellman operator is a crucial tool for reasoning about MDPs. Intuitively, it answers the following question: if we evaluate the next state using \\(v\\), how good is the current state, if we take a single action from the given policy?\n\n\nCode\ndef bellman_operator_looping(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"\n    Looping definition of the Bellman operator.\n    Concise version is below\n    \"\"\"\n    v_new = jnp.zeros((mdp.S,))\n    for s in range(mdp.S):\n        for a in range(mdp.A):\n            for s_next in range(mdp.S):\n                v_new[s] += (\n                    policy[s, a]\n                    * mdp.P[s, a, s_next]\n                    * (mdp.r[s, a] + mdp.gamma * v[s_next])\n                )\n    return v_new\n\n# Note that we can concisely implement this using the `q_to_v` and `v_to_q` utilities from above:\n\ndef bellman_operator(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"For a known finite MDP, the Bellman operator can be exactly evaluated.\"\"\"\n    return q_to_v(policy, v_to_q(mdp, v))  # equivalent\n    return jnp.sum(policy * (mdp.r + mdp.gamma * mdp.P @ v), axis=1)\n\nlatex(\n    bellman_operator_looping,\n    id_to_latex={\"v_new\": r\"v^\\text{lhs}\", \"policy\": r\"\\pi\", \"s_next\": r\"s'\"},\n    trim_prefixes={\"mdp\"},\n    subscript_type=\"call\",\n)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{bellman\\_operator\\_looping}(\\mathrm{mdp}: \\mathrm{MDP}, \\pi: \\mathbb{R}^{S \\times A}, v: \\mathbb{R}^{S}) \\\\ \\hspace{1em} \\textrm{\"\n    Looping definition of the Bellman operator.\n    Concise version is below\n    \"} \\\\ \\hspace{1em} v^\\text{lhs} \\gets \\mathbf{0}^{S} \\\\ \\hspace{1em} \\mathbf{for} \\ s \\in \\mathrm{range} \\mathopen{}\\left( S \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathrm{range} \\mathopen{}\\left( A \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathbf{for} \\ s' \\in \\mathrm{range} \\mathopen{}\\left( S \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{4em} v^\\text{lhs}(s) \\gets v^\\text{lhs}(s) + \\pi(s, a) P(s, a, s') \\cdot \\mathopen{}\\left( r(s, a) + \\gamma v(s') \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ v^\\text{lhs} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 2.7: An algorithm for computing the Bellman operator for a finite state and action space.\n\n\n\n\nThe Bellman operator also gives us a concise way to express the Bellman consistency equations (eq. 2.9) for the value function:\n\\[\nV_h^\\pi = \\mathcal{J}^{\\pi_h}(V_{h+1}^\\pi)\n\\tag{2.13}\\]\nThe Bellman consistency equations (eq. 2.9) give us a convenient algorithm for evaluating stationary policies: it expresses the value function at timestep \\(h\\) as a function of the value function at timestep \\(h+1\\). This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman operator to compute the value function at each time step.\n\n\nCode\ndef dp_eval_finite(mdp: MDP, policy: Float[Array, \"S A\"]) -&gt; Float[Array, \"H S\"]:\n    \"\"\"Evaluate a policy using dynamic programming.\"\"\"\n    V = np.zeros((mdp.H + 1, mdp.S))  # initialize to 0 at end of time horizon\n    for h in range(mdp.H - 1, -1, -1):\n        V[h] = bellman_operator(mdp, policy[h], V[h + 1])\n    return V[:-1]\n\nlatex(\n    dp_eval_finite,\n    id_to_latex={\"bellman_operator\": r\"\\mathcal{J}\", \"policy\": r\"\\pi\", \"V\": \"V^\\pi\"},\n)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{dp\\_eval\\_finite}(\\mathrm{mdp}: \\mathrm{MDP}, \\pi: \\mathbb{R}^{S \\times A}) \\\\ \\hspace{1em} \\textrm{\"Evaluate a policy using dynamic programming.\"} \\\\ \\hspace{1em} V^\\pi \\gets \\mathbf{0}^{\\mathrm{mdp}.H + 1 \\times \\mathrm{mdp}.S} \\\\ \\hspace{1em} \\mathbf{for} \\ h \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{mdp}.H - 1, -1, -1 \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} V^\\pi_{h} \\gets \\mathcal{J} \\mathopen{}\\left( \\mathrm{mdp}, \\pi_{h}, V^\\pi_{h + 1} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ V^\\pi_{:-1} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 2.8: A dynamic programming algorithm for evaluating a policy in a finite-horizon MDP.\n\n\n\n\nThis runs in time \\(O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)\\). Note that the implementation of the Bellman operator in fig. 2.7 can be easily modified to compute \\(Q^\\pi\\) as an intermediate step. Do you see how?\n\nExample 2.6 (Tidying policy evaluation) Let’s evaluate the policy from ex. 2.4 in the tidying MDP that tidies if and only if the room is messy. We’ll use the Bellman consistency equation to compute the value function at each time step.\n\\[\n\\begin{aligned}\nV_{H-1}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) \\\\\n&= 1 \\\\\nV_{H-1}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) \\\\\n&= 0 \\\\\nV_{H-2}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-1}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1 + 0.3 \\cdot 0 \\\\\n&= 1.7 \\\\\nV_{H-2}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-1}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 \\\\\nV_{H-3}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-2}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1.7 + 0.3 \\cdot 1 \\\\\n&= 2.49 \\\\\nV_{H-3}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-2}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1.7\n\\end{aligned}\n\\]\netc. You may wish to repeat this computation for the other policies to get a better sense of this algorithm.\n\n\n\nCode\nV_messy = dp_eval_finite(tidy_mdp, tidy_policy_messy_only)\nMarkdown(tabulate(\n    zip(*[\n        [\"orderly\", \"messy\"],\n        *V_messy,\n    ]),\n    [\"$s$\"] + [f\"$h={h}$\" for h in range(len(V_messy))]\n))\n\n\n\n\n\n\\(s\\)\n\\(h=0\\)\n\\(h=1\\)\n\\(h=2\\)\n\\(h=3\\)\n\\(h=4\\)\n\\(h=5\\)\n\\(h=6\\)\n\n\n\n\norderly\n5.56217\n4.79277\n4.0241\n3.253\n2.49\n1.7\n1\n\n\nmessy\n4.79277\n4.0241\n3.253\n2.49\n1.7\n1\n0\n\n\n\n\n\n\n\n2.3.2 Optimality\nWe’ve just seen how to evaluate a given policy. But how can we find the best policy for a given environment? We must first define what it means for a policy to be optimal. We’ll investigate optimality for history-independent policies. In Theorem 2.6, we’ll see that constraining ourselves to history-independent policies isn’t actually a constraint: the best history-independent policy performs at least as well as the best history-dependent policy, in all scenarios!\n\nRemark 2.4 (Intuition). How is this possible? Recall the Markov property (def. 2.5), which says that once we know the current state, the next state becomes independent from the past history. This means that history-dependent policies, intuitively speaking, don’t benefit over simply history-independent ones in MDPs.\n\n\nDefinition 2.11 (Optimal policies) Let \\(\\pi^\\star\\) be a history-independent policy. We say that \\(\\pi^\\star\\) is optimal if \\(V_h^{\\pi^\\star}(s) \\ge V_h^{\\pi}(s)\\) for any history-independent policy \\(\\pi\\) at any time \\(h\\in [H]\\). In other words, an optimal history-independent policy achieves the highest possible expected remaining reward in every state at every time.\n\n\nExample 2.7 (Optimal policy in the tidying MDP) For the tidying MDP (ex. 2.3), you might guess that the optimal strategy is\n\\[\n\\pi(s) = \\begin{cases}\n\\text{tidy} & s = \\text{messy} \\\\\n\\text{ignore} & \\text{otherwise}\n\\end{cases}\n\\tag{2.14}\\]\nsince this keeps the room in the “orderly” state in which reward can be obtained.\n\n\nDefinition 2.12 (Optimal value function) Given a state \\(s\\) at time \\(h\\), the optimal value \\(V^\\star_h(s)\\) is the maximum expected remaining reward achievable by any history-independent policy \\(\\pi\\):\n\\[\nV^\\star_h(s) := \\max_{\\pi} V^{\\pi}_h(s).\n\\tag{2.15}\\]\nThe optimal action-value function is defined analogously:\n\\[\nQ^\\star_h(s, a) := \\max_{\\pi} Q^{\\pi}_h(s, a).\n\\tag{2.16}\\]\n\nThese satisfy the following relationship (cf Theorem 2.2):\n\nTheorem 2.4 (Relating \\(V^\\star\\) and \\(Q^\\star\\)) The optimal value of a state is the maximum value across actions from that state:\n\\[\nV_h^\\star(s) = \\max_{a \\in \\mathcal{A}} Q^\\star_h(s, a).\n\\tag{2.17}\\]\nThe optimal value of an action is the immediate reward plus the expected value from the next state:\n\\[\nQ^\\star_h(s, a) = r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [V^\\star_{h+1}(s')].\n\\tag{2.18}\\]\n\n\nProof. We first prove eq. 2.17. We begin by expanding the definition of the optimal value function using eq. 2.7:\n\\[\n\\begin{aligned}\nV_h^\\star(s)\n&= \\max_\\pi V^\\pi_h(s) \\\\\n&= \\max_\\pi \\mathop{\\mathbb{E}}_{a \\sim \\pi_h(s)} [\n    Q^\\pi_h(s, a)\n].\n\\end{aligned}\n\\tag{2.19}\\]\nNow note that \\(Q^\\pi_h\\) doesn’t depend on \\(\\pi_h\\), so we can split the maximization over \\(\\pi = (\\pi_0, \\dots, \\pi_{H-1})\\) into an outer optimization over the immediate action \\(\\pi_h\\) and an inner optimization over the remaining actions \\(\\pi_{h+ 1}, \\dots, \\pi_{H-1}\\):\n\\[\n\\max_\\pi \\mathop{\\mathbb{E}}_{a \\sim \\pi_h(s)} \\Big[\n    Q^\\pi_h(s, a)\n]\n= \\max_{\\pi_h} \\mathop{\\mathbb{E}}_{a \\sim \\pi_h(s)} \\Big[\n    \\max_{\\pi_{h+1}, \\dots, \\pi_{H-1}}\n    Q^\\pi_h(s, a)\n\\Big].\n\\tag{2.20}\\]\nBut now the inner quantity is exactly \\(Q^\\star_h(s, a)\\) as defined in eq. 2.16. Now note that maximizing over \\(\\pi_h\\) reduces to just maximizing over the action taken:\n\\[\n\\max_{\\pi_h} \\mathop{\\mathbb{E}}_{a \\sim \\pi_h(s)} [\\cdots] = \\max_{a \\in \\mathcal{A}} [\\cdots].\n\\tag{2.21}\\]\nThis proves eq. 2.17.\nWe now prove eq. 2.18. We begin by using eq. 2.8:\n\\[\n\\begin{aligned}\nQ^\\star_h(s, a) &= \\max_\\pi Q^\\pi_h(s, a) \\\\\n&= \\max_\\pi \\Big[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}[ V^\\pi_{h+1}(s') ] \\Big]\n\\end{aligned}\n\\]\nWe can move the maximization over \\(\\pi\\) under the expectation since \\(s, a\\), and \\(s'\\) are all independent of \\(\\pi\\). Substituting the definition of \\(V^\\star\\) concludes the proof.\n\nAs in the Bellman consistency equations (Theorem 2.3), combining eq. 2.17 and eq. 2.18 gives the Bellman optimality equations.\n\nTheorem 2.5 (Bellman optimality equations) The optimal value function function (def. 2.12) satisfies, for all timesteps \\(h\\in [H-1]\\),\n\\[\nV_h^\\star(s) = \\max_a r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [V_{h+1}^\\star(s')].\n\\tag{2.22}\\]\nLike the Bellman consistency equations (eq. 2.9), This is a system of \\(H |\\mathcal{S}|\\) equations in \\(H |\\mathcal{S}|\\) unknowns, so \\(V^\\star\\) is the unique solution (assuming all the states are distinguishable from each other).\nNote that the letter \\(\\pi\\) doesn’t appear. This will prove useful when we discuss off-policy algorithms later in the course.\n\nWe now construct a deterministic history-independent optimal policy by acting greedily with respect to the optimal action-value function:\n\nDefinition 2.13 (Greedy policies) For any sequence of functions \\(q_h : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) for \\(h = 0, \\dots, H-1\\), we define the greedy policy \\(\\pi^q\\) to be the deterministic policy that selects the action with the highest value according to \\(q\\) at each state:\n\\[\n\\pi_h^q(s) := \\arg\\max_{a \\in \\mathcal{A}} q_h(s, a).\n\\tag{2.23}\\]\nNote that it is not true in general that \\(Q^{\\pi^q} = q\\) for a greedy policy! For one, \\(q\\) might not even be a consistent Q function.\n\n\n\nCode\ndef q_to_greedy(q: Float[Array, \"S A\"]) -&gt; Float[Array, \"S A\"]:\n    \"\"\"\n    Get the (deterministic) greedy policy with respect to an action-value function.\n    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.\n    \"\"\"\n    A = q.shape[1]\n    a_ary = jnp.argmax(q, axis=1)\n    return jnp.eye(A)[a_ary]\n\n\ndef v_to_greedy(mdp: MDP, v: Float[Array, \" S\"]) -&gt; Float[Array, \"S A\"]:\n    \"\"\"Get the (deterministic) greedy policy with respect to a value function.\"\"\"\n    return q_to_greedy(v_to_q(mdp, v))\n\n\n\nTheorem 2.6 (A greedy optimal policy) The greedy policy \\(\\pi^{Q^\\star}\\), where \\(Q^\\star\\) is the optimal action-value function (eq. 2.16), is an optimal policy (def. 2.11). Furthermore, \\(\\pi^{Q^\\star}\\) is optimal across all policies \\(\\pi^\\text{any}\\), including history-dependent ones, in the sense that for any partial trajectory\n\\[\n\\tau_h= (s_0, a_0, r_0, \\dots, s_{h-1}, a_{h-1}, r_{h-1}, s_h),\n\\]\nit achieves a higher expected remaining reward:\n\\[\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^{Q^\\star}}} [\n    r_h+ \\cdots + r_{H-1}\n    \\mid \\tau_h\n]\n\\ge\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^\\text{any}}} [\n    r_h+ \\cdots + r_{H-1}\n    \\mid \\tau_h\n].\n\\tag{2.24}\\]\nBy conditioning on \\(\\tau_h\\), we mean to condition on the event that \\(s_{h'}, a_{h'}\\) are the state and action visited at time \\(h' &lt; h\\), and \\(s_h\\) is the state at time \\(h\\). (If the reward function were stochastic, we would condition on the observed rewards as well.)\n\nWe will now prove by induction that \\(\\pi^{Q^\\star}\\) is optimal. Essentially, we begin at the end of the trajectory, where the optimal action is simply the one that yields highest immediate reward. Once the optimal values at time \\(H-1\\) are known, we use these to compute the optimal values at time \\(H-2\\) using the Bellman optimality equations (eq. 2.38), and proceed backward through the trajectory.\n\n\n\n\n\n\ngraph RL\n    Q0[\"$$Q^\\star_{H-1} = r$$\"] -- \"$$\\max_{a \\in \\mathcal{A}}$$\" --&gt; V0\n    Q0 -- \"$$\\arg\\max_{a \\in \\mathcal{A}}$$\" --&gt; P0[\"$$\\pi^\\star_{H-1}$$\"]\n    V0[\"$$V^\\star_{H-1}$$\"] -- \"Bellman equations\" --&gt; Q1\n    \n    Q1[\"$$Q^\\star_{H-2}$$\"] -- \"$$\\max_{a \\in \\mathcal{A}}$$\" --&gt; V1\n    Q1 -- \"$$\\arg\\max_{a \\in \\mathcal{A}}$$\" --&gt; P1[\"$$\\pi^\\star_{H-2}$$\"]\n    V1[\"$$V^\\star_{H-2}$$\"] -- \"Bellman equations\" --&gt; Q2\n\n    Q2[\"...\"]\n\n\n\n\nFigure 2.9: Illustrating a dynamic programming algorithm for computing the optimal policy in a finite-horizon MDP.\n\n\n\n\n\n\nProof. At the end of the trajectory (time step \\(H-1\\)), we can’t take any more actions, so the \\(Q\\)-function simply returns the immediate reward:\n\\[\nQ^\\star_{H-1}(s, a) = r(s, a).\n\\tag{2.25}\\]\n\\(\\pi^{Q^\\star}\\) is then optimal across all policies at time \\(H-1\\), since the policy only determines a single action, the one which maximizes (expected) remaining reward:\n\\[\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^{Q^\\star}}}[ r_{H-1} \\mid s_{H- 1} = s ]\n= \\max_{a \\in \\mathcal{A}} r(s, a).\n\\tag{2.26}\\]\nFor the inductive step, suppose \\(\\pi^{Q^\\star}\\) is optimal across all policies at time \\(h+1\\). Note that this implies \\(V^{\\pi^{Q^\\star}} = V^\\star\\). Then for any other policy \\(\\pi^\\text{any}\\) (which could be history-dependent), and for any partial trajectory \\(\\tau_{h}\\),\n\\[\n\\begin{aligned}\n&{} \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^\\text{any}}} [\n    r_h+ \\cdots + r_{H-1} \\mid \\tau_{h}\n] \\\\\n={}&\n\\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi^\\text{any}_h(\\tau_h) \\\\ s' \\sim P(s_h, a)}} \\left[\n    r_h+ \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^\\text{any}}} [\n        r_{h+1} + \\cdots + r_{H-1}\n        \\mid\n        \\tau_{h},\n        a_h= a,\n        s_{h+1} = s'\n    ]\n\\right] \\\\\n\\le{}&\n\\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi^\\text{any}_h(\\tau_h) \\\\ s' \\sim P(s_h, a)}} \\left[\n    r_h+ V^\\star_{h+1}(s')\n\\right] \\\\\n\\le{}&\n\\max_{a \\in \\mathcal{A}}\nr(s_h, a)\n+\n\\mathop{\\mathbb{E}}_{s' \\sim P(s_h, a)} \\left[\n    V^\\star_{h+1}(s')\n\\right] \\\\\n={}& V^\\star_h(s_h).\n\\end{aligned}\n\\tag{2.27}\\]\nThis completes the inductive step, which shows that \\(\\pi^{Q^\\star}\\) is optimal across all policies at every time \\(h\\in [H]\\).\n\n\n\nCode\n# TODO fix latexify bugs\ndef find_optimal_policy(mdp: MDP):\n    Q = np.zeros((mdp.H, mdp.S, mdp.A))\n    policy = np.zeros((mdp.H, mdp.S, mdp.A))\n    V = np.zeros((mdp.H+1, mdp.S))  # initialize to 0 at end of time horizon\n\n    for h in range(mdp.H - 1, -1, -1):\n        Q[h] = mdp.r + mdp.P @ V[h + 1]\n        policy[h] = jnp.eye(mdp.S)[jnp.argmax(Q[h], axis=1)]  # one-hot\n        V[h] = jnp.max(Q[h], axis=1)\n\n    Q = jnp.stack(Q)\n    policy = jnp.stack(policy)\n    V = jnp.stack(V[:-1])\n\n    return policy, V, Q\n\nlatex(find_optimal_policy, id_to_latex={\"policy\": r\"\\pi\"}, trim_prefixes={\"mdp\"})\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{find\\_optimal\\_policy}(\\mathrm{mdp}: \\mathrm{MDP}) \\\\ \\hspace{1em} Q \\gets \\mathbf{0}^{H \\times S \\times A} \\\\ \\hspace{1em} \\pi \\gets \\mathbf{0}^{H \\times S \\times A} \\\\ \\hspace{1em} V \\gets \\mathbf{0}^{H + 1 \\times S} \\\\ \\hspace{1em} \\mathbf{for} \\ h \\in \\mathrm{range} \\mathopen{}\\left( H - 1, -1, -1 \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} Q_{h} \\gets r + P V_{h + 1} \\\\ \\hspace{2em} \\pi_{h} \\gets \\mathrm{jnp}.\\mathrm{eye} \\mathopen{}\\left( S \\mathclose{}\\right)_{\\mathrm{jnp}.\\mathrm{argmax} \\mathopen{}\\left( Q_{h} \\mathclose{}\\right)} \\\\ \\hspace{2em} V_{h} \\gets \\mathrm{jnp}.\\mathrm{max} \\mathopen{}\\left( Q_{h} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} Q \\gets \\mathrm{jnp}.\\mathrm{stack} \\mathopen{}\\left( Q \\mathclose{}\\right) \\\\ \\hspace{1em} \\pi \\gets \\mathrm{jnp}.\\mathrm{stack} \\mathopen{}\\left( \\pi \\mathclose{}\\right) \\\\ \\hspace{1em} V \\gets \\mathrm{jnp}.\\mathrm{stack} \\mathopen{}\\left( V_{:-1} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{return} \\ \\mathopen{}\\left( \\pi, V, Q \\mathclose{}\\right) \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 2.10: Pseudocode for the dynamic programming algorithm.\n\n\n\n\nAt each of the \\(H\\) timesteps, we must compute \\(Q^{\\star}_h\\) for each of the \\(|\\mathcal{S}| |\\mathcal{A}|\\) state-action pairs. Each computation takes \\(|\\mathcal{S}|\\) operations to evaluate the average value over \\(s'\\). Computing \\(\\pi^\\star_h\\) and \\(V^\\star_h\\) then requires computing the value-maximizing action in each state, which is an additional \\(O(|\\mathcal{S}| \\mathcal{A}|)\\) comparisons. This is dominated by the earlier term, resulting in a total computation time of \\(O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)\\).\nNote that this algorithm is identical to the policy evaluation algorithm (fig. 2.8), but instead of averaging over the actions chosen by a policy, we instead take a maximum over the action-values. We’ll see this relationship between policy evaluation and optimal policy computation show up again in the infinite-horizon setting.\n\n\nCode\npi_opt, V_opt, Q_opt = find_optimal_policy(tidy_mdp)\nassert jnp.allclose(pi_opt, tidy_policy_messy_only)\nassert jnp.allclose(V_opt, V_messy)\nassert jnp.allclose(Q_opt[:-1], v_ary_to_q_ary(tidy_mdp, V_messy)[1:])\n\"Assertions passed (the 'tidy when messy' policy is optimal)\"\n\n\n\"Assertions passed (the 'tidy when messy' policy is optimal)\"\n\n\nLet us review some equivalent definitions of an optimal policy:\n\nRemark 2.5 (Equivalent definitions of an optimal policy). Let \\(\\pi\\) be a stationary policy. Then the following are all equivalent:\n\n\\(\\pi\\) is optimal across history-independent policies (def. 2.11).\n\\(\\pi\\) is optimal across all policies (eq. 2.24).\n\\(V^\\pi = V^\\star\\).\n\\(Q^\\pi = Q^\\star\\).\n\\(V^\\pi\\) satisfies the Bellman optimality equations (eq. 2.38).\n\n1 and 3 are the same by definition. 3 and 4 are the same since \\(V^\\star\\) and \\(Q^\\star\\) uniquely define each other by eq. 2.17 and \\(V^\\pi\\) and \\(Q^\\pi\\) uniquely define each other by eq. 2.7 and eq. 2.8. 3 and 5 are the same by the Bellman optimality equations (Theorem 2.5). 1 and 2 are the same as shown in the proof of Theorem 2.6.\n\n\nRemark 2.6 (Why stochastic policies). Given that there exists an optimal deterministic policy, why do we try ever try to learn stochastic policies? We will see a partial answer to this when we discuss the exploration-exploitation tradeoff in Chapter 4. So far, we’ve assumed that the environment is totally known. If it isn’t, however, we need some way to explore different possible actions, and stochastic policies provide a natural way to do this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-infinite-horizon-mdps",
    "href": "mdps.html#sec-infinite-horizon-mdps",
    "title": "2  Markov Decision Processes",
    "section": "2.4 Infinite-horizon Markov decision processes",
    "text": "2.4 Infinite-horizon Markov decision processes\nWhat if we allow trajectories to continue for an infinite amount of time, that is, set the time horizon \\(H\\) to infinity? This might seem impractical, since in the real world, most of the trajectories we care about terminate after some fixed number of steps. However, we will see that infinite-horizon MDPs are simpler in certain regards and can serve as good approximations to finite-horizon tasks. A crucial result is that Bellman operators (def. 2.10) in this setting are contraction mappings (Theorem 2.8), which yield simple fixed-point iteration algorithms for policy evaluation (Section 2.4.3.2) and computing the optimal value function (Section 2.4.4.1). Finally, we’ll present the policy iteration algorithm (Section 2.4.4.2). In addition to being an effective tool in its own right, we will find that policy iteration serves as a useful framework for designing and analyzing later algorithms.\n\n2.4.1 Differences from the finite horizon setting\n\nRemark 2.7 (Discounted rewards). First of all, note that the total reward \\(r_0 + r_{1} + \\cdots\\) might blow up to infinity. To ensure that we work with bounded quantities, we insert a discount factor \\(\\gamma \\in [0, 1)\\) such that rewards become less valuable the further into the future they are:\n\\[\nr_0 + \\gamma r_{1} + \\gamma^2 r_{2} + \\cdots\n=\n\\sum_{h=0}^\\infty \\gamma^hr_{h}\n\\le R \\frac{1}{1 - \\gamma},\n\\tag{2.28}\\]\nwhere \\(R\\) is the maximum possible reward. We can think of \\(\\gamma\\) as measuring how much we care about the future: if it’s close to \\(0\\), we only care about the near-term rewards; if it’s close to \\(1\\), we put more weight into future rewards.\nYou can also analyze \\(\\gamma\\) as the probability of continuing the trajectory at each time step. (This is equivalent to \\(H\\) being distributed by a First Success distribution with success probability \\(\\gamma\\).) This accords with the above interpretation: if \\(\\gamma\\) is close to \\(0\\), the trajectory will likely be very short, while if \\(\\gamma\\) is close to \\(1\\), the trajectory will likely continue for a long time.\n\nThe other components of the MDP remain from the finite-horizon setting (def. 2.4):\n\\[\n\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_0, P, r, \\gamma).\n\\]\n\n\nCode\n# Code-wise, we can reuse the `MDP` class from before @def-finite-horizon-mdp and set `mdp.H = float('inf')`.\n\ntidy_mdp_inf = tidy_mdp._replace(H=float(\"inf\"), gamma=0.95)\n\n\n\nRemark 2.8 (Stationary policies). Time-dependent policies become difficult to handle in the infinite-horizon case. Not only would they be computationally impossible to store, but also, many of the DP approaches we saw required us to start at the end of the trajectory, which is no longer possible. We’ll shift to stationary policies \\(\\pi : \\mathcal{S} \\to \\mathcal{A}\\) (deterministic) or \\(\\triangle(\\mathcal{A})\\) (stochastic), which don’t explicitly depend on the timestep.\n\n\nExercise 2.6 (Stationary policy examples) Which of the policies in ex. 2.4 (policies in the tidying MDP) are stationary?\n\n\nDefinition 2.14 (Value functions) We’ll also consider stationary value functions \\(V^\\pi : \\mathcal{S} \\to \\mathbb{R}\\) and \\(Q^\\pi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\). These now represent the expected total discounted reward:\n\\[\n\\begin{aligned}\n    V^\\pi(s) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} [r_0 + \\gamma r_{1} + \\gamma^2 r_{2} + \\cdots \\mid s_0 = s] \\\\\n    Q^\\pi(s, a) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} [r_0 + \\gamma r_{1} + \\gamma^2 r_{2} + \\cdots \\mid s_0 = s, a_0 = a] \\\\\n\\end{aligned}\n\\tag{2.29}\\]\n\n\nExercise 2.7 (Time-independent) Note that we add up the rewards starting from time \\(0\\), whereas in the finite-horizon case, we needed to explicitly add the rewards from time \\(h\\) onwards. Heuristically speaking, why does it no longer matter which time step we start on when defining the value function? Refer back to the Markov property (def. 2.5).\n\n\nTheorem 2.7 (Bellman consistency equations) The Bellman consistency equations play the same role they did in the finite-horizon setting (Theorem 2.3). We need to insert a factor of \\(\\gamma\\) to account for the discounting, and the \\(h\\) subscript is gone since \\(V^\\pi\\) is stationary:\n\\[\nV^\\pi(s) = \\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma V^\\pi(s')].\n\\tag{2.30}\\]\nThe value function and action-value function are still related in the same way:\n\\[\n\\begin{aligned}\nV^\\pi(s) &= \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} [Q(s, a)] \\\\\nQ^\\pi(s, a) &= r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [V^\\pi(s')].\n\\end{aligned}\n\\tag{2.31}\\]\n\n\n\n2.4.2 Contraction mappings\nRecall that a policy’s Bellman operator takes a guess for the policy’s value function and returns an improved guess using one step of the policy (def. 2.10). In the infinite-horizon setting, this has the same form:\n\nDefinition 2.15 (Bellman operator) Let \\(\\pi : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\) be a stationary policy. Its Bellman operator is defined\n\\[\n\\mathcal{J}^{\\pi}(v) := \\Big(\n    s \\mapsto \\mathop{\\mathbb{E}}_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma v(s')]\n\\Big).\n\\tag{2.32}\\]\n\nThe crucial property of the Bellman operator is that it is a contraction mapping for any policy. Intuitively, if we start with two guesses \\(v, u : \\mathcal{S} \\to \\mathbb{R}\\), if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate.\n\nDefinition 2.16 (Contraction mapping) Let \\(X\\) be a set with a norm \\(\\|\\cdot\\|\\). We call an operator \\(f: X \\to X\\) a contraction mapping if for any \\(x, y \\in X\\),\n\\[\n\\|f(x) - f(y)\\| \\le \\gamma \\|x - y\\|\n\\]\nfor some fixed \\(\\gamma \\in (0, 1)\\).\n\n\nExercise 2.8 (Contraction mappings pull points together) Show that for a contraction mapping \\(f\\) with coefficient \\(\\gamma\\), for all \\(t \\in \\mathbb{N}\\),\n\\[\n\\|f^{(t)}(x) - f^{(t)}(y)\\| \\le \\gamma^t \\|x - y\\|,\n\\tag{2.33}\\]\ni.e. any two points will be pushed closer by at least a factor of \\(\\gamma\\) at each iteration.\n\nIt is a powerful fact, known as the Banach fixed-point theorem, that every contraction mapping has a unique fixed point \\(x^\\star\\) such that \\(f(x^\\star) = x^\\star\\). This means that if we repeatedly apply \\(f\\) to any starting point, we will eventually converge to \\(x^\\star\\):\n\\[\n\\|f^{(t)}(x) - x^\\star\\| \\le \\gamma^t \\|x - x^\\star\\|.\n\\tag{2.34}\\]\nLet’s return to the RL setting and apply this result to the Bellman operator. How can we measure the distance between two functions \\(v, u : \\mathcal{S} \\to \\mathbb{R}\\)? We’ll take the supremum norm as our distance metric:\n\\[\n\\| v - u \\|_{\\infty} := \\sup_{s \\in \\mathcal{S}} |v(s) - u(s)|,\n\\]\ni.e. we compare the functions on the state that causes the biggest gap between them. The Bellman consistency equations (Theorem 2.7) state that \\(V^\\pi\\) is the fixed point of \\(\\mathcal{J}^\\pi\\). Then eq. 2.34 implies that if we repeatedly apply \\(\\mathcal{J}^\\pi\\) to any starting guess, we will eventually converge to \\(V^\\pi\\):\n\\[\n\\|(\\mathcal{J}^\\pi)^{(t)}(v) - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v - V^\\pi\\|_{\\infty}.\n\\tag{2.35}\\]\nWe’ll use this useful fact to prove the convergence of several algorithms later on.\n\nTheorem 2.8 (The Bellman operator is a contraction mapping) Let \\(\\pi\\) be a stationary policy. There exists \\(\\gamma \\in (0, 1)\\) such that for any two functions \\(u, v : \\mathcal{S} \\to \\mathbb{R}\\),\n\\[\n\\|\\mathcal{J}^{\\pi} (v) - \\mathcal{J}^{\\pi} (u) \\|_{\\infty} \\le \\gamma \\|v - u \\|_{\\infty}.\n\\]\n\n\nProof. For all states \\(s \\in \\mathcal{S}\\),\n\\[\n\\begin{aligned}\n|[\\mathcal{J}^{\\pi} (v)](s) - [\\mathcal{J}^{\\pi} (u)](s)|&= \\Big| \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v(s') \\right] \\\\\n&\\qquad - \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} u(s') \\right] \\Big| \\\\\n&= \\gamma \\left|\\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [v(s') - u(s')] \\right| \\\\\n&\\le \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}|v(s') - u(s')| \\qquad \\text{(Jensen's inequality)} \\\\\n&\\le \\gamma \\max_{s'} |v(s') - u(s')| \\\\\n&= \\gamma \\|v - u \\|_{\\infty}.\n\\end{aligned}\n\\]\n\n\n\n2.4.3 Policy evaluation\nThe backwards DP technique we used in the finite-horizon case (Section 2.3.1) no longer works since there is no “final timestep” to start from. We’ll need another approach to policy evaluation.\nThe Bellman consistency equations (Theorem 2.7) yield a system of \\(|\\mathcal{S}|\\) equations we can solve to evaluate a deterministic policy exactly. For a faster approximate solution, we can iterate the policy’s Bellman operator, since we know that it has a unique fixed point at the true value function.\n\n2.4.3.1 Matrix inversion for deterministic policies\nNote that when the policy \\(\\pi\\) is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:\n\\[\n\\begin{aligned}\n    r^{\\pi} &\\in \\mathbb{R}^{|\\mathcal{S}|} & P^{\\pi} &\\in [0, 1]^{|\\mathcal{S}| \\times |\\mathcal{S}|} & P_0 &\\in [0, 1]^{|\\mathcal{S}|} \\\\\n    \\pi &\\in \\mathcal{A}^{|\\mathcal{S}|} & V^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}|} & Q^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}.\n\\end{aligned}\n\\]\nFor \\(P^\\pi\\), we’ll treat the rows as the states and the columns as the next states. Then \\(P^\\pi_{s, s'}\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) under policy \\(\\pi\\).\n\nExample 2.8 (Tidying MDP) The tabular MDP from ex. 2.3 has \\(|\\mathcal{S}| = 2\\) and \\(|\\mathcal{A}| = 2\\). Let’s write down the quantities for the policy \\(\\pi\\) that tidies if and only if the room is messy:\n\\[\nr^{\\pi} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n\\quad\nP^{\\pi} = \\begin{bmatrix} 0.7 & 0.3 \\\\ 1 & 0 \\end{bmatrix},\n\\quad\nP_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n\\]\nWe’ll see how to evaluate this policy in the next section.\n\nThe Bellman consistency equation for a deterministic policy can be written in tabular notation as\n\\[\nV^\\pi = r^\\pi + \\gamma P^\\pi V^\\pi.\n\\]\n(Unfortunately, this notation doesn’t simplify the expression for \\(Q^\\pi\\).) This system of equations can be solved with a matrix inversion:\n\\[\nV^\\pi = (I - \\gamma P^\\pi)^{-1} r^\\pi.\n\\tag{2.36}\\]\n\nExercise 2.9 (Matrix invertibility) Note we’ve assumed that \\(I - \\gamma P^\\pi\\) is invertible. Can you see why this is the case? (Recall that a linear operator, i.e. a square matrix, is invertible if and only if its null space is trivial; that is, it doesn’t map any nonzero vector to zero. Show that \\(I - \\gamma P^\\pi\\) maps any nonzero vector to another nonzero vector.)\n\n\n\nCode\ndef eval_deterministic_infinite(\n    mdp: MDP, policy: Float[Array, \"S A\"]\n) -&gt; Float[Array, \" S\"]:\n    pi = jnp.argmax(policy, axis=1)  # un-one-hot\n    P_pi = mdp.P[jnp.arange(mdp.S), pi]\n    r_pi = mdp.r[jnp.arange(mdp.S), pi]\n    return jnp.linalg.solve(jnp.eye(mdp.S) - mdp.gamma * P_pi, r_pi)\n\n\n\nExample 2.9 (Tidying policy evaluation) Let’s use the same policy \\(\\pi\\) that tidies if and only if the room is messy. Setting \\(\\gamma = 0.95\\), we must invert\n\\[\nI - \\gamma P^{\\pi} = \\begin{bmatrix} 1 - 0.95 \\times 0.7 & - 0.95 \\times 0.3 \\\\ - 0.95 \\times 1 & 1 - 0.95 \\times 0 \\end{bmatrix} = \\begin{bmatrix} 0.335 & -0.285 \\\\ -0.95 & 1 \\end{bmatrix}.\n\\]\nThe inverse to two decimal points is\n\\[\n(I - \\gamma P^{\\pi})^{-1} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix}.\n\\]\nThus the value function is\n\\[\nV^{\\pi} = (I - \\gamma P^{\\pi})^{-1} r^{\\pi} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 15.56 \\\\ 14.79 \\end{bmatrix}.\n\\]\nLet’s sanity-check this result. Since rewards are at most \\(1\\), the maximum cumulative return of a trajectory is at most \\(1/(1-\\gamma) = 20\\). We see that the value function is indeed slightly lower than this.\n\n\n\nCode\neval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[0])\n\n\nArray([15.56419, 14.78598], dtype=float32)\n\n\n\n\n2.4.3.2 Iterative policy evaluation\nThe matrix inversion above takes roughly \\(O(|\\mathcal{S}|^3)\\) time. It also only works for deterministic policies. Can we trade off the requirement of finding the exact value function for a faster approximate algorithm that will also extend to stochastic policies?\nLet’s use the Bellman operator to define an iterative algorithm for computing the value function. We’ll start with an initial guess \\(v^{(0)}\\) with elements in \\([0, 1/(1-\\gamma)]\\) and then iterate the Bellman operator:\n\\[\nv^{(t+1)} = \\mathcal{J}^{\\pi}(v^{(t)}),\n\\]\ni.e. \\(v^{(t)} = (\\mathcal{J}^{\\pi})^{(t)} (v^{(0)})\\). Note that each iteration takes \\(O(|\\mathcal{S}|^2)\\) time for the matrix-vector multiplication.\n\n\nCode\ndef supremum_norm(v):\n    return jnp.max(jnp.abs(v))  # same as jnp.linalg.norm(v, jnp.inf)\n\n\ndef loop_until_convergence(op, v, ε=1e-6):\n    \"\"\"Repeatedly apply op to v until convergence (in supremum norm).\"\"\"\n    while True:\n        v_new = op(v)\n        if supremum_norm(v_new - v) &lt; ε:\n            return v_new\n        v = v_new\n\n\ndef iterative_evaluation(mdp: MDP, pi: Float[Array, \"S A\"], ε=1e-6) -&gt; Float[Array, \" S\"]:\n    op = partial(bellman_operator, mdp, pi)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\n\nThen, as we showed in eq. 2.35, by the Banach fixed-point theorem:\n\\[\n\\|v^{(t)} - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v^{(0)} - V^\\pi\\|_{\\infty}.\n\\]\n\n\nCode\niterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[0])\n\n\nArray([15.564166, 14.785956], dtype=float32)\n\n\n\nRemark 2.9 (Convergence of iterative policy evaluation). How many iterations do we need for an \\(\\epsilon\\)-accurate estimate? We can work backwards to solve for \\(t\\) (note that \\(\\log \\gamma &lt; 0\\)):\n\\[\n\\begin{aligned}\n    \\gamma^t \\|v^{(0)} - V^\\pi\\|_{\\infty} &\\le \\epsilon \\\\\n    t &\\ge \\frac{\\log (\\epsilon / \\|v^{(0)} - V^\\pi\\|_{\\infty})}{\\log \\gamma} \\\\\n    &= \\frac{\\log (\\|v^{(0)} - V^\\pi\\|_{\\infty} / \\epsilon)}{\\log (1 / \\gamma)},\n\\end{aligned}\n\\]\nand so the number of iterations required for an \\(\\epsilon\\)-accurate estimate is\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\\]\nNote that we’ve applied the inequalities \\(\\|v^{(0)} - V^\\pi\\|_{\\infty} \\le 1/(1-\\gamma)\\) and \\(\\log (1/x) \\ge 1-x\\).\n\n\n\n\n2.4.4 Optimality\nNow let’s move on to solving for an optimal policy in the infinite-horizon case. As in def. 2.11, an optimal policy \\(\\pi^\\star\\) is one that does at least as well as any other policy in all situations. That is, for all policies \\(\\pi\\), times \\(h\\in \\mathbb{N}\\), and initial trajectories \\(\\tau_{\\le h} = (s_0, a_0, r_0, \\dots, s_h)\\),\n\\[\n\\begin{aligned}\n    &\n    \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi^{\\star}}}[\n        r_h+ \\gamma r_{h+1} + \\gamma^2 r_{h+2} + \\cdots \\mid \\tau_{\\le h}] \\\\\n    \\ge{}\n    &\n    \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi}}[\n        r_h+ \\gamma r_{h+1} + \\gamma^2 r_{h+2} + \\cdots \\mid \\tau_{\\le h}]\n\\end{aligned}\n\\tag{2.37}\\]\nOnce again, all optimal policies share the same optimal value function \\(V^\\star\\), and the greedy policy with respect to this value function is optimal.\n\nExercise 2.10 (The greedy optimal policy) Verify this by modifying the proof Theorem 2.6 from the finite-horizon case.\n\nSo how can we compute such an optimal policy? We can’t use the backwards DP approach from the finite-horizon case fig. 2.10 since there’s no “final timestep” to start from. Instead, we’ll exploit the fact that the Bellman consistency equation eq. 2.29 for the optimal value function doesn’t depend on any policy:\n\\[\nV^\\star(s) = \\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V^\\star(s'). \\right]\n\\tag{2.38}\\]\nAs before, thinking of the r.h.s. of eq. 2.38 as an operator on value functions gives the Bellman optimality operator\n\\[\n[\\mathcal{J}^{\\star}(v)](s) = \\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v(s') \\right]\n\\tag{2.39}\\]\n\n\nCode\ndef bellman_optimality_operator(mdp: MDP, v: Float[Array, \" S\"]) -&gt; Float[Array, \" S\"]:\n    return jnp.max(mdp.r + mdp.gamma * mdp.P @ v, axis=1)\n\n\ndef check_optimal(v: Float[Array, \" S\"], mdp: MDP):\n    return jnp.allclose(v, bellman_optimality_operator(v, mdp))\n\n\n\n2.4.4.1 Value iteration\nSince the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as value iteration.\n\n\nCode\ndef value_iteration(mdp: MDP, ε: float = 1e-6) -&gt; Float[Array, \" S\"]:\n    \"\"\"Iterate the Bellman optimality operator until convergence.\"\"\"\n    op = partial(bellman_optimality_operator, mdp)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\n\n\n\nCode\nvalue_iteration(tidy_mdp_inf)\n\n\nArray([15.564166, 14.785956], dtype=float32)\n\n\nNote that the runtime analysis for an \\(\\epsilon\\)-optimal value function is exactly the same as Section 2.4.3.2! This is because value iteration is simply the special case of applying iterative policy evaluation to the optimal value function.\nAs the final step of the algorithm, to return an actual policy \\(\\hat \\pi\\), we can simply act greedily with respect to the final iteration \\(v^{(T)}\\) of our above algorithm:\n\\[\n\\hat \\pi(s) = \\arg\\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v^{(T)}(s') \\right].\n\\tag{2.40}\\]\nWe must be careful, though: the value function of this greedy policy, \\(V^{\\hat \\pi}\\), is not the same as \\(v^{(T)}\\), which need not even be a well-defined value function for some policy!\nThe bound on the policy’s quality is actually quite loose: if \\(\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\epsilon\\), then the greedy policy \\(\\hat \\pi\\) satisfies \\(\\|V^{\\hat \\pi} - V^\\star\\|_{\\infty} \\le \\frac{2\\gamma}{1-\\gamma} \\epsilon\\), which might potentially be very large.\n\nTheorem 2.9 (Greedy policy value worsening) \\[\n\\|V^{\\hat \\pi} - V^\\star \\|_{\\infty} \\le \\frac{2 \\gamma}{1-\\gamma} \\|v - V^\\star\\|_{\\infty}\n\\]\nwhere \\(\\hat \\pi(s) = \\arg\\max_a q(s, a)\\) is the greedy policy with respect to\n\\[\nq(s, a) = r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v(s').\n\\]\n\n\nProof. We first have\n\\[\n\\begin{aligned}\n        V^{\\star}(s) - V^{\\hat \\pi}(s) &= Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\\\\\n        &= [Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s))] + [Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))].\n\\end{aligned}\n\\]\nLet us bound these two quantities separately.\nFor the first quantity, note that by the definition of \\(\\hat \\pi\\), we have\n\\[\nq(s, \\hat \\pi(s)) \\ge q(s,\\pi^\\star(s)).\n\\]\nLet’s add \\(q(s, \\hat \\pi(s)) - q(s,\\pi^\\star(s)) \\ge 0\\) to the first term to get\n\\[\n\\begin{aligned}\n        Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s)) &\\le [Q^{\\star}(s,\\pi^\\star(s))- q(s,\\pi^\\star(s))] + [q(s, \\hat \\pi(s)) - Q^{\\star}(s, \\hat \\pi(s))] \\\\\n        &= \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi^{\\star}(s))} [ V^{\\star}(s') - v(s') ] + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\hat \\pi(s))} [ v(s') - V^{\\star}(s') ] \\\\\n        &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty}.\n\\end{aligned}\n\\]\nThe second quantity is bounded by\n\\[\n\\begin{aligned}\n        Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\n        &=\n        \\gamma \\mathop{\\mathbb{E}}_{s'\\sim P(s, \\hat \\pi(s))}\\left[ V^\\star(s') - V^{\\hat \\pi}(s') \\right] \\\\\n        & \\leq\n        \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty\n\\end{aligned}\n\\]\nand thus\n\\[\n\\begin{aligned}\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty} + \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty \\\\\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le \\frac{2 \\gamma \\|v - V^{\\star}\\|_{\\infty}}{1-\\gamma}.\n\\end{aligned}\n\\]\n\nSo in order to compensate and achieve \\(\\|V^{\\hat \\pi} - V^{\\star}\\| \\le \\epsilon\\), we must have\n\\[\n\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\frac{1-\\gamma}{2 \\gamma} \\epsilon.\n\\]\nThis means, using Remark 2.9, we need to run value iteration for\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{\\gamma}{\\epsilon (1-\\gamma)^2}\\right) \\right)\n\\]\niterations to achieve an \\(\\epsilon\\)-accurate estimate of the optimal value function.\n\n\n2.4.4.2 Policy iteration\nCan we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function together? This is the idea behind policy iteration. In each step, we simply set the policy to act greedily with respect to its own value function.\n\n\nCode\ndef policy_iteration(mdp: MDP, ε=1e-6) -&gt; Float[Array, \"S A\"]:\n    \"\"\"Iteratively improve the policy and value function.\"\"\"\n    def op(pi):\n        return v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))\n    pi_init = jnp.ones((mdp.S, mdp.A)) / mdp.A  # uniform random policy\n    return loop_until_convergence(op, pi_init, ε)\n\n\n\n\nCode\npolicy_iteration(tidy_mdp_inf)\n\n\nArray([[1., 0.],\n       [0., 1.]], dtype=float32)\n\n\nAlthough PI appears more complex than VI, we’ll use Theorem 2.8 to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an \\(\\epsilon\\)-optimal value function (Remark 2.9), although in practice, PI often converges in fewer iterations. Why so? Intuitively, by iterating through actual policies, we can “skip over” different functions that represent the same policy, which value iteration might iterate through. For a concrete example, suppose we have an MDP with three states \\(\\mathcal{S} = \\{\\text A, \\text B, \\text C\\}\\). Compare the two functions below:\n\nCode\ndisplay(Markdown(tabulate(\n    {\n        r\"$s$\": [\"A\", \"B\", \"C\"],\n        r\"$a = 0$\": [0.1, 0.8, 0.4],\n        r\"$a = 1$\": [0.2, 0.9, 0.5],\n    },\n    \"keys\"\n)))\n\ndisplay(Markdown(tabulate(\n    {\n        r\"$s$\": [\"A\", \"B\", \"C\"],\n        r\"$a = 0$\": [0.1, 0.8, 0.4],\n        r\"$a = 1$\": [0.3, 1.0, 0.6],\n    },\n    \"keys\"\n)))\n\n\n\n\nTable 2.2: Two action-value functions that result in the same greedy policy\n\n\n\n\n\n\n\n(a) One Q-function\n\n\n\n\n\n\\(s\\)\n\\(a = 0\\)\n\\(a = 1\\)\n\n\n\n\nA\n0.1\n0.2\n\n\nB\n0.8\n0.9\n\n\nC\n0.4\n0.5\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Q-function\n\n\n\n\n\n\\(s\\)\n\\(a = 0\\)\n\\(a = 1\\)\n\n\n\n\nA\n0.1\n0.3\n\n\nB\n0.8\n1\n\n\nC\n0.4\n0.6\n\n\n\n\n\n\n\n\n\n\n\nThese both map to the same greedy policy, so policy iteration would never iterate through both of these functions, while value iteration might.\n\nTheorem 2.10 (Policy Iteration runtime and convergence) The number of iterations required for an \\(\\epsilon\\)-accurate estimate of the optimal value function is\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\\]\n\n\nProof. This bound follows from the contraction property eq. 2.35:\n\\[\n\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.\n\\]\nWe’ll prove that the iterates of PI respect the contraction property by showing that the policies improve monotonically:\n\\[\nV^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s).\n\\]\nThen we’ll use this to show \\(V^{\\pi^{t+1}}(s) \\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s)\\). Note that\n\\[\n\\begin{aligned}\n(s) &= \\max_a \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V^{\\pi^{t}}(s') \\right] \\\\\n    &= r(s, \\pi^{t+1}(s)) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi^{t+1}(s))} V^{\\pi^{t}}(s')\n\\end{aligned}\n\\]\nSince \\([\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s) \\ge V^{\\pi^{t}}(s)\\), we then have\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) \\\\\n    &= \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right].\n\\end{aligned}\n\\tag{2.41}\\]\nBut note that the expression being averaged is the same as the expression on the l.h.s. with \\(s\\) replaced by \\(s'\\). So we can apply the same inequality recursively to get\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge  \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge \\gamma^2 \\mathop{\\mathbb{E}}_{\\substack{s' \\sim P(s, \\pi^{t+1}(s)) \\\\ s'' \\sim P(s', \\pi^{t+1}(s'))}} \\left[V^{\\pi^{t+1}}(s'') -  V^{\\pi^{t}}(s'') \\right]\\\\\n    &\\ge \\cdots\n\\end{aligned}\n\\]\nwhich implies that \\(V^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s)\\) for all \\(s\\) (since the r.h.s. converges to zero). We can then plug this back into eq. 2.41 to get the desired result:\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) &= \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge 0 \\\\\n    V^{\\pi^{t+1}}(s) &\\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s)\n\\end{aligned}\n\\]\nThis means we can now apply the Bellman convergence result eq. 2.35 to get\n\\[\n\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\|\\mathcal{J}^{\\star} (V^{\\pi^{t}}) - V^{\\star}\\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#key-takeaways",
    "href": "mdps.html#key-takeaways",
    "title": "2  Markov Decision Processes",
    "section": "2.5 Key takeaways",
    "text": "2.5 Key takeaways\nMarkov decision processes (MDPs) are a framework for sequential decision making under uncertainty. They consist of a state space \\(\\mathcal{S}\\), an action space \\(\\mathcal{A}\\), an initial state distribution \\(P_0 \\in \\triangle(\\mathcal{S})\\), a transition function \\(P(s' \\mid s, a)\\), and a reward function \\(r(s, a)\\). They can be finite-horizon (ends after \\(H\\) timesteps) or infinite-horizon (where rewards scale by \\(\\gamma \\in (0, 1)\\) at each timestep). Our goal is to find a policy \\(\\pi\\) that maximizes expected total reward. Policies can be deterministic or stochastic, history-dependent or history-independent, stationary or time-dependent. A policy induces a distribution over trajectories.\nWe can evaluate a policy by computing its value function \\(V^\\pi(s)\\), which is the expected total reward starting from state \\(s\\) and following policy \\(\\pi\\). We can also compute the state-action value function \\(Q^\\pi(s, a)\\), which is the expected total reward starting from state \\(s\\), taking action \\(a\\), and then following policy \\(\\pi\\). In the finite-horizon setting, these also depend on the timestep \\(h\\).\nThe Bellman consistency equation is an equation that the value function must satisfy. It can be used to solve for the value functions exactly. Thinking of the r.h.s. of this equation as an operator on value functions gives the Bellman operator.\nIn the finite-horizon setting, we can compute the optimal policy using dynamic programming. In the infinite-horizon setting, we can compute the optimal policy using value iteration or policy iteration.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-mdps-bib",
    "href": "mdps.html#sec-mdps-bib",
    "title": "2  Markov Decision Processes",
    "section": "2.6 Bibliographic notes and further reading",
    "text": "2.6 Bibliographic notes and further reading\nThe MDP framework can be traced to Puterman (1994). The proof of Theorem 2.6 can be found in Agarwal et al. (2022).\nMDPs are the most common framework used throughout RL. See Section 1.4 for a list of popular textbooks on RL.\nIn most real-world problems, we don’t observe the entire state of the environment. Instead, we only get access to what our senses can perceive. This is what makes games like hide-and-seek enjoyable. We can model such environment as partially observed MDPs (POMDPs). Powell (2022) presents a unified framework for sequential decision problems.\nAnother important bandit algorithm is Gittins indices (Gittins, 2011). The derivation of this algorithm is out of scope of this course.\n\n\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022). Reinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nFrans Berkelaar. (2009). Container ship MSC davos - westerschelde - zeeland [Graphic]. https://www.flickr.com/photos/28169156@N03/52957948820/\n\n\nGittins, J. C. (2011). Multi-armed bandit allocation indices (2nd ed). Wiley.\n\n\nGPA Photo Archive. (2017). Robotic arm [Graphic]. https://www.flickr.com/photos/iip-photo-archive/36123310136/\n\n\nGuy, R. (2006). Chess [Graphic]. https://www.flickr.com/photos/romainguy/230416692/\n\n\nPowell, W. B. (2022). Reinforcement learning and stochastic optimization: A unified framework for sequential decisions. Wiley.\n\n\nPuterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "control.html",
    "href": "control.html",
    "title": "3  Linear Quadratic Regulators",
    "section": "",
    "text": "3.1 Introduction\nIn Chapter 2, we considered decision problems with finitely many states and actions. However, in many applications, states and actions may take on continuous values. For example, consider autonomous driving, controlling a robot’s joints, and automated manufacturing. How can we teach computers to solve these kinds of problems? This is the task of continuous control.\nAside from the change in the state and action spaces, the general problem setup remains the same: we seek to construct an optimal policy that outputs actions to solve the desired task. We will see that many key ideas and algorithms, in particular dynamic programming algorithms (Section 2.3.2), carry over to this new setting.\nThis chapter introduces a fundamental tool to solve a simple class of continuous control problems: the linear quadratic regulator (LQR). We can use the LQR model as a building block for solving more complex problems.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom utils import latex\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport control",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#introduction",
    "href": "control.html#introduction",
    "title": "3  Linear Quadratic Regulators",
    "section": "",
    "text": "Solving a Rubik’s Cube with a robot hand\n\n\n\n\n\n\n\nBoston Dynamics’s Spot robot\n\n\n\n\n\n\n\n\nRemark 3.1 (Control vs RL). Control theory is often considered a distinct, though related, field from RL. In both fields, the central aim is to solve sequential decision problems, i.e. find a strategy for taking actions in the environment in order to achieve some goal that is measured by a scalar signal. The two fields arose rather independently and with rather different problem settings, and use different mathematical terminology as a result.\nControl theory has close ties to electrical and mechanical engineering. Control theorists typically work in a continuous-time setting in which the dynamics can be described by systems of differential equations. The goal is typically to ensure stability of the system and minimize some notion of “cost” (e.g. wasted energy in controlling the system). Rather than learning the system from data, one typically supposes a particular structure of the environment and solves for the optimal controller using analytical or numerical methods. As such, most control theory algorithms, like the ones we will explore in this chapter, are planning methods that do not require learning from data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#optimal-control",
    "href": "control.html#optimal-control",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.2 Optimal control",
    "text": "3.2 Optimal control\nLet’s first look at a simple example of a continuous control problem:\n\nExample 3.1 (CartPole) Try to balance a pencil on its point on a flat surface. It’s much more difficult than it may first seem: the position of the pencil varies continuously, and the state transitions governing the system, i.e. the laws of physics, are highly complex. This task is equivalent to the classic control problem known as CartPole:\n\n\n\nA snapshot of the cart pole environment\n\n\nThe state \\(x\\in \\mathbb{R}^4\\) can be described by four real variables:\n\nthe position of the cart;\nthe velocity of the cart;\nthe angle of the pole;\nthe angular velocity of the pole.\n\nWe can control the cart by applying a horizontal force \\(u\\in \\mathbb{R}\\).\nGoal: Stabilize the cart around an ideal state and action \\((x^\\star, u^\\star)\\).\n\nA continuous control environment is a special case of an MDP (def. 2.4). Recall that a finite-horizon MDP\n\\[\n\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_0, P, r, H).\n\\tag{3.1}\\]\nis defined by its state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), initial state distribution \\(P_0\\), state transitions \\(P\\), reward function \\(r\\), and time horizon \\(H\\). These each have equivalents in the control setting.\n\nDefinition 3.1 (Continuous control environment) A continuous control environment is defined by the following components:\n\nThe state and action spaces \\(\\mathcal{S}, \\mathcal{A}\\) are continuous rather than finite. That is, \\(\\mathcal{S} \\subseteq \\mathbb{R}^{n_x}\\) and \\(\\mathcal{A} \\subseteq \\mathbb{R}^{n_u}\\), where \\(n_x\\) and \\(n_u\\) are the number of coordinates required to specify a single state or action respectively. For example, in robotic control, \\(n_x\\) might be the number of sensors and \\(n_u\\) might be the number of actuators on the robot.\n\\(P_0 \\in \\triangle(\\mathcal{S})\\) denotes the initial state distribution.\nWe call the state transitions the dynamics of the system and denote them by \\(f\\): \\[\nx_{h+1} = f_h(x_h, u_h, w_h),\n\\tag{3.2}\\] where \\(w_h\\) denotes noise drawn from the distribution \\(\\nu_h\\in \\triangle(\\mathbb{R}^{n_w})\\). We allow the dynamics to vary at each timestep \\(h\\).\nInstead of maximizing the reward function, we seek to minimize the cost function \\(c_h: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\). Often, the cost function describes how far away we are from a target state-action pair \\((x^\\star, u^\\star)\\).\nThe agent acts over a finite time horizon \\(H\\in \\mathbb{N}\\).\n\nTogether, these constitute a description of the environment\n\\[\n\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_0, f, \\nu, c, H).\n\\tag{3.3}\\]\n\n\nDefinition 3.2 (Optimal control) For a continuous control environment \\(\\mathcal{M}\\), the optimal control problem is to compute a policy \\(\\pi\\) that minimizes the total cost\n\\[\n\\begin{aligned}\n    \\min_{\\pi_0, \\dots, \\pi_{H-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\mathop{\\mathbb{E}}\\left[\n        \\sum_{h=0}^{H-1} c_h(x_h, u_h)\n    \\right] \\\\\n    \\text{where} \\quad & x_{h+1} = f_h(x_h, u_h, w_h), \\\\\n    & u_h= \\pi_h(x_h) \\\\\n    & x_0 \\sim P_0 \\\\\n    & w_h\\sim \\nu_h.\n\\end{aligned}\n\\tag{3.4}\\]\n\nIn this chapter, we will only consider deterministic, time-dependent policies\n\\[\n\\pi = (\\pi_0, \\dots, \\pi_{H-1})\n\\quad \\text{where} \\quad\n\\pi_h : \\mathcal{S} \\to \\mathcal{A}\n\\text{ for each }\nh\\in [H].\n\\tag{3.5}\\]\nTo make the explicit conditioning more concise, we let \\(\\rho^\\pi\\) denote the trajectory distribution induced by the policy \\(\\pi\\). That is, if we sample initial states from \\(P_0\\), act according to \\(\\pi\\), and transition according to the dynamics \\(f\\), the resulting distribution over trajectories is \\(\\rho^\\pi\\). Put another way, the procedure for sampling from \\(\\rho^\\pi\\) is to take rollouts in the environment using \\(\\pi\\).\n\nTheorem 3.1 (Trajectory distributions) Let \\(\\pi = \\{ \\pi_0, \\dots, \\pi_{H-1} \\}\\) be a deterministic, time-dependent policy. We include states \\(x_h\\), actions \\(u_h\\), and noise terms \\(w_h\\) in the trajectory. Then the density of a trajectory \\(\\tau = (x_0, u_0, w_0, \\dots, w_{H-2}, x_{H-1}, u_{H-1})\\) can be expressed as follows:\n\\[\n\\begin{aligned}\n\\rho^{\\pi}(\\tau)\n&:= P_0(x_0) \\times \\mathbf{1}\\left\\{u_0 = \\pi_0(x_0)\\right\\} \\\\\n&\\quad {} \\times \\nu(w_0) \\times \\mathbf{1}\\left\\{x_1 = f(x_0, u_0, w_0)\\right\\} \\\\\n&\\quad {} \\times \\cdots \\\\\n&\\quad {} \\times \\nu(w_{H-2})\n    \\times \\mathbf{1}\\left\\{x_{H-1} = f(x_{H-2}, u_{H-2}, w_{H-2})\\right\\}\n    \\times \\mathbf{1}\\left\\{u_{H-1} = \\pi_{H-1}(x_{H-1})\\right\\} \\\\\n&= P_0(x_0) \\mathbf{1}\\left\\{u_0 = \\pi_0(x_0)\\right\\}\n    \\prod_{h=1}^{H-1} \\nu(w_{h-1})\n    \\mathbf{1}\\left\\{x_{h} = f(x_{h-1}, u_{h-1}, w_{h-1})\\right\\}\n    \\mathbf{1}\\left\\{u_{h} = \\pi_{h}(x_{h})\\right\\}\n\\end{aligned}\n\\tag{3.6}\\]\n\nThis expression may seem intimidating, but on closer examination, it simply uses indicator variables to enforce that only valid trajectories have nonzero probability. Henceforth, we will write \\(\\tau \\sim \\rho^\\pi\\) instead of the explicit conditioning in eq. 3.4.\nAs in Chapter 2, value functions will be crucial for constructing the optimal policy via dynamic programming.\n\nDefinition 3.3 (Value functions in LQR) Given a policy \\(\\mathbf{\\pi} = (\\pi_0, \\dots, \\pi_{H-1})\\), we can define its value function \\(V^\\pi_h: \\mathcal{S} \\to \\mathbb{R}\\) at time \\(h\\in [H]\\) as the cost-to-go incurred by that policy in expectation:\n\\[\nV^\\pi_h(x) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} \\left[\n    \\sum_{h'=h}^{H-1} c(x_{h'}, u_{h'})\n    \\mid x_h= x\n\\right]\n\\tag{3.7}\\]\n\n\nDefinition 3.4 (Q function in LQR) The Q-function additionally conditions on the first action we take:\n\\[\nQ^\\pi_h(x, u) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} \\left[\n    \\sum_{h'=h}^{H-1} c(x_{h'}, u_{h'})\n    \\mid (x_h, u_h) = (x, u)\n\\right]\n\\tag{3.8}\\]\n\nSince we use a cost function \\(c\\) instead of a reward function \\(r\\), the best policies in a given state (or state-action pair) are the ones with smaller \\(V^\\pi\\) and \\(Q^\\pi\\).\n\n3.2.1 A first attempt: Discretization\nCan we solve this problem using tools from the finite MDP setting? If \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) were finite, then we could use dynamic programming (fig. 2.10) to compute the optimal policy exactly. This inspires us to try discretizing the problem.\nSuppose \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) are bounded, that is, \\(\\max_{x\\in \\mathcal{S}} \\|x\\| \\le B_x\\) and \\(\\max_{u\\in \\mathcal{A}} \\|u\\| \\le B_u\\). To make \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) finite, let’s choose some small positive \\(\\epsilon\\), and simply round each coordinate to the nearest multiple of \\(\\epsilon\\). For example, if \\(\\epsilon = 0.01\\), then we round each element of \\(x\\) and \\(u\\) to two decimal spaces.\nWhat goes wrong? The discretized \\(\\widetilde{\\mathcal{S}}\\) and \\(\\widetilde{\\mathcal{A}}\\) may be finite, but for all practical purposes, they are far too large: we must divide each dimension into intervals of length \\(\\varepsilon\\), resulting in the state and action spaces\n\\[\n|\\widetilde{\\mathcal{S}}| = (B_x/\\varepsilon)^{n_x}\n\\text{ and }\n|\\widetilde{\\mathcal{A}}| = (B_u/\\varepsilon)^{n_u}.\n\\tag{3.9}\\]\nTo get a sense of how quickly this grows, consider \\(\\varepsilon = 0.01, n_x= n_u= 4\\), and \\(B_x= B_u= 1\\). Then the number of elements in the transition matrix would be \\(|\\widetilde{\\mathcal{S}}|^2 |\\widetilde{\\mathcal{A}}| = (100^{4})^2 (100^{4}) = 10^{24}\\)! (That’s a million million million million.)\nWhat properties of the problem could we instead make use of? Note that by discretizing the state and action spaces, we implicitly assumed that rounding each state or action vector by some tiny amount \\(\\varepsilon\\) wouldn’t change the behaviour of the system by much; namely, that the cost and dynamics were relatively continuous. Can we use this continuous structure in other ways? Yes! We will see that the linear quadratic regulator makes better use of this assumption.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-lqr",
    "href": "control.html#sec-lqr",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.3 The Linear Quadratic Regulator",
    "text": "3.3 The Linear Quadratic Regulator\nThe optimal control problem def. 3.2 is quite general. Is there a relevant simplification that we can solve, where the dynamics \\(f\\) and cost function \\(c\\) have some special structure? The linear quadratic regulator (LQR) is a special case of a continuous control environment (def. 3.1) where the dynamics \\(f\\) are linear and the cost function \\(c\\) is an upward-curved quadratic. We also assume that the environment is time-invariant. This is an important environment class in which the optimal policy and value function can be solved for exactly.\n\nDefinition 3.5 (Linear quadratic regulator) The dynamics \\(f\\) are specified by the matrices \\(A \\in \\mathbb{R}^{n_x\\times n_x}\\) and \\(B \\in \\mathbb{R}^{n_x\\times n_u}\\):\n\\[\nx_{h+1} = f(x_h, u_h, w_h) = A x_h+ B u_h+ w_h\n\\tag{3.10}\\]\nfor all \\(h\\in [H-1]\\), where \\(w_h\\sim \\mathcal{N}(0, \\sigma^2) I\\).\nThe cost function is specified by the symmetric positive definite matrices \\(Q \\in \\mathbb{R}^{n_x\\times n_x}\\) and \\(R \\in \\mathbb{R}^{n_u\\times n_u}\\):\n\\[\nc(x_h, u_h) = x_h^\\top Q x_h+ u_h^\\top R u_h\n\\tag{3.11}\\]\nfor all \\(h\\in [H]\\). We use the same letter as the Q-function (def. 3.4); the context should make it clear which is intended.\n\n\nRemark 3.2 (Properties of LQR). Recall that \\(w_h\\) is a noise term that makes the dynamics random. By setting its covariance matrix to \\(\\sigma^2 I\\), we mean that there is Gaussian noise of standard deviation \\(\\sigma\\) added independently to each coordinate of the state. Setting \\(\\sigma = 0\\) gives us deterministic state transitions. Surprisingly, the optimal policy doesn’t depend on the amount of noise, although the optimal value function and Q-function do. (We will show this in a later derivation.)\nThe LQR cost function attempts to stabilize the state and action to \\((x^\\star, u^\\star) = (0, 0)\\). We require \\(Q\\) and \\(R\\) to both be positive definite matrices so that \\(c\\) has a unique minimum. We can furthermore assume without loss of generality that they are both symmetric (see exercise below). This greatly simplifies later computations.\n\n\nExercise 3.1 (Symmetric \\(Q\\) and \\(R\\)) Show that replacing \\(Q\\) and \\(R\\) with \\((Q + Q^\\top) / 2\\) and \\((R + R^\\top) / 2\\) (which are symmetric) yields the same cost function.\n\n\nExample 3.2 (Double integrator) Consider a ball moving along a line. Let’s first frame this as a continuous-time dynamical system, which is often more natural for physical phenomena, and then discretize it to solve it with the discrete-time LQR.\nFrom elementary physics, the ball’s velocity \\(\\dot p(t)\\) is the instantaneous change in its position, and its acceleration \\(\\ddot p(t)\\) is the instantaneous change in its velocity. Suppose we can apply a force \\(u(t)\\) to accelerate the ball. This dynamical system is known as the double integrator. Our goal is to move the ball so that it is stationary at position \\(p^\\star = 0\\).\nHow would we frame this as an LQR problem? The effect of the control on the ball’s position is nonlinear. However, if we define our state as including both the position and velocity, i.e.\n\\[\nx(t) = \\begin{pmatrix}\np(t) \\\\\n\\dot p(t)\n\\end{pmatrix},\n\\tag{3.12}\\]\nthen we end up with the following first-order linear differential equation:\n\\[\n\\dot x(t) = \\begin{pmatrix}\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}\nx(t)\n+\n\\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix}\nu(t).\n\\tag{3.13}\\]\nThere are many ways to turn this into a discrete-time problem. The simplest way is the (forward) Euler method, where we choose a small step size \\(\\Delta t\\), and explicitly use the limit definition of the derivative to obtain\n\\[\nx(t + \\Delta t) \\approx x(t) + \\Delta t \\cdot \\dot x(t).\n\\tag{3.14}\\]\nApplying this to eq. 3.13 gives the system\n\\[\nx_{h+1} = \\begin{pmatrix}\n1 & \\Delta t \\\\\n0 & 1\n\\end{pmatrix}\nx_h\n+\n\\begin{pmatrix}\n0 \\\\\n\\Delta t\n\\end{pmatrix}\nu_h,\n\\tag{3.15}\\]\nor explicitly in terms of the position and velocity,\n\\[\n\\begin{aligned}\np_{h+1} &= p_h+ \\Delta t \\cdot \\dot p_h, \\\\\n\\dot p_{h+ 1} &= \\dot p_h+ \\Delta t \\cdot u_h.\n\\end{aligned}\n\\tag{3.16}\\]\n\nCode\ndt = 0.1\n\nA = np.array([\n    [1, dt],\n    [0, 1]\n])\nB = np.array([\n    [0],\n    [dt]\n])\n\nQ = np.eye(2)\nR = np.array([[0.1]])\nK, S, E = control.dlqr(A, B, Q, R)\n\nN = 50                 # number of discrete steps\nx = np.zeros((2, N+1)) # state array: 2 rows (pos & vel), N+1 columns (time steps)\nx[:, 0] = [1.0, 0.0]   # initial condition: position=1, velocity=0\nu = np.zeros((N, 1))   # control input array with correct shape (N, 1)\n\nfor k in range(N):\n    u[k] = -K @ x[:, k].reshape(2, 1)\n    x[:, k+1] = A @ x[:, k] + B @ u[k].flatten()\n\ntime = np.arange(N+1) * dt\n\nfig, ax = plt.subplots(2, 1, figsize=(10,6))\n\nax[0].plot(time, x[0, :], '-o', label='position $p_h$')\nax[0].plot(time, x[1, :], '-o', label='velocity $\\dot p_h$')\nax[0].set_title('State evolution')\nax[0].set_xlabel('Time')\nax[0].grid(True)\nax[0].legend()\n\nax[1].step(time[:-1], u.flatten(), where='post', label='Control input $u_h$')\nax[1].set_title('Control input')\nax[1].set_xlabel('Time')\nax[1].grid(True)\nax[1].legend()\n\nfig.tight_layout()\nplt.show()\n\nplt.plot(x[0, :], x[1, :], 'o-', label='state $x_h$')\nfor k in range(N):\n    dx = x[0, k+1] - x[0, k]\n    dy = x[1, k+1] - x[1, k]\n    plt.arrow(x[0, k], x[1, k], dx, dy, \n              head_width=0.02, length_includes_head=True, color='gray', alpha=0.6)\n\nplt.grid(True)\nplt.xlabel('Position')\nplt.ylabel('Velocity')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The state and control input over time.\n\n\n\n\n\n\n\n\n\n\n\n(b) The trajectory of \\(x_h\\) plotted in the phase space.\n\n\n\n\n\n\n\nFigure 3.1: Visualization of a double integrator LQR system.\n\n\n\nWe see that the LQR control smoothly controls the position of the ball.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-optimal-lqr",
    "href": "control.html#sec-optimal-lqr",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.4 Optimality and the Riccati Equation",
    "text": "3.4 Optimality and the Riccati Equation\nIn this section, we’ll compute the optimal value function \\(V^\\star_h\\), Q-function \\(Q^\\star_h\\), and policy \\(\\pi^\\star_h\\) for an LQR problem (def. 3.5) using dynamic programming. The algorithm is identical to the one in Section 2.3.1, except here states and actions are vector-valued. Recall the definition of the optimal value function:\n\nDefinition 3.6 (Optimal value function in LQR) The optimal value function is the one that, at any time and in any state, achieves minimum cost across all policies \\(\\pi\\) that are history-independent, time-dependent, and deterministic.\n\\[\n\\begin{aligned}\n    V^\\star_h(x) &= \\min_{\\pi} V^\\pi_h(x) \\\\\n    &= \\min_{\\pi} \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} \\left[\n            \\sum_{h'=h}^{H-1} \\left( x_{h'}^\\top Q x_{h'} + u_{h'}^\\top R u_{h'} \\right)\n            \\mid x_h= x\n        \\right] \\\\\n\\end{aligned}\n\\tag{3.17}\\]\n\n\nDefinition 3.7 (Optimal Q function in LQR) The optimal Q-function is defined in the same way, conditioned on the starting state-action pair:\n\\[\n\\begin{aligned}\n    Q^\\star_h(x, u) &= \\min_{\\pi} Q^\\pi_h(x, u) \\\\\n    &= \\min_{\\pi} \\mathop{\\mathbb{E}}\\left[\n        \\sum_{h'=h}^{H-1}\\left(  x_{h'}^\\top Q x_{h'} + u_{h'}^\\top R u_{h'} \\right)\n        \\mid x_h= x, u_h= u\n    \\right] \\\\\n\\end{aligned}\n\\tag{3.18}\\]\n\n\nRemark 3.3 (Minimum over all policies). By Theorem 2.6, we do not lose any generality by only considering policies that are history-independent, time-dependent, and deterministic. That is, the optimal policy within this class is also globally optimal across all policies, in the sense that it achieves lower cost for any given trajectory prefix.\n\nThe solution has very simple structure: \\(V_h^\\star\\) and \\(Q^\\star_h\\) are upward-curved quadratics and \\(\\pi_h^\\star\\) is linear and furthermore does not depend on the amount of noise!\n\nTheorem 3.2 (Optimal value function in LQR is an upward-curved quadratic) At each timestep \\(h\\in [H]\\),\n\\[\nV^\\star_h(x) = x^\\top P_hx+ p_h\n\\tag{3.19}\\]\nfor some symmetric, positive-definite \\(n_x\\times n_x\\) matrix \\(P_h\\) and scalar \\(p_h\\in \\mathbb{R}\\).\n\n\nTheorem 3.3 (Optimal policy in LQR is linear) At each timestep \\(h\\in [H]\\),\n\\[\n\\pi^\\star_h(x) = - K_hx\n\\tag{3.20}\\]\nfor some \\(K_h\\in \\mathbb{R}^{n_u\\times n_x}\\). (The negative is due to convention.)\n\nThe construction (and inductive proof) proceeds similarly to the one in the MDP setting (Section 2.3.1). We leave the full proof, which involves a significant amount of linear algebra, to Section B.1.\n\n\n\n\n\n\ngraph RL\n    Q0[\"$$Q^\\star_{H-1} = r$$\"] -- \"$$\\max_{a \\in \\mathcal{A}}$$\" --&gt; V0\n    Q0 -- \"$$\\arg\\max_{a \\in \\mathcal{A}}$$\" --&gt; P0[\"$$\\pi^\\star_{H-1}$$\"]\n    V0[\"$$V^\\star_{H-1}$$\"] -- \"Bellman equations\" --&gt; Q1\n    \n    Q1[\"$$Q^\\star_{H-2}$$\"] -- \"$$\\max_{a \\in \\mathcal{A}}$$\" --&gt; V1\n    Q1 -- \"$$\\arg\\max_{a \\in \\mathcal{A}}$$\" --&gt; P1[\"$$\\pi^\\star_{H-2}$$\"]\n    V1[\"$$V^\\star_{H-2}$$\"] -- \"Bellman equations\" --&gt; Q2\n\n    Q2[\"...\"]\n\n\n\n\nFigure 3.2: Illustrating a dynamic programming algorithm for computing the optimal policy in an LQR environment.\n\n\n\n\n\nHere we provide the concrete \\(P_h, p_h\\), and \\(K_h\\) used to compute the quantities above.\n\nDefinition 3.8 (Riccati equation) It turns out that the matrices \\(P_0, \\dots, P_{H-1}\\) used to define \\(V^\\star\\) obey a recurrence relation known as the Riccati equation:\n\\[\nP_h= Q + A^\\top P_{h+1} A - A^\\top P_{h+1} B (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A,\n\\tag{3.21}\\]\nwhere \\(A \\in \\mathbb{R}^{n_x\\times n_x}\\) and \\(B \\in \\mathbb{R}^{n_x\\times n_u}\\) are the matrices used to define the dynamics \\(f\\) and \\(Q \\in \\mathbb{R}^{n_x\\times n_x}\\) and \\(R \\in \\mathbb{R}^{n_u\\times n_u}\\) are the matrices used to define the cost function \\(c\\) (def. 3.5).\n\nThe scalars \\(p_h\\) obey the recurrence relation\n\\[\np_h= \\mathrm{Tr}(\\sigma^2 P_{h+1}) + p_{h+1}\n\\tag{3.22}\\]\nand the matrices \\(K_0, \\dots, K_{H-1}\\) for defining the policy satisfy\n\\[\nK_h= (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A.\n\\tag{3.23}\\]\nBy setting \\(P_H= 0\\) and \\(p_H= 0\\),\n\n\nCode\ndef dp_lqr(A, B, Q, R, sigma, H):\n    n_x, n_u = B.shape\n    P = jnp.zeros(n_x, n_x)\n    p = 0\n    P_ary, p_ary = [], []\n    for h in range(H):\n        P = Q + A.T @ P @ A - A.T @ P @ B @ np.inv(R + B.T @ P @ B) @ B.T @ P @ A\n        p = sigma ** 2 * np.trace(P) + p\n        P_ary.append(P)\n        p_ary.append(p)\n    return reversed(P_ary), reversed(p)\n\n\n\nDefinition 3.9 (Dynamic programming in LQR) Let \\(A, B, Q, R\\) be the matrices that define the LQR problem, and let \\(\\sigma^2\\) be the variance of the noise in the dynamics (def. 3.5).\n\nInitialize \\(P_H= 0, p_H= 0\\).\nFor each \\(h= H- 1, \\dots, 0\\):\n\nCompute \\(P_h\\) using the Riccati equation (def. 3.8) and \\(p_h\\) using eq. 3.22.\nCompute \\(K_h\\) using eq. B.5.\nThese define the functions \\(V^\\star_h\\) and \\(\\pi^\\star_h\\) by Theorem 3.2 and Theorem 3.3.\n\n\n\n\n3.4.1 Expected state at time \\(h\\)\nLet’s consider how the state at time \\(h\\) behaves when we act according to this optimal policy. In the field of control theory, which is often used in high-stakes circumstances such as aviation or architecture, it is important to understand the way the state evolves under the proposed controls.\nHow can we compute the expected state at time \\(h\\) when acting according to the optimal policy? Let’s first express \\(x_h\\) in a cleaner way in terms of the history. Having linear dynamics makes it easy to expand terms backwards in time:\n\\[\n\\begin{aligned}\n    x_h& = A x_{h-1} + B u_{h-1} + w_{h-1}                                 \\\\\n            & = A (Ax_{h-2} + B u_{h-2} + w_{h-2}) + B u_{h-1} + w_{h-1} \\\\\n            & = \\cdots                                                                     \\\\\n            & = A^hx_0 + \\sum_{i=0}^{h-1} A^i (B u_{h-i-1} + w_{h-i-1}).\n\\end{aligned}\n\\tag{3.24}\\]\nLet’s consider the average state at this time, given all the past states and actions. Since we assume that \\(\\mathop{\\mathbb{E}}[w_h] = 0\\) (this is the zero vector in \\(d\\) dimensions), when we take an expectation, the \\(w_h\\) term vanishes due to linearity, and so we’re left with\n\\[\n\\mathop{\\mathbb{E}}[x_h\\mid x_{0:(h-1)}, u_{0:(h-1)}] = A^hx_0 + \\sum_{i=0}^{h-1} A^i B u_{h-i-1}.\n\\tag{3.25}\\]\n\nExercise 3.2 (Expected state) Show that if we choose actions according to the optimal policy Lemma B.2, eq. 3.25 becomes\n\\[\n\\mathop{\\mathbb{E}}[x_h\\mid x_0, u_i = \\pi^\\star_i(x_i)\\quad \\forall i \\le h] = \\left( \\prod_{i=0}^{h-1} (A - B K_i) \\right) x_0.\n\\]\n\nThis introdces the quantity \\(A - B K_i\\), which shows up frequently in control theory. For example, one important question is: will \\(x_h\\) remain bounded, or will it go to infinity as time goes on? To answer this, let’s imagine for simplicity that these \\(K_i\\)s are equal (call this matrix \\(K\\)). Then the expression above becomes \\((A-BK)^hx_0\\). Now consider the maximum eigenvalue \\(\\lambda_{\\max}\\) of \\(A - BK\\). If \\(|\\lambda_{\\max}| &gt; 1\\), then there’s some nonzero initial state \\(\\bar x_0\\), the corresponding eigenvector, for which\n\\[\n\\lim_{h\\to \\infty} (A - BK)^h\\bar x_0\n    = \\lim_{h\\to \\infty} \\lambda_{\\max}^h\\bar x_0\n    = \\infty.\n\\]\nOtherwise, if \\(|\\lambda_{\\max}| &lt; 1\\), then it’s impossible for your original state to explode as dramatically.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#extensions",
    "href": "control.html#extensions",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.5 Extensions",
    "text": "3.5 Extensions\nWe’ve now formulated an optimal solution for the time-homogeneous LQR and computed the expected state under the optimal policy. However, real world tasks rarely have such simple dynamics, and we may wish to design more complex cost functions. In this section, we’ll consider more general extensions of LQR where some of the assumptions we made above are relaxed. Specifically, we’ll consider:\n\nTime-dependency, where the dynamics and cost function might change depending on the timestep.\nGeneral quadratic cost, where we allow for linear terms and a constant term.\nTracking a goal trajectory rather than aiming for a single goal state-action pair.\n\nCombining these will allow us to use the LQR solution to solve more complex setups by taking Taylor approximations of the dynamics and cost functions.\n\n3.5.1 Time-dependent dynamics and cost function\nSo far, we’ve considered the time-homogeneous case, where the dynamics and cost function stay the same at every timestep. However, this might not always be the case. As an example, in many sports, the rules and scoring system might change during an overtime period. To address such tasks, we can loosen the time-homogeneous restriction, and consider the case where the dynamics and cost function are time-dependent. Our analysis remains almost identical; in fact, we can simply add a time index to the matrices \\(A\\) and \\(B\\) that determine the dynamics and the matrices \\(Q\\) and \\(R\\) that determine the cost.\nThe modified problem is now defined as follows:\n\nDefinition 3.10 (Time-dependent LQR) \\[\n\\begin{aligned}\n        \\min_{\\pi_{0}, \\dots, \\pi_{H-1}} \\quad & \\mathop{\\mathbb{E}}\\left[ \\left( \\sum_{h=0}^{H-1} (x_h^\\top Q_hx_h) + u_h^\\top R_hu_h\\right) + x_H^\\top Q_Hx_H\\right] \\\\\n        \\textrm{where} \\quad                      & x_{h+1} = f_h(x_h, u_h, w_h) = A_hx_h+ B_hu_h+ w_h\\\\\n                                                  & x_0 \\sim \\mu_0                                                                                                                                   \\\\\n                                                  & u_h= \\pi_h(x_h)                                                                                                                       \\\\\n                                                  & w_h\\sim \\mathcal{N}(0, \\sigma^2 I).\n\\end{aligned}\n\\]\n\nThe derivation of the optimal value functions and the optimal policy remains almost exactly the same, and we can modify the Riccati equation accordingly:\n\nDefinition 3.11 (Time-dependent Riccati Equation) \\[\nP_h= Q_h+ A_h^\\top P_{h+1} A_h- A_h^\\top P_{h+1} B_h(R_h+ B_h^\\top P_{h+1} B_h)^{-1} B_h^\\top P_{h+1} A_h.\n\\]\nNote that this is just the time-homogeneous Riccati equation (def. 3.8), but with the time index added to each of the relevant matrices.\n\n\nExercise 3.3 (Time dependent LQR proof) Walk through the proof in Section 3.4 to verify that we can simply add \\(h\\) for the time-dependent case.\n\nAdditionally, by allowing the dynamics to vary across time, we gain the ability to locally approximate nonlinear dynamics at each timestep. We’ll discuss this later in the chapter.\n\n\n3.5.2 More general quadratic cost functions\nOur original cost function had only second-order terms with respect to the state and action, incentivizing staying as close as possible to \\((x^\\star, u^\\star) = (0, 0)\\). We can also consider more general quadratic cost functions that also have first-order terms and a constant term. Combining this with time-dependent dynamics results in the following expression, where we introduce a new matrix \\(M_h\\) for the cross term, linear coefficients \\(q_h\\) and \\(r_h\\) for the state and action respectively, and a constant term \\(c_h\\):\n\\[\nc_h(x_h, u_h) = ( x_h^\\top Q_hx_h+ x_h^\\top M_hu_h+ u_h^\\top R_hu_h) + (x_h^\\top q_h+ u_h^\\top r_h) + c_h.\n\\tag{3.26}\\]\nSimilarly, we can also include a constant term \\(v_h\\in \\mathbb{R}^{n_x}\\) in the dynamics. This is deterministic, unlike the stochastic noise \\(w_h\\):\n\\[\nx_{h+1} = f_h(x_h, u_h, w_h) = A_hx_h+ B_hu_h+ v_h+ w_h.\n\\]\n\nExercise 3.4 (General cost function) Derive the optimal solution. You will need to slightly modify the proof in Section 3.4.\n\n\n\n3.5.3 Tracking a predefined trajectory\nConsider applying LQR to a task like autonomous driving, where the target state-action pair changes over time. We might want the vehicle to follow a predefined trajectory of states and actions \\((x_h^\\star, u_h^\\star)_{h=0}^{H-1}\\). To express this as a control problem, we’ll need a corresponding time-dependent cost function:\n\\[\nc_h(x_h, u_h) = (x_h- x^\\star_h)^\\top Q (x_h- x^\\star_h) + (u_h- u^\\star_h)^\\top R (u_h- u^\\star_h).\n\\]\nNote that this punishes states and actions that are far from the intended trajectory. By expanding out these multiplications, we can see that this is actually a special case of the more general quadratic cost function above eq. 3.26:\n\\[\nM_h= 0, \\qquad q_h= -2Q x^\\star_h, \\qquad r_h= -2R u^\\star_h, \\qquad c_h= (x^\\star_h)^\\top Q (x^\\star_h) + (u^\\star_h)^\\top R (u^\\star_h).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-approx-nonlinear",
    "href": "control.html#sec-approx-nonlinear",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.6 Approximating nonlinear dynamics",
    "text": "3.6 Approximating nonlinear dynamics\nThe LQR algorithm solves for the optimal policy when the dynamics are linear and the cost function is an upward-curved quadratic. However, real settings are rarely this simple! Let’s return to the CartPole example from the start of the chapter (ex. 3.1). The dynamics (physics) aren’t linear. How can we approximate this by an LQR problem?\nConcretely, let’s consider a noise-free problem since, as we saw, the noise doesn’t factor into the optimal policy. Let’s assume the dynamics and cost function are stationary, and ignore the terminal state for simplicity:\n\nDefinition 3.12 (Nonlinear control problem) \\[\n\\begin{aligned}\n        \\min_{\\pi_0, \\dots, \\pi_{H-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad\n        & \\mathop{\\mathbb{E}}_{x_0 \\sim P_0} \\left[\n            \\sum_{h=0}^{H-1} c(x_h, u_h)\n        \\right] \\\\\n        \\text{where} \\quad                                  & x_{h+1} = f(x_h, u_h)                                   \\\\\n                                                            & u_h= \\pi_h(x_h)                                          \\\\\n                                                            & x_0 \\sim \\mu_0                                                     \\\\\n                                                            & c(x, u) = d(x, x^\\star) + d(u, u^\\star).\n\\end{aligned}\n\\]\nHere, \\(d\\) denotes a function that measures the “distance” between its two arguments.\n\nThis is now only slightly simplified from the general optimal control problem (see def. 3.2). Here, we don’t know an analytical form for the dynamics \\(f\\) or the cost function \\(c\\), but we assume that we’re able to query/sample/simulate them to get their values at a given state and action. To clarify, consider the case where the dynamics are given by real world physics. We can’t (yet) write down an expression for the dynamics that we can differentiate or integrate analytically. However, we can still simulate the dynamics and cost function by running a real-world experiment and measuring the resulting states and costs. How can we adapt LQR to this more general nonlinear case?\n\n3.6.1 Local linearization\nHow can we apply LQR when the dynamics are nonlinear or the cost function is more complex? We’ll exploit the useful fact that we can take a function that’s locally continuous around \\((x^\\star, u^\\star)\\) and approximate it nearby with low-order polynomials (i.e. its Taylor approximation). In particular, as long as the dynamics \\(f\\) are differentiable around \\((x^\\star, u^\\star)\\) and the cost function \\(c\\) is twice differentiable at \\((x^\\star, u^\\star)\\), we can take a linear approximation of \\(f\\) and a quadratic approximation of \\(c\\) to bring us back to the regime of LQR.\nLinearizing the dynamics around \\((x^\\star, u^\\star)\\) gives:\n\\[\n\\begin{gathered}\n    f(x, u) \\approx f(x^\\star, u^\\star) + \\nabla_xf(x^\\star, u^\\star) (x- x^\\star) + \\nabla_uf(x^\\star, u^\\star) (u- u^\\star) \\\\\n    (\\nabla_xf(x, u))_{ij} = \\frac{d f_i(x, u)}{d x_j}, \\quad i, j \\le n_x\\qquad (\\nabla_uf(x, u))_{ij} = \\frac{d f_i(x, u)}{d u_j}, \\quad i \\le n_x, j \\le n_u\n\\end{gathered}\n\\]\nand quadratizing the cost function around \\((x^\\star, u^\\star)\\) gives:\n\\[\n\\begin{aligned}\n    c(x, u) & \\approx c(x^\\star, u^\\star) \\quad \\text{constant term}                                                                                      \\\\\n                 & \\qquad + \\nabla_xc(x^\\star, u^\\star) (x- x^\\star) + \\nabla_uc(x^\\star, u^\\star) (a - u^\\star) \\quad \\text{linear terms} \\\\\n                 & \\left. \\begin{aligned}\n                               & \\qquad + \\frac{1}{2} (x- x^\\star)^\\top \\nabla_{xx} c(x^\\star, u^\\star) (x- x^\\star)       \\\\\n                               & \\qquad + \\frac{1}{2} (u- u^\\star)^\\top \\nabla_{uu} c(x^\\star, u^\\star) (u- u^\\star) \\\\\n                               & \\qquad + (x- x^\\star)^\\top \\nabla_{xu} c(x^\\star, u^\\star) (u- u^\\star)\n                          \\end{aligned} \\right\\} \\text{quadratic terms}\n\\end{aligned}\n\\]\nwhere the gradients and Hessians are defined as\n\\[\n\\begin{aligned}\n    (\\nabla_xc(x, u))_{i}         & = \\frac{d c(x, u)}{d x_i}, \\quad i \\le n_x\n                                          & (\\nabla_uc(x, u))_{i}                                               & = \\frac{d c(x, u)}{d u_i}, \\quad i \\le n_u\\\\\n    (\\nabla_{xx} c(x, u))_{ij}  & = \\frac{d^2 c(x, u)}{d x_i d x_j}, \\quad i, j \\le n_x\n                                          & (\\nabla_{uu} c(x, u))_{ij}                                       & = \\frac{d^2 c(x, u)}{d u_i d u_j}, \\quad i, j \\le n_u\\\\\n    (\\nabla_{xu} c(x, u))_{ij} & = \\frac{d^2 c(x, u)}{d x_i d u_j}. \\quad i \\le n_x, j \\le n_u\n\\end{aligned}\n\\]\n\nExercise 3.5 (Expressing as quadratic) Note that this cost can be expressed in the general quadratic form seen in eq. 3.26. Derive the corresponding quantities \\(Q, R, M, q, r, c\\).\n\n\n\n3.6.2 Finite differencing\nTo calculate these gradients and Hessians in practice, we use a method known as finite differencing for numerically computing derivatives. Namely, we can simply use the limit definition of the derivative, and see how the function changes as we add or subtract a tiny \\(\\delta\\) to the input.\n\\[\n\\frac{d}{dx} f(x) = \\lim_{\\delta \\to 0} \\frac{f(x + \\delta) - f(x)}{\\delta}\n\\]\nThis only requires us to be able to query the function, not to have an analytical expression for it, which is why it’s so useful in practice.\n\n\n3.6.3 Local convexification\nHowever, simply taking the second-order approximation of the cost function is insufficient, since for the LQR setup we required that the \\(Q\\) and \\(R\\) matrices were positive definite, i.e. that all of their eigenvalues were positive.\nOne way to naively force some symmetric matrix \\(D\\) to be positive definite is to set any non-positive eigenvalues to some small positive value \\(\\varepsilon &gt; 0\\). Recall that any real symmetric matrix \\(D \\in \\mathbb{R}^{n \\times n}\\) has an basis of eigenvectors \\(u_1, \\dots, u_n\\) with corresponding eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) such that \\(D u_i = \\lambda_i u_i\\). Then we can construct the positive definite approximation by\n\\[\n\\widetilde{D} = \\left( \\sum_{i=1, \\dots, n \\mid \\lambda_i &gt; 0} \\lambda_i u_i u_i^\\top \\right) + \\varepsilon I.\n\\]\nExercise: Convince yourself that \\(\\widetilde{D}\\) is indeed positive definite.\nNote that Hessian matrices are generally symmetric, so we can apply this process to \\(Q\\) and \\(R\\) to obtain the positive definite approximations \\(\\widetilde{Q}\\) and \\(\\widetilde{R}\\). Now that we have an upward-curved quadratic approximation to the cost function, and a linear approximation to the state transitions, we can simply apply the time-homogenous LQR methods from Section 3.4.\nBut what happens when we enter states far away from \\(x^\\star\\) or want to use actions far from \\(u^\\star\\)? A Taylor approximation is only accurate in a local region around the point of linearization, so the performance of our LQR controller will degrade as we move further away. We’ll see how to address this in the next section using the iterative LQR algorithm.\n\n\n\nLocal linearization might only be accurate in a small region around the point of linearization.\n\n\n\n\n3.6.4 Iterative LQR\nTo address these issues with local linearization, we’ll use an iterative approach, where we repeatedly linearize around different points to create a time-dependent approximation of the dynamics, and then solve the resulting time-dependent LQR problem to obtain a better policy. This is known as iterative LQR or iLQR:\n\nDefinition 3.13 (Iterative LQR) For each iteration of the algorithm:\n\nForm a time-dependent LQR problem (Section 3.5.1) around the current candidate trajectory using local linearization.\nCompute the optimal policy of the time-dependent LQR problem.\nSample a trajectory using this optimal policy.\nCompute a better candidate trajectory by interpolating between the current and proposed actions.\n\n\n\n\n\n\n\n\ngraph LR\n    Approx[\"Locally approximate a linear model of the environment\"] -- \"$$f, c$$\" --&gt; Opt\n    Opt[\"Solve for the optimal policy in the model\"] -- \"$$\\pi^\\star$$\" --&gt; Approx\n\n\n\n\nFigure 3.3: iLQR can be phrased in the style of policy iteration (Section 2.4.4.2). It alternates between computing a locally linear model around the current policy’s trajectories and computing the optimal policy that solves the linear model.\n\n\n\n\n\nNow let’s go through the details of each step. We’ll use superscripts to denote the iteration of the algorithm. We’ll also denote \\(\\bar x_0 = \\mathop{\\mathbb{E}}_{x_0 \\sim \\mu_0} [x_0]\\) as the expected initial state.\nAt iteration \\(i\\) of the algorithm, we begin with a candidate trajectory \\(\\bar \\tau^i = (\\bar x^i_0, \\bar u^i_0, \\dots, \\bar x^i_{H-1}, \\bar u^i_{H-1})\\).\nStep 1: Form a time-dependent LQR problem. At each timestep \\(h\\in [H]\\), we use the techniques from Section 3.6 to linearize the dynamics and quadratize the cost function around \\((\\bar x^i_h, \\bar u^i_h)\\):\n\\[\n\\begin{aligned}\n    f_h(x, u) & \\approx f(\\bar {x}^i_h, \\bar {u}^i_h) + \\nabla_{x} f(\\bar {x}^i_h, \\bar {u}^i_h)(x- \\bar {x}^i_h) + \\nabla_{u} f(\\bar {x}^i_h, \\bar {u}^i_h)(u- \\bar {u}^i_h)                         \\\\\n    c_h(x, u) & \\approx c(\\bar {x}^i_h, \\bar {u}^i_h) + \\begin{bmatrix}\n                                                              x- \\bar {x}^i_h& u- \\bar {u}^i_h\n                                                          \\end{bmatrix} \\begin{bmatrix}\n                                                                            \\nabla_{x} c(\\bar {x}^i_h, \\bar {u}^i_h)\\\\\n                                                                            \\nabla_{u} c(\\bar {x}^i_h, \\bar {u}^i_h)\n                                                                        \\end{bmatrix}                                                      \\\\\n                     & \\qquad + \\frac{1}{2} \\begin{bmatrix}\n                                                x- \\bar {x}^i_h& u- \\bar {u}^i_h\n                                            \\end{bmatrix} \\begin{bmatrix}\n                                                              \\nabla_{xx} c(\\bar {x}^i_h, \\bar {u}^i_h)  & \\nabla_{xu} c(\\bar {x}^i_h, \\bar {u}^i_h)  \\\\\n                                                              \\nabla_{ux} c(\\bar {x}^i_h, \\bar {u}^i_h) & \\nabla_{uu} c(\\bar {x}^i_h, \\bar {u}^i_h)\n                                                          \\end{bmatrix}\n    \\begin{bmatrix}\n        x- \\bar {x}^i_h\\\\\n        u- \\bar {u}^i_h\n    \\end{bmatrix}.\n\\end{aligned}\n\\]\nStep 2: Compute the optimal policy. We solve the time-dependent LQR problem using the time-dependent algebraic Riccati equation (def. 3.11) to compute the optimal policy \\(\\pi^i_0, \\dots, \\pi^i_{H-1}\\).\nStep 3: Generate a new series of actions. We generate a new sample trajectory by taking actions according to this optimal policy:\n\\[\n\\bar x^{i+1}_0 = \\bar x_0, \\qquad \\widetilde u_h= \\pi^i_h(\\bar x^{i+1}_h), \\qquad \\bar x^{i+1}_{h+1} = f(\\bar x^{i+1}_h, \\widetilde u_h).\n\\]\nNote that the states are sampled according to the true dynamics, which we assume we have query access to.\nStep 4: Compute a better candidate trajectory. Note that we’ve denoted these actions as \\(\\widetilde u_h\\) and aren’t directly using them for the next iteration \\(\\bar u^{i+1}_h\\). Rather, we want to interpolate between them and the actions from the previous iteration \\(\\bar u^i_0, \\dots, \\bar u^i_{H-1}\\). This is so that the cost will increase monotonically, since if the new policy turns out to actually be worse, we can stay closer to the previous trajectory. The new policy might be worse, for example, if the states in the new trajectory are far enough from the original states that the local linearization is no longer accurate.\nFormally, we want to find \\(\\alpha \\in [0, 1]\\) to generate the next iteration of actions \\(\\bar u^{i+1}_0, \\dots, \\bar u^{i+1}_{H-1}\\) such that the cost is minimized:\n\\[\n\\begin{aligned}\n    \\min_{\\alpha \\in [0, 1]} \\quad & \\sum_{h=0}^{H-1} c(x_h, \\bar u^{i+1}_h)                     \\\\\n    \\text{where} \\quad             & x_{h+1} = f(x_h, \\bar u^{i+1}_h)                             \\\\\n                                   & \\bar u^{i+1}_h= \\alpha \\bar u^i_h+ (1-\\alpha) \\widetilde u_h\\\\\n                                   & x_0 = \\bar x_0.\n\\end{aligned}\n\\]\nNote that this optimizes over the closed interval \\([0, 1]\\), so by the Extreme Value Theorem, it’s guaranteed to have a global maximum.\nThe final output of this algorithm is a policy \\(\\pi^{n_\\text{steps}}\\) derived after \\(n_\\text{steps}\\) of the algorithm. Though the proof is somewhat complex, one can show that for many nonlinear control problems, this solution converges to a locally optimal solution (in the policy space).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#key-takeaways",
    "href": "control.html#key-takeaways",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.7 Key takeaways",
    "text": "3.7 Key takeaways\nThis chapter introduced some approaches to solving simple variants of the optimal control problem def. 3.2. We began with the simple case of linear dynamics and an upward-curved quadratic cost. This model is called the linear quadratic regulator (LQR). The optimal policy can be solved using dynamic programming. We then extended these results to the more general nonlinear case via local linearization. We finally studied the iterative LQR algorithm for solving nonlinear control problems.\nIn the big picture, the LQR algorithm can be seen as an extension of the dynamic programming algorithms in Chapter 2 to continuous state and action spaces. Both of these methods assume that the environment is fully known. iLQR loosens this assumption slightly: as long as we have “query access” to the environment, that is, we can efficiently sample the state following any given state-action pair, we can locally approximate the dynamics using finite differences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-control-bib",
    "href": "control.html#sec-control-bib",
    "title": "3  Linear Quadratic Regulators",
    "section": "3.8 Bibliographic notes and further reading",
    "text": "3.8 Bibliographic notes and further reading\nThe field of control theory generally predates that of RL and can be seen as a precursor to the model-based planning algorithms used for RL tasks. People have attempted to solve the optimal control problem since antiquity: the first recorded control device (i.e. our notion of a policy, put into practice) is thought to be a water clock from the third century BCE that kept time by controlling the amount of water flowing out of a vessel (Keviczky et al., 2019; Mayr, 1970).\nWatt’s 1788 centrifugal governor for controlling the pace of a steam engine was one of the greatest inventions in control at the time. We refer the reader to (Bellman, 1961; MacFarlane, 1979) for more on the history of control systems.\nThe first attempt to formalize control theory as a field is attributed to Maxwell (1867). Maxwell (well known for Maxwell’s equations) demonstrated the utility of analyzing mathematical models to design stable systems. Other researchers also studied the stability of differential equations. The work of Lyapunov (1892) has been widely influential in this field; his seminal work did not appear in English until Ljapunov & Fuller (1992).\nA number of excellent textbooks have been published on optimal control (Athans & Falb, 1966; Bellman, 1957; Berkovitz, 1974; Lewis et al., 2012). We refer interested readers to these for further reading, particularly about the continuous-time case.\n\n\n\n\nAthans, M., & Falb, P. L. (1966). Optimal control: An introduction to the theory and its applications. McGraw-Hill. https://books.google.com?id=pfJHAQAAIAAJ\n\n\nBellman, R. (1957). Dynamic programming. Princeton University Press. https://books.google.com?id=rZW4ugAACAAJ\n\n\nBellman, R. (1961). Adaptive control processes: A guided tour. Princeton University Press. https://books.google.com?id=POAmAAAAMAAJ\n\n\nBerkovitz, L. D. (1974). Optimal control theory. Springer Science+Business Media LLC.\n\n\nKeviczky, L., Bars, R., Hetthéssy, J., & Bányász, C. (2019). Control engineering. Springer. https://doi.org/10.1007/978-981-10-8297-9\n\n\nLewis, F. L., Vrabie, D. L., & Syrmos, V. L. (2012). Optimal control (3rd ed). John Wiley & Sons. https://doi.org/10.1002/9781118122631\n\n\nLjapunov, A. M., & Fuller, A. T. (1992). The general problem of the stability of motion. Taylor & Francis.\n\n\nLyapunov, A. M. (1892). The general problem of the stability of motion. University of Kharkov.\n\n\nMacFarlane, A. (1979). The development of frequency-response methods in automatic control [perspectives]. IEEE Transactions on Automatic Control, 24(2), 250–265. IEEE Transactions on Automatic Control. https://doi.org/10.1109/TAC.1979.1101978\n\n\nMaxwell, J. C. (1867). On governors. Proceedings of the Royal Society of London, 16, 270–283. https://www.jstor.org/stable/112510\n\n\nMayr, E. (1970). Populations, species and evolution: An abridgment of animal species and evolution. Belknap Press of Harvard University Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "bandits.html",
    "href": "bandits.html",
    "title": "4  Multi-Armed Bandits",
    "section": "",
    "text": "4.1 Introduction\nThe multi-armed bandit (MAB) problem is a simple setting for studying the basic challenges of sequential decision-making. In this setting, an agent repeatedly chooses from a fixed set of actions, called arms, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period. Here are a few examples:\nThe MAB is the simplest setting for exploring the exploration-exploitation tradeoff: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?\nIn this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.\nCode\nfrom jaxtyping import Float, Array\nimport numpy as np\nimport latexify\nfrom typing import Callable, Union\nimport matplotlib.pyplot as plt\nimport utils\n\nimport solutions.bandits as solutions\n\nnp.random.seed(184)\n\ndef random_argmax(ary: Array) -&gt; int:\n    \"\"\"Take an argmax and randomize between ties.\"\"\"\n    max_idx = np.flatnonzero(ary == ary.max())\n    return np.random.choice(max_idx).item()\n\n\n# used as decorator\nlatex = latexify.algorithmic(\n    trim_prefixes={\"mab\"},\n    id_to_latex={\"arm\": \"a_\\ep\", \"reward\": \"r\", \"means\": \"\\mu\", \"t\": r\"\\ep\", \"T\": r\"\\Ep\"},\n    use_math_symbols=True,\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#introduction",
    "href": "bandits.html#introduction",
    "title": "4  Multi-Armed Bandits",
    "section": "",
    "text": "Example 4.1 (Online advertising) Let’s suppose you, the agent, are an advertising company. You have \\(K\\) different ads that you can show to users. Let’s suppose you are targeting a single user. You receive \\(1\\) reward if the user clicks the ad, and \\(0\\) otherwise. Thus, the unknown reward distribution associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.\n\n\n\n\n\n\nDeciding which ad to put on a billboard can be treated as a MAB problem. Image from Negative Space (2015).\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4.2 (Clinical trials) Suppose you’re a pharmaceutical company, and you’re testing a new drug. You have \\(K\\) different dosages of the drug that you can administer to patients. You receive \\(1\\) reward if the patient recovers, and \\(0\\) otherwise. Thus, the unknown reward distribution associated with each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.\n\n\n\n\n\n\nOptimizing between different medications can be treated as a MAB problem. Image from Pixabay (2016a).\n\n\n\n\n\n\n\n\n\nRemark 4.1 (Etymology). The name “multi-armed bandit” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.\n\n\n\n\nTable 4.1: How bandits leads into the full RL problem.\n\n\n\n\n\n\n\n\n\n\nEnvironment is…\nknown (planning/optimal control)\nunknown (learning from experience)\n\n\n\n\nstateless/invariant (\\(\\vert\\mathcal{S}\\vert = 1\\))\n(trivial)\nmulti-armed bandits\n\n\nstateful/dynamic\nMDP planning (dynamic programming)\n“full” RL",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-bandit-problem",
    "href": "bandits.html#sec-bandit-problem",
    "title": "4  Multi-Armed Bandits",
    "section": "4.2 The multi-armed bandit problem",
    "text": "4.2 The multi-armed bandit problem\nLet’s frame the MAB problem mathematically. In both ex. 4.1 and ex. 4.2, the outcome of each decision is binary (i.e. \\(0\\) or \\(1\\)). This is the Bernoulli bandit problem, which we’ll use throughout the chapter.\n\n\n\n\n\n\n\nDefinition 4.1 (Bernoulli bandit problem) Let \\(K\\) denote the number of arms. We’ll label them \\(0, \\dots, K-1\\) and use superscripts to indicate the arm index \\(k\\). Since we seldom need to raise a number to a power, this won’t cause much confusion.\nLet \\(\\mu^k\\) be the mean of arm \\(k\\), such that pulling arm \\(k\\) either returns reward \\(1\\) with probability \\(\\mu^k\\) or \\(0\\) otherwise.\nThe agent gets \\(T\\) chances to pull an arm. The task is to determine a strategy for maximizing the total reward.\n\n\n\n\n\n\nEach arm \\(k\\) in a Bernoulli bandit is a biased coin with probability \\(\\mu^k\\) of showing heads. Each “pull” corresponds to flipping a coin. The goal is to maximize the number of heads given \\(T\\) flips. Image from Pixabay (2016b).\n\n\n\n\n\n\n\nCode\nclass MAB:\n    \"\"\"The Bernoulli multi-armed bandit environment.\"\"\"\n\n    def __init__(self, means: Float[Array, \" K\"], T: int):\n        \"\"\"Constructor.\n\n        :param means: the means (success probabilities) of the reward distributions for each arm\n        :param T: the time horizon\n        \"\"\"\n        assert all(0 &lt;= p &lt;= 1 for p in means)\n        self.means = means\n        self.T = T\n        self.K = self.means.size\n        self.best_arm = random_argmax(self.means)\n\n    def pull(self, k: int) -&gt; int:\n        \"\"\"Pull the `k`-th arm and sample from its (Bernoulli) reward distribution.\"\"\"\n        reward = np.random.rand() &lt; self.means[k].item()\n        return +reward\n\n\n\n\nCode\nmab = MAB(means=np.array([0.1, 0.8, 0.4]), T=100)\n\n\n\nRemark 4.2 (Connection between MAB and MDP settings). A multi-armed bandit is a special case of a finite-horizon Markov decision process with a single state (\\(|\\mathcal{S}| = 1\\)) and a horizon of \\(H= 1\\). (Conceptually speaking, the environment is “stateless”, but an MDP with “no states” doesn’t make sense.) As such, Each arm is an action: \\(|\\mathcal{A}| = K\\). the expected value of pulling arm \\(k\\), which we denote \\(\\mu^k\\) in this chapter, is exactly the Q-function (def. 2.9):\n\\[\n\\mu^k= Q(k)\n\\tag{4.1}\\]\nNote that we omit the arguments for \\(\\pi, h\\), and \\(s\\), which are meaningless in the MAB setting.\n\nIn pseudocode, the agent’s interaction with the MAB environment can be described by the following process:\n\n\nCode\ndef mab_loop(mab: MAB, agent: \"Agent\") -&gt; int:\n    for t in range(mab.T):\n        arm = agent.choose_arm()  # in 0, ..., K-1\n        reward = mab.pull(arm)\n        agent.update_history(arm, reward)\n\n\nlatex(mab_loop)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{mab\\_loop}(\\mathrm{mab}: \\mathrm{MAB}, \\mathrm{agent}: \\textrm{\"Agent\"}) \\\\ \\hspace{1em} \\mathbf{for} \\ t\\in \\mathrm{range} \\mathopen{}\\left( T\\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} a_t\\gets \\mathrm{agent}.\\mathrm{choose\\_arm} \\mathopen{}\\left( \\mathclose{}\\right) \\\\ \\hspace{2em} r \\gets \\mathrm{pull} \\mathopen{}\\left( a_t\\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{agent}.\\mathrm{update\\_history} \\mathopen{}\\left( a_t, r \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 4.1: The MAB interaction loop. \\(a_t\\in [K]\\) denotes the \\(t\\)th arm pulled.\n\n\n\n\n\n\nCode\nclass Agent:\n    \"\"\"Stores the pull history and uses it to decide which arm to pull next.\n        \n    Since we are working with Bernoulli bandit,\n    we can summarize the pull history concisely in a (K, 2) array.\n    \"\"\"\n\n    def __init__(self, K: int, T: int):\n        \"\"\"The MAB agent that decides how to choose an arm given the past history.\"\"\"\n        self.K = K\n        self.T = T\n        self.rewards = []  # for plotting\n        self.choices = []\n        self.history = np.zeros((K, 2), dtype=int)\n\n    def choose_arm(self) -&gt; int:\n        \"\"\"Choose an arm of the MAB. Algorithm-specific.\"\"\"\n        ...\n\n    def count(self) -&gt; int:\n        \"\"\"The number of pulls made. Also the current step index.\"\"\"\n        return len(self.rewards)\n\n    def update_history(self, arm: int, reward: int):\n        self.rewards.append(reward)\n        self.choices.append(arm)\n        self.history[arm, reward] += 1\n\n\nWhat’s the optimal strategy for the agent, i.e. the one that achieves the highest expected reward? Convince yourself that the agent should try to always pull the arm with the highest expected reward:\n\nDefinition 4.2 (Optimal arm) We call the arm whose reward distribution has the highest mean the optimal arm:\n\\[\n\\begin{aligned}\nk^\\star &:= \\arg\\max_{k\\in [K]} \\mu^k\\\\\n\\mu^\\star &:= \\mu^{k^\\star} = \\max_{k\\in [K]} \\mu^k.\n\\end{aligned}\n\\tag{4.2}\\]\n\n\n4.2.1 Regret\nWhen analyzing an MAB algorithm, rather than measuring its performance in terms of the (expected) total reward, we instead compare it to the “oracle” strategy that knows the optimal arm in advance. This gives rise to a measure of performance known as regret, which answers, how much better could you have done had you known the optimal arm from the beginning? This provides a more meaningful baseline than the expected total reward, whose magnitude may vary widely depending on the bandit problem.\n\nExample 4.3 (Regret provides a more meaningful baseline) Consider the problem of grade inflation or deflation. If you went to a different school or university, you might have gotten a very different grade, independently of your actual ability. This is like comparing bandit algorithms based on the expected total reward: the baseline varies depending on the problem (i.e. school), making it hard to compare the algorithms (i.e. students) themselves. Instead, it makes more sense to measure you relative to the (theoretical) best student from your school. This is analogous to measuring algorithm performance using regret.\n\n\nDefinition 4.3 (Regret) The agent’s (cumulative) regret after \\(T\\) pulls is defined as\n\\[\n\\text{Regret}_T:= \\sum_{t=0}^{T-1} \\mu^\\star - \\mu^{a_t},\n\\tag{4.3}\\]\nwhere \\(a_t\\in [K]\\) is the arm chosen on the \\(t\\)th pull.\n\n\n\nCode\ndef regret_per_step(mab: MAB, agent: Agent):\n    \"\"\"Get the difference from the average reward of the optimal arm. The sum of these is the regret.\"\"\"\n    return [mab.means[mab.best_arm] - mab.means[arm] for arm in agent.choices]\n\n\nThis depends on the true means of the pulled arms, and is independent of the observed rewards. The regret \\(\\text{Regret}_T\\) is a random variable where the randomness comes from the agent’s strategy (i.e. the sequence of actions \\(a_0, \\dots, a_{K-1}\\)).\nAlso note that we care about the total regret across all decisions, rather than just the final decision. If we only cared about the quality of the final decision, the best strategy would be to learn as much as possible about the environment, and then use that knowledge to make the best possible decision at the last step. This is a pure exploration problem since the agent is never penalized for taking suboptimal actions up to the final decision. Minimizing the cumulative regret (or maximizing the cumulative reward) means we must strike a balance between exploration and exploitation throughout the entire decision-making process.\nThroughout the chapter, we will try to upper bound the regret of various algorithms in two different senses:\n\nUpper bound the expected regret, i.e. show that for some upper bound \\(M_T\\), \\[\n\\mathop{\\mathbb{E}}[\\text{Regret}_T] \\le M_T.\n\\tag{4.4}\\]\nFind a high-probability upper bound on the regret, i.e. show that for some small failure probability \\(\\delta &gt; 0\\), \\[\n\\mathbb{P}(\\text{Regret}_T\\le M_{T, \\delta}) \\ge 1-\\delta\n\\tag{4.5}\\] for some upper bound \\(M_{T, \\delta}\\).\n\nThe first approach says that the regret is at most \\(M_T\\) in expectation. However, the agent might still achieve a higher or lower regret on a particular randomization. The second approach says that, with probability at least \\(1-\\delta\\), the agent will achieve regret at most \\(M_{T, \\delta}\\). However, it doesn’t say anything about the regret in the remaining \\(\\delta\\) fraction of runs, which could be arbitrarily high.\n\nExercise 4.1 (Expected regret bounds yield probabilistic regret bounds) Suppose we have an upper bound \\(M_T\\) on the expected regret. Show that, with probability \\(1-\\delta\\), \\(M_{T, \\delta} := M_T/ \\delta\\) upper bounds the regret. Note this is a much higher bound! For example, if \\(\\delta = 0.01\\), the high-probability bound is \\(100\\) times as large.\n\n\nRemark 4.3 (Issues with regret). Is regret the right way to measure performance? Consider the following two algorithms:\n\nAn algorithm which sometimes chooses very bad arms.\nAn algorithm which often chooses slightly suboptimal arms.\n\nWe might care a lot about this distinction in practice, for example, in healthcare, where an occasional very bad decision could have extreme consequences. However, these two algorithms could achieve the exact same expected total regret.\nOther performance measures include probably approximately correct (PAC) bounds and uniform high-probability regret bounds. There is also a Uniform-PAC bound. We won’t discuss these in detail here; see the bibliographic notes for more details.\n\nWe’d like to achieve sublinear regret in expectation:\n\\[\n\\mathop{\\mathbb{E}}[\\text{Regret}_T] = o(T).\n\\tag{4.6}\\]\n(See Section A.1 if you’re unfamiliar with big-oh notation.) An algorithm with sublinear regret will eventually do as well as the oracle strategy as \\(T\\to \\infty\\).\n\nRemark 4.4 (Interpreting sublinear regret). An algorithm with sublinear regret in expectation doesn’t necessarily always choose the correct arm. In fact, such an algorithm can still choose a suboptimal arm infinitely many times. Consider a two-armed Bernoulli bandit with \\(\\mu^0 = 0\\) and \\(\\mu^1 = 1\\). An algorithm that chooses arm \\(0\\) \\(\\sqrt{T}\\) times out of \\(T\\) still achieves \\(O(\\sqrt{T})\\) regret in expectation.\nAlso, sublinear regret measures the asymptotic performance of the algorithm as \\(T\\to \\infty\\). It doesn’t tell us about the rate at which the algorithm approaches optimality.\n\nIn the remaining sections, we’ll walk through a series of MAB algorithms and see how they make different tradeoffs between exploration and exploitation.\n\n\nCode\ndef plot_strategy(mab: MAB, agent: Agent):\n    plt.figure(figsize=(4, 2))\n\n    # plot reward and cumulative regret\n    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label=\"reward\")\n    cum_regret = np.cumsum(regret_per_step(mab, agent))\n    plt.plot(np.arange(mab.T), cum_regret, label=\"cumulative regret\")\n\n    # draw colored circles for arm choices\n    colors = [\"red\", \"green\", \"blue\"]\n    color_array = [colors[k] for k in agent.choices]\n    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c=color_array)\n\n    # Add legend entries for each arm\n    for i, color in enumerate(colors):\n        plt.scatter([], [], c=color, label=f'arm {i}')\n\n    # labels and title\n    plt.xlabel(\"Pull index\")\n    plt.legend()\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-pure-exploration",
    "href": "bandits.html#sec-pure-exploration",
    "title": "4  Multi-Armed Bandits",
    "section": "4.3 Pure exploration",
    "text": "4.3 Pure exploration\nA trivial strategy is to always choose arms at random (i.e. “pure exploration”).\n\nDefinition 4.4 (Pure exploration) On the \\(t\\)th pull, the arm \\(a_t\\) is sampled uniformly at random from all arms:\n\\[\na_t\\sim \\text{Unif}([K])\n\\tag{4.7}\\]\n\n\n\nCode\nclass PureExploration(Agent):\n    def choose_arm(self):\n        \"\"\"Choose an arm uniformly at random.\"\"\"\n        return solutions.pure_exploration_choose_arm(self)\n\n\n\nTheorem 4.1 (Pure exploration expected regret) The pure exploration strategy achieves regret \\(\\Theta(T)\\) in expectation.\n\n\nProof. By choosing arms uniformly at random,\n\\[\n\\mathop{\\mathbb{E}}_{a_t\\sim \\text{Unif}([K])}[\\mu^{a_t}]\n=\n\\bar \\mu\n:=\n\\frac{1}{K} \\sum_{k=0}^{K-1} \\mu^k\n\\tag{4.8}\\]\nso the expected regret is simply\n\\[\n\\begin{aligned}\n    \\mathop{\\mathbb{E}}[\\text{Regret}_T]\n    &= \\sum_{t= 0}^{T-1} \\mathop{\\mathbb{E}}[\\mu^\\star - \\mu^{a_t}]\n    \\\\\n    &= T\\cdot (\\mu^\\star - \\bar \\mu).\n\\end{aligned}\n\\tag{4.9}\\]\nThis scales as \\(\\Theta(T)\\) in expectation, i.e. linear in the number of pulls \\(T\\).\n\n\n\nCode\nagent = PureExploration(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.2: Reward and cumulative regret of the pure exploration algorithm.\n\n\n\n\n\nPure exploration doesn’t use any information about the environment to improve its strategy. The distribution over its arm choices always appears “(uniformly) random”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-pure-greedy",
    "href": "bandits.html#sec-pure-greedy",
    "title": "4  Multi-Armed Bandits",
    "section": "4.4 Pure greedy",
    "text": "4.4 Pure greedy\nHow might we improve on pure exploration? Instead, we could try each arm once, and then commit to the one with the highest observed reward. We’ll call this the pure greedy strategy.\n\nDefinition 4.5 (Pure greedy) On the \\(t\\)th pull, the arm \\(a_t\\) is chosen according to\n\\[\na_t:= \\begin{cases}\nt& t&lt; K\\\\\n\\arg\\max_{k\\in [K]} r_k& t\\ge K\n\\end{cases}\n\\tag{4.10}\\]\nwhere \\(r_t\\) denotes the reward obtained on the \\(t\\)th pull.\n\n\n\nCode\nclass PureGreedy(Agent):\n    def choose_arm(self):\n        \"\"\"Choose the arm with the highest observed reward on its first pull.\"\"\"\n        return solutions.pure_greedy_choose_arm(self)\n\n\nHow does the expected regret of this strategy compare to that of pure exploration? We’ll do a more general analysis in the following section. Now, for intuition, suppose there’s just \\(K=2\\) arms with means \\(\\mu^1 &gt; \\mu^0\\). If \\(r_1 &gt; r_0\\), then we repeatedly pull arm \\(1\\) and achieve zero regret per pull. If \\(r_0 &gt; r_1\\), then we repeatedly pull arm \\(0\\) and incur \\(\\mu^1 - \\mu^0\\) regret per pull. Accounting for the first two pulls and the case of a tie, we obtain the following lower bound:\n\\[\n\\mathop{\\mathbb{E}}[\\text{Regret}_T] \\ge \\mathbb{P}(r^0 &gt; r^1) T\\cdot (\\mu^1 - \\mu^0)\n\\tag{4.11}\\]\nThis is still \\(\\Theta(T)\\), the same as pure exploration!\n\n\nCode\nagent = PureGreedy(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.3: Reward and cumulative regret of the pure greedy algorithm.\n\n\n\n\n\nThe cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, we see in fig. 4.3 that the greedy algorithm can get lucky on the first set of pulls and act entirely optimally for that experiment! But its average regret is still linear, since the chance of sticking with a suboptimal arm is nonzero.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-etc",
    "href": "bandits.html#sec-etc",
    "title": "4  Multi-Armed Bandits",
    "section": "4.5 Explore-then-commit",
    "text": "4.5 Explore-then-commit\nWe can improve the pure greedy algorithm (def. 4.5) as follows: let’s reduce the variance of the reward estimates by pulling each arm \\(N_\\text{explore} \\ge 1\\) times before committing. This is called the explore-then-commit strategy.\n\nDefinition 4.6 (Explore-then-commit) The explore-then-commit algorithm first pulls each arm \\(N_\\text{explore}\\) times. Let\n\\[\n\\hat \\mu^k:= \\frac{1}{N_\\text{explore}} \\sum_{n=0}^{N_\\text{explore}-1} r_{kN_\\text{explore} + n}\n\\tag{4.12}\\]\ndenote the empirical mean reward of arm \\(k\\) during these pulls. Afterwards, on the \\(t\\)th pull (for \\(t\\ge N_\\text{explore} K\\)), it chooses\n\\[\na_t:= \\arg\\max_{k\\in [K]} \\hat \\mu^k.\n\\tag{4.13}\\]\n\nNote that the “pure greedy” strategy above is just the special case where \\(N_\\text{explore} = 1\\).\n\n\nCode\nclass ExploreThenCommit(Agent):\n    def __init__(self, K: int, T: int, N_explore: int):\n        super().__init__(K, T)\n        self.N_explore = N_explore\n\n    def choose_arm(self):\n        return solutions.etc_choose_arm(self)\n\n\n\n\nCode\nagent = ExploreThenCommit(mab.K, mab.T, mab.T // 15)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.4: Reward and cumulative regret of the explore-then-commit algorithm\n\n\n\n\n\nThis algorithm finds the true optimal arm more frequently than the pure greedy strategy. We would expect ETC to then have a lower expected regret. We will show that ETC achieves the following regret bound:\n\nTheorem 4.2 (ETC high-probability regret bound) The regret of ETC satisfies, with high probability,\n\\[\n\\text{Regret}_T= \\widetilde{O}(T^{2/3} K^{1/3}),\n\\tag{4.14}\\]\nwhere the tilde means we ignore logarithmic factors.\n\n\nProof. Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up into the exploration and exploitation phases.\nExploration phase. This phase takes \\(N_\\text{explore} K\\) pulls. Since at each step we incur at most \\(1\\) regret, the total regret is at most \\(N_\\text{explore} K\\).\nExploitation phase. This will take a bit more effort. We’ll prove that for any total time \\(T\\), we can choose \\(N_\\text{explore}\\) such that with arbitrarily high probability, the regret is sublinear.\nLet \\(\\widehat{k}\\) denote the arm chosen after the exploration phase. We know the regret from the exploitation phase is\n\\[\nT_\\text{exploit} (\\mu^\\star - \\mu^{\\widehat{k}}) \\qquad \\text{where} \\qquad T_\\text{exploit} := T- N_\\text{explore} K.\n\\tag{4.15}\\]\nSo we’d need \\(\\mu^\\star - \\mu^{\\widehat{k}} \\to 0\\) as \\(T\\to \\infty\\) in order to achieve sublinear regret. How can we do this?\nLet’s define \\(\\Delta^k:= \\hat \\mu^k- \\mu^k\\) to denote how far the mean estimate for arm \\(k\\) is from the true mean. How can we bound this quantity? We’ll use the following useful inequality for i.i.d. bounded random variables:\n\nTheorem 4.3 (Hoeffding’s inequality) Let \\(X_0, \\dots, X_{N-1}\\) be i.i.d. random variables with \\(X_n \\in [0, 1]\\) almost surely for each \\(n \\in [N]\\). Then for any \\(\\delta &gt; 0\\),\n\\[\n\\mathbb{P}\\left(\n    \\left| \\frac{1}{N} \\sum_{n=1}^N (X_n - \\mathop{\\mathbb{E}}[X_n]) \\right| &gt; \\sqrt{\\frac{\\ln(2/\\delta)}{2N}}\n\\right) \\le \\delta.\n\\tag{4.16}\\]\n\n\nProof. The proof of this inequality is beyond the scope of this book. See Vershynin (2018, ch. 2.2).\n\nWe can apply this directly to the rewards for a given arm \\(k\\), since the rewards from that arm are i.i.d.:\n\\[\n\\mathbb{P}\\left(|\\Delta^k| &gt; \\sqrt{\n    \\frac{\\ln(2/\\delta)}{2N_\\text{explore}}\n} \\right) \\le \\delta.\n\\tag{4.17}\\]\nThis can be thought of as a \\(1-\\delta\\) confidence interval for the true arm mean \\(\\mu^k\\). But we can’t apply this to arm \\(\\widehat{k}\\) directly since \\(\\widehat{k}\\) is itself a random variable. Instead, we need to bound the error across all the arms simultaneously, so that the resulting bound will apply no matter what \\(\\widehat{k}\\) “crystallizes” to. The union bound provides a simple way to do this. (See Theorem A.1 for a review.)\nIn our case, we have one event per arm, stating that the CI for that arm contains the true mean. The probability that all of the CIs are accurate is then\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left(\n        \\forall k\\in [K] : |\\Delta^k|\n        \\le\n        \\sqrt{\\frac{\\ln(2/\\delta)}{2N_\\text{explore}}}\n    \\right) &\\ge 1- K\\delta.\n\\end{aligned}\n\\tag{4.18}\\]\nThis lower-bounds the probability that the CI for arm \\(\\widehat{k}\\) is accurate. Let us return to our original task of bounding \\(\\mu^\\star - \\mu^{\\widehat{k}}\\). We apply the useful trick of “adding zero”:\n\\[\n\\begin{aligned}\n    \\mu^{k^\\star} - \\mu^{\\widehat{k}} &= \\mu^{k^\\star} - \\mu^{\\widehat{k}} + (\\hat \\mu^{k^\\star} - \\hat \\mu^{k^\\star}) + (\\hat \\mu^{\\widehat{k}} - \\hat \\mu^{\\widehat{k}}) \\\\\n    &= \\Delta^{\\widehat{k}} - \\Delta^{k^\\star} + \\underbrace{(\\hat \\mu^{k^\\star} - \\hat \\mu^{\\widehat{k}})}_{\\le 0 \\text{ by definition of } \\widehat{k}} \\\\\n    &\\le 2 \\sqrt{\\frac{\\ln(2 K/ \\delta')}{2N_\\text{explore}}} \\text{ with probability at least } 1-\\delta'\n\\end{aligned}\n\\tag{4.19}\\]\nwhere we’ve set \\(\\delta' := K\\delta\\). Putting this all together, we’ve shown that, with probability \\(1 - \\delta'\\),\n\\[\n\\text{Regret}_T\\le N_\\text{explore} K+ T_\\text{exploit} \\cdot \\sqrt{\\frac{2\\ln(2 K/\\delta')}{N_\\text{explore}}}.\n\\tag{4.20}\\]\nNote that it suffices for \\(N_\\text{explore}\\) to be on the order of \\(\\sqrt{T}\\) to achieve sublinear regret. In particular, we can find the optimal \\(N_\\text{explore}\\) by setting the derivative of eq. 4.20 with respect to \\(N_\\text{explore}\\) to zero:\n\\[\n\\begin{aligned}\n    0 &= K- T_\\text{exploit} \\cdot \\frac{1}{2} \\sqrt{\\frac{2\\ln(2 K/\\delta')}{N_\\text{explore}^3}} \\\\\n    N_\\text{explore} &= \\left(\n        T_\\text{exploit} \\cdot \\frac{\\sqrt{\\ln(2 K/ \\delta')/2}}{ K}\n    \\right)^{2/3}\n\\end{aligned}\n\\tag{4.21}\\]\nPlugging this into the expression for the regret, we have (still with probability \\(1-\\delta'\\)):\n\\[\n\\begin{aligned}\n    \\text{Regret}_T&\\le 3 T^{2/3} \\sqrt[3]{K\\ln(2 K/\\delta') / 2} \\\\\n    &= \\widetilde{O}(T^{2/3} K^{1/3}),\n\\end{aligned}\n\\tag{4.22}\\]\nsatisfying Theorem 4.2.\n\nThe ETC algorithm is rather “abrupt” in that it switches from exploration to exploitation after a fixed number of pulls. What if the total number of pulls \\(T\\) isn’t known in advance? We’ll need to use a more gradual transition, which brings us to the epsilon-greedy algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-epsilon-greedy",
    "href": "bandits.html#sec-epsilon-greedy",
    "title": "4  Multi-Armed Bandits",
    "section": "4.6 Epsilon-greedy",
    "text": "4.6 Epsilon-greedy\nInstead of doing all of the exploration and then all of the exploitation in separate phases, which requires knowing the time horizon beforehand, we can instead interleave exploration and exploitation by, at each pull, choosing a random action with some probability. We call this the epsilon-greedy algorithm.\n\nDefinition 4.7 (Epsilon-greedy) Let \\(N^k_t\\) denote the number of times that arm \\(k\\) is pulled within the first \\(t\\) overall pulls, and let \\(\\hat \\mu^k_t\\) denote the sample mean of the corresponding rewards:\n\\[\n\\begin{aligned}\n    N^k_t&:= \\sum_{t' = 0}^{t-1} \\mathbf{1} \\{ a_{t'} = k\\}, \\\\\n    \\hat \\mu^k_t&:= \\frac{1}{N^k_t} \\sum_{t' = 0}^{t-1} \\mathbf{1} \\{ a_{t'} = k\\} r_{t'}. \\\\\n\\end{aligned}\n\\tag{4.23}\\]\nOn the \\(t\\)th pull, the arm \\(a_t\\) is chosen according to\n\\[\na_t:= \\begin{cases}\n\\text{sample from } \\text{Unif}([K]) & \\text{with probability } \\epsilon_t\\\\\n\\arg\\max_{k\\in [K]} \\hat \\mu^k_t& \\text{with probability } 1 - \\epsilon_t,\n\\end{cases}\n\\tag{4.24}\\]\nwhere \\(\\epsilon_t\\in [0, 1]\\) is the probability of choosing a random action.\n\n\n\nCode\nclass EpsilonGreedy(Agent):\n    def __init__(\n        self,\n        K: int,\n        T: int,\n        ε_array: Float[Array, \" T\"],\n    ):\n        super().__init__(K, T)\n        self.ε_array = ε_array\n\n    def choose_arm(self):\n        return solutions.epsilon_greedy_choose_arm(self)\n\n\n\n\nCode\nagent = EpsilonGreedy(mab.K, mab.T, np.full(mab.T, 0.1))\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.5: Reward and cumulative regret of the epsilon-greedy algorithm\n\n\n\n\n\nWe let the exploration probability \\(\\epsilon_t\\) vary over time. This lets us gradually decrease \\(\\epsilon_t\\) as we learn more about the reward distributions and no longer need to explore as much.\n\nExercise 4.2 (Regret for constant epsilon) What is the asymptotic expected regret of the algorithm if we set \\(\\epsilon\\) to be a constant? Is this different from pure exploration (def. 4.4)?\n\n\nTheorem 4.4 (Epsilon-greedy expected regret) Let \\(\\epsilon_t:= \\sqrt[3]{K\\ln(t) / t}\\). Then the epsilon-greedy achieves a regret of \\(\\widetilde O(t^{2/3} K^{1/3})\\) in expectation (ignoring logarithmic factors).\n\n\nProof. We won’t prove this here. See Agarwal et al. (2022) for a proof.\n\nIn ETC, we had to set \\(N_\\text{explore}\\) based on the total number of pulls \\(T\\). But the epsilon-greedy algorithm handles the exploration incrementally: the regret rate holds for any \\(t\\), and doesn’t depend on the total number of pulls \\(T\\).\nBut the way epsilon-greedy and ETC explore is rather naive: they explore uniformly across all the arms. But what if we could be smarter about it, and explore more for arms that we’re less certain about?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-ucb",
    "href": "bandits.html#sec-ucb",
    "title": "4  Multi-Armed Bandits",
    "section": "4.7 Upper Confidence Bound (UCB)",
    "text": "4.7 Upper Confidence Bound (UCB)\nThe upper confidence bound algorithm is the first strategy we study that explores adaptively: it identifies arms it is less certain about, and explicitly trades off between learning more about these and exploiting arms that already seem good.\nTo quantify how certain we are about the mean of each arm, UCB computes statistical confidence intervals (CIs) for the arm means, and then chooses the arm with the highest upper confidence bound (hence its name). Concretely, after \\(t\\) pulls, we compute upper confidence bounds \\(M^k_t\\) for each arm \\(k\\) such that \\(\\mu^k\\le M^k_t\\) with high probability, and select \\(a_t:= \\arg \\max_{k\\in [K]} M^k_t\\). This operates on the principle of the benefit of the doubt (i.e. optimism in the face of uncertainty): we’ll choose the arm that we’re most optimistic about.\n\n\nCode\nmeans = [0.1, 0.8, 0.4]\nlower_bounds = [-0.4745434, 0.2002283, 0.04175852]\nupper_bounds = [0.9745434, 0.9247717, 0.95824148]\n\nwhiskers = np.array([lower_bounds, upper_bounds])\n\nfig, ax = plt.subplots()\nfor i in range(len(means)):\n    ax.plot([i, i], [whiskers[0, i], whiskers[1, i]], color='black')\n    ax.plot([i - 0.1, i + 0.1], [whiskers[0, i], whiskers[0, i]], color='black')\n    ax.plot([i - 0.1, i + 0.1], [whiskers[1, i], whiskers[1, i]], color='blue')\n\nax.scatter(range(len(means)), means, color='red', zorder=5, label=\"$\\mu^k$\")\nax.set_xticks(range(len(means)))\nax.set_xticklabels([f'Arm {i}' for i in range(3)])\nax.set_ylabel(r'Mean reward')\nfig.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.6: Visualization of confidence intervals for the arm means. UCB would choose arm \\(0\\) since it has the highest upper confidence bound.\n\n\n\n\n\n\nTheorem 4.5 (Confidence intervals for arm means) As in our definition of the epsilon-greedy algorithm (def. 4.7), let \\(N^k_t\\) denote the number of times we pull arm \\(k\\) within the first \\(t\\) pulls, and let \\(\\hat \\mu^k_t\\) denote the sample mean of the corresponding rewards. Then with probability at least \\(1 - \\delta\\), for all \\(t\\ge K\\),\n\\[\n\\mu^k\\in \\left[\n    \\hat \\mu^k_t- \\frac{1}{\\sqrt{N^k_t}} \\cdot \\sqrt{\n    \\frac{\\ln(2 t/\\delta)}{2}\n    },\n    \\hat \\mu^k_t+ \\frac{1}{\\sqrt{N^k_t}} \\cdot \\sqrt{\n    \\frac{\\ln(2 t/\\delta)}{2}\n    }\n\\right].\n\\tag{4.25}\\]\nWe assume that during the first \\(K\\) pulls, we pull each arm once.\n\n\nDefinition 4.8 (Upper confidence bound algorithm (Auer, 2002)) The UCB algorithm first pulls each arm once. Then it chooses the \\(t\\)th pull (for \\(t\\ge K\\)) by taking\n\\[\na_t:= \\arg\\max_{k\\in [K]} \\hat \\mu^k_t+ \\sqrt{\n    \\frac{\\ln(2 t/\\delta)}{2 N^k_t}\n},\n\\tag{4.26}\\]\nwhere \\(\\delta \\in (0, 1)\\) is a parameter that controls the width of the confidence interval.\n\nIntuitively, UCB prioritizes arms where:\n\n\\(\\hat \\mu^k_t\\) is large, i.e. the arm’s corresponding sample mean is high, and we’d choose it for exploitation, and\n\\(\\sqrt{\\frac{\\ln(2 t/\\delta)}{2N^k_t}}\\) is large, i.e. \\(N^k_t\\) is small and we’re still uncertain about the arm, and we’d choose it for exploration.\n\n\\(\\delta\\) is formally the coverage probability of the confidence interval, but we can also treat it as a parameter that trades off between exploration and exploitation:\n\nA smaller \\(\\delta\\) would give us a larger interval, emphasizing the exploration term.\nA larger \\(\\delta\\) would give a tighter interval, prioritizing the current sample means.\n\nWe now prove Theorem 4.5.\n\nProof. Our proof is similar to that in Theorem 4.2: we use Hoeffding’s inequality to construct a CI for the mean of a bounded random variable (i.e. the rewards from a given arm). However, we must be careful, since the number of samples from arm \\(k\\) up to time \\(t\\), \\(N^k_t\\), is a random variable.\nHoeffding’s inequality (Theorem 4.3) tells us that, for \\(N\\) i.i.d. samples \\(\\widetilde r_0, \\dots, \\widetilde r_{N-1}\\) from arm \\(k\\),\n\\[\n| \\widetilde \\mu^k_N - \\mu^k| \\le \\sqrt{\n    \\frac{\\ln( 2 / \\delta )}{2 N}\n}\n\\tag{4.27}\\]\nwith probability at least \\(1 - \\delta\\), where \\(\\widetilde \\mu^k_N = \\frac{1}{N} \\sum_{n=0}^{N-1} \\widetilde r_n\\). The union bound then tells us that with probability at least \\(1 - \\delta'\\), for all \\(N = 1, \\dots, t\\),\n\\[\n\\forall N = 1, \\dots, t:\n| \\widetilde \\mu^k_N - \\mu^k| \\le \\sqrt{\n    \\frac{\\ln( 2 t/ \\delta' )}{2 N}\n}.\n\\tag{4.28}\\]\nSince \\(N^k_t\\in \\{ 1, \\dots, t\\}\\) (we assume each arm is pulled once at the start), eq. 4.28 implies that\n\\[\n| \\widetilde \\mu^k_{N^k_t} - \\mu^k| \\le \\sqrt{\n    \\frac{\\ln( 2 t/ \\delta' )}{N}\n}\n\\tag{4.29}\\]\nwith probability at least \\(1 - \\delta'\\) as well. But notice that \\(\\widetilde \\mu^k_{N^k_t}\\) is identically distributed to \\(\\hat \\mu^k_t\\): both refer to the sample mean of the first \\(N^k_t\\) pulls from arm \\(k\\). This gives the bound in Theorem 4.5 (after relabelling \\(\\delta' \\mapsto \\delta\\)).\n\n\n\nCode\nclass UCB(Agent):\n    def __init__(self, K: int, T: int, delta: float):\n        super().__init__(K, T)\n        self.delta = delta\n\n    def choose_arm(self):\n        return solutions.ucb_choose_arm(self)\n\n\n\n\nCode\nagent = UCB(mab.K, mab.T, 0.9)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.7: Reward and cumulative regret of the upper confidence bound algorithm\n\n\n\n\n\nAs desired, this explores in a smarter, adaptive way compared to the previous algorithms. Does it achieve lower regret?\n\nTheorem 4.6 (UCB high-probability regret bound) With probability \\(1-\\delta''\\),\n\\[\n\\text{Regret}_T= \\widetilde O(K\\sqrt{T})\n\\tag{4.30}\\]\n\n\nProof. First we’ll bound the regret incurred at each timestep. Then we’ll bound the total regret across timesteps.\nFor the sake of analysis, we’ll use a slightly looser bound that applies across the whole time horizon and across all arms. We’ll omit the derivation since it’s very similar to the above (walk through it yourself for practice).\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left(\\forall k\\le K, t&lt; T: |\\hat \\mu^k_t- \\mu^k| \\le B^k_t\\right) &\\ge 1-\\delta'' \\\\\n    \\text{where} \\quad B^k_t&:= \\sqrt{\\frac{\\ln(2 TK/\\delta'')}{2 N^k_t}}.\n\\end{aligned}\n\\tag{4.31}\\]\nIntuitively, \\(B^k_t\\) denotes the width of the CI for arm \\(k\\) at time \\(t\\). Then, assuming the above uniform bound holds (which occurs with probability \\(1-\\delta''\\)), we can bound the regret at each timestep as follows:\n\\[\n\\begin{aligned}\n    \\mu^\\star - \\mu^{a_t} &\\le \\hat \\mu^{k^\\star}_t+ B_t^{k^\\star} - \\mu^{a_t} && \\text{applying UCB to arm } k^\\star \\\\\n    &\\le \\hat \\mu^{a_t}_t+ B^{a_t}_t- \\mu^{a_t} && \\text{since UCB chooses } a_t= \\arg \\max_{k\\in [K]} \\hat \\mu^k_t+ B_t^{k} \\\\\n    &\\le 2 B^{a_t}_t&& \\text{since } \\hat \\mu^{a_t}_t- \\mu^{a_t} \\le B^{a_t}_t\\text{ by definition of } B^{a_t}_t\\\\\n\\end{aligned}\n\\tag{4.32}\\]\nSumming this across timesteps gives\n\\[\n\\begin{aligned}\n    \\text{Regret}_T&\\le \\sum_{t= 0}^{T-1} 2 B^{a_t}_t\\\\\n    &= \\sqrt{2\\ln(2 TK/\\delta'')} \\sum_{t= 0}^{T-1} (N^{a_t}_t)^{-1/2} \\\\\n    \\sum_{t= 0}^{T-1} (N^{a_t}_t)^{-1/2} &= \\sum_{t= 0}^{T-1} \\sum_{k=0}^{K-1} \\mathbf{1}\\{ a_t= k \\} (N^k_t)^{-1/2} \\\\\n    &= \\sum_{k=0}^{K-1} \\sum_{n=1}^{N_T^k} n^{-1/2} \\\\\n    &\\le K \\sum_{n=1}^Tn^{-1/2} \\\\\n    \\sum_{n=1}^Tn^{-1/2} &\\le 1 + \\int_1^Tx^{-1/2} \\ \\mathrm{d}x \\\\\n    &= 1 + (2 \\sqrt{x})_1^T\\\\\n    &= 2 \\sqrt{T} - 1 \\\\\n    &\\le 2 \\sqrt{T} \\\\\n\\end{aligned}\n\\]\nPutting everything together gives, with probability \\(1-\\delta''\\),\n\\[\n\\begin{aligned}\n    \\text{Regret}_T&\\le 2 K\\sqrt{2 T\\ln(2 TK/\\delta'')} \\\\\n    &= \\widetilde O(K\\sqrt{T}),\n\\end{aligned}\n\\tag{4.33}\\]\nas in Theorem 4.6.\n\nIn fact, we can do a more sophisticated analysis to trim off a factor of \\(\\sqrt{K}\\) and show \\(\\text{Regret}_T= \\widetilde O(\\sqrt{TK})\\).\n\n4.7.1 Lower bound on regret (intuition)\nIs it possible to do better than \\(\\Omega(\\sqrt{T})\\) in general? In fact, no! We can show that any algorithm must incur \\(\\Omega(\\sqrt{T})\\) regret in the worst case. We won’t rigorously prove this here, but the intuition is as follows.\nThe Central Limit Theorem tells us that with \\(T\\) i.i.d. samples from some distribution, we can only learn the mean of the distribution to within \\(\\Omega(1/\\sqrt{T})\\) (the standard deviation). Then, since we get \\(T\\) samples spread out across the arms, we can only learn each arm’s mean to an even looser degree.\nThat is, if two arms have means that are within about \\(1/\\sqrt{T}\\), we won’t be able to confidently tell them apart, and will sample them about equally. But then we’ll incur regret \\[\n\\Omega((T/2) \\cdot (1/\\sqrt{T})) = \\Omega(\\sqrt{T}).\n\\tag{4.34}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-thompson-sampling",
    "href": "bandits.html#sec-thompson-sampling",
    "title": "4  Multi-Armed Bandits",
    "section": "4.8 Thompson sampling and Bayesian bandits",
    "text": "4.8 Thompson sampling and Bayesian bandits\nSo far, we’ve treated the parameters \\(\\mu^0, \\dots, \\mu^{K-1}\\) of the reward distributions as fixed. Instead, we can take a Bayesian approach where we treat them as random variables from some prior distribution. Then, upon pulling an arm and observing a reward, we can simply condition on this observation to exactly describe the posterior distribution over the parameters. This fully describes the information we gain about the parameters from observing the reward.\nFrom this Bayesian perspective, the Thompson sampling algorithm follows naturally: just sample from the distribution of the optimal arm, given the observations!\n\n\nCode\nclass Distribution:\n    def sample(self) -&gt; Float[Array, \" K\"]:\n        \"\"\"Sample a vector of means for the K arms.\"\"\"\n        ...\n\n    def update(self, arm: int, reward: float):\n        \"\"\"Condition on obtaining `reward` from the given arm.\"\"\"\n        ...\n\n\n\n\nCode\nclass ThompsonSampling(Agent):\n    def __init__(self, K: int, T: int, prior: Distribution):\n        super().__init__(K, T)\n        self.distribution = prior\n\n    def choose_arm(self):\n        means = self.distribution.sample()\n        return random_argmax(means)\n\n    def update_history(self, arm: int, reward: int):\n        super().update_history(arm, reward)\n        self.distribution.update(arm, reward)\n\n\nIn other words, we sample each arm proportionally to how likely we think it is to be optimal, given the observations so far. This strikes a good exploration-exploitation tradeoff: we explore more for arms that we’re less certain about, and exploit more for arms that we’re more certain about. Thompson sampling is a simple yet powerful algorithm that achieves state-of-the-art performance in many settings.\n\nExample 4.4 (Bayesian Bernoulli bandit) We’ve been working in the Bernoulli bandit setting, where arm \\(k\\) yields a reward of \\(1\\) with probability \\(\\mu^k\\) and no reward otherwise. The vector of success probabilities \\(\\boldsymbol{\\mu} = (\\mu^0, \\dots, \\mu^{K-1})\\) thus describes the entire MAB.\nUnder the Bayesian perspective, we think of \\(\\boldsymbol{\\mu}\\) as a random vector drawn from some prior distribution \\(p \\in \\triangle([0, 1]^K)\\). For example, we might have \\(p\\) be the Uniform distribution over the unit hypercube \\([0, 1]^K\\), that is,\n\\[\np(\\boldsymbol{\\mu}) = \\begin{cases}\n    1 & \\text{if } \\boldsymbol{\\mu}\\in [0, 1]^K\\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\tag{4.35}\\]\nIn this case, upon viewing some reward, we can exactly calculate the posterior distribution of \\(\\boldsymbol{\\mu}\\) using Bayes’s rule (i.e. the definition of conditional probability):\n\\[\n\\begin{aligned}\n    \\mathbb{P}(\\boldsymbol{\\mu} \\mid a_0, r_0) &\\propto \\mathbb{P}(r_0 \\mid a_0, \\boldsymbol{\\mu}) \\mathbb{P}(a_0 \\mid \\boldsymbol{\\mu}) \\mathbb{P}(\\boldsymbol{\\mu}) \\\\\n    &\\propto (\\mu^{a_0})^{r_0} (1 - \\mu^{a_0})^{1-r_0}.\n\\end{aligned}\n\\]\nThis is the PDF of the \\(\\text{Beta}(1 + r_0, 1 + (1 - r_0))\\) distribution, which is a conjugate prior for the Bernoulli distribution. That is, if we start with a Beta prior on \\(\\mu^k\\) (note that \\(\\text{Unif}([0, 1]) = \\text{Beta}(1, 1)\\)), then the posterior, after conditioning on samples from \\(\\text{Bern}(\\mu^k)\\), will also be Beta. This is a very convenient property, since it means we can simply update the parameters of the Beta distribution upon observing a reward, rather than having to recompute the entire posterior distribution from scratch.\n\n\n\nCode\nclass Beta(Distribution):\n    def __init__(self, K: int, alpha: int = 1, beta: int = 1):\n        self.alphas = np.full(K, alpha)\n        self.betas = np.full(K, beta)\n\n    def sample(self):\n        return np.random.beta(self.alphas, self.betas)\n\n    def update(self, arm: int, reward: int):\n        self.alphas[arm] += reward\n        self.betas[arm] += 1 - reward\n\n\n\n\nCode\nbeta_distribution = Beta(mab.K)\nagent = ThompsonSampling(mab.K, mab.T, beta_distribution)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\nFigure 4.8: Reward and cumulative regret of the Thompson sampling algorithm\n\n\n\n\n\nIt turns out that asymptotically, Thompson sampling is optimal in the following sense. Lai & Robbins (1985) prove an instance-dependent lower bound that says for any bandit algorithm,\n\\[\n\\liminf_{T\\to \\infty} \\frac{\\mathop{\\mathbb{E}}[N_T^k]}{\\ln(T)} \\ge \\frac{1}{\\mathrm{KL}\\left(\\mu^k\\parallel\\mu^\\star\\right)}\n\\tag{4.36}\\]\nwhere\n\\[\n\\mathrm{KL}\\left(\\mu^k\\parallel\\mu^\\star\\right) := \\mu^k\\ln \\frac{\\mu^k}{\\mu^\\star} + (1 - \\mu^k) \\ln \\frac{1 - \\mu^k}{1 - \\mu^\\star}\n\\tag{4.37}\\]\nmeasures the Kullback-Leibler divergence from the Bernoulli distribution with mean \\(\\mu^k\\) to the Bernoulli distribution with mean \\(\\mu^\\star\\). It turns out that Thompson sampling achieves this lower bound with equality! That is, not only is the error rate optimal, but the constant factor is optimal as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-contextual-bandits",
    "href": "bandits.html#sec-contextual-bandits",
    "title": "4  Multi-Armed Bandits",
    "section": "4.9 Contextual bandits",
    "text": "4.9 Contextual bandits\nIn the multi-armed bandits setting described above, each pull is exactly the same. We don’t obtain any prior information about the reward distributions. However, consider the online advertising example (ex. 4.1), where each arm corresponds to an ad we could show a given user, and we receive a reward if and only if the user clicks on the ad. Suppose Alice is interested in sports and politics, while Bob is interested in news and entertainment. The ads that Alice is likely to click differ from those that Bob is likely to click. That is, the reward distributions of the arms depend on some prior context. We can model such environments using contextual bandits.\n\nDefinition 4.9 (Contextual bandit) At each pull \\(t\\), a context \\(x_t\\) is chosen from the set \\(\\mathcal{X}\\) of all possible contexts. The learner gets to observe the context before they choose an action. That is, we treat the action chosen on the \\(t\\)th pull as a function of the context: \\(a_t(x_t)\\). Then, the learner observes the reward from the chosen arm, where the reward distribution also depends on the context.\n\n\nExample 4.5 (Online advertising as a contextual bandit problem) Suppose you represent an online advertising company and you are tasked with placing ads on the New York Times website. Articles are categorized into six sections, each with its own web page: U.S., World, Business, Arts, Lifestyle, and Opinion. You might decide to frame this as a contextual bandits problem with \\(|\\mathcal{X}| = 6\\). This is because the demographic of readers likely differs across the different pages, and so the reward distributions differ across pages accordingly.\n\nSuppose our context is discrete, as in ex. 4.5. Then we could treat each context as its own MAB, assigning each context-arm pair to a distinct arm in an enlarged MAB of \\(K|\\mathcal{X}|\\) arms.\n\nExercise 4.3 (Naive UCB for contextual bandits) Write down the UCB algorithm for this enlarged MAB. That is, write an expression \\(M^k_t\\) in terms of \\(K\\) and \\(|\\mathcal{X}|\\) such that \\(a_t(x_t) = \\arg\\max_{k\\in [K]} M^k_t\\).\n\nRecall that executing UCB for \\(T\\) timesteps on an MAB with \\(K\\) arms achieves a regret bound of \\(\\widetilde{O}(\\sqrt{TK})\\) (Theorem 4.6). So in this problem, we would achieve regret \\(\\widetilde{O}(\\sqrt{TK|\\mathcal{X}|})\\) in the contextual MAB, which has a polynomial dependence on \\(|\\mathcal{X}|\\). But in a situation where we have too many possible contexts, or the context is a continuous value, this algorithm no longer works.\nNote that this “enlarged MAB” treats the different contexts as entirely unrelated to each other, while in practice, often contexts are related to each other in some way. In the medical trial example (ex. 4.2), clients with similar health situations may benefit from the same medications. How can we incorporate this structure into our solution?\n\n4.9.1 Linear contextual bandits\n\nRemark 4.5 (Supervised learning). The material in this section depends on basic knowledge of supervised learning (Chapter 5).\n\nSuppose now that we observe a vector-valued context, that is, \\(\\mathcal{X} = \\mathbb{R}^D\\). For example, in a medical trial (ex. 4.2), we might know the patient’s height, age, blood pressure, etc., each of which would be a separate element of the vector. The approach of treating each context as its own MAB problem no longer works out-of-the-box since the number of contexts is infinite. Instead, we assume that the mean reward of each arm \\(k\\) is a function of the context: \\(\\mu^k: \\mathcal{X} \\to \\mathbb{R}\\).\nRather than the Bernoulli bandit problem (def. 4.1), we’ll assume that the mean reward is linear in the context. (This model is easier to work with for our purposes.) That is, a reward \\(r^k\\) from arm \\(k\\) is drawn according to\n\\[\nr^k= x^\\top \\theta^k+ \\varepsilon,\n\\tag{4.38}\\]\nwhere \\(\\theta^k\\in \\mathbb{R}^D\\) describes a feature direction for arm \\(k\\) and \\(\\varepsilon\\) is some independent noise with mean \\(0\\) and variance \\(\\sigma^2\\). This clearly has\n\\[\n\\mu^k(x) = x^\\top \\theta^k.\n\\tag{4.39}\\]\nThen, on the \\(t\\)th pull, given context \\(x_t\\), we can model the mean reward from arm \\(k\\) as\n\\[\n\\hat \\mu^k_t(x_t) := x_t^\\top \\hat \\theta^k_t,\n\\tag{4.40}\\]\nwhere \\(\\hat \\theta^k_t\\) is some estimator of \\(\\theta^k\\) computed from the first \\(t\\) pulls.\n\nRemark 4.6 (Assumptions for linear models). Why did we switch from Bernoulli reward distributions to the additive model in eq. 4.38? The estimator eq. 4.40 is a bit odd for the Bernoulli bandit problem: since \\(\\mu^k\\) is the success probability of arm \\(k\\), it would have to lie between \\(0\\) and \\(1\\), but \\(\\hat \\mu^k_t(x_t)\\) extends outside this interval. The question of how to model the unknown parameters of a distribution is a supervised learning problem (Chapter 5). For instance, in Bernoulli bandit, we could instead use a logistic regression model for each arm’s success probability. We won’t dive into more advanced supervised learning topics here.\n\nOur goal is still to develop an algorithm that chooses an arm \\(a_t(x_t)\\) to pull based on the experience gained during the first \\(t\\) trials. We will use supervised learning, in particular, empirical risk minimization (Section 5.3), to estimate \\(\\theta^k\\) for each arm. That is, we compute \\(\\hat \\theta^k_t\\) as the vector that minimizes squared error from the observed rewards:\n\\[\n\\hat \\theta_t^k:= \\arg\\min_{\\theta \\in \\mathbb{R}^D}\n    \\sum_{t' \\in \\mathcal{I}^k_t}\n    (r_{t'} - x_{t'}^\\top \\theta)^2,\n\\tag{4.41}\\]\nwhere\n\\[\n\\mathcal{I}^k_t:= \\{ t' \\in [t] \\mid a_{t'} = k\\}\n\\tag{4.42}\\]\ndenotes the subset of \\([t]\\) at which arm \\(k\\) was pulled. Note that \\(|\\mathcal{I}^k_t| = N^k_t\\) as defined in eq. 4.23. The ERM problem in eq. 4.41 has the closed-form solution known as the ordinary least squares (OLS) estimator:\n\\[\n\\begin{aligned}\n    \\hat \\theta_t^k&= (\\widehat \\Sigma_t^k)^{-1}\\left(\n        \\frac{1}{N^k_t} \\sum_{t' \\in \\mathcal{I}^k_t} x_{t'} r_{t'}\n    \\right) \\\\\n    \\text{where} \\quad \\widehat \\Sigma_t^k&= \\frac{1}{N^k_t} \\sum_{t' \\in \\mathcal{I}^k_t} x_{t'} x_{t'}^\\top.\n\\end{aligned}\n\\tag{4.43}\\]\n\\(\\widehat \\Sigma_t^k\\) is the (biased) empirical covariance matrix of the contexts across the trials where arm \\(k\\) was pulled (out of the first \\(t\\) trials).\n\nRemark 4.7 (Invertibility). In eq. 4.43, we write the expression \\((\\widehat \\Sigma_t^k)^{-1}\\), assuming that \\(\\widehat \\Sigma_t^k\\) is invertible. This only holds if the context features are linearly independent, that is, there does not exist some coordinate \\(d \\in [D]\\) that can be written as a linear combination of the other coordinates. One way to ensure that \\(\\widehat \\Sigma_t^k\\) is invertible is to add a \\(\\lambda I\\) regularization term to it. This is equivalent to solving a ridge regression problem instead of the unregularized least squares problem.\n\nNow, given a sequence of \\(t\\) contexts, pulls, and rewards \\(x_0, a_0, r_0, \\dots, x_{t-1}, a_{t-1}, r_{t-1}\\), we can estimate \\(\\mu^k(x_t)\\) for each arm. This is helpful, but we haven’t yet solved the problem of deciding which arms to pull in each context to trade off between exploration and exploitation.\nThe upper confidence bound (UCB) algorithm (def. 4.8) was a useful strategy. What would we need to adapt UCB to this new setting? Recall that at each step, UCB estimates a confidence interval for each arm mean, and chooses the arm with the highest upper confidence bound. We already have an estimator \\(\\hat \\mu^k_t(x_t)\\) of each arm mean, so all that is left is to compute the width of the confidence interval itself.\n\nTheorem 4.7 (LinUCB confidence interval) Suppose we pull each arm once during the first \\(K\\) trials. Then at each pull \\(t\\ge K\\), for all arms \\(k\\in [K]\\) and all contexts \\(x \\in \\mathcal{X}\\), we have that with probability at least \\(1 - 1/\\beta^2\\),\n\\[\n\\mu^k(x) \\in \\left[\n    x^\\top \\hat \\theta^k_t\n    - \\beta \\frac{\\sigma}{\\sqrt{N^k_t}} \\cdot \\sqrt{x^\\top (\\widehat \\Sigma^k_t)^{-1} x},\n    x^\\top \\hat \\theta^k_t\n    + \\beta \\frac{\\sigma}{\\sqrt{N^k_t}} \\cdot \\sqrt{x^\\top (\\widehat \\Sigma^k_t)^{-1} x}\n\\right],\n\\tag{4.44}\\]\nwhere\n\n\\(\\sigma\\) is the standard deviation of the reward (eq. 4.38),\n\\(N^k_t\\) is the number of times arm \\(k\\) was pulled in the first \\(t\\) pulls (eq. 4.23),\n\\(\\hat \\theta^k_t\\) is the OLS estimator for arm \\(k\\) (eq. 4.43),\nand \\(\\widehat \\Sigma^k_t\\) is the sample covariance matrix of the contexts given arm \\(k\\) (eq. 4.43).\n\n\nWe can then plug this interval into the UCB strategy (def. 4.8) to obtain what is known as the LinUCB algorithm:\n\nDefinition 4.10 (LinUCB (Li et al., 2010)) LinUCB first pulls each arm once. Then, for pull \\(t\\ge K\\), LinUCB chooses an arm according to\n\\[\na_t(x_t) := \\arg\\max_{k\\in [K]}\n    x^\\top \\hat \\theta^k_t\n    +\n    \\beta \\frac{\\sigma}{\\sqrt{N^k_t}} \\sqrt{x_t^\\top (\\widehat \\Sigma^k_t)^{-1} x_t},\n\\tag{4.45}\\]\nwhere \\(\\sigma, N^k_t, \\hat \\theta^k_t, \\widehat \\Sigma^k_t\\) are defined as in Theorem 4.7, and \\(\\beta &gt; 0\\) controls the width of the confidence interval (and thereby the exploration-exploitation tradeoff).\n\nThe first term is exactly our predicted mean reward \\(\\hat \\mu^k_t(x_t)\\). We can think of it as the term encouraging exploitation of an arm if its predicted mean reward is high. The second term is the width of the confidence interval given by Theorem 4.7. This is the exploration term that encourages an arm when \\(x_t\\) is not aligned with the data seen so far, or if arm \\(k\\) has not been explored much and so \\(N_t^k\\) is small.\nNow we prove Theorem 4.7.\n\nProof. How can we construct the upper confidence bound in Theorem 4.7? Previously, we treated the pulls of an arm as i.i.d. samples and used Hoeffding’s inequality to bound the distance of the sample mean, our estimator, from the true mean. However, now our estimator is not a sample mean, but rather the OLS estimator above eq. 4.43. We will construct the confidence interval using Chebyshev’s inequality, which gives a confidence interval for the mean of a distribution based on that distribution’s variance.\n\nTheorem 4.8 (Chebyshev’s inequality) For a random variable \\(Y\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\n|Y - \\mu| \\le \\beta \\sigma \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\\tag{4.46}\\]\n\nWe want to construct a CI for \\(x^\\top \\theta^k\\), the mean reward from arm \\(k\\). Our estimator is \\(x^\\top \\hat \\theta^k_t\\). Applying Chebyshev’s inequality requires us to compute the variance of our estimator.\n\nTheorem 4.9 (Variance of OLS) Given the linear reward model in eq. 4.38,\n\\[\n\\text{Var}(x^\\top \\hat \\theta_t^k)\n=\n\\frac{\\sigma^2}{N^k_t} x^\\top (\\widehat \\Sigma^t_k)^{-1} x,\n\\tag{4.47}\\]\nwhere \\(\\widehat \\Sigma^t_k\\) is defined in eq. 4.43.\n\n\nProof. We leave the proof as an exercise. It follows from applications of the law of iterated expectations.\n\nNow we can substitute eq. 4.47 into eq. 4.46 to obtain the desired CI:\n\\[\n\\begin{aligned}\n    x_t^\\top \\theta^k\\le x_t^\\top \\hat \\theta_t^k\n    + \\beta \\sqrt{x_t^\\top (A_t^k)^{-1} x_t} \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\\end{aligned}\n\\tag{4.48}\\]\n\n\n\nCode\nclass LinUCBPseudocode(Agent):\n    def __init__(\n        self, K: int, T: int, D: int, lam: float, get_c: Callable[[int], float]\n    ):\n        super().__init__(K, T)\n        self.lam = lam\n        self.get_c = get_c\n        self.contexts = [None for _ in range(K)]\n        self.A = np.repeat(lam * np.eye(D)[...], K)  # regularization\n        self.targets = np.zeros(K, D)\n        self.w = np.zeros(K, D)\n\n    def choose_arm(self, context: Float[Array, \" D\"]):\n        c = self.get_c(self.count)\n        scores = self.w @ context + c * np.sqrt(\n            context.T @ np.linalg.solve(self.A, context)\n        )\n        return random_argmax(scores)\n\n    def update_history(self, context: Float[Array, \" D\"], arm: int, reward: int):\n        self.A[arm] += np.outer(context, context)\n        self.targets[arm] += context * reward\n        self.w[arm] = np.linalg.solve(self.A[arm], self.targets[arm])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#key-takeaways",
    "href": "bandits.html#key-takeaways",
    "title": "4  Multi-Armed Bandits",
    "section": "4.10 Key takeaways",
    "text": "4.10 Key takeaways\nIn this chapter, we explored the multi-armed bandit setting for analyzing sequential decision-making in an unknown environment. An MAB consists of multiple arms, each with an unknown reward distribution. The agent’s task is to learn about these through interaction, eventually minimizing the regret, which measures how suboptimal the chosen arms were.\nWe saw algorithms such as upper confidence bound and Thompson sampling that handle the tradeoff between exploration and exploitation, that is, the tradeoff between choosing arms that the agent is uncertain about and arms the agent already supposes are be good.\nWe finally discussed contextual bandits, in which the agent gets to observe some context that affects the reward distributions. We can approach these problems through supervised learning approaches.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#bibliographic-notes-and-further-reading",
    "href": "bandits.html#bibliographic-notes-and-further-reading",
    "title": "4  Multi-Armed Bandits",
    "section": "4.11 Bibliographic notes and further reading",
    "text": "4.11 Bibliographic notes and further reading\nThe problem that a bandit algorithm faces after \\(t\\) pulls is to infer differences between the means of a set of distributions given \\(t\\) samples spread out across those distributions. This is a well-studied problem in statistics.\nA common practice in various fields such as psychology is to compare within-group variances to the variance of the group means. This popular class of methods is known as analysis of variance (ANOVA). It has been studied at least since Laplace in the 1770s (Stigler, 2003) and was further popularized by Fisher (1925). William Thompson studied the two-armed mean distinction problem in Thompson (1933) and Thompson (1935). Our focus has been on the decision-making aspect of the bandit problem, rather than the hypothesis testing aspect.\nAdding the sequential decision-making aspect turns the above into the full multi-armed bandit problem. Wald (1949) studies the sequential analysis problem in depth. Robbins (1952) then builds on Wald’s work and provides a clear exposition of the bandit problem in the form presented in this chapter. Berry & Fristedt (1985) is a comprehensive treatment of bandit problems from a statistical perspective.\n\nRemark 4.8 (Disambiguation of LinUCB). The name “LinUCB” is used for various similar algorithms throughout the literature. Here we use it as defined in Li et al. (2010). This algorithm was also considered in Auer (2002), the paper that introduced UCB, under the name “Associative Reinforcement Learning with Linear Value Functions”.\n\n\n\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022). Reinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nAuer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3, 397–422. https://www.jmlr.org/papers/v3/auer02a.html\n\n\nBerry, D. A., & Fristedt, B. (1985). Bandit problems. Springer Netherlands. https://doi.org/10.1007/978-94-015-3711-7\n\n\nFisher, R. A. (1925). Statistical methods for research workers, 11th ed. rev. Edinburgh.\n\n\nLai, T. L., & Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1), 4–22. https://doi.org/10.1016/0196-8858(85)90002-8\n\n\nLi, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. Proceedings of the 19th International Conference on World Wide Web, 661–670. https://doi.org/10.1145/1772690.1772758\n\n\nNegative Space. (2015). Photo of commercial district during dawn [Graphic]. https://www.pexels.com/photo/photo-of-commercial-district-during-dawn-34639/\n\n\nPixabay. (2016a). 20 mg label blister pack [Graphic]. https://www.pexels.com/photo/20-mg-label-blister-pack-208512/\n\n\nPixabay. (2016b). Coins on brown wood [Graphic]. https://www.pexels.com/photo/coins-on-brown-wood-210600/\n\n\nRobbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5), 527–535. https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full\n\n\nStigler, S. M. (2003). The history of statistics: The measurement of uncertainty before 1900 (9. print). Belknap Pr. of Harvard Univ. Pr.\n\n\nThompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286\n\n\nThompson, W. R. (1935). On the theory of apportionment. American Journal of Mathematics, 57(2), 450–456. https://doi.org/10.2307/2371219\n\n\nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science. Cambridge University Press. https://books.google.com?id=NDdqDwAAQBAJ\n\n\nWald, A. (1949). Statistical decision functions. The Annals of Mathematical Statistics, 20(2), 165–205. https://doi.org/10.1214/aoms/1177730030",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html",
    "href": "supervised_learning.html",
    "title": "5  Supervised learning",
    "section": "",
    "text": "5.1 Introduction\nCode\nfrom utils import Float, Array, Callable, plt, np, latex\nfrom torchvision.datasets import MNIST\nfrom config import MNIST_PATH\nSupervised learning (SL) is a core subfield of machine learning alongside RL and unsupervised learning. The typical SL task is to approximate an unknown function given a dataset of input-output examples from that function.\nWhen might function approximation be useful in RL? Recall that the definition of an MDP includes the state transitions \\(P : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S})\\) and a reward function \\(r : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) (def. 2.4). If we know both of these functions, in the sense that we are able to query them on arbitrary inputs, we can use powerful planning algorithms to solve for the optimal policy exactly (see Section 2.3.2). Thus, if either (or both) of these is not known, we can use an SL algorithm to model the environment and then solve the modeled environment using dynamic programming. This approach is called fitted DP and will be covered in Chapter 6.\nWe do not seek to comprehensively discuss supervised learning; see the bibiographic notes at the end of the chapter (Section 5.6) for further resources. We hope to to leave you with an understanding of what types of problems SL can solve and how SL algorithms can be applied to RL.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#sec-sl-intro",
    "href": "supervised_learning.html#sec-sl-intro",
    "title": "5  Supervised learning",
    "section": "",
    "text": "Example 5.1 (Image classification) One of the most common examples of an SL problem is the task of image classification: Given a dataset of images and their respective labels, construct a function that takes an image and outputs the correct label.\nfig. 5.1 illustrates two samples (that is, input-output pairs) from the MNIST database of handwritten digits (Deng, 2012). This is a task that most humans can easily accomplish. By providing many samples of digits and their labels to a machine, SL algorithms can learn to solve this task as well.\n\n\nCode\ndata = MNIST(MNIST_PATH, train=True, download=True)\n\nplt.figure(figsize=(2, 2))\nplt.axis('off')\nplt.imshow(data.data[0], cmap='gray')\n# plt.title(f\"Label: {data.targets[0]}\")\nplt.show()\n\nplt.figure(figsize=(2, 2))\nplt.axis('off')\nplt.imshow(data.data[1], cmap='gray')\n# plt.title(f\"Label: {data.targets[1]}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The handwritten digit \\(5\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) The handwritten digit \\(0\\).\n\n\n\n\n\n\n\nFigure 5.1: The MNIST image classification dataset of \\(28   imes 28\\) handwritten digits.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#sec-sl-task",
    "href": "supervised_learning.html#sec-sl-task",
    "title": "5  Supervised learning",
    "section": "5.2 The supervised learning task",
    "text": "5.2 The supervised learning task\nIn SL, we are given a dataset of labelled samples \\((x_1, y_1), \\dots, (x_N, y_N)\\) that are independently sampled from some data generating process. Mathematically, we describe this data generating process as a joint distribution \\(p \\in \\triangle(\\mathcal{X} \\times \\mathcal{Y})\\), where \\(\\mathcal{X}\\) is the space of possible inputs and \\(\\mathcal{Y}\\) is the space of possible outputs. Note that, by the chain rule of probability, this can be factored as \\(p(x, y) = p(y \\mid x) p(x)\\).\n\nExample 5.2 (Joint distributions for image classification) For example, in ex. 5.1, the marginal distribution over \\(x\\) is assumed to be the distribution of handwritten digits by humans, scanned as \\(28 \\times 28\\) grayscale images. The conditional distribution \\(y \\mid x\\) is assumed to be the distribution over \\(\\{ 0, \\dots, 9 \\}\\) that a human would assign to the image \\(x\\).\n\nOur task is to compute a “good” prediction rule \\(\\hat f : \\mathcal{X} \\to \\mathcal{Y}\\) that takes an input and tries to predict the corresponding output.\n\n5.2.1 Loss functions\nHow can we measure how “good” a prediction rule is? The most common way is to use a loss function \\(\\ell : \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}\\) that compares the guess \\(\\hat y := \\hat f(x)\\) with the true output \\(y\\). \\(\\ell(\\hat y, y)\\) should be low if the prediction rule accurately guessed the output, and high if the prediction was incorrect.\n\nExample 5.3 (Zero-one loss) In the image classification task ex. 5.1, we have \\(X = [0, 1]^{28 \\times 28}\\) (the space of \\(28\\)-by-\\(28\\) grayscale images) and \\(Y = \\{ 0, \\dots, 9 \\}\\) (the image’s label). We could use the zero-one loss function,\n\\[\n\\ell(\\hat y, y) = \\begin{cases}\n0 & \\hat y = y \\\\\n1 & \\hat y \\ne y\n\\end{cases}\n\\tag{5.1}\\]\nto measure the accuracy of the prediction rule. That is, if the prediction rule assigns the wrong label to an image, it incurs a loss of one for that sample.\n\n\nExample 5.4 (Square loss) For a continuous output (i.e. \\(\\mathcal{Y} \\subseteq \\mathbb{R}\\)), we typically use the squared difference as the loss function:\n\\[\n\\ell(\\hat y, y) = (\\hat y - y)^2\n\\tag{5.2}\\]\nThe square loss is nice to work with analytically since its derivative with respect to \\(\\hat y\\) is simply \\(2 (\\hat y - y)\\). (Sometimes authors define the square loss as half of the above value to cancel the factor of \\(2\\) in the derivative. Generally speaking, scaling the loss by some constant scalar has no practical effect.)\n\n\n\nCode\nx = np.linspace(-1, 1, 20)\ny = x ** 2\nplt.plot(x, y)\nplt.xlabel(r\"$\\hat y - y$\")\nplt.ylabel(r\"$\\ell(\\hat y, y)$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5.2: Square loss.\n\n\n\n\n\n\n\n5.2.2 Model selection\nUltimately, we want a prediction rule that does well on new, unseen samples from the data generating process. We can thus ask, how much loss does the prediction rule incur in expectation? This is called the prediction rule’s generalization error or test error.\n\nDefinition 5.1 (Generalization error) Given a loss function \\(\\ell\\) and a prediction rule \\(\\hat f\\), the generalization error of the prediction rule is defined as the expected loss over the data generating process.\n\\[\n\\text{error}_{\\text{g}}(\\hat f) := \\mathop{\\mathbb{E}}_{(x, y) \\sim p} [ \\ell(\\hat f(x), y) ]\n\\tag{5.3}\\]\nSuppose we sample a new input and output from the data generating process, make a guess according to our prediction rule, and use the loss function to compare our guess to the true output. If we repeat this many times, the average loss would approach the generalization error.\n\nThe goal of SL is then to find the prediction rule that minimizes the test error. For certain loss functions, the theoretical optimum can be analytically computed, such as for squared error.\n\nTheorem 5.1 (The conditional expectation minimizes mean squared error) An important result is that, under the square loss, the optimal prediction rule is the conditional expectation:\n\\[\n\\arg\\min_{f} \\mathop{\\mathbb{E}}[(y - f(x))^2] = (x \\mapsto \\mathop{\\mathbb{E}}[y \\mid x])\n\\tag{5.4}\\]\n\n\nProof. We can decompose the mean squared error as\n\\[\n\\begin{aligned}\n\\mathop{\\mathbb{E}}[(y - f(x))^2] &= \\mathop{\\mathbb{E}}[ (y - \\mathop{\\mathbb{E}}[y \\mid x] + \\mathop{\\mathbb{E}}[y \\mid x] - f(x))^2 ] \\\\\n&= \\mathop{\\mathbb{E}}[ (y - \\mathop{\\mathbb{E}}[y \\mid x])^2 ] + \\mathop{\\mathbb{E}}[ (\\mathop{\\mathbb{E}}[y \\mid x] - f(x))^2 ] \\\\\n&\\quad {} + 2 \\mathop{\\mathbb{E}}[ (y - \\mathop{\\mathbb{E}}[y \\mid x])(\\mathop{\\mathbb{E}}[y \\mid x] - f(x)) ] \\\\\n\\end{aligned}\n\\tag{5.5}\\]\nWe leave it as an exercise to show that the last term is zero. (Hint: use the law of iterated expectations.) The first term is the noise, or irreducible error, that doesn’t depend on \\(f\\), and the second term is the error due to the approximation, which is minimized at \\(0\\) when \\(f(x) = \\mathop{\\mathbb{E}}[y \\mid x]\\).\n\nIn most applications, such as in ex. 5.2, we can’t integrate over the joint distribution of \\(x, y\\), and so we can’t evaluate \\(\\mathop{\\mathbb{E}}[y \\mid x]\\) analytically. Instead, all we get are \\(N\\) samples from the joint distribution of \\(x\\) and \\(y\\). How might we use these to approximate the generalization error?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#sec-erm",
    "href": "supervised_learning.html#sec-erm",
    "title": "5  Supervised learning",
    "section": "5.3 Empirical risk minimization",
    "text": "5.3 Empirical risk minimization\nTo estimate the generalization error, we could simply take the sample mean of the loss over the training data. This is called the training loss or empirical risk:\n\nDefinition 5.2 (Empirical risk) Given a dataset \\((x_1, y_1), \\dots, (x_N, y_N)\\) sampled i.i.d. from the data generating process, and a loss function \\(\\ell\\), the empirical risk of the prediction rule \\(\\hat f\\) is the average loss across the dataset:\n\\[\n\\text{training loss}(\\hat f) := \\frac 1 N \\sum_{n=1}^N \\ell(\\hat f(x_n), y_n).\n\\tag{5.6}\\]\n\nBy the law of large numbers, as \\(N\\) grows to infinity, the training loss converges to the generalization error (def. 5.1).\nThe empirical risk minimization (ERM) approach is to find a prediction rule that minimizes the empirical risk.\n\nDefinition 5.3 (Empirical risk minimization) An ERM algorithm requires two ingredients to be chosen based on our domain knowledge about the DGP:\n\nA function class \\(\\mathcal{F}\\), that is, the space of functions to consider.\nA fitting method that uses the dataset to find the element of \\(\\mathcal{F}\\) that minimizes the training loss.\n\nThis allows us to compute the empirical risk minimizer:\n\\[\n\\begin{aligned}\n\\hat f_\\text{ERM} &:= \\arg\\min_{f \\in \\mathcal{F}} \\text{training loss}(f) \\\\\n&= \\arg\\min_{f \\in \\mathcal{F}}\\frac 1 N \\sum_{n=1}^N \\ell(f(x_n), y_n).\n\\end{aligned}\n\\tag{5.7}\\]\n\n\n5.3.1 Function classes\nHow should we choose the correct function class? In fact, why do we need to constrain our search at all?\n\nExercise 5.1 (Overfitting) Suppose we are trying to approximate a relationship between real-valued inputs and outputs using square loss as our loss function. Consider the prediction rule (visualized in fig. 5.3)\n\\[\n\\hat f(x) = \\sum_{n=1}^N y_n \\mathbf{1}\\left\\{x = x_n\\right\\}.\n\\tag{5.8}\\]\nWhat is the empirical risk of this function? How well does it perform on newly generated samples?\n\n\nCode\nn = 1000\nx_axis = np.linspace(-1, +1, n)\n\nfor _ in range(2):\n    x_train = np.random.uniform(-1, +1, 10)\n    y_train = np.sin(np.pi * x_train)\n    y_hat = np.where(np.isclose(x_axis[:, None], x_train, atol=2/n), y_train, 0).sum(axis=-1)\n\n    plt.plot(x_axis, y_hat, label=r'$\\hat f(x)$')\n    plt.scatter(x_train, y_train, color='red', marker='x', label='training data')\n    plt.legend()\n    plt.gcf().set_size_inches(3, 2)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) One training dataset.\n\n\n\n\n\n\n\n\n\n\n\n(b) Another training dataset.\n\n\n\n\n\n\n\nFigure 5.3: A pathological prediction rule.\n\n\n\nThe choice of \\(\\mathcal{F}\\) depends on our domain knowledge about the task. On one hand, \\(\\mathcal{F}\\) should be large enough to contain the true relationship, but on the other, it shouldn’t be too expressive; otherwise, it will overfit to random noise in the labels. The larger and more complex the function class, the more accurately we will be able to approximate any particular training dataset (i.e. smaller bias), but the more drastically the function will vary for different training datasets (i.e. larger variance). For most loss functions, including the square loss, it is possible to express the generalization error (def. 5.1) as a sum of a bias term and a variance term. The mathematical details of this so-called bias-variance tradeoff can be found, for example, in Hastie et al. (2013, Chapter 2.9).\n\nCode\nn_samples = 10\ndegrees = [2, 5, 50]\nx_axis = np.linspace(-1, +1, 50)\n\n\ndef generate_data(sigma=0.2):\n    x_train = np.random.uniform(-1, +1, n_samples)\n    y_train = np.sin(np.pi * x_train) + sigma * np.random.normal(size=n_samples)\n    return x_train, y_train\n\n\ndef transform(x: Float[Array, \" N\"], d: int):\n    return np.column_stack([\n        x ** d_\n        for d_ in range(d + 1)\n    ])\n\n\nfor d in degrees:\n    for _ in range(2):\n        x_train, y_train = generate_data()\n\n        x_features = transform(x_train, d)\n        w = np.linalg.lstsq(x_features, y_train)[0]\n        y_hat = transform(x_axis, d) @ w\n\n        color = 'blue' if _ == 0 else 'red'\n        plt.scatter(x_train, y_train, color=color, marker='x')\n        plt.plot(x_axis, y_hat, color=color)\n    plt.xlim(-1, +1)\n    plt.ylim(-1.2, 1.2)\n    plt.gcf().set_size_inches(2, 2)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Degree \\(2\\) polynomials\n\n\n\n\n\n\n\n\n\n\n\n(b) Degree \\(5\\) polynomials\n\n\n\n\n\n\n\n\n\n\n\n(c) Degree \\(50\\) polynomials\n\n\n\n\n\n\n\nFigure 5.4: Demonstrating the bias-variance tradeoff through polynomial regression. Increasing the degree increases the complexity of the polynomial function class.\n\n\n\nWe must also consider practical constraints on the function class. We need an efficient algorithm to actually compute the function in the class that minimizes the training error. This point should not be underestimated! The success of modern deep learning, for example, is in large part due to hardware developments that have made certain operations more efficient than others.\n\n\n5.3.2 Parameterized function classes\nBoth of the function classes we will consider, linear maps and neural networks, are finite-dimensional, a.k.a. parameterized. The notion of a parameterized function class is best illustrated by example:\n\nExample 5.5 (Quadratic functions) consider the class of quadratic functions, i.e. polynomials of degree \\(2\\). This is a three-dimensional function space (\\(D = 3\\)), since we can describe any quadratic \\(p\\) as\n\\[\np(x) = a x^2 + b x + c,\n\\tag{5.9}\\]\nwhere \\(a, b, c\\) are the three parameters. We could also use a different parameterization:\n\\[\np(x) = a' (x - b')^2 + c'.\n\\tag{5.10}\\]\n\n\nDefinition 5.4 (Parameters) Let \\(\\mathcal{F}\\) be a class of functions mapping from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\). We say that \\(\\mathcal{F}\\) is a parameterized function class if each \\(f \\in \\mathcal{F}\\) can be identified using \\(D\\) parameters.\n\n\nExercise 5.2 (Parameterization matters) Note that the choice of parameterization can impact the performance of the chosen fitting method. What is the derivative of eq. 5.9 with respect to \\(a, b, c\\)? Compare this to the derivative of eq. 5.10 with respect to \\(a', b', c'\\). This shows that gradient-based fitting methods may change their behaviour depending on the parameterization.\n\nUsing a parameterized function class allows us to reframe the ERM problem def. 5.3 in terms of optimizing over the parameters instead of over the functions they represent:\n\\[\n\\begin{aligned}\n\\hat \\theta_\\text{ERM} &:= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\text{training loss}(f_\\theta) \\\\\n&= \\frac{1}{N} \\sum_{n=1}^N (y_n - f_\\theta(x_n))^2\n\\end{aligned}\n\\tag{5.11}\\]\nIn general, optimizing over a finite-dimensional space is much, much easier than optimizing over an infinite-dimensional space.\n\n\n5.3.3 Gradient descent\nOne widely applicable fitting method for parameterized function classes is gradient descent.\nLet \\(L(\\theta) = \\text{training loss}(f_\\theta)\\) denote the empirical risk in terms of the parameters. The gradient descent algorithm iteratively updates the parameters according to the rule\n\\[\n\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(\\theta^t)\n\\]\nwhere \\(\\eta &gt; 0\\) is the learning rate and \\(\\nabla_\\theta L(\\theta^t)\\) indicates the gradient of \\(L\\) at the point \\(\\theta^t\\). Recall that the gradient of a function at a point is a vector in the direction that increases the function’s value the most within a neighborhood. So by taking small steps in the oppposite direction, we obtain a solution that achieves a slightly lower loss than the current one.\n\n\n\n\nCode\nParams = Float[Array, \" D\"]\n\n\ndef gradient_descent(\n    loss: Callable[[Params], float],\n    θ_init: Params,\n    η: float,\n    epochs: int,\n):\n    \"\"\"\n    Run gradient descent to minimize the given loss function\n    (expressed in terms of the parameters).\n    \"\"\"\n    θ = θ_init\n    for _ in range(epochs):\n        θ = θ - η * grad(loss)(θ)\n    return θ\n\n\n\nFigure 5.5\n\n\n\nIn Section 7.3.1, we will discuss methods for implementing the grad function above, which takes in a function and returns its gradient, which can then be evaluated at a point.\nWhy do we need to scale down the step size by \\(\\eta\\)? The key word above is “neighborhood”. The gradient only describes the function within a local region around the point, whose size depends on the function’s smoothness. If we take a step that’s too large, we might end up with a worse solution by overshooting the region where the gradient is accurate. Note that, as a result, we can’t guarantee finding a global optimum of the function; we can only find local optima that are the best parameters within some neighborhood.\nAnother issue is that it’s often expensive to compute \\(\\nabla_\\theta L\\) when \\(N\\) is very large. Instead of calculating the gradient for every point in the dataset and averaging these, we can simply draw a batch of samples from the dataset and average the gradient across just these samples. Note that this is an unbiased random estimator of the true gradient. This algorithm is known as stochastic gradient descent. The added noise sometimes helps to jump to better solutions with a lower overall empirical risk.\nStepping for a moment back into the world of RL, you might wonder, why can’t we simply apply gradient descent to the total reward? It turns out that the gradient of the total reward with respect to the policy parameters known as the policy gradient, is challenging but possible to approximate. In Chapter 7, we will do exactly this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#examples-of-parameterized-function-classes",
    "href": "supervised_learning.html#examples-of-parameterized-function-classes",
    "title": "5  Supervised learning",
    "section": "5.4 Examples of parameterized function classes",
    "text": "5.4 Examples of parameterized function classes\n\n5.4.1 Linear regression\nIn linear regression, we assume that the function \\(f\\) is linear in the parameters:\n\\[\n\\mathcal{F} = \\{ x \\mapsto \\theta^\\top x \\mid \\theta \\in \\mathbb{R}^D \\}\n\\]\nYou may already be familiar with linear regression from an introductory statistics course. This function class is extremely simple and only contains linear functions, whose graphs look like “lines of best fit” through the training data. It turns out that, when minimizing the squared error, the empirical risk minimizer has a closed-form solution, known as the ordinary least squares estimator. Let us write \\(Y = (y_1, \\dots, y_n)^\\top \\in \\mathbb{R}^N\\) and \\(X = (x_1, \\dots, x_N)^\\top \\in \\mathbb{R}^{N \\times D}\\). Then we can write\n\\[\n\\begin{aligned}\n\\hat \\theta &= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\frac{1}{2} \\sum_{n=1}^N (y_n - \\theta^\\top x_n)^2 \\\\\n&= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\frac 1 2 \\|Y - X \\theta \\|^2 \\\\\n&= (X^\\top X)^{-1} X^\\top Y,\n\\end{aligned}\n\\tag{5.12}\\]\nwhere we have assumed that the columns of \\(X\\) are linearly independent so that the matrix \\(X^\\top X\\) is invertible.\nWhat happens if the columns aren’t linearly independent? In this case, out of the possible solutions with the minimum empirical risk, we typically choose the one with the smallest norm.\n\nExercise 5.3 (Gradient descent finds the minimum norm solution) Gradient descent on the ERM problem (eq. 5.12), initialized at the origin and using a small enough step size, eventually finds the parameters with the smallest norm. In practice, since the squared error gradient is convenient to compute, running gradient descent can be faster than explicitly computing the inverse (or pseudoinverse) of a matrix.\nAssume that \\(N &lt; D\\) and that the data points are linearly independent.\n\nLet \\(\\hat{\\theta}\\) be the solution found by gradient descent. Show that \\(\\hat{\\theta}\\) is a linear combination of the data points, that is, \\(\\hat{\\theta} = X^\\top a\\), where \\(a \\in \\mathbb{R}^N\\).\nLet \\(w \\in \\mathbb{R}^D\\) be another empirical risk minimizer i.e. \\(X w = y\\). Show that \\(\\hat{\\theta}^\\top (w - \\hat{\\theta}) = 0\\).\nUse this to show that \\(\\|\\hat{\\theta}\\| \\le \\|w\\|\\), showing that the gradient descent solution has the smallest norm out of all solutions that fit the data. (No need for algebra; there is a nice geometric solution!)\n\n\nThough linear regression may appear trivially simple, it is a very powerful tool for more complex models to build upon. For instance, to expand the expressiveness of linear models, we can first transform the input \\(x\\) using some feature mapping \\(\\phi\\), i.e. \\(\\widetilde x = \\phi(x)\\), and then fit a linear model in the transformed space instead. By using domain knowledge to choose a useful feature mapping, we can obtain a powerful SL method for a particular task.\n\n\n5.4.2 Neural networks\nIn neural networks, we assume that the function \\(f\\) is a composition of linear functions (represented by matrices \\(W_i\\)) and non-linear activation functions (denoted by \\(\\sigma\\)):\n\\[\n\\mathcal{F} = \\{ x \\mapsto \\sigma(W_L \\sigma(W_{L-1} \\dots \\sigma(W_1 x + b_1) \\dots + b_{L-1}) + b_L) \\}\n\\]\nwhere \\(W_\\ell \\in \\mathbb{R}^{D_{\\ell+1} \\times D_\\ell}\\) and \\(b_\\ell \\in \\mathbb{R}^{D_{\\ell+1}}\\) are the parameters of the \\(i\\)-th layer, and \\(\\sigma\\) is the activation function.\nThis function class is highly expressive and allows for more parameters. This makes it more susceptible to overfitting on smaller datasets, but also allows it to represent more complex functions. In practice, however, neural networks exhibit interesting phenomena during training, and are often able to generalize well even with many parameters.\nAnother reason for their popularity is the efficient backpropagation algorithm for computing the gradient of the output with respect to the parameters. Essentially, the hierarchical structure of the neural network, i.e. computing the output of the network as a composition of functions, allows us to use the chain rule to compute the gradient of the output with respect to the parameters of each layer.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#key-takeaways",
    "href": "supervised_learning.html#key-takeaways",
    "title": "5  Supervised learning",
    "section": "5.5 Key takeaways",
    "text": "5.5 Key takeaways\nSupervised learning is a field of machine learning that seeks to approximate some unknown function given a dataset of example input-output pairs from that function. In particular, we typically seek to compute a prediction rule that takes in an input value and returns a good guess for the corresponding output. We score prediction rules using a loss function that measures how incorrectly it guesses. We want to find a prediction rule that achieves low loss on unseen data points. We do this by searching over a class of functions to find one that minimizes the empirical risk over the training dataset. We finally saw two popular examples of parameterized function classes: linear regression and neural networks.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#sec-sl-bib",
    "href": "supervised_learning.html#sec-sl-bib",
    "title": "5  Supervised learning",
    "section": "5.6 Bibliographic notes and further reading",
    "text": "5.6 Bibliographic notes and further reading\nSupervised learning is the largest subfield of machine learning; we do not attempt to comprehensively survey recent progress here. Rather, here are some textbooks for interested students to read further.\nJames et al. (2023) provides an accessible introduction to supervised learning. Hastie et al. (2013) examines the subject in even further depth and covers many relevant supervised learning methods. Nielsen (2015) provides a comprehensive introduction to neural networks and backpropagation. Vapnik (2000) is another prominent textbook on classical statistical learning from before the “deep learning era”. Bishop (2006) focuses on the Bayesian perspective.\n\n\n\n\nBishop, C. M. (2006). Pattern recognition and machine learning. Springer.\n\n\nDeng, L. (2012). The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), 141–142. IEEE Signal Processing Magazine. https://doi.org/10.1109/MSP.2012.2211477\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2013). The elements of statistical learning: Data mining, inference, and prediction. Springer Science & Business Media. https://books.google.com?id=yPfZBwAAQBAJ\n\n\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: With applications in python. Springer International Publishing. https://doi.org/10.1007/978-3-031-38747-0\n\n\nNielsen, M. A. (2015). Neural networks and deep learning. Determination Press. http://neuralnetworksanddeeplearning.com/\n\n\nVapnik, V. N. (2000). The nature of statistical learning theory. Springer. https://doi.org/10.1007/978-1-4757-3264-1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html",
    "href": "fitted_dp.html",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "",
    "text": "6.1 Fitted policy evaluation\nIn Chapter 2, we discussed how to solve known tabular Markov decision processes where the state and action spaces \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) were finite and small and we knew the reward function and state transitions of the environment. For most interesting tasks, however, we don’t know the analytical form of the reward functions or state transitions. In such settings, we must learn through interaction with the environment.\nIn this short chapter, we will tackle the full RL problem by extending DP algorithms to account for unknown environments. We will also relax the assumption that the state space is small and finite: the methods we will cover support large or continuous state spaces by using parameterized function approximation. This will require learning about the environment by collecting data through interaction and then applying supervised learning (see Chapter 5) to approximate relevant functions.\nEach of the algorithms we’ll cover is analogous to one of the algorithms from Chapter 2:\nThe term “fitted” means that, since we can no longer compute the Bellman consistency equations or Bellman optimality equations exactly, we plug data from the environment into a supervised learning algorithm to approximately solve these equations. As such, these are also termed “approximate DP” algorithms, or “neuro-DP” algorithms, as in Bertsekas & Tsitsiklis (1996). We will also introduce some useful language for classifying RL algorithms.\nRecall the task of policy evaluation: given a policy \\(\\pi : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\), compute its Q function \\(Q^\\pi\\), which expresses the expected total reward in a given starting state-action pair.\nHowever, computing the update step in eq. 6.2 requires computing an expectation over the state transitions \\(s' \\sim P(\\cdot \\mid s, a)\\). As mentioned in the introduction, this is intractable for most real-world tasks. Either the state space is too large, or we simply don’t know what \\(P\\) is. Instead, we will apply supervised learning methods to approximately solve the Bellman consistency equations using data.\nRecall that supervised learning is good at learning conditional expectations of the form\n\\[\nf(x) = \\mathop{\\mathbb{E}}[ y \\mid x ],\n\\tag{6.3}\\]\nwhere \\(x\\) are the input features and \\(y\\) is the scalar response. Can we rewrite the one-step value target (eq. 6.1) as a conditional expectation? In fact, it already is! Let us use notation that makes this more clear. We explicitly treat \\(s', a'\\) as random variables and move the conditioning on \\(s, a\\) into the brackets:\n\\[\nQ^\\pi(s, a) = \\mathop{\\mathbb{E}}[ r(s, a) + Q^\\pi(s', a') \\mid s, a ].\n\\tag{6.4}\\]\nWe can see that the input \\(x\\) corresponds to \\(s, a\\) and the response \\(y\\) corresponds to \\(r(s, a) + q^i(s', a')\\), where \\(q^i\\) is our current estimate of \\(Q^\\pi\\). Now we just need to obtain a dataset of input-output pairs and run empirical risk minimization (Section 5.3). We can classify data collection strategies as either offline or online.\nWe’ll begin with an offline version of fitted policy evaluation and then see an online version.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#sec-fit-pe",
    "href": "fitted_dp.html#sec-fit-pe",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "",
    "text": "Remark 6.3 (Value function vs Q function). In Chapter 2, we sought to compute the value function \\(V^\\pi\\). Here, we’ll instead approximate the action-value function \\(Q^\\pi\\), since we’ll need the ability to take the action with the maximum expected remaining reward. If we don’t know the environment, we can’t compute the action-values using only \\(V^\\pi\\).\n\n\nRemark 6.4 (Fixed-point policy evaluation review). The fixed-point policy evaluation algorithm (Section 2.4.3.2) makes use of the Bellman consistency equations (Theorem 2.3), restated here for the Q-function:\n\\[\nQ^\\pi(s, a) = r(s, a) + \\mathop{\\mathbb{E}}_{\\substack{\n    s' \\sim P(\\cdot \\mid s, a)\\\\\n    a' \\sim \\pi(\\cdot \\mid s')\n}} [Q^\\pi(s', a')].\n\\tag{6.1}\\]\nIn fixed-point iteration, we treat eq. 6.1 not as an equation, but as an “operator” that takes in a function \\(q^i : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) and returns an updated function \\(q^{i+1}\\) by substituting \\(q^i\\) in place of \\(Q^\\pi\\).\n\\[\nq^{i+1}(s, a) := r(s, a) + \\mathop{\\mathbb{E}}_{\\substack{\n    s' \\sim P(\\cdot \\mid s, a)\\\\\n    a' \\sim \\pi(\\cdot \\mid s')\n}} [q^i(s', a')].\n\\tag{6.2}\\]\nRoughly speaking, \\(q^{i+1}\\) approximates \\(Q^\\pi\\) slightly better than \\(q^i\\) since it incorporates a single step of reward from the environment. Since this operator is a contraction mapping (Theorem 2.8), by iterating this process, \\(q\\) will eventually converge to the true policy \\(Q^\\pi\\).\n\n\n\n\n\n\n\n\nDefinition 6.1 (Offline and online algorithms) We say that a learning algorithm works offline if the learning is performed as a function of a static dataset, without requiring further interaction with the environment. In contrast, online learning algorithms require interaction with the environment during learning.\n\n\n\n6.1.1 Offline fitted policy evaluation\nIn particular, we use \\(\\pi\\), the policy we’re trying to evaluate, to obtain a dataset of \\(N\\) trajectories \\(\\tau^1, \\dots, \\tau^N \\sim \\rho^{\\pi}\\). Let us indicate the trajectory index in the superscript, so that\n\\[\n\\tau^n = \\{ s_0^n, a_0^n, r_0^n, s_1^n, a_1^n, r_1^n, \\dots, s_{H-1}^n, a_{H-1}^n, r_{H-1}^n \\}.\n\\tag{6.5}\\]\nThis would give us \\(N (H- 1)\\) samples in the dataset. We subtract one from the horizon since each \\((x, y)\\) sample requires a pair of consecutive timesteps:\n\\[\nx^n_{h} = (s_h^n, a_h^n) \\qquad y^n_{h} = r(s_h^n, a_h^n) + q^i(s^n_{h+ 1}, a^n_{h+1})\n\\tag{6.6}\\]\nwhere \\(q^i\\) is our current estimate of \\(Q^\\pi\\). This makes our new algorithm a fixed-point algorithm as well, since it uses \\(q^i\\) to compute the next iterate \\(q^{i+1}\\).\nNow we can use empirical risk minimization to find a function \\(q^I\\) that approximates the optimal Q-function.\n\n\n\n\n\n\ngraph LR\n    A[\"$$\\text{Use } \\pi \\text{ to collect data } \\tau^1, \\dots, \\tau^N$$\"] --&gt; B\n    B[\"$$\\text{Compute labels } y^n_h \\text{ using Bellman consistency equations for } q$$\"] --&gt; C\n    C[\"$$\\text{Update } q \\text{ by fitting to data}$$\"] --&gt; B\n\n\n\n\nFigure 6.1: Fitted policy evaluation.\n\n\n\n\n\n\nDefinition 6.2 (Fitted policy evaluation) Let \\(\\pi : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\) be the policy we wish to evaluate. Our goal is to compute an approximation of its Q-function \\(Q^\\pi\\).\n\nCollect a dataset \\(\\tau^1, \\dots, \\tau^N \\sim \\rho^\\pi\\).\nInitialize some function \\(q^0 : \\mathcal{S} \\times \\mathcal{A} \\in \\mathbb{R}\\).\nFor \\(i = 0, \\dots, I-1\\):\n\nGenerate labels \\(y^1, \\dots, y^N\\) from the trajectories and the current estimate \\(q^i\\), where the labels come from eq. 6.1.\nSet \\(q^{i+1}\\) to the function that minimizes the empirical risk:\n\n\n\\[\nq \\gets \\arg\\min_{q} \\frac{1}{N H} \\sum_{n=1}^N \\sum_{h=1}^{H-2} (y^n_h- q(x^n_h))^2.\n\\tag{6.7}\\]\n\n\n\nCode\ndef fitted_evaluation(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    π: Policy,\n    epochs: int,\n    Q_init: QFunction,\n) -&gt; QFunction:\n    \"\"\"\n    Run fitted policy evaluation using the given dataset.\n    Returns an estimate of the Q-function of the given policy.\n    \"\"\"\n    Q_hat = Q_init\n    X = get_X(trajectories)\n    for epoch in tqdm(range(epochs)):\n        y = get_y(trajectories, Q_hat, π)\n        Q_hat = fit(X, y)\n    return Q_hat\n\nlatex(fitted_evaluation)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{fitted\\_evaluation}(\\mathrm{trajectories}: \\mathrm{list}_{\\mathrm{Trajectory}}, \\mathrm{fit}: \\mathrm{FittingMethod}, π: \\mathrm{Policy}, \\mathrm{epochs}: \\mathbb{Z}, Q_{\\mathrm{init}}: \\mathrm{QFunction}) \\\\ \\hspace{1em} \\textrm{\"\n    Run fitted policy evaluation using the given dataset.\n    Returns an estimate of the Q-function of the given policy.\n    \"} \\\\ \\hspace{1em} \\widehat{Q} \\gets Q_{\\mathrm{init}} \\\\ \\hspace{1em} X \\gets \\mathrm{get\\_X} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{epoch} \\in \\mathrm{tqdm} \\mathopen{}\\left( \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} y \\gets \\mathrm{get\\_y} \\mathopen{}\\left( \\mathrm{trajectories}, \\widehat{Q}, π \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{Q} \\gets \\mathrm{fit} \\mathopen{}\\left( X, y \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\widehat{Q} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFitted policy evaluation is offline since the “interaction phase” and the “learning phase” are disjoint. In other words, we can think of fitted policy evaluation as taking a dataset of trajectories collected by some unknown policy \\(\\pi_\\text{data}\\), and then approximating the Q function of \\(\\pi_\\text{data}\\).\nFitted policy evaluation is on-policy because the update rule uses trajectories sampled from \\(\\pi\\). Where do we need this assumption? Pay close attention to the target, and compare it to the true one-step value target (eq. 6.1):\n\\[\n\\begin{aligned}\ny_h&= r(s_h, a_h) + q(s_{h+1}, a_{h+1}) \\\\\nQ^\\pi(s, a) &= r(s, a) + \\mathop{\\mathbb{E}}_{\\substack{\n    s' \\sim P(\\cdot \\mid s, a) \\\\\n    a' \\sim \\pi(\\cdot \\mid s')\n}} Q^\\pi(s', a').\n\\end{aligned}\n\\tag{6.8}\\]\nNotice that \\((s_{h+1}, a_{h+1})\\) is a single sample from the joint distribution \\(s' \\sim P(\\cdot \\mid s, a)\\) and \\(a' \\sim \\pi(\\cdot \\mid s')\\). If the trajectories were collected from a different policy, then \\(a_{h+1}\\) would not be a sample from \\(\\pi(\\cdot \\mid s')\\), making the target a biased sample for evaluating \\(\\pi\\).\n\nDefinition 6.3 (On-policy and off-policy algorithms) We say that a learning algorithm is on-policy if the update rule must use data collected by the current policy. On the other hand, we call a learning algorithm off-policy if its update rule doesn’t care about how the data was collected.\n\n\nExercise 6.1 (Off-policy fitted policy evaluation) Now suppose you are given a dataset of trajectories sampled from a policy \\(\\pi_\\text{data}\\), and you want to evaluate a different policy \\(\\pi\\). You are not given access to the environment. How could you use the dataset to evaluate \\(\\pi\\)? Explain what makes this an off-policy algorithm.\n\n\n\nCode\ndef collect_data(\n    env: gym.Env, N: int, H: int, key: rand.PRNGKey, π: Optional[Policy] = None\n) -&gt; list[Trajectory]:\n    \"\"\"Collect a dataset of trajectories from the given policy (or a random one).\"\"\"\n    trajectories = []\n    seeds = [rand.bits(k).item() for k in rand.split(key, N)]\n    for i in tqdm(range(N)):\n        τ = []\n        s, _ = env.reset(seed=seeds[i])\n        for h in range(H):\n            # sample from a random policy\n            a = π(s, h) if π else env.action_space.sample()\n            s_next, r, terminated, truncated, _ = env.step(a)\n            τ.append(Transition(s, a, r))\n            if terminated or truncated:\n                break\n            s = s_next\n        trajectories.append(τ)\n    return trajectories\n\n\ndef get_X(trajectories: list[Trajectory]):\n    \"\"\"\n    We pass the state and timestep as input to the Q-function\n    and return an array of Q-values.\n    \"\"\"\n    rows = [(τ[h].s, τ[h].a, h) for τ in trajectories for h in range(len(τ))]\n    return [np.stack(ary) for ary in zip(*rows)]\n\n\ndef get_y(\n    trajectories: list[Trajectory],\n    f: Optional[QFunction] = None,\n    π: Optional[Policy] = None,\n):\n    \"\"\"\n    Transform the dataset of trajectories into a dataset for supervised learning.\n    If `π` is None, instead estimates the optimal Q function.\n    Otherwise, estimates the Q function of π.\n    \"\"\"\n    f = f or Q_zero(get_num_actions(trajectories))\n    y = []\n    for τ in trajectories:\n        for h in range(len(τ) - 1):\n            s, a, r = τ[h]\n            Q_values = f(s, h + 1)\n            y.append(r + (Q_values[π(s, h + 1)] if π else Q_values.max()))\n        y.append(τ[-1].r)\n    return np.array(y)\n\n\n\n\n6.1.2 Bootstrapping and target networks\nUsing the current guess \\(q\\) to compute the labels is known as bootstrapping. (This has nothing to do with the term bootstrapping in statistical inference.)\nThis term comes from the following metaphor: if you are trying to get on a horse, and there’s nobody to help you up, you need to “pull yourself up by your bootstraps,” or in other words, start from your existing resources to build up to something more effective.\nUsing a bootstrapped estimate makes the optimization more complicated. Since we are constantly updating our Q function estimate, the labels are also constantly changing, destabilizing learning. Sutton & Barto (2018) calls bootstrapping one prong of the deadly triad of deep reinforcement learning, alongside function approximation and off-policy learning. When an algorithm relies on all three of these properties, it is possible for learning to diverge, so that the fitted Q function is far from the true Q function (van Hasselt et al., 2018).\nMany tricks exist to mitigate this issue in practice. One such trick is to use a target network. That is, when computing \\(y\\), instead of using \\(q\\), which is constantly changing, we maintain another target network \\(q_\\text{target}\\) that “updates more slowly.” Concretely, \\(q_\\text{target}\\) is an exponential moving average of the iterates of \\(q\\). Whenever we update \\(q\\), we update \\(q_\\text{target}\\) accordingly:\n\\[\nq_\\text{target} \\gets (1-\\lambda_{\\text{target}}) q_\\text{target} + \\lambda_\\text{target} q,\n\\tag{6.9}\\]\nwhere \\(\\lambda_\\text{target} \\in (0, 1)\\) is some mixing parameter: the larger it is, the more we update towards the current estimate \\(q\\).\n\n\n6.1.3 Online fitted policy evaluation\nIn the offline algorithm above, we collect the whole dataset before starting the learning process. What could go wrong with this? Since the environment is unknown, the dataset only contains information about some portion of the environment. When we update \\(q\\), it may only learn to approximate \\(Q^\\pi\\) well for states that are in the initial dataset. It would be much better if \\(q\\) were accurate in states that the policy will actually find itself in. This leads to the following simple shift: collect trajectories inside the iteration loop! This results in an online algorithm for fitted policy evaluation, also known as \\(\\text{TD}(0)\\).\n\n\nCode\ndef fitted_policy_evaluation_online(\n    env,\n    pi: Policy,\n    epochs: int,\n    learning_rate: float,\n    q_init,\n    *,\n    key\n):\n    q = q_init\n    for epoch in range(epochs):\n        trajectory = collect_data(env, N=1, H=20, key=key, pi=pi)\n        for (s, a, r), (s_next, a_next, _) in zip(trajectory[:-1], trajectory[1:]):\n            target = r + q[s_next, a_next]\n            q[s, a] = q[s, a] - learning_rate * (q[s, a] - target)\n    return q\n\nlatex(fitted_policy_evaluation_online)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{fitted\\_policy\\_evaluation\\_online}(\\mathrm{env}, \\pi: \\mathrm{Policy}, \\mathrm{epochs}: \\mathbb{Z}, \\mathrm{learning\\_rate}: \\mathbb{R}, q_{\\mathrm{init}}) \\\\ \\hspace{1em} q \\gets q_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{epoch} \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectory} \\gets \\mathrm{collect\\_data} \\mathopen{}\\left( \\mathrm{env} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ \\mathopen{}\\left( \\mathopen{}\\left( s, a, r \\mathclose{}\\right), \\mathopen{}\\left( s_{\\mathrm{next}}, a_{\\mathrm{next}}, \\mathrm{\\_} \\mathclose{}\\right) \\mathclose{}\\right) \\in \\mathrm{zip} \\mathopen{}\\left( \\mathrm{trajectory}_{:-1}, \\mathrm{trajectory}_{1:} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathrm{target} \\gets r + q_{s_{\\mathrm{next}}, a_{\\mathrm{next}}} \\\\ \\hspace{3em} q_{s, a} \\gets q_{s, a} - \\mathrm{learning\\_rate} \\cdot \\mathopen{}\\left( q_{s, a} - \\mathrm{target} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ q \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 6.2: Pseudocode for online policy evaluation (TD(0))\n\n\n\n\nNote that we explicitly write out one step of gradient descent on the squared “temporal difference error”.\n\nRemark 6.5 (On notation). What does \\(\\text{TD}(0)\\) stand for? This notation suggests the existence of other \\(\\text{TD}(\\lambda)\\) algorithms. In fact, \\(\\text{TD}(\\lambda)\\) is a well-studied algorithm with a parameter \\(\\lambda \\in [0, 1]\\), and \\(\\text{TD}(0)\\) is simply the special case \\(\\lambda = 0\\). TD stands for “temporal difference”: we use the value function at a different point in time to estimate the value of the current state at the current time step. The parameter \\(\\lambda\\) is one way to average between the near-term estimates and the long-term estimates at later timesteps.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#sec-fit-vi",
    "href": "fitted_dp.html#sec-fit-vi",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "6.2 Fitted value iteration",
    "text": "6.2 Fitted value iteration\nWe’ll now explore an algorithm for computing the optimal value function \\(V^\\star\\) when the environment is unknown. This method is analogous to value iteration (Section 2.4.4.1), except instead of solving the Bellman optimality equations (eq. 2.38) exactly, which would require full knowledge of the environment, we will collect a dataset of trajectories and apply supervised learning to solve the Bellman optimality equations approximately. This is exactly analogous to fitted policy evaluation (Section 6.1), except we use the Bellman optimality equations (eq. 2.38) instead of the Bellman consistency equations (eq. 2.9) for a given policy.\n\n\n\nTable 6.2: How fitted value iteration relates to existing algorithms.\n\n\n\n\n\n\n\n\n\n\n\nKnown environment\nUnknown environment\n\n\n\n\nBellman consistency equations (evaluation)\nPolicy evaluation\nFitted policy evaluation\n\n\nBellman optimality equations (optimization)\nValue iteration\nFitted value iteration\n\n\n\n\n\n\n\nRemark 6.6 (Value iteration review). Value iteration was an algorithm we used to compute the optimal value function \\(V^\\star : \\mathcal{S} \\to \\mathbb{R}\\) in an infinite-horizon MDP (Section 2.4.4.1). Here, we will present the equivalent algorithm for the optimal action-value function \\(Q^\\star : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\). The optimal action-value function satisfies the Bellman optimality equations\n\\[\nQ^\\star(s, a) = r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\cdot \\mid s, a)}[\n    \\max_{a' \\in \\mathcal{A}} Q^\\star(s', a')\n].\n\\tag{6.10}\\]\nNow let us treat eq. 6.10 as an “operator” instead of an equation, that is,\n\\[\nq(s, a) \\gets r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\cdot \\mid s, a)}[\n    \\max_{a' \\in \\mathcal{A}} q(s', a')\n].\n\\tag{6.11}\\]\nIf we start with some guess \\(q : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\), and repeatedly apply the update step eq. 6.11, the iterates will eventually converge to \\(Q^\\star\\) since eq. 6.11 is a contraction mapping.\n\nWhen we can’t compute expectations over \\(s' \\sim P(\\cdot \\mid s, a)\\), we can instead apply supervised learning to approximately solve the Bellman optimality equations. As before, we can write \\(Q^\\star\\) explicitly as a conditional expectation\n\\[\nQ^\\star(s, a) = \\mathop{\\mathbb{E}}\\left[\n    r(s, a) + \\max_{a' \\in \\mathcal{A}}Q^\\star(s', a') \\mid s, a\n\\right].\n\\tag{6.12}\\]\nFrom this expression, we can read off the inputs \\(x\\) and the targets \\(y\\) for supervised learning. As before, since we don’t know \\(Q^\\star\\), we replace it with our current guess \\(q \\approx Q^\\star\\):\n\\[\n\\begin{aligned}\nx &:= (s, a) \\\\\ny &:= r(s, a) + \\max_{a' \\in \\mathcal{A}} q(s', a').\n\\end{aligned}\n\\tag{6.13}\\]\nThe only difference from fitted policy evaluation (Section 6.1) is how we compute the targets \\(y\\). Instead of using the next action from the trajectory, we use the action with the maximum value. Notice that these equations don’t reference a policy anywhere! In other words, fitted value iteration is an off-policy algorithm.\n\n\n\n\n\n\ngraph LR\n    A[\"$$\\text{Use } \\pi \\text{ to collect data } \\tau^1, \\dots, \\tau^N$$\"] --&gt; B\n    B[\"$$\\text{Compute labels } y^n_h \\text{ using Bellman optimality equations for } q$$\"] --&gt; C\n    C[\"$$\\text{Update } q \\text{ by fitting to data}$$\"] --&gt; B\n\n\n\n\nFigure 6.3: Fitted policy evaluation.\n\n\n\n\n\n\n6.2.1 Offline fitted value iteration\nTo construct an offline algorithm, we take some dataset of trajectories, and then do all of the learning without ever interacting with the environment.\n\nDefinition 6.4 (Fitted value iteration) Suppose we have some dataset of trajectories \\(\\tau^1, \\dots, \\tau^N\\) collected by interacting with the environment.\n\nInitialize some function \\(q^0(s, a, h) \\in \\mathbb{R}\\).\nCompute \\(x^n_h= (s^n_h, a^n_h)\\).\nFor \\(t = 0, \\dots, T-1\\):\n\nUse \\(q^t\\) to generate the targets \\[\ny^n_h= r^n_h+ \\max_{a'} q^t(s^n_h, a').\n\\tag{6.14}\\]\nSet \\(q^{t+1}\\) to the function that minimizes the empirical risk:\n\n\n\\[\nf^{t+1} \\gets \\arg\\min_f \\frac{1}{N (H-1)} \\sum_{n=1}^N  \\sum_{h=0}^{H-2} (y^n_h- f(x^n_h))^2,\n\\tag{6.15}\\]\n\n\n\nCode\ndef fitted_q_iteration(\n    trajectories,\n    fit,\n    epochs,\n    Q_init,\n) -&gt; QFunction:\n    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))\n    X = get_X(trajectories)\n    for _ in range(epochs):\n        y = get_y(trajectories, Q_hat)\n        Q_hat = fit(X, y)\n    return Q_hat\n\nlatex(fitted_q_iteration)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{fitted\\_q\\_iteration}(\\mathrm{trajectories}, \\mathrm{fit}, \\mathrm{epochs}, Q_{\\mathrm{init}}) \\\\ \\hspace{1em} \\widehat{Q} \\gets Q_{\\mathrm{init}} \\lor Q_{\\mathrm{zero}} \\mathopen{}\\left( \\mathrm{get\\_num\\_actions} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} X \\gets \\mathrm{get\\_X} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{\\_} \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} y \\gets \\mathrm{get\\_y} \\mathopen{}\\left( \\mathrm{trajectories}, \\widehat{Q} \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{Q} \\gets \\mathrm{fit} \\mathopen{}\\left( X, y \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\widehat{Q} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 6.4: Pseudocode for fitted value iteration.\n\n\n\n\n\n\n6.2.2 Q-learning\nIn fitted value iteration (fig. 6.4), we collect the whole dataset beforehand from some unknown policy (or policies). This doesn’t interact with the environment during the learning process. What could go wrong? Since the environment is unknown, the dataset only contains information about some portion of the environment. When we update \\(q\\), it may therefore only learn to approximate \\(Q^\\star\\) well for states that are in the initial dataset. It would be much better if \\(q\\) was accurate in states that the policy will actually find itself in. Given access to the environment, we can instead collect trajectories while learning. This turns fitted value interation into an online algorithm known as Q-learning.\n\n\n\n\n\n\ngraph LR\n    A[Collect trajectories from data collection policy] --&gt; B\n    B[Compute targets by bootstrapping with q] --&gt; C\n    C[Update q by empirical risk minimization] --&gt; B\n\n\n\n\nFigure 6.5: Fitted value iteration\n\n\n\n\n\n\n\n\n\n\n\ngraph LR\n    A[Collect trajectories from epsilon-greedy policy for current guess for Q] --&gt; B\n    B[Compute targets by bootstrapping with q] --&gt; C\n    C[Update q by empirical risk minimization] --&gt; A\n\n\n\n\nFigure 6.6: Q learning\n\n\n\n\n\n\n\nCode\ndef q_learning(env, Q_init, epochs: int):\n    q = Q_init\n    for _ in range(epochs):\n        trajectories = collect_trajectories(env, EpsilonGreedyPolicy(theta))\n        X = get_X(trajectories)\n        y = get_y(trajectories, q)\n        q = fit(X, y)\n    return q\n\nlatex(q_learning, id_to_latex={\"q_learning\": r\"\\text{Q-learning}\"})\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\text{Q-learning}(\\mathrm{env}, Q_{\\mathrm{init}}, \\mathrm{epochs}: \\mathbb{Z}) \\\\ \\hspace{1em} q \\gets Q_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{\\_} \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{collect\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\mathrm{EpsilonGreedyPolicy} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{2em} X \\gets \\mathrm{get\\_X} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} y \\gets \\mathrm{get\\_y} \\mathopen{}\\left( \\mathrm{trajectories}, q \\mathclose{}\\right) \\\\ \\hspace{2em} q \\gets \\mathrm{fit} \\mathopen{}\\left( X, y \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ q \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nNote that it doesn’t actually matter how the trajectories are collected, making Q-learning an off-policy algorithm. One common choice is to collect trajectories using an epsilon-greedy policy with respect to the current guess \\(q\\).\nAnother common trick used in practice is to grow the dataset, called a replay buffer, at each iteration. Then, in the improvement step, we randomly sample a batch of \\((x, y)\\) samples from the replay buffer and use these for empirical risk minimization.\n\n\nCode\nimport random\n\ndef q_learning_with_buffer(env, Q_init, epochs: int, batch_size):\n    q = Q_init\n    buffer = []\n    for _ in range(epochs):\n        trajectories = collect_trajectories(env, EpsilonGreedyPolicy(theta))\n        buffer.extend(trajectories)\n\n        batch = random.sample(trajectories, batch_size)\n        X = get_X(batch)\n        y = get_y(batch, q)\n        q = fit(X, y)\n    return q\n\nlatex(q_learning, id_to_latex={\"q_learning_with_buffer\": r\"\\text{Q-learning-buffer}\"})\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ q_{\\mathrm{learning}}(\\mathrm{env}, Q_{\\mathrm{init}}, \\mathrm{epochs}: \\mathbb{Z}) \\\\ \\hspace{1em} q \\gets Q_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{\\_} \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{collect\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\mathrm{EpsilonGreedyPolicy} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{2em} X \\gets \\mathrm{get\\_X} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} y \\gets \\mathrm{get\\_y} \\mathopen{}\\left( \\mathrm{trajectories}, q \\mathclose{}\\right) \\\\ \\hspace{2em} q \\gets \\mathrm{fit} \\mathopen{}\\left( X, y \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ q \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#sec-fit-pi",
    "href": "fitted_dp.html#sec-fit-pi",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "6.3 Fitted policy iteration",
    "text": "6.3 Fitted policy iteration\nWe can use fitted policy evaluation to extend policy iteration (Section 2.4.4.2) to this new, more general setting. The algorithm remains exactly the same – repeatedly make the policy greedy with respect to its own value function – except now we evaluate the policy \\(\\pi\\) (i.e. estimate \\(Q^\\pi\\)) using fitted policy evaluation.\n\n\nCode\ndef fitted_policy_iteration(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    epochs: int,\n    evaluation_epochs: int,\n    π_init: Optional[Policy] = lambda s, h: 0,  # constant zero policy\n):\n    \"\"\"Run fitted policy iteration using the given dataset.\"\"\"\n    π = π_init\n    for _ in range(epochs):\n        Q_hat = fitted_evaluation(trajectories, fit, π, evaluation_epochs)\n        π = q_to_greedy(Q_hat)\n    return π\n\nlatex(fitted_policy_iteration, id_to_latex={\"fitted_policy_iteration\": r\"\\text{fitted-policy-iteration}\"})\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\text{fitted-policy-iteration}(\\mathrm{trajectories}: \\mathrm{list}_{\\mathrm{Trajectory}}, \\mathrm{fit}: \\mathrm{FittingMethod}, \\mathrm{epochs}: \\mathbb{Z}, \\mathrm{evaluation\\_epochs}: \\mathbb{Z}, π_{\\mathrm{init}}: \\mathrm{Optional}_{\\mathrm{Policy}}) \\\\ \\hspace{1em} \\textrm{\"Run fitted policy iteration using the given dataset.\"} \\\\ \\hspace{1em} π \\gets π_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{\\_} \\in \\mathrm{range} \\mathopen{}\\left( \\mathrm{epochs} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\widehat{Q} \\gets \\mathrm{fitted\\_evaluation} \\mathopen{}\\left( \\mathrm{trajectories}, \\mathrm{fit}, π, \\mathrm{evaluation\\_epochs} \\mathclose{}\\right) \\\\ \\hspace{2em} π \\gets \\mathrm{q\\_to\\_greedy} \\mathopen{}\\left( \\widehat{Q} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ π \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 6.7: Pseudocode for fitted policy iteration.\n\n\n\n\n\nExercise 6.2 (Classification of fitted policy iteration) Is fitted policy iteration online or offline? On-policy or off-policy?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#key-takeaways",
    "href": "fitted_dp.html#key-takeaways",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "6.4 Key takeaways",
    "text": "6.4 Key takeaways\nIn a finite MDP where the environment is known, we can apply dynamic programming (Section 2.3.2) to compute the optimal policy. But in most real-world settings, we don’t know the state transitions \\(P\\). This means we can’t exactly compute the Bellman consistency or optimality equations (Theorem 2.3), which require taking an expectation over the next state.\nIn an unknown environment, we must learn from data collected from the environment. Model-based methods learn a model of the environment, and then plan within that model to choose the best action. Model-free methods instead use the data to approximate quantities of interest (e.g. the optimal Q function or optimal policy) directly.\nWe began by considering offline algorithms that used a dataset of interactions to learn some quantity of interest. We then saw the online equivalents that observe data by interacting with the environment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#sec-fit-bib",
    "href": "fitted_dp.html#sec-fit-bib",
    "title": "6  Fitted Dynamic Programming Algorithms",
    "section": "6.5 Bibliographic notes and further reading",
    "text": "6.5 Bibliographic notes and further reading\nThe fitted dynamic programming algorithms we discuss in this chapter are a special case of temporal difference (TD) methods, an important class of reinforcement learning algorithms. In particular, this chapter only discusses one-step methods, which can be directly generalized to \\(n\\)-step methods and the \\(\\lambda\\)-return, the formulation most commonly used in practice. This allows us to draw direct parallels to the tabular DP methods of Chapter 2 (see tbl. 6.1). TD learning is perhaps the “central and novel [idea of] reinforcement learning” (Sutton & Barto, 2018, ch. 6).\nTD methods are based in the psychology of animal learning\nWitten (1977), which proposed the tabular \\(\\text{TD}(0)\\) method, appears to be the earliest instance of a temporal difference algorithm.\nRichard Sutton’s PhD thesis (Sutton, 1988) proved theoretical guarantees for many of the algorithms presented here. Sutton & Barto (2018) gives a comprehensive discussion of the methods discussed here.\nIn later work, Hausknecht & Stone (2017) made the Q-function recurrent, so that it depends on the past history of states. The resulting policy is non-Markovian, which additionally accounts for the partial observability of the environment.\nMaei et al. (2009) further discusses the deadly triad of off-policy sampling, function approximation, and value bootstrapping.\nDeep RL rose to prominence when Mnih et al. (2013) used a deep Q network to achieve state of the art performance on the Atari games. However, the combination of deep learning and the underlying variance in RL problems made it challenging to optimize these neural networks. Many changes have been proposed. Wang et al. (2016) suggests learning the value function and advantage function separately in a “dueling” architecture. Hasselt (2010) suggested the “double learning” algorithm to compensate for Q-learning’s tendency to overestimate the true values. Hasselt et al. (2016) applied double learning to deep neural networks. Hessel et al. (2018) combined\nBertsekas & Tsitsiklis (1996) coined the term “neuro-dynamic programming” to refere to the combination of (artificial) neural networks with dynamic programming techniques.\n\n\n\n\nBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena scientific.\n\n\nHasselt, H. (2010). Double q-learning. Advances in Neural Information Processing Systems, 23. https://proceedings.neurips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html\n\n\nHasselt, H. van, Guez, A., & Silver, D. (2016). Deep reinforcement learning with double q-learning. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2094–2100.\n\n\nHausknecht, M., & Stone, P. (2017, January 11). Deep recurrent q-learning for partially observable mdps. https://doi.org/10.48550/arXiv.1507.06527\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, 3215–3222.\n\n\nMaei, H., Szepesvári, C., Bhatnagar, S., Precup, D., Silver, D., & Sutton, R. S. (2009). Convergent temporal-difference learning with arbitrary smooth function approximation. Advances in Neural Information Processing Systems, 22. https://papers.nips.cc/paper_files/paper/2009/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. A. (2013). Playing atari with deep reinforcement learning. CoRR, abs/1312.5602. http://arxiv.org/abs/1312.5602\n\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3(1), 9–44. https://doi.org/10.1007/BF00115009\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf\n\n\nvan Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., & Modayil, J. (2018, December 6). Deep reinforcement learning and the deadly triad. https://doi.org/10.48550/arXiv.1812.02648\n\n\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. Proceedings of The 33rd International Conference on Machine Learning, 1995–2003. https://proceedings.mlr.press/v48/wangf16.html\n\n\nWitten, I. H. (1977). An adaptive optimal controller for discrete-time markov environments. Information and Control, 34(4), 286–295. https://doi.org/10.1016/S0019-9958(77)90354-0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "pg.html",
    "href": "pg.html",
    "title": "7  Policy Gradient Methods",
    "section": "",
    "text": "7.1 Introduction\nThe core task of RL is finding the optimal policy in a given environment. This is essentially an optimization problem (i.e. computing a minimum or maximum): out of some class of policies \\(\\Pi\\), we want to find the one that achieves the maximum expected total reward:\n\\[\n\\hat \\pi = \\arg\\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} [R(\\tau)] \\quad \\text{where} \\quad R(\\tau) := \\sum_{h=0}^{H-1} r(s_h, a_h).\n\\tag{7.1}\\]\nIn a known environment with a finite set of states and actions, we can compute the optimal policy using dynamic programming in \\(O( H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)\\) steps (Section 2.3.2). In more general settings, though, such as when the environment is unknown, it is typically intractable to compute the optimal policy exactly. Instead, we start from some random policy, and then iteratively improve it by interacting with the environment. Policy iteration (Section 2.4.4.2) and iterative LQR (Section 3.6.4) are two algorithms we’ve seen that do this.\nIn this chapter, we will explore policy gradient algorithms, which also iteratively improve a policy. The gradient of a function at a specific input point is the direction that increases the output value most rapidly. If we think of the expected total reward as a function of the policy, we can repeatedly shift the policy by the gradient of that function to obtain policies that achieve higher expected total rewards.\nPolicy gradient methods are responsible for groundbreaking applications including AlphaGo, OpenAI Five, and large language models. This chapter will explore:\nCode\n%load_ext autoreload\n%autoreload 2\nCode\nfrom utils import Array, Float, Callable, jax, jnp, rng, latex, gym\nfrom jaxtyping import UInt\nimport jax.random as jr\nimport functools as ft\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nrng = rng(184)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#introduction",
    "href": "pg.html#introduction",
    "title": "7  Policy Gradient Methods",
    "section": "",
    "text": "Examples of parameterized policies that can be expressed in terms of a finite set of parameters.\nGradient descent, a general optimization algorithm for differentiable functions.\nDifferent estimators of the policy gradient, enabling us to apply (stochastic) gradient descent to RL.\nProximal (policy) optimization techniques that ensure the steps taken are “not too large”. These are some of the most popular and widely used RL algorithms at the time of writing.\n\n\nRemark 7.1 (Conventions). Policy gradient algorithms typically optimize over stochastic policies (def. 2.6). In this chapter, we will only discuss the case of stochastic policies, though there exist policy gradient algorithms for deterministic policies as well (Silver et al., 2014).\nTo ease notation, we will treat the horizon \\(H\\) as finite and avoid adding a discount factor \\(\\gamma\\). We make the policy’s dependence on the timestep \\(h\\) implicit by assuming that the state space conveys information about the timestep. We also use\n\\[\nR(\\tau) := \\sum_{h=0}^{H-1} r_h\n\\tag{7.2}\\]\nto denote the total reward in a trajectory.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-parameterizations",
    "href": "pg.html#sec-parameterizations",
    "title": "7  Policy Gradient Methods",
    "section": "7.2 Parameterized policies",
    "text": "7.2 Parameterized policies\nOptimizing over the entire space of policies is usually intractable: there’s just too many! There are there are \\(|\\mathcal{A}|^{|\\mathcal{S}|}\\) deterministic mappings from state to actions. In continuous state spaces, \\(|\\mathcal{S}|\\) is infinite, so the space of policies becomes infinite-dimensional. Infinite-dimensional spaces are usually hard to optimize over. Instead, we choose a parameterized policy class with a finite number \\(D\\) of adjustable parameters. Each parameter vector corresponds to a mapping from states to actions. (We also discussed the notion of a parameterized function class in Section 5.3.2). The following examples seek to make this more concrete.\n\nExample 7.1 (Tabular representation) If both the state and action spaces are finite, perhaps we could simply learn a preference value \\(\\theta_{s,a}\\) for each state-action pair, resulting in a table of \\(|\\mathcal{S}| |\\mathcal{A}|\\) values. These are the \\(D\\) parameters of the policy. Then, for each state, to compute the distribution over actions, we perform a softmax operation: we exponentiate each of the values, and then normalize to form a valid distribution.\n\\[\n\\pi^\\text{softmax}_\\theta(a \\mid s) = \\frac{\\exp(\\theta_{s,a})}{\\sum_{s,a'} \\exp (\\theta_{s,a'})}.\n\\tag{7.3}\\]\nIn this way, we turn a vector of length \\(|\\mathcal{S}| |\\mathcal{A}|\\) into a mapping from \\(\\mathcal{S} \\to \\triangle(\\mathcal{A})\\). However, this doesn’t make use of any structure in the states or actions, so while this is flexible, it is also prone to overfitting. For small state and action spaces, it makes more sense to use a dynamic programming algorithm from Chapter 2.\n\nFor a given parameterization, such as the one above, we call the set of resulting policies the parameterized policy class, that is,\n\\[\n\\{ \\pi_\\theta \\mid \\theta \\in \\mathbb{R}^D \\},\n\\tag{7.4}\\]\nwhere \\(D\\) is the fixed number of parameters. Let us explore some more examples of parameterized policy classes.\n\nExample 7.2 (Linear in features) Another approach is to map each state-action pair into some feature space \\(\\phi(s, a) \\in \\mathbb{R}^D\\). Then, to map a feature vector to a probability, we take a linear combination of the features and take a softmax. The parameters \\(\\theta\\) are the coefficients of the linear combination:\n\\[\n\\pi^\\text{linear}_{\\theta}(a \\mid s) = \\frac{\\exp(\\theta^\\top \\phi(s, a))}{\\sum_{a'} \\exp(\\theta^\\top \\phi(s, a'))}.\n\\tag{7.5}\\]\nAnother interpretation is that \\(\\theta\\) represents the feature vector of the “desired” state-action pair, as state-action pairs whose features align closely with \\(\\theta\\) are given higher probability.\n\n\nExample 7.3 (Neural policies) More generally, we could map states and actions to unnormalized scores via some parameterized function \\(f_\\theta : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\), such as a neural network, and choose actions according to a softmax:\n\\[\n\\pi^\\text{neural}_\\theta(a \\mid s) = \\frac{\\exp(f_{\\theta}(s, a))}{\\sum_{a'} \\exp(f_{\\theta}(s, a'))}.\n\\tag{7.6}\\]\n\n\nRemark 7.2 (Why softmax). The three parameterized policy classes above all use the softmax operation. Of course, this isn’t the only way to turn a list of values into a probability distribution: for example, you could also subtract by the minimum value and divide by the range. Why is softmax preferred in practice? One reason is that it is a smooth, differentiable function of the input values. It is also nice to work with analytically, and has other interpretations from physics, where it is known as the Gibbs or Boltzmann distribution, and economics, where it is known as the Bradley-Terry model for ranking choices.\n\n\nExample 7.4 (Diagonal Gaussian policies for continuous action spaces) Consider an \\(N\\)-dimensional action space \\(\\mathcal{A} = \\mathbb{R}^{N}\\). Then for a stochastic policy, we could predict the mean action and then add some random noise to it. For example, we could use a linear model to predict the mean action and then add some noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\) to it.\n\\[\n\\pi_\\theta(\\cdot \\mid s) = \\mathcal{N}(\\theta^\\top \\phi(s), \\sigma^2 I).\n\\]\n\nNow we’ve seen some examples of parameterized policies. Optimizing over a parameterized policy class makes the policy optimization problem finite-dimensional:\n\\[\n\\begin{aligned}\n\\hat \\theta &= \\arg\\max_{\\theta \\in \\mathbb{R}^D} J(\\theta) \\\\\n\\text{where}\\quad J(\\theta) &:= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\sum_{h=0}^{H-1} r(s_h, a_h).\n\\end{aligned}\n\\tag{7.7}\\]\nThis enables us to apply one of the most popular and general optimization algorithms: gradient descent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-gd",
    "href": "pg.html#sec-gd",
    "title": "7  Policy Gradient Methods",
    "section": "7.3 Gradient descent",
    "text": "7.3 Gradient descent\nGradient descent is an optimization algorithm that can be applied to any differentiable function. That is, it is a tool for solving\n\\[\n\\arg\\min_{\\theta \\in \\mathbb{R}^D} J(\\theta),\n\\tag{7.8}\\]\nwhere \\(J(\\theta) \\in \\mathbb{R}\\) is the function to be minimized. For two-dimensional inputs, a suitable analogy for this algorithm is making your way down a hill (with no cliffs), where you keep taking steps in the steepest direction downwards from your current position. Your horizontal position \\((x, z)\\) is the input and your vertical position \\(y\\) is the function to be minimized. The slope of the mountain at your current position can be expressed using the gradient, written \\(\\nabla y(x, z) \\in \\mathbb{R}^2\\). This can be computed as the vector of partial derivatives,\n\\[\n\\nabla y(x, z) = \\begin{pmatrix}\n\\frac{\\partial y}{\\partial x} \\\\\n\\frac{\\partial y}{\\partial z}\n\\end{pmatrix}.\n\\tag{7.9}\\]\n\n\nCode\ndef f(x, y):\n    return 100.0 * (y - x**2)**2 + (1 - x)**2\n\n# Create a grid of points\nX, Y = jnp.mgrid[0:1:40j, 0:1:40j]\nZ = f(X, Y)\n\nfig, ax = plt.subplots(figsize=(4, 3))\ncontourf = ax.contourf(X, Y, Z, levels=20, alpha=0.6)\nfig.colorbar(contourf, ax=ax)\n\ntx, ty = 0.5, 0.8\ngx, gy = -0.001 * jnp.asarray(jax.grad(f, argnums=(0, 1))(tx, ty))\n\nax.arrow(tx, ty, gx, gy, fc='blue', ec='blue', width=0.01, head_width=0.05, head_length=0.1, zorder=5)\nax.scatter(tx, ty, color='red', s=100, zorder=5)\n\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.1: Example of gradient descent on the Rosenbrock function.\n\n\n\n\n\nTo calculate the slope (aka “directional derivative”) of the mountain in a given direction \\((\\Delta x, \\Delta z)\\), you take the dot product of the difference vector with the gradient:\n\\[\n\\Delta y = \\begin{pmatrix}\n\\Delta x \\\\ \\Delta z\n\\end{pmatrix}\n\\cdot\n\\nabla y(x, z),\n\\tag{7.10}\\]\nwhere \\(x, z\\) is your current position. What direction should we walk to go down the hill as quickly as possible? That is, we want to find the direction \\((\\Delta x, \\Delta z)\\) that minimizes the slope:\n\\[\n\\arg\\min_{\\Delta x, \\Delta z} \\Delta y.\n\\tag{7.11}\\]\nWe use the useful fact that, for a given vector \\(v\\), the direction \\(u\\) that minimizes the dot product \\(u \\cdot v\\) points in the opposite direction to \\(v\\). This should make intuitive sense, since \\(u \\cdot v = \\|u\\| \\|v\\| \\cos \\theta\\), where \\(\\theta\\) is the angle between \\(u\\) and \\(v\\), and the cosine function is minimized at \\(\\theta = \\pi\\). Applying this to eq. 7.10, we see that the direction of steepest decrease is opposite to the gradient. “Walking” in that direction corresponds to subtracting a multiple of the gradient from your current position:\n\\[\n\\begin{pmatrix}\nx^{t+1} \\\\ z^{t+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx^{t} \\\\ z^{t}\n\\end{pmatrix}\n-\n\\eta \\nabla y(x^{t}, z^{t})\n\\]\nwhere \\(t\\) denotes the iteration of the algorithm and \\(\\eta &gt; 0\\) is a “step size” hyperparameter that controls the size of the steps we take. (Note that we could also vary the step size across iterations, that is, \\(\\eta^0, \\dots, \\eta^T\\).)\n\nRemark 7.3 (Minimization vs maximization). Optimization problems are usually posed as minimization problems by convention. To solve a maximization problem with gradient descent, you could simply take steps in the direction of the positive gradient. This also corresponds to flipping the sign of the objective function.\n\nThe case of a two-dimensional input is easy to visualize. The analogy to climbing a hill (if we were maximizing the function) is why gradient descent is sometimes called hill climbing and the graph of the objective function is called the loss landscape. (The term “loss” comes from supervised learning, which you can read about in Chapter 5.) But this idea can be straightforwardly extended to higher-dimensional inputs. From now on, we’ll use \\(J\\) to denote the function we’re trying to maximize, and \\(\\theta\\) to denote the parameters being optimized over. (In the above example, \\(\\theta = \\begin{pmatrix} x & z \\end{pmatrix}^\\top\\)). Let’s summarize the algorithm in general:\n\nDefinition 7.1 (Gradient descent) Suppose we are trying to solve the optimization problem\n\\[\n\\theta^\\star = \\arg\\min_{\\theta \\in \\mathbb{R}^D} J(\\theta),\n\\tag{7.12}\\]\nwhere \\(J(\\theta) \\in \\mathbb{R}\\) is the differentiable function to be minimized. Gradient descent starts with an initial guess \\(\\theta^0\\) and then takes steps in the direction of \\(- \\nabla J(\\theta^t)\\), where \\(\\theta^t\\) is the current iterate:\n\\[\n\\theta^{t+1} = \\theta^t - \\eta \\nabla J(\\theta^t).\n\\tag{7.13}\\]\nNote that we scale \\(\\nabla J(\\theta^t)\\) by the step size \\(\\eta &gt; 0\\), also known as the learning rate.\n\n\n\nCode\ndef gradient_descent(\n    theta_init: Float[Array, \" D\"],\n    objective: Callable[[Float[Array, \" D\"]], float],\n    eta: float,\n    n_steps: UInt,\n):\n    # `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.\n    theta = theta_init\n    history = [theta]\n    for step in range(n_steps):\n        theta = theta + eta * jax.grad(objective)(theta)\n        history.append(theta)\n    return theta, jnp.array(history)\n\nlatex(gradient_descent, id_to_latex={\"objective\": \"J\"})\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{gradient\\_descent}(\\theta_{\\mathrm{init}}: \\mathbb{R}^{D}, J: (\\mathbb{R}^{D}) \\rightarrow \\mathbb{R}, \\eta: \\mathbb{R}, n_{\\mathrm{steps}}: \\mathbb{N}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathrm{history} \\gets \\mathopen{}\\left[ \\theta \\mathclose{}\\right] \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{step} \\in \\mathrm{range} \\mathopen{}\\left( n_{\\mathrm{steps}} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\theta \\gets \\theta + \\eta \\cdot \\nabla \\mathopen{}\\left(J\\mathclose{}\\right) \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{history}.\\mathrm{append} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\mathopen{}\\left( \\theta, \\mathrm{jnp}.\\mathrm{array} \\mathopen{}\\left( \\mathrm{history} \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 7.2: Pseudocode for gradient descent.\n\n\n\n\nNotice that the parameters will stop changing once \\(\\nabla J(\\theta) = 0\\). Once we reach this stationary point, our current parameters are ‘locally optimal’ (assuming we’re at a local minimum): it’s impossible to increase the function by moving in any direction. If \\(J\\) is convex (i.e. the function looks like an upward-curved bowl), then the only point where this happens is at the global optimum. Otherwise, if \\(J\\) is nonconvex, the best we can hope for is a local optimum. We won’t go deeper into the theory of convex functions here. For more details, refer to the textbook of Boyd & Vandenberghe (2004).\n\nRemark 7.4 (Limitations of gradient descent). Gradient descent and its variants are responsible for most of the major achievements in modern machine learning. It is important to note, however, that for many problems, gradient descent is not a good optimization algorithm to reach for! If you have more information about the problem, such as access to second derivatives, you can apply more powerful optimization algorithms that converge much more rapidly. Read Nocedal & Wright (2006) if you’re curious about the field of numerical optimization.\n\n\n7.3.1 Computing derivatives\nHow does a computer compute the gradient of a function?\nOne way is symbolic differentiation, which is similar to the way you might compute it by hand: the computer applies a list of rules to transform the symbols involved. Python’s sympy package (Meurer et al., 2017) supports symbolic differentiation. However, functions implemented as algorithms in code may not always have a straightforward symbolic representation.\nAnother way is numerical differentiation, which is based on the limit definition of a (directional) derivative:\n\\[\n\\nabla_{\\boldsymbol{u}} J(\\boldsymbol{x}) = \\lim_{\\varepsilon \\to 0}\n\\frac{J(\\boldsymbol{x} + \\varepsilon \\boldsymbol{u}) - J(\\boldsymbol{x})}{\\varepsilon}\n\\tag{7.14}\\]\nThen, we can substitute a small value of \\(\\varepsilon\\) on the r.h.s. to approximate the directional derivative. How small, though? Depending on how smooth the function is and how accurate our estimate needs to be, we may need such a small value of \\(\\varepsilon\\) that typical computers will run into rounding errors. Also, to compute the full gradient, we would need to compute the r.h.s. once for each input dimension. This is an issue if computing \\(J\\) is expensive.\nAutomatic differentiation achieves the best of both worlds. Like symbolic differentiation, we manually implement the derivative rules for a few basic operations. However, instead of executing these on the symbols, we execute them on the values when the function gets called, like in numerical differentiation. This allows us to differentiate through programming constructs such as branches or loops, and doesn’t involve any arbitrarily small values. Baydin et al. (2018) provides an accessible survey of automatic differentiation. At the time of writing, all of the popular Python libraries for machine learning, such as PyTorch (Ansel et al., 2024) and Jax (Bradbury et al., 2018), use automatic differentiation.\n\n\n7.3.2 Stochastic gradient descent\nIn real applications, computing the gradient of the target function is not so simple. As an example from supervised learning, \\(J(\\theta)\\) might be the sum of squared prediction errors across an entire training dataset. If our dataset is very large, it might not fit into our computer’s memory, making it impossible to evaluate \\(\\nabla J(\\theta)\\) at once. We will see that computing the exact gradient in RL faces a similar challenge where computing the gradient would require computing a complicated integral.\nIn these cases, we can compute some gradient estimate\n\\[\ng_x(\\theta) \\approx \\nabla J(\\theta)\n\\tag{7.15}\\]\nof the gradient at each step, using some observed data \\(x\\), and walk in that direction instead. This is called stochastic gradient descent. In the SL example above, we might randomly choose a minibatch of samples and use them to estimate the true prediction error.\n\n\nCode\ndef sgd(\n    rng: jr.PRNGKey,\n    theta_init: Float[Array, \" D\"],\n    estimate_gradient: Callable[[jr.PRNGKey, Float[Array, \" D\"]], Float[Array, \" D\"]],\n    eta: float,\n    n_steps: int,\n):\n    # Perform `n_steps` steps of SGD.\n\n    # `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.\n    theta = theta_init\n    rngs = jr.split(rng, n_steps)\n    history = [theta]\n    for step in range(n_steps):\n        theta = theta + eta * estimate_gradient(rngs[step], theta)\n        history.append(theta)\n    return theta, jnp.array(history)\n\n\nlatex(sgd, id_to_latex={\"estimate_gradient\": \"g\"})\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{sgd}(\\mathrm{rng}: \\mathrm{jr}.\\mathrm{PRNGKey}, \\theta_{\\mathrm{init}}: \\mathbb{R}^{D}, g: (\\mathrm{jr}.\\mathrm{PRNGKey} \\times \\mathbb{R}^{D}) \\rightarrow \\mathbb{R}^{D}, \\eta: \\mathbb{R}, n_{\\mathrm{steps}}: \\mathbb{Z}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathrm{rngs} \\gets \\mathrm{jr}.\\mathrm{split} \\mathopen{}\\left( \\mathrm{rng}, n_{\\mathrm{steps}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathrm{history} \\gets \\mathopen{}\\left[ \\theta \\mathclose{}\\right] \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{step} \\in \\mathrm{range} \\mathopen{}\\left( n_{\\mathrm{steps}} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\theta \\gets \\theta + \\eta \\cdot g \\mathopen{}\\left( \\mathrm{rngs}_{\\mathrm{step}}, \\theta \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{history}.\\mathrm{append} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\mathopen{}\\left( \\theta, \\mathrm{jnp}.\\mathrm{array} \\mathopen{}\\left( \\mathrm{history} \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 7.3: Pseudocode for stochastic gradient descent.\n\n\n\n\n\nCode\nX = 2 * jr.normal(next(rng), (100, 2))\nw_true = jnp.ones(2)\ny = X @ w_true\nw_init = jnp.zeros(2)\nlr = 0.1\nn_steps = 10\n\ndef estimate_gradient(rng, theta, batch_size):\n    batch_idx = jr.randint(rng, batch_size, 0, X.shape[0])\n    X_batch = X[batch_idx]\n    y_batch = y[batch_idx]\n    return - (1 / batch_size) * X_batch.T @ (X_batch @ theta - y_batch)\n\ndef loss(theta):\n    return - jnp.mean((X @ theta - y) ** 2)\n\n# loss contour\nw_lim = slice(-0.2, 1.5, 20j)\nw0, w1 = jnp.mgrid[w_lim, w_lim]\nthetas = jnp.stack([w0, w1], axis=2)\nz = jax.vmap(jax.vmap(loss))(thetas)\n\ndef plot_history(history):\n    plt.contourf(w0, w1, z, levels=10, alpha=0.3)\n    for i in range(len(history) - 1):\n        plt.arrow(history[i, 0], history[i, 1], history[i+1, 0] - history[i, 0], history[i+1, 1] - history[i, 1],\n                width=0.03, fc='black', ec='black', zorder=5)\n    plt.xlabel(r\"$w_0$\")\n    plt.ylabel(r\"$w_1$\")\n    plt.xlim(w_lim.start, w_lim.stop)\n    plt.ylim(w_lim.start, w_lim.stop)\n    plt.gcf().set_size_inches((3, 2))\n    plt.show()\n\n# also plot full gradient\n_, history_gd = gradient_descent(w_init, loss, lr, n_steps)\n_, history_sgd_1 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=1), lr, n_steps)\n_, history_sgd_5 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=5), lr, n_steps)\n_, history_sgd_10 = sgd(next(rng), w_init, ft.partial(estimate_gradient, batch_size=10), lr, n_steps)\n\nplot_history(history_gd)\nplot_history(history_sgd_1)\nplot_history(history_sgd_5)\nplot_history(history_sgd_10)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Full-batch GA.\n\n\n\n\n\n\n\n\n\n\n\n(b) SGD with a batch size of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) SGD with a batch size of 5.\n\n\n\n\n\n\n\n\n\n\n\n(d) SGD with a batch size of 10.\n\n\n\n\n\n\n\nFigure 7.4: GA and SGD on a two-dimensional convex optimization task. Note that a larger batch size reduces the variance in the step direction.\n\n\n\nWhat makes one gradient estimator better than another? Ideally, we want this estimator to be unbiased; that is, on average, it matches a single true gradient step.\n\nDefinition 7.2 (Unbiased gradient estimator) We call the gradient estimator \\(g\\) unbiased if, for all \\(\\theta \\in \\mathbb{R}^D\\),\n\\[\n\\mathop{\\mathbb{E}}_{x \\sim p_\\theta} [g_x(\\theta)] = \\nabla J(\\theta),\n\\tag{7.16}\\]\nwhere \\(p_\\theta\\) denotes the distribution of the observed data \\(x\\).\n\nWe also want the variance of the estimator to be low so that its performance doesn’t change drastically at each step.\nWe can actually show that, for many “nice” functions, in a finite number of steps, SGD will find a \\(\\theta\\) that is “close” to a stationary point. In another perspective, for such functions, the local “landscape” of \\(J\\) around \\(\\theta\\) becomes flatter and flatter the longer we run SGD.\n\nTheorem 7.1 (SGD convergence) More formally, suppose we run SGD for \\(K\\) steps, using an unbiased gradient estimator. Let the step size \\(\\eta^k\\) scale as \\(O(1/\\sqrt{k}).\\) Then if \\(J\\) is bounded and \\(\\beta\\)-smooth (see below), and the norm of the gradient estimator has a bounded second moment \\(\\sigma^2,\\)\n\\[\n\\|\\nabla J({\\theta^i})\\|^2 \\le O \\left( M \\beta \\sigma^2 / K\\right).\n\\]\nWe call a function \\(\\beta\\)-smooth if its gradient is Lipschitz continuous with constant \\(\\beta\\):\n\\[\n\\|\\nabla J(\\theta) - \\nabla J(\\theta')\\| \\le \\beta \\|\\theta - \\theta'\\|.\n\\]\n\nWe’ll now see a concrete application of stochastic gradient descent in the context of policy optimization.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-pg-intro",
    "href": "pg.html#sec-pg-intro",
    "title": "7  Policy Gradient Methods",
    "section": "7.4 Policy (stochastic) gradient descent",
    "text": "7.4 Policy (stochastic) gradient descent\nPolicy gradient methods boil down to applying gradient descent to the policy optimization problem for a chosen parameterized policy class (eq. 7.4):\n\\[\n\\theta^{t+1} = \\theta^t + \\eta \\nabla J(\\theta^t)\n\\quad \\text{where} \\quad\nJ(\\theta) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\sum_{h=0}^{H-1} r(s_h, a_h) \\right].\n\\tag{7.17}\\]\nThe challenges lie mostly in computing unbiased gradient estimators (def. 7.2) and in constraining the size of each update to improve stability of the learning algorithm.\nTo provide some intuition for the gradient estimators we will later derive, the following section constructs a general policy gradient algorithm in a “bottom-up” way. The remainder of the chapter focuses on methods for improving the stability of the policy gradient algorithm.\n\nRemark 7.5 (On-policy). Note that since the gradient estimator is computed using data collected by the current policy, policy gradient algorithms are generally on-policy (def. 6.3). Note that on-policy algorithms generally suffer from worse sample efficiency than off-policy algorithms: while off-policy algorithms (such as Q-learning (Section 6.2.2)) can use data collected by any method, on-policy algorithms can only make use of data from the current policy.\n\n\n7.4.1 Introduction to policy gradient methods\nIn this section, we’ll intuitively construct an iterative algorithm for improving the parameters. We’ll build up to true policy gradient estimators by considering a series of update rules.\nRemember that in RL, the primary goal is to find the optimal policy that achieves the highest total reward. Put simply, we want policies that take better actions more often. Breaking that down, we need\n\na way to shift the policy to make certain actions more likely, and\na way to measure how good an action is in a given state.\n\nLet’s tackle the first task: getting the policy to take action \\(a\\) in state \\(s\\). Back in the tabular setting, for a deterministic policy \\(\\pi\\), we simply assigned \\(\\pi(s) \\gets a\\). Now that we’re using a parameterized policy class, we can’t directly assign a value to \\(\\pi(s)\\). Instead, we adjust the parameters \\(\\theta\\) of the policy to maximize the probability of taking \\(a\\):\n\\[\n\\hat \\theta = \\arg\\max_{\\theta} \\pi_{\\theta}(a \\mid s).\n\\tag{7.18}\\]\nAssuming the output of the policy is differentiable with respect to its parameters, we can apply gradient descent:\n\\[\n\\begin{aligned}\n\\theta^{t+1} &= \\theta^t + \\eta g_{s, a}(\\theta^t) \\\\\n\\text{where} \\quad g_{s, a}(\\theta) &:= \\nabla \\pi_{\\theta}(a \\mid s).\n\\end{aligned}\n\\tag{7.19}\\]\nThe notation \\(g\\) is chosen to remind you of a gradient estimator (eq. 7.15). We will draw the relationship between our intuitive approach and SGD more concretely later on.\nNow let’s approach the second task: how do we choose which actions to take more often? Suppose we have some random variable \\(\\psi\\) (that’s the Greek letter “psi”) that is correlated with how “good” action \\(a\\) is in state \\(s\\). Later in the chapter, we’ll explore concrete choices for \\(\\psi\\), but for now, we’ll leave its identity a mystery. Then, to update the policy, we could sample an action \\(a \\sim \\pi_{\\theta^i}(\\cdot \\mid s)\\), and weight the corresponding gradient by \\(\\psi\\). We will therefore call \\(\\psi\\) the gradient coefficient.\n\\[\ng_{s, a}(\\theta) := \\psi \\nabla \\pi_{\\theta}(a \\mid s).\n\\tag{7.20}\\]\nTo illustrate this, suppose the policy takes one action \\(a_0\\) and obtains a coefficient of \\(\\psi_0 = 2\\) and then takes a second action \\(a_1\\) with a coefficient of \\(\\psi_1 = -1\\). Then we would update \\(\\theta \\gets \\theta + 2 \\eta \\nabla \\pi_{\\theta}(a_0 \\mid s)\\) and then \\(\\theta \\gets \\theta - \\eta \\nabla \\pi_{\\theta}(a_1 \\mid s)\\).\n\nExercise 7.1 (An alternative approach) Compare this with the policy iteration update (Section 2.4.4.2), where we updated the deterministic policy according to \\(\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} Q^\\pi(s, a)\\). In our current setting, why not just solve for \\(\\theta^{t+1} = \\arg\\max_{\\theta} \\pi_{\\theta}(a^\\star \\mid s)\\), where \\(a^\\star = \\arg\\max_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta^i}}(a \\mid s)\\), instead of sampling an action from our policy? What type of action space does this approach assume? What does the added stochasticity grant you?\n\nBut the gradient estimator eq. 7.20 has an issue: the amount that we encourage action \\(a\\) depends on how often the policy takes it. This could lead to a positive feedback loop where the most common action becomes more and more likely, regardless of its quality. To cancel out this factor, we divide by the action’s likelihood:\n\\[\n\\begin{aligned}\ng_{s, a}(\\theta) &:= \\psi \\frac{\\nabla \\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta}(a \\mid s)} \\\\\n&= \\psi \\nabla \\log \\pi_{\\theta}(a \\mid s).\n\\end{aligned}\n\\tag{7.21}\\]\nNow we can extend this across the entire time horizon. Suppose we use \\(\\pi_{\\theta^i}\\) to roll out a trajectory \\(\\tau = (s_0, a_0, \\dots, s_{H-1}, a_{H-1})\\) and compute eq. 7.21 at each step of the trajectory. We compute a gradient coefficient at each timestep, so we denote each instance by \\(\\psi_h(\\tau)\\).\n\\[\ng_\\tau(\\theta)\n:=\n\\sum_{h=0}^{H-1}\n\\psi_h(\\tau) \\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\tag{7.22}\\]\nTo reduce the variance, we could roll out multiple trajectories, and average the gradient steps across them. This gives us the general form of the policy gradient algorithm:\n\nDefinition 7.3 (General policy gradient algorithm) Suppose we are given an expression for the gradient coefficients \\(\\psi_h(\\tau)\\). Then we can perform policy gradient optimization as follows.\nAt each iteration \\(t = 0, \\dots, T-1\\) of the algorithm, we sample \\(N\\) trajectories \\(\\tau^n = (s^n_0, a^n_0, r^n_0, \\dots, s^n_{H-1}, a^n_{H-1}, r^n_{H-1})\\), and compute the update rule\n\\[\n\\begin{aligned}\n\\theta^{t+1} &= \\theta^t + \\eta \\frac{1}{N} \\sum_{n=1}^N g_{\\tau^n}(\\theta^t) \\\\\n\\text{where} \\quad g_{\\tau}(\\theta) &= \\sum_{h=0}^{H-1}\n\\psi_h(\\tau) \\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.23}\\]\n\nThis algorithm allows us to optimize a policy by sampling trajectories from it and computing the gradient-log-likelihoods (sometimes called the scores) of the chosen actions. Then we can update the parameters \\(\\theta\\) in the direction given by eq. 7.23 to obtain a new policy that chooses better actions more often.\n\n\nCode\ndef policy_gradient(env: gym.Env, pi, theta: Float[Array, \" D\"], get_psi: Callable[[list[\"Transition\"]], Float[Array, \" H\"]]):\n    \"\"\"Estimate the policy gradient using REINFORCE.\"\"\"\n    g = jnp.zeros_like(theta)\n    tau = sample_trajectory(env, pi(theta))\n    psis = get_psi(pi(theta), tau)\n    for (s, a, r), psi in zip(tau, psis):\n        def policy_log_likelihood(theta: Float[Array, \" D\"]) -&gt; float:\n            return log(pi(theta)(s, a))\n        g += psi * jax.grad(policy_log_likelihood)(theta)\n    return g\n\nlatex(policy_gradient, id_to_latex={\"jax.grad\": r\"\\nabla\"})\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{policy\\_gradient}(\\mathrm{env}: \\mathrm{gym}.\\mathrm{Env}, \\pi, \\theta: \\mathbb{R}^{D}, \\mathrm{get\\_psi}: (\\mathrm{list}_{\\textrm{\"Transition\"}}) \\rightarrow \\mathbb{R}^{H}) \\\\ \\hspace{1em} \\textrm{\"Estimate the policy gradient using REINFORCE.\"} \\\\ \\hspace{1em} g \\gets \\mathrm{jnp}.\\mathrm{zeros\\_like} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\tau \\gets \\mathrm{sample\\_trajectory} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathrm{psis} \\gets \\mathrm{get\\_psi} \\mathopen{}\\left( \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), \\tau \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathopen{}\\left( \\mathopen{}\\left( s, a, r \\mathclose{}\\right), \\psi \\mathclose{}\\right) \\in \\mathrm{zip} \\mathopen{}\\left( \\tau, \\mathrm{psis} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{policy\\_log\\_likelihood}(\\theta: \\mathbb{R}^{D}) \\\\ \\hspace{3em} \\mathbf{return} \\ \\log \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} g \\gets g + \\psi \\cdot \\nabla \\mathopen{}\\left(\\mathrm{policy\\_log\\_likelihood}\\mathclose{}\\right) \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ g \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 7.5: Pseudocode for the general policy gradient algorithm.\n\n\n\n\n\nRemark 7.6 (Summary of intuitive derivation). Let us review how we arrived at this expression:\n\nWe take the gradient of the policy to encourage certain actions (eq. 7.19).\nWe encourage actions proportionally to their advantage (eq. 7.20).\nWe correct for the policy’s sampling distribution (eq. 7.21).\nWe extend this to each step of the trajectory (eq. 7.22).\nWe sample multiple trajectories to reduce variance (eq. 7.23).\n\nThe last piece is to figure out what \\(\\psi\\) stands for.\n\n\nExercise 7.2 (Brainstorming) Can you think of any possibilities? \\(\\psi_h(\\tau)\\) should correlate with the quality of the action taken at time \\(h\\). It may depend on the current policy \\(\\pi_{\\theta^i}\\) or any component of the trajectory \\((s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1})\\).\n\nWe won’t keep you waiting: it turns out that if we set \\(\\psi_h(\\tau)\\) to\n\n\\(R(\\tau)\\) (the total reward of the trajectory) (eq. 7.25),\n\\(\\sum_{h'=h}^{H-1} r(s_{h'}, a_{h'})\\) (the remaining reward in the trajectory),\n\\(Q^{\\pi_{\\theta^i}}(s_h, a_h)\\) (the current policy’s Q-function), or\n\\(A^{\\pi_{\\theta^i}}(s_h, a_h)\\) (the current policy’s advantage function),\n\namong other possibilities, the gradient term of eq. 7.23 is actually an unbiased estimator (def. 7.2) of the true “policy gradient” \\(\\nabla J(\\theta)\\) (see eq. 7.7). That is, for any of the \\(\\psi\\) above, updating the parameters according to the general policy gradient algorithm eq. 7.23 is (minibatch) stochastic gradient descent on the expected total reward \\(J(\\theta)\\), with the gradient estimator\n\\[\n\\begin{aligned}\ng_{\\tau}(\\theta)\n&=\n\\sum_{h=0}^{H-1}\n\\psi_h(\\tau) \\nabla \\log \\pi_{\\theta}(a_h\\mid s_h) \\\\\n\\text{where} \\quad\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}}[ g_{\\tau}(\\theta) ]\n&=\n\\nabla J(\\theta).\n\\end{aligned}\n\\tag{7.24}\\]\n\n\n7.4.2 The REINFORCE policy gradient\nWe begin by showing that setting \\(\\psi_h= \\sum_{h'=0}^{H-1} r_{h'}\\), i.e. the total reward of the trajectory, provides an unbiased gradient estimator.\n\nTheorem 7.2 (Using the total reward is unbiased) Substituting\n\\[\n\\psi_h(\\tau) := R(\\tau) := \\sum_{h'=0}^{H-1} r_{h'}\n\\tag{7.25}\\]\ninto the general policy gradient estimator eq. 7.23 gives an unbiased estimator. That is,\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ g^\\text{R}_\\tau(\\theta) ] \\\\\n\\text{where} \\quad\ng^\\text{R}_{\\tau} (\\theta)\n&:=\n\\sum_{h=0}^{H-1}\nR(\\tau)\n\\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.26}\\]\n\nThe “R” stands for REINFORCE, which stands for “REward Increment \\(=\\) Nonnegative Factor \\(\\times\\) Offset Reinforcement \\(\\times\\) Characteristic Eligibility” (Williams, 1992, p. 234). (We will not elaborate further on this etymology.)\n\nProof (Proof via calculus). As our first step towards constructing an unbiased policy gradient estimator, let us simplify the expression\n\\[\n\\nabla J(\\theta) = \\nabla \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [R(\\tau)].\n\\tag{7.27}\\]\nIn supervised learning, we were able to swap the gradient and expectation. That was because the function being averaged depended on the parameters, not the distribution itself:\n\\[\n\\nabla \\mathop{\\mathbb{E}}_{(x, y) \\sim p}[\n    L(f_\\theta(x), y)\n] = \\mathop{\\mathbb{E}}_{(x, y) \\sim p} [\\nabla L(f_\\theta(x), y) ].\n\\tag{7.28}\\]\nHere, though, the distribution depends on the parameters, and the function being averaged does not. One way to compute this type of derivative is to use the identity\n\\[\n\\nabla \\rho^{\\pi_\\theta}(\\tau)\n=\n\\rho^{\\pi_\\theta}(\\tau) \\nabla \\log \\rho^{\\pi_\\theta}(\\tau).\n\\tag{7.29}\\]\nBy expanding the definition of expected value, we can compute the correct value to be\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) & = \\nabla \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [R(\\tau)] \\\\\n& = \\int \\nabla \\rho^{\\pi_\\theta}(\\tau) R(\\tau) \\\\\n& = \\int \\rho^{\\pi_\\theta}(\\tau) \\nabla \\log \\rho^{\\pi_\\theta}(\\tau) R(\\tau) \\\\\n& = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ \\nabla \\log \\rho^{\\pi_\\theta}(\\tau) R(\\tau) ].\n\\end{aligned}\n\\tag{7.30}\\]\nNow we deal with the \\(\\nabla \\log \\rho^{\\pi_\\theta}(\\tau)\\) term, that is, the gradient-log-likelihood (aka score) of the trajectory. Recall Theorem 2.1, in which we showed that when the state transitions are Markov (i.e. \\(s_{h}\\) only depends on \\(s_{h-1}, a_{h-1}\\)) and the policy is history-independent (i.e. \\(a_h\\sim \\pi_\\theta (\\cdot \\mid s_h)\\)), we can autoregressively write out the likelihood of a trajectory under the policy \\(\\pi_\\theta\\). Taking the log of the trajectory likelihood turns the products into sums:\n\\[\n\\log \\rho^{\\pi_\\theta}(\\tau) = \\log P_0(s_0) + \\sum_{h=0}^{H-1} \\Big(\n    \\log \\pi_\\theta(a_h\\mid s_h) + \\log P(s_{h+1} \\mid s_h, a_h)\n\\Big)\n\\tag{7.31}\\]\nWhen we take the gradient with respect to the parameters \\(\\theta\\), only the \\(\\log \\pi_\\theta(a_h\\mid s_h)\\) terms depend on \\(\\theta\\):\n\\[\n\\nabla \\log \\rho^{\\pi_\\theta}(\\tau)\n=\n\\sum_{h=0}^{H-1} \\nabla \\log \\pi_\\theta(a_h\\mid s_h).\n\\tag{7.32}\\]\nSubstituting this into eq. 7.30 gives\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}}\n\\left[\n    \\sum_{h=0}^{H-1} R(\\tau) \\nabla \\log \\pi_\\theta(a_h\\mid s_h)\n\\right] \\\\\n&= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [g^\\text{R}_\\theta(\\tau)],\n\\end{aligned}\n\\]\nshowing that the REINFORCE policy gradient (eq. 7.25) is unbiased.\n\n\nProof (Proof via importance sampling.). Another way of deriving Theorem 7.2 involves a technique known as importance sampling. We’ll demonstrate this approach here since it will come in handy later on. Importance sampling is useful when we want to estimate an expectation over a distribution that is hard to sample from. Instead, we can sample from a different distribution that supports the same values, and then reweight the samples according to the likelihood ratio between the two distributions.\n\nTheorem 7.3 (Importance sampling) Consider some random variable \\(x \\in \\mathcal{X}\\) with density function \\(p\\). Let \\(q\\) be the density function of another distribution on \\(\\mathcal{X}\\) that supports all of \\(p\\), that is, \\(q(x) = 0\\) only if \\(p(x) = 0\\). Then\n\\[\n\\mathop{\\mathbb{E}}_{x \\sim p}[f(x)] = \\mathop{\\mathbb{E}}_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} f(x) \\right].\n\\tag{7.33}\\]\n\n\nProof. We expand the definition of expected value:\n\\[\n\\begin{aligned}\n\\mathop{\\mathbb{E}}_{x \\sim p}[f(x)] &= \\sum_{x \\in \\mathcal{X}} f(x) p(x) \\\\\n&= \\sum_{x \\in \\mathcal{X}} f(x) \\frac{p(x)}{q(x)} q(x) \\\\\n&= \\mathop{\\mathbb{E}}_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} f(x) \\right].\n\\end{aligned}\n\\tag{7.34}\\]\n\n\nExercise 7.3 (Importance sampling for a biased coin) Suppose you are a student and you determine your study routine by flipping a biased coin. Let \\(x \\in \\{ \\text{heads}, \\text{tails} \\}\\) be the result of the coin flip. The coin shows heads twice as often as it shows tails:\n\\[\np(x) = \\begin{cases}\n2/3 & x = \\text{heads} \\\\\n1/3 & x = \\text{tails}.\n\\end{cases}\n\\tag{7.35}\\]\nSuppose you study for \\(f(x)\\) hours, where\n\\[\nf(x) = \\begin{cases}\n1 & x = \\text{heads} \\\\\n2 & x = \\text{tails}.\n\\end{cases}\n\\tag{7.36}\\]\nOne day, you lose your coin, and have to replace it with a fair one, i.e.\n\\[\nq(x) = 1/2,\n\\tag{7.37}\\]\nbut you want to study for the same amount on average. Suppose you decide to do this by importance sampling with the new coin. Now, upon flipping heads or tails, you study for\n\\[\n\\begin{aligned}\n\\frac{p(\\text{heads})}{q(\\text{heads})} f(\\text{heads}) = \\frac{4}{3} \\\\\n\\frac{p(\\text{tails})}{q(\\text{tails})} f(\\text{tails}) = \\frac{2}{3}\n\\end{aligned}\n\\tag{7.38}\\]\nhours respectively. Verify that your expected time spent studying is the same as before. Now compute the variance in the time you spend studying. Does it change?\n\nReturning to the RL setting, we can compute the policy gradient by importance sampling from any trajectory distribution \\(\\rho\\). (All gradients are being taken with respect to \\(\\theta\\).)\n\\[\n\\begin{aligned}\n    \\nabla J(\\theta) & = \\nabla \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ R(\\tau) ] \\\\\n                     & = \\nabla \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho} \\left[ \\frac{\\rho^{\\pi_\\theta}(\\tau)}{\\rho(\\tau)} R(\\tau) \\right] \\\\\n                     & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho} \\left[ \\frac{\\nabla \\rho^{\\pi_\\theta}(\\tau)}{\\rho(\\tau)} R(\\tau) \\right].\n\\end{aligned}\n\\]\nSetting \\(\\rho = \\rho^{\\pi_\\theta}\\) reveals eq. 7.30, and we can then proceed as we did in the previous proof.\n\nLet us reiterate some intuition into how this method works. Recall that we update our parameters according to\n\\[\n\\begin{aligned}\n    \\theta_{t+1} &= \\theta_t + \\eta \\nabla J(\\theta_t) \\\\\n    &= \\theta^t + \\eta \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\theta_t}} [\\nabla \\log \\rho^{\\theta^t}(\\tau) R(\\tau)].\n\\end{aligned}\n\\tag{7.39}\\]\nConsider the “good” trajectories where \\(R(\\tau)\\) is large. Then \\(\\theta\\) gets updated so that these trajectories become more likely. To see why, recall that \\(\\log \\rho^\\theta(\\tau)\\) is the log-likelihood of the trajectory \\(\\tau\\) under the policy \\(\\pi_\\theta\\), so the gradient points in the direction that makes \\(\\tau\\) more likely.\nHowever, the REINFORCE gradient estimator \\(g^\\text{R}_\\tau(\\theta)\\) has large variance. Intuitively, this is because it uses the total reward from the entire trajectory, which depends on the entire sequence of interactions with the environment, each step of which introducesrandomness. The rest of this chapter investigates ways to find lower-variance policy gradient estimators.\n\n\n7.4.3 Baselines and advantages\nA central idea from statistical learning is the bias-variance decomposition, which shows that the mean squared error of an estimator is the sum of its squared bias and its variance. All of the policy gradient estimators we will see in this chapter are already unbiased, i.e., their mean over trajectories equals the true policy gradient (def. 7.2). Can we construct estimators with lower variance as well?\nAs a first step, note that the action taken at step \\(h\\) does not causally affect the reward from previous timesteps, since they’re already in the past. So we should only use the reward from the current timestep onwards to estimate the policy gradient.\n\nTheorem 7.4 (Using the remaining reward is unbiased) Substituting the reamining reward \\(\\sum_{h'=h}^{H-1} r_{h'}\\) for \\(\\psi_h(\\tau)\\) into the general policy gradient estimator eq. 7.23 gives an unbiased estimator. That is,\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ g^\\text{rem}_\\tau(\\theta) ] \\\\\n\\text{where} \\quad\ng^\\text{rem}_{\\tau} (\\theta)\n&:=\n\\sum_{h=0}^{H-1}\n\\left( \\sum_{h'=h}^{H-1} r_{h'} \\right)\n\\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.40}\\]\n\n\nExercise 7.4 (Unbiasedness of remaining reward estimator) We leave the proof of Theorem 7.4 as an exercise.\n\nBy a conditioning argument, we can replace the remaining reward with the policy’s Q-function, evaluated at the current state. By the same reasoning as above, this also reduces the variance, since the only stochasticity in the expression \\(Q^{\\pi_\\theta}(s_h, a_h)\\) comes from the current state and action.\n\nTheorem 7.5 (Using the Q function is unbiased) Substituting \\(Q^{\\pi_\\theta}(s_h, a_h)\\) for \\(\\psi_h(\\tau)\\) into the general policy gradient estimator eq. 7.23 gives an unbiased estimator. That is,\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ g^\\text{Q}_\\tau(\\theta) ] \\\\\n\\text{where} \\quad\ng^\\text{Q}_{\\tau} (\\theta)\n&:=\n\\sum_{h=0}^{H-1}\nQ^{\\pi_\\theta}(s_h, a_h)\n\\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.41}\\]\n\n\nExercise 7.5 (Unbiasedness of remaining reward estimator) We also leave the proof of Theorem 7.5 as an exercise.\n\nWe can further reduce variance by subtracting a baseline function \\(b_\\theta : \\mathcal{S} \\to \\mathbb{R}\\). Note that this function could also depend on the current policy parameters.\n\nTheorem 7.6 (Subtracting a baseline function preserves unbiasedness) Let \\(b_\\theta : \\mathcal{S} \\to \\mathbb{R}\\) be some baseline function, and let \\(\\psi\\) be a gradient coefficient function that yields an unbiased policy gradient estimator (e.g. eq. 7.25 or eq. 7.43). Substituting\n\\[\n\\psi^{\\text{bl}}_h(\\tau) := \\psi_h(\\tau) - b_\\theta(s_h)\n\\tag{7.42}\\]\ninto the general policy gradient estimator eq. 7.23 gives an unbiased policy gradient estimator. That is,\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ g^\\text{bl}_\\tau(\\theta) ] \\\\\n\\text{where} \\quad\ng^\\text{bl}_{\\tau} (\\theta)\n&:=\n\\sum_{h=0}^{H-1}\n(\\psi_h(\\tau) - b_\\theta(s_h))\n\\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.43}\\]\n\n\nExercise 7.6 (Unbiasedness of baseline estimator) We leave the proof of Theorem 7.6 as an exercise as well.\n\nFor example, we might want \\(b_h\\) to estimate the average remaining reward at a given timestep:\n\\[\nb_\\theta(s_h) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[\n    \\sum_{h'=h}^{H-1} r_{h'}\n\\right].\n\\tag{7.44}\\]\nAs a better baseline, we could instead choose the value function of \\(\\pi_{\\theta}\\). For any policy \\(\\pi\\), note that the random variable \\(Q^\\pi_h(s, a) - V^\\pi_h(s)\\), where the randomness is taken over the action \\(a\\), is centered around zero. (Recall \\(V^\\pi_h(s) = \\mathop{\\mathbb{E}}_{a \\sim \\pi} Q^\\pi_h(s, a).\\)) This quantity matches the intuition given in Section 7.4: it is positive for actions that are better than average (in state \\(s\\)), and negative for actions that are worse than average. In fact, it has a particular name: the advantage function.\n\nDefinition 7.4 (Advantage function) For a policy \\(\\pi\\), its advantage function \\(A^\\pi\\) at time \\(h\\) is given by\n\\[\nA^\\pi_h(s, a) := Q^\\pi_h(s, a) - V^\\pi_h(s).\n\\tag{7.45}\\]\n\nNote that for an optimal policy \\(\\pi^\\star\\), the advantage of a given state-action pair is always zero or negative.\nWe can now use \\(A^{\\pi_{\\theta}}(s_h, a_h)\\) for the gradient coefficients to obtain the ultimate unbiased policy gradient estimator.\n\nTheorem 7.7 (Using the advnatage function is unbiased) Substituting\n\\[\n\\psi_h(\\tau) := A^{\\pi_\\theta}(s_h, a_h)\n\\tag{7.46}\\]\ninto the general policy gradient estimator eq. 7.23 gives an unbiased estimator. That is,\n\\[\n\\begin{aligned}\n\\nabla J(\\theta) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} [ g^\\text{adv}_\\tau(\\theta) ] \\\\\n\\text{where} \\quad\ng^\\text{adv}_{\\tau} (\\theta)\n&:=\n\\sum_{h=0}^{H-1}\nA^{\\pi_\\theta}(s_h, a_h)\n\\nabla \\log \\pi_{\\theta}(a_h\\mid s_h).\n\\end{aligned}\n\\tag{7.47}\\]\n\n\nProof. This follows directly from Theorem 7.5 and Theorem 7.6.\n\nNote that to avoid correlations between the gradient estimator and the value estimator (i.e. baseline), we must estimate them with independently sampled trajectories:\n\n\n\nCode\ndef pg_with_learned_baseline(env: gym.Env, pi, eta: float, theta_init, K: int, N: int) -&gt; Float[Array, \" D\"]:\n    theta = theta_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, pi(theta), N)\n        V_hat = fit_value(trajectories)\n        tau = sample_trajectories(env, pi(theta), 1)\n        nabla_hat = jnp.zeros_like(theta)  # gradient estimator\n\n        for h, (s, a) in enumerate(tau):\n            def log_likelihood(theta_opt):\n                return jnp.log(pi(theta_opt)(s, a))\n            nabla_hat = nabla_hat + jax.grad(log_likelihood)(theta) * (return_to_go(tau, h) - V_hat(s))\n        \n        theta = theta + eta * nabla_hat\n    return theta\n\nlatex(pg_with_learned_baseline)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{pg\\_with\\_learned\\_baseline}(\\mathrm{env}: \\mathrm{gym}.\\mathrm{Env}, \\pi, \\eta: \\mathbb{R}, \\theta_{\\mathrm{init}}, K: \\mathbb{Z}, N: \\mathbb{Z}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( K \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), N \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{V} \\gets \\mathrm{fit\\_value} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\tau \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), 1 \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{\\nabla} \\gets \\mathrm{jnp}.\\mathrm{zeros\\_like} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ \\mathopen{}\\left( h, \\mathopen{}\\left( s, a \\mathclose{}\\right) \\mathclose{}\\right) \\in \\mathrm{enumerate} \\mathopen{}\\left( \\tau \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathbf{function} \\ \\mathrm{log\\_likelihood}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{4em} \\mathbf{return} \\ \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ function} \\\\ \\hspace{3em} \\widehat{\\nabla} \\gets \\widehat{\\nabla} + \\nabla \\mathopen{}\\left(\\mathrm{log\\_likelihood}\\mathclose{}\\right) \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\cdot \\mathopen{}\\left( \\mathrm{return\\_to\\_go} \\mathopen{}\\left( \\tau, h \\mathclose{}\\right) - \\widehat{V} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\theta \\gets \\theta + \\eta \\widehat{\\nabla} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nNote that you could also generalize this by allowing the learning rate \\(\\eta\\) to vary across steps, or take multiple trajectories \\(\\tau\\) and compute the sample mean of the gradient estimates.\nThe baseline estimation step fit_value can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.\n\nExample 7.5 (Policy gradient for the linear-in-features parameterization) The gradient-log-likelihood for the linear parameterization ex. 7.2 is also quite elegant:\n\\[\n\\begin{aligned}\n        \\nabla \\log \\pi_\\theta(a|s) &= \\nabla \\left( \\theta^\\top \\phi(s, a) - \\log \\left( \\sum_{a'} \\exp(\\theta^\\top \\phi(s, a')) \\right) \\right) \\\\\n        &= \\phi(s, a) - \\mathop{\\mathbb{E}}_{a' \\sim \\pi_\\theta(s)} \\phi(s, a')\n\\end{aligned}\n\\]\nPlugging this into our policy gradient expression, we get\n\\[\n\\begin{aligned}\n    \\nabla J(\\theta) & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[\n    \\sum_{t=0}^{T-1} \\nabla \\log \\pi_\\theta(a_h| s_h) A_h^{\\pi_\\theta}\n    \\right]                                                                                                                    \\\\\n                     & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[\n    \\sum_{t=0}^{T-1} \\left( \\phi(s_h, a_h) - \\mathop{\\mathbb{E}}_{a' \\sim \\pi(s_h)} \\phi(s_h, a') \\right) A_h^{\\pi_\\theta}(s_h, a_h)\n    \\right]                                                                                                                    \\\\\n                     & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\sum_{t=0}^{T-1} \\phi(s_h, a_h) A_h^{\\pi_\\theta} (s_h, a_h) \\right]\n\\end{aligned}\n\\]\nWhy can we drop the \\(\\mathop{\\mathbb{E}}\\phi(s_h, a')\\) term? By linearity of expectation, consider the dropped term at a single timestep: \\(\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\left( \\mathop{\\mathbb{E}}_{a' \\sim \\pi(s_h)} \\phi(s, a') \\right) A_h^{\\pi_\\theta}(s_h, a_h) \\right].\\) By Adam’s Law, we can wrap the advantage term in a conditional expectation on the state \\(s_h.\\) Then we already know that \\(\\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} A_h^{\\pi}(s, a) = 0,\\) and so this entire term vanishes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#comparing-policy-gradient-algorithms-to-policy-iteration",
    "href": "pg.html#comparing-policy-gradient-algorithms-to-policy-iteration",
    "title": "7  Policy Gradient Methods",
    "section": "7.5 Comparing policy gradient algorithms to policy iteration",
    "text": "7.5 Comparing policy gradient algorithms to policy iteration\nWhat advantages do policy gradient algorithms have over the policy iteration algorithms covered in Section 2.4.4.2?\n\nRemark 7.7 (Policy iteration review). Recall that policy iteration is an algorithm for MDPs with unknown state transitions where we alternate between the following two steps:\n\nEstimating the \\(Q\\)-function (or advantage function) of the current policy;\nUpdating the policy to be greedy with respect to this approximate \\(Q\\)-function (or advantage function).\n\n\nTo analyze the difference between them, we’ll make use of the performance difference lemma, which provides an expression for comparing the difference between two value functions.\n\nTheorem 7.8 (Performance difference lemma (S. Kakade & Langford, 2002, Lemma 6.1)) Suppose Alice is playing a game (an MDP). Bob is spectating, and can evaluate how good an action is compared to his own strategy. (That is, Bob can compute his advantage function \\(A_h^{\\text{Bob}}(s_h, a_h)\\)). The performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:\n\\[\nV_0^{\\text{Alice}}(s) - V_0^{\\text{Bob}}(s) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\text{Alice}} \\left[ \\sum_{h=0}^{H-1} A_h^{\\text{Bob}} (s_h, a_h) \\mid s_0 = s \\right]\n\\tag{7.48}\\]\nwhere \\(\\rho^\\text{Alice}\\) denotes Alice’s trajectory distribution (def. 2.7).\n\nTo see why, consider a specific step \\(h\\) in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average. But this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!\n\nProof. Formally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that\n\\[\n\\begin{aligned}\nA^\\pi_h(s_h, a_h) &= Q^\\pi_h(s_h, a_h) - V^\\pi_h(s_h) \\\\\n&= r_h(s_h, a_h) + \\mathop{\\mathbb{E}}_{s_{h+1} \\sim P(\\cdot \\mid s_h, a_h)} [V^\\pi_{h+1}(s_{h+1})] - V^\\pi_h(s_h)\n\\end{aligned}\n\\tag{7.49}\\]\nso expanding out the r.h.s. expression of eq. 7.48 and grouping terms together gives\n\\[\n\\begin{aligned}\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\text{Alice}} \\left[ \\sum_{h=0}^{H-1} A_h^{\\text{Bob}} (s_h, a_h) \\mid s_0 = s \\right]\n&= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho_\\text{Alice}} \\bigg[\n    \\left( \\sum_{h=0}^{H-1} r_h(s_h, a_h) \\right) \\\\\n& \\quad {} + \\left( V^{\\text{Bob}}_1(s_1) + \\cdots + V^{\\text{Bob}}_H(s_H) \\right) \\\\\n& \\quad {} - \\left( V^{\\text{Bob}}_0(s_0) + \\cdots + V^{\\text{Bob}}_{H-1}(s_{H-1}) \\right)\n    \\mid s_0 = s \\bigg] \\\\\n&= V^{\\text{Alice}}_0(s) - V^{\\text{Bob}}_0(s).\n\\end{aligned}\n\\tag{7.50}\\]\nas desired. Note that the “inner” expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid. Also note that \\(V_H^\\pi\\), the value after reaching a terminal state, is always zero for any policy \\(\\pi\\).\n\nThe PDL gives insight into why fitted approaches such as PI don’t work as well in the “full” RL setting. To see why, let’s consider a single iteration of policy iteration, where policy \\(\\pi\\) gets updated to \\(\\widetilde \\pi\\). We’ll assume these policies are deterministic. Define \\(\\Delta_\\infty\\) to be the most negative advantage:\n\\[\n\\Delta_\\infty = \\min_{s \\in \\mathcal{S}} A^{\\pi}_h(s, \\widetilde \\pi(s)).\n\\tag{7.51}\\]\nSuppose \\(\\Delta_\\infty &lt; 0\\), i.e. there exists a state \\(s\\) such that\n\\[\nA^\\pi(s, \\widetilde \\pi(s)) &lt; 0,\n\\tag{7.52}\\]\nthat is, if \\(\\widetilde \\pi\\) acts for just one turn from state \\(s\\) and then \\(\\pi\\) acts thereafter, the result would be worse on average than allowing \\(\\pi\\) to act. Plugging this into the PDL (Theorem 7.8) gives\n\\[\n\\begin{aligned}\nV_0^{\\widetilde \\pi}(s) - V_0^{\\pi}(s) &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\widetilde \\pi}} \\left[\n\\sum_{h=0}^{H-1} A_h^{\\pi}(s_h, a_h)\n\\mid s_0 = s\n\\right] \\\\\n&\\ge H \\Delta_\\infty \\\\\nV_0^{\\widetilde \\pi}(s) &\\ge V_0^{\\pi}(s) - H|\\Delta_\\infty|.\n\\end{aligned}\n\\]\nThat is, for some state \\(s\\), the lower bound on the performance of \\(\\widetilde \\pi\\) is lower than the performance of \\(\\pi\\). This doesn’t state that \\(\\widetilde \\pi\\) will necessarily perform worse than \\(\\pi\\), only suggests that it might be possible. If these worst case states do exist, though, PI does not avoid situations where the new policy often visits them; It does not enforce that the trajectory distributions \\(\\rho^\\pi\\) and \\(\\rho^{\\widetilde \\pi}\\) be close to each other. In other words, PI falls prey to distributional shift: the “training distribution” that our prediction rule is fitted on, \\(\\rho^\\pi\\), may differ significantly from the “evaluation distribution” \\(\\rho^{\\widetilde \\pi}\\).\nOn the other hand, policy gradient methods do, albeit implicitly, encourage \\(\\rho^\\pi\\) and \\(\\rho^{\\widetilde \\pi}\\) to be similar. Suppose that the mapping from policy parameters to trajectory distributions is relatively smooth. Then, by adjusting the parameters only a small distance, the new policy will also have a similar trajectory distribution. But this is not very rigorous, and in practice the parameter-to-distribution mapping may not be so smooth. Can we constrain the distance between the resulting distributions more explicitly?\nThis brings us to the following local policy optimization methods:\n\ntrust region policy optimization (TRPO), which explicitly constrains the difference between the distributions before and after each step;\nthe natural policy gradient (NPG), a first-order approximation of TRPO;\nproximal policy optimization (PPO-penalty), a “soft relaxation” of TRPO;\nthe clipped surrogate objective (PPO-clip), a version of PPO that is popular in practice.\n\n\nRemark 7.8 (Ordering of algorithms). Chronologically, NPG was developed first, followed by TRPO and later PPO. We begin with TRPO since it sets up the intuition behind these constrained methods.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-trpo",
    "href": "pg.html#sec-trpo",
    "title": "7  Policy Gradient Methods",
    "section": "7.6 Trust region policy optimization",
    "text": "7.6 Trust region policy optimization\nWe saw above that policy gradient methods are effective because they implicitly constrain how much the policy changes at each iteration in terms of its trajectory distribution \\(\\rho^\\pi\\). What happens if we explicitly constrain the distance between the new and old trajectory distributions? This requires some way to measure the distance between two trajectory distributions. For this, we introduce the Kullback-Leibler divergence.\n\nDefinition 7.5 (Kullback-Leibler divergence) For two PDFs \\(p, q\\),\n\\[\n\\mathrm{KL}\\left(p\\parallel q\\right) := \\mathop{\\mathbb{E}}_{x \\sim p} \\left[ \\log \\frac{p(x)}{q(x)} \\right].\n\\tag{7.53}\\]\n\nThe Kullback-Leibler divergence can be interpreted in many different ways, many stemming from information theory. One such interpretation is that \\(\\mathrm{KL}\\left(p\\parallel q\\right)\\) describes how much more surprised you are if you think data is being generated by \\(q\\) but it’s actually generated by \\(p\\), compared to someone who knows the true distribution \\(p\\). (The surprise of an event with probability \\(p\\) is \\(- \\log_2 p\\).)\nIt can be shown that \\(\\mathrm{KL}\\left(p\\parallel q\\right) = 0\\) if and only if \\(p = q\\). Also note that it is generally not symmetric, that is, \\(\\mathrm{KL}\\left(p\\parallel q\\right) \\neq \\mathrm{KL}\\left(q\\parallel p\\right)\\). How can we interpret this asymmetry?\n\nRemark 7.9 (Asymmetry of the Kullback-Leibler divergence). Note that the KL divergence gets large if \\(p(x)/q(x)\\) is very large for some \\(x\\), that is, \\(q\\) assigns low probability to a common event under \\(p\\). So if we minimize the KL divergence with respect to the second argument \\(q\\), the “prediction” distribution, \\(q\\) will “spread out” to cover all common events under \\(p\\). If we minimize the KL divergence with respect to the first argument \\(p\\), the data generating distribution, \\(p\\) will “squeeze under” \\(q\\), so that \\(p(x)\\) is small wherever \\(q(x)\\) is small.\n\n\n\nCode\ndef mixture_pdf(x, mus, sigmas, weights):\n    pdf_val = np.zeros_like(x, dtype=float)\n    for mu, sigma, w in zip(mus, sigmas, weights):\n        pdf_val += w * norm.pdf(x, loc=mu, scale=sigma)\n    return pdf_val\n\ndef kl_divergence(P, Q, x_grid):\n    epsilon = 1e-12  # avoid division by zero\n    P_safe = np.maximum(P, epsilon)\n    Q_safe = np.maximum(Q, epsilon)\n    dx = x_grid[1] - x_grid[0]\n    return np.sum(P_safe * np.log(P_safe / Q_safe)) * dx\n\ndef Q_pdf(x):\n    # 1/3 N(-5, 1) + 2/3 N(2, 1)\n    return mixture_pdf(x, mus=[-5, 2], sigmas=[1, 1], weights=[1/3, 2/3])\n\n# Single Gaussian for the candidate\ndef single_gaussian_pdf(x, params):\n    mu, log_sigma = params\n    sigma = np.exp(log_sigma)\n    return norm.pdf(x, loc=mu, scale=sigma)\n\ndef kl_objective_PQ(params, x_grid, Q_vals):\n    # minimize KL(P_candidate || Q).\n    P_candidate_vals = single_gaussian_pdf(x_grid, params)\n    return kl_divergence(P_candidate_vals, Q_vals, x_grid)\n\ndef kl_objective_QP(params, x_grid, Q_vals):\n    # minimize KL(Q || P_candidate).\n    P_candidate_vals = single_gaussian_pdf(x_grid, params)\n    return kl_divergence(Q_vals, P_candidate_vals, x_grid)\n\nx_grid = np.linspace(-10, 10, 201)\nQ_vals = Q_pdf(x_grid)\n\n# Minimize each objective separately\ninit_params = [0.0, 0.0]  # mu=0, log_sigma=0 -&gt; sigma=1\nres_min_PQ = minimize(kl_objective_PQ, x0=init_params, args=(x_grid, Q_vals), method='L-BFGS-B')\nres_min_QP = minimize(kl_objective_QP, x0=init_params, args=(x_grid, Q_vals), method='L-BFGS-B')\n\nplt.figure(figsize=(4, 2))\nplt.plot(x_grid, Q_vals, '-', label=\"q\")\nplt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_QP.x), '--', label=r\"$\\arg\\min_q \\text{KL}(p \\parallel q)$\")\nplt.plot(x_grid, single_gaussian_pdf(x_grid, res_min_PQ.x), '--', label=r\"$\\arg\\min_p \\text{KL}(p \\parallel q)$\")\nplt.xlabel('x'), plt.ylabel('PDF')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7.6: Minimizing the forward and backward Kullback-Leibler divergence against a bimodal distribution with respect to a unimodal distribution\n\n\n\n\n\nFor trajectory distributions \\(\\rho^\\pi_{\\theta^i}\\) and \\(\\rho^\\pi_{\\theta'}\\) (def. 2.7), the KL divergence can be broken down into a sum over timesteps:\n\\[\n\\begin{aligned}\n\\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right)\n&= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\log \\rho^{\\pi_{\\theta^i}}(\\tau) - \\log \\rho^{\\pi_{\\theta'}}(\\tau)\n\\right] \\\\\n&= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1} \\log \\pi_{\\theta^i}(a_h\\mid s_h) - \\log \\pi_{\\theta'}(a_h\\mid s_h)\n\\right] \\\\\n\\end{aligned}\n\\tag{7.54}\\]\nsince the terms corresponding to the state transitions and initial state distribution cancel out.\nWe can now use the KL divergence to explicitly constrain the distance between the new and old trajectory distributions:\n\\[\n\\begin{aligned}\n\\theta^{t+1} &\\gets \\arg\\max_{{\\theta'}\\in \\mathbb{R}^D} J({\\theta'}) \\\\\n& \\text{where } \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right) &lt; \\delta\n\\end{aligned}\n\\tag{7.55}\\]\nNote that we place \\(\\rho^\\pi_{\\theta'}\\) in the second argument to the KL divergence. This ensures that \\(\\rho^\\pi_{\\theta'}\\) supports all of the trajectories under \\(\\rho^\\pi_{\\theta^i}\\) (see Remark 7.9).\nIn place of \\(J\\), if we use the performance difference lemma (Theorem 7.8) to compare the performance of the new policy to the old one, we obtain an\n\nDefinition 7.6 (TRPO update rule) Let \\({\\theta^i}\\in \\mathbb{R}^D\\) denote the current policy parameter vector. The TRPO update rule is\n\\[\n\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{{\\theta'}\\in \\mathbb{R}^D}\n\\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1}\n    \\mathop{\\mathbb{E}}_{a'_h\\sim \\pi^{{\\theta'}}(\\cdot \\mid s_h)}\n    A^{\\pi_{\\theta^i}}(s_h, a'_h)\n\\right] \\\\\n& \\text{where } \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right) &lt; \\delta\n\\end{aligned}\n\\tag{7.56}\\]\n\n\nRemark 7.10 (Drawing states from old policy). Note that we have made a small change to the eq. 7.48: we use the current policy’s trajectory distribution, and re-sample actions from the updated policy. This allows us to reuse a single batch of trajectories from \\(\\pi_{\\theta^i}\\) rather than sample new batches from \\(\\pi_{\\theta'}\\) when solving the optimization problem eq. 7.56. This approximation also matches the r.h.s. of the PDL to first order in \\(\\theta\\). (We will elaborate more on this later.)\n\n\n\nCode\ndef trpo(env, δ, theta_init, n_interactions):\n    theta = theta_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, pi(theta), n_interactions)\n        A_hat = fit_advantage(trajectories)\n        \n        def approximate_gain(theta_opt):\n            A_total = 0\n            for tau in trajectories:\n                for s, _a, _r in tau:\n                    for a in env.action_space:\n                        A_total += pi(theta)(s, a) * A_hat(s, a)\n            return A_total\n        \n        def constraint(theta_opt):\n            return kl_div_trajectories(pi, theta, theta_opt, trajectories) &lt;= δ\n        \n        theta = optimize(approximate_gain, constraint)\n\n    return theta\n\nlatex(trpo)\n\n\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{trpo}(\\mathrm{env}, δ, \\theta_{\\mathrm{init}}, n_{\\mathrm{interactions}}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( K \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), n_{\\mathrm{interactions}} \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{A} \\gets \\mathrm{fit\\_advantage} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{approximate\\_gain}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} A_{\\mathrm{total}} \\gets 0 \\\\ \\hspace{3em} \\mathbf{for} \\ \\tau \\in \\mathrm{trajectories} \\ \\mathbf{do} \\\\ \\hspace{4em} \\mathbf{for} \\ \\mathopen{}\\left( s, \\mathrm{\\_a}, \\mathrm{\\_r} \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{5em} \\mathbf{for} \\ a \\in \\mathrm{env}.\\mathrm{action\\_space} \\ \\mathbf{do} \\\\ \\hspace{6em} A_{\\mathrm{total}} \\gets A_{\\mathrm{total}} + \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\cdot \\widehat{A} \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{5em} \\mathbf{end \\ for} \\\\ \\hspace{4em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{return} \\ A_{\\mathrm{total}} \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{constraint}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} \\mathbf{return} \\ \\mathrm{kl\\_div\\_trajectories} \\mathopen{}\\left( \\pi, \\theta, \\theta_{\\mathrm{opt}}, \\mathrm{trajectories} \\mathclose{}\\right) \\le δ \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\theta \\gets \\mathrm{optimize} \\mathopen{}\\left( \\mathrm{approximate\\_gain}, \\mathrm{constraint} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFigure 7.7: Pseudocode for trust region policy optimization\n\n\n\n\nThe above isn’t entirely complete: we still need to solve the actual optimization problem at each step. Unless we know additional properties of the problem, this is still a nonconvex constrained optimization problem that might be inefficient to solve. Do we need to solve for the exact objective function, though? Instead, if we assume that both the objective function and the constraint are somewhat smooth in terms of the policy parameters, we can use their Taylor expansions to give us a simpler optimization problem with a closed-form solution. This brings us to the natural policy gradient algorithm.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-npg",
    "href": "pg.html#sec-npg",
    "title": "7  Policy Gradient Methods",
    "section": "7.7 Natural policy gradient",
    "text": "7.7 Natural policy gradient\nIn some sense, the natural policy gradient algorithm (S. M. Kakade, 2001) is an implementation of trust region policy optimization (fig. 7.7). Recall that in each TRPO update, we seek to maximize the expected total reward while keeping the updated policy close to the current policy in terms of Kullback-Leibler divergence (def. 7.5):\n\\[\n\\begin{gathered}\n\\theta^{k+1} \\gets \\arg\\max_{{\\theta'}\\in \\mathbb{R}^D} J({\\theta'}) \\\\\n\\text{where } \\mathrm{KL}\\left({\\theta^i}\\parallel{\\theta'}\\right) \\le \\delta\n\\end{gathered}\n\\tag{7.57}\\]\nNPG uses the following simplification: we take local approximations of the objective and constraint functions, which results in a simple problem with a closed-form solution.\nConcretely, we take a first-order Taylor approximation to the objective function about the current iterate \\({\\theta^i}\\):\n\\[\nJ({\\theta'}) = J({\\theta^i}) + ({\\theta'}- {\\theta^i})^\\top \\nabla J({\\theta^i}) + O(\\|{\\theta'}- {\\theta^i}\\|^2).\n\\tag{7.58}\\]\nWe also take a second-order Taylor approximation to the constraint function:\n\nTheorem 7.9 (Quadratic approximation to KL divergence.) The second-order approximation to \\(d^i(\\theta) := \\mathrm{KL}\\left(p_{\\theta^i}\\parallel p_\\theta\\right)\\) is given by\n\\[\n\\begin{aligned}\nd^i(\\theta)\n&=\n\\frac{1}{2} (\\theta - {\\theta^i})^\\top F_{\\theta^i}(\\theta - {\\theta^i})\n+\nO(\\|\\theta - {\\theta^i}\\|^3),\n\\end{aligned}\n\\tag{7.59}\\]\nwhere \\(F_{{\\theta^i}}\\) is the Fisher information matrix (FIM) of the trajectory distribution \\(\\rho^{\\pi_{\\theta^i}}\\). (We define the FIM below in def. 7.7.)\n\n\nProof. We leave the details as an exercise. Here is an outline:\n\nWrite down the Taylor expansion of \\(d^i(\\theta)\\) around \\({\\theta^i}\\).\nShow that the zeroth-order term \\(d^i({\\theta^i})\\) is zero.\nShow that the gradient \\(\\nabla d^i(\\theta) \\vert_{\\theta = {\\theta^i}}\\) is zero.\nShow that the Hessian \\(\\nabla^2 d^i(\\theta) \\vert_{\\theta = {\\theta^i}}\\) equals \\(F_{\\theta^i}\\).\n\n\n\nDefinition 7.7 (Fisher information matrix) Let \\(p_\\theta\\) denote a distribution parameterized by \\(\\theta\\). Its Fisher information matrix \\(F_\\theta\\) can be defined equivalently as:\n\\[\n\\begin{aligned}\n        F_{\\theta} & = \\mathop{\\mathbb{E}}_{x \\sim p_\\theta} \\left[ (\\nabla_\\theta \\log p_\\theta(x)) (\\nabla_\\theta \\log p_\\theta(x))^\\top \\right] \\\\\n                   & = \\mathop{\\mathbb{E}}_{x \\sim p_{\\theta}} [- \\nabla_\\theta^2 \\log p_\\theta(x)].\n\\end{aligned}\n\\tag{7.60}\\]\n\n\nRemark 7.11 (Interpretation of the Fisher information matrix). The Fisher information matrix is an important quantity when working with parameterized distributions. It has many possible interpretations. The first expression in eq. 7.60 shows that it is the covariance matrix of the gradient-log-probability (also known as the score), and the second expression shows that it is the expected Hessian matrix of the negative-log-likelihood of the underlying distribution. It can also be shown that it is precisely the Hessian of the KL divergence (with respect to either one of the parameters).\nRecall that the Hessian of a function describes its curvature. Concretely, suppose we have some parameter vector \\(\\theta \\in \\mathbb{R}^D\\) and we seek to measure how much \\(p_\\theta\\) changes if we shift \\(\\theta\\) by \\(\\delta \\in \\mathbb{R}^D\\). The quantity \\(\\delta^\\top F_\\theta \\delta\\) describes how rapidly the negative log-likelihood changes if we move by \\(\\delta\\). (The zeroth and first order terms are both zero in our case due to properties of the KL divergence.)\n\nPutting this together results in the following update rule:\n\nDefinition 7.8 (Natural policy gradient update) We aim to solve the constrained optimization problem eq. 7.57. Upon taking first- and second-order approximations of the objective function and constraint function respectively, we obtain the update rule\n\\[\n\\begin{gathered}\n    \\theta^{k+1} \\gets {\\theta^i}+ \\arg\\max_{\\Delta\\theta \\in \\mathbb{R}^D} \\nabla J({\\theta^i})^\\top \\Delta\\theta \\\\\n    \\text{where } \\frac{1}{2} \\Delta\\theta^\\top F_{{\\theta^i}} \\Delta\\theta \\le \\delta,\n\\end{gathered}\n\\tag{7.61}\\]\nwhere \\(F_{\\theta^i}\\) is the Fisher information matrix of \\(\\rho^\\pi_{\\theta^i}\\).\n\neq. 7.61 is a convex optimization problem with a closed-form solution. To see why, it helps to visualize the case where \\(\\theta\\) is two-dimensional: the constraint describes the inside of an ellipse, and the objective function is linear, so we can find the extreme point on the boundary of the ellipse by setting the gradient of the Lagrangian to zero:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\theta, \\alpha)                     & = \\nabla J({\\theta^i})^\\top (\\theta - {\\theta^i}) - \\alpha \\left[ \\frac{1}{2} (\\theta - {\\theta^i})^\\top F_{{\\theta^i}} (\\theta - {\\theta^i}) - \\delta \\right] \\\\\n    \\nabla \\mathcal{L}(\\theta^{k+1}, \\alpha) & := 0                                                                                                                                                             \\\\\n    \\implies \\nabla J({\\theta^i})        & = \\alpha F_{{\\theta^i}} (\\theta^{k+1} - {\\theta^i})\n\\end{aligned}\n\\tag{7.62}\\]\nRearranging gives the NPG update rule:\n\\[\n\\begin{aligned}\n    \\theta^{k+1}                           & = {\\theta^i}+ \\eta F_{{\\theta^i}}^{-1} \\nabla J({\\theta^i})                                                                                             \\\\\n    \\text{where } \\eta                     & = \\sqrt{\\frac{2 \\delta}{\\nabla J({\\theta^i})^\\top F_{{\\theta^i}}^{-1} \\nabla J({\\theta^i})}}\n\\end{aligned}\n\\tag{7.63}\\]\nThis gives us the closed-form update.\n\nRemark 7.12 (Scalability of NPG). Having a closed-form solution might at first seem like brilliant news. Is there a catch? The challenge lies in computing the inverse Fisher information matrix (FIM) \\(F_{{\\theta^i}}^{-1}\\). Since it is an expectation over trajectories, computing it exactly is typically intractable. Instead, we could collect trajectories from the environment and approximate the expectation by a sample mean, since we can write the Fisher information matrix as\n\\[\nF_{\\theta} = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_\\theta}} \\left[ \\sum_{h=0}^{H-1} (\\nabla \\log \\pi_\\theta (a_h\\mid s_h)) (\\nabla \\log \\pi_\\theta(a_h\\mid s_h))^\\top \\right].\n\\tag{7.64}\\]\nNote that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.\nIt turns out that to estimate the FIM to a relative error of \\(\\epsilon\\), we need \\(O(D / \\epsilon^2)\\) samples (Vershynin, 2018, Remark 4.7.2). In order for the estimated FIM to be accurate enough to be useful, this can be too large to be practical. Taking the inverse also takes \\(O(D^3)\\) time, which can be expensive if the parameter space is large.\n\n\nRemark 7.13 (NPG accounts for curvature in the parameter space). Let us compare the original policy gradient update (eq. 7.17) to the NPG update (eq. 7.63):\n\\[\n\\begin{aligned}\n\\theta^{t+1} &= \\theta^t + \\eta \\nabla J(\\theta^t) & \\text{Policy gradient} \\\\\n\\theta^{t+1} &= \\theta^t + \\eta F_{\\theta^t}^{-1} \\nabla J(\\theta^t) & \\text{Natural policy gradient}\n\\end{aligned}\n\\tag{7.65}\\]\nThe NPG update preconditions the gradient by the inverse Fisher information matrix. Speaking abstractly, this matrix accounts for the geometry of the parameter space in the following sense.\nThe typical gradient descent algorithm implicitly measures distances between parameters using the typical “flat” Euclidean distance:\n\\[\n\\text{distance}(\\theta_0, \\theta_1) = \\| \\theta_0 - \\theta_1 \\|.\n\\tag{7.66}\\]\nThe NPG update measures distance between parameters as the KL divergence (def. 7.5) between their induced trajectory distributions:\n\\[\n\\text{distance}(\\theta_0, \\theta_1) = \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta_0}}\\parallel\\rho^{\\pi_{\\theta_1}}\\right).\n\\tag{7.67}\\]\nUsing a parameterized policy class is just a means to the end of optimizing the trajectory distribution, so it makes sense to optimize over the trajectory distributions directly. In fact, the NPG update is the only update that is invariant to reparameterizations of the policy space: if instead of \\(\\theta\\), we used some transformation \\(\\phi(\\theta)\\) to parameterize the policy, the NPG update would remain the same. This is why NPG is called a coordinate-free optimization algorithm.\nWe can illustrate this with the following example:\n\n\nExample 7.6 (Natural gradient on a simple problem) Let’s step away from RL and consider a simple optimization problem over Bernoulli distributions \\(p_\\theta \\in \\triangle(\\{ 0, 1 \\})\\). This distribution space is parameterized by the success probability \\(\\theta \\in [0, 1]\\). The per-sample objective function is \\(100\\) if the Bernoulli trial succeeds and \\(1\\) if the trial fails.\n\\[\nf(x) = \\begin{cases}\n100 & x = 1 \\\\\n1 & x = 0.\n\\end{cases}\n\\tag{7.68}\\]\nThe objective \\(J(\\theta)\\) is the average of \\(f\\) over \\(x \\sim p_\\theta\\):\n\\[\n\\begin{aligned}\n    J(\\theta) & = \\mathop{\\mathbb{E}}_{x \\sim p_\\theta} [f(x)] \\\\\n    &= 100 \\theta + 1 (1 - \\theta)\n\\end{aligned}\n\\tag{7.69}\\]\nWe can think of the space of such distributions as the line between \\((0, 1)\\) to \\((1, 0)\\) on the Cartesian plane:\n\n\nCode\nx = jnp.linspace(0, 1, 50)\ny = 1 - x\nplt.figure(figsize=(3, 2))\nplt.plot(x, y)\nplt.xlabel(r\"$\\pi(0)$\")\nplt.ylabel(r\"$\\pi(1)$\")\nplt.show()\n\n\n\n\n\nThe space of Bernoulli distributions.\n\n\n\n\nClearly the optimal distribution is the constant one \\(\\pi(1) = 1\\). Suppose we optimize over the parameterized family \\(\\pi_\\theta(1) = \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}\\). Then our optimization algorithm should set \\(\\theta\\) to be unboundedly large. Then the “parameter-space” gradient is\n\\[\n\\nabla_\\theta J(\\pi_\\theta) = \\frac{99 \\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\\]\nNote that as \\(\\theta \\to \\infty\\) that the increments get closer and closer to \\(0\\); the rate of increase becomes exponentially slow.\nHowever, if we compute the Fisher information “matrix” (which is just a scalar in this case), we can account for the geometry induced by the parameterization.\n\\[\n\\begin{aligned}\n        F_\\theta & = \\mathop{\\mathbb{E}}_{x \\sim \\pi_\\theta} [ (\\nabla_\\theta \\log \\pi_\\theta(x))^2 ] \\\\\n                 & = \\frac{\\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\\end{aligned}\n\\]\nThis gives the natural gradient update\n\\[\n\\begin{aligned}\n        \\theta^{k+1} & = {\\theta^i}+ \\eta F_{{\\theta^i}}^{-1} \\nabla_ \\theta J({\\theta^i}) \\\\\n                     & = {\\theta^i}+ 99 \\eta\n\\end{aligned}\n\\]\nwhich increases at a constant rate, i.e. improves the objective more quickly than parameter-space gradient descent.\n\nThough the NPG now gives a closed-form optimization step, it requires estimating and computing the inverse Fisher information matrix, which can be difficult or slow, especially as the parameter space gets large (Remark 7.12). Can we achieve a similar effect without the inverse Fisher information matrix? This brings us to the proximal policy optimization algorithm.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-ppo-kl",
    "href": "pg.html#sec-ppo-kl",
    "title": "7  Policy Gradient Methods",
    "section": "7.8 Penalty-based proximal policy optimization",
    "text": "7.8 Penalty-based proximal policy optimization\nWe can relax the TRPO optimization problem (eq. 7.56) in a different way: Rather than imposing a hard constraint\n\\[\n\\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right) &lt; \\delta,\n\\tag{7.70}\\]\nwe can instead impose a soft constraint by subtracting \\(\\lambda \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right)\\) from the function to be maximized. \\(\\lambda &gt; 0\\) is a coefficient that controls the tradeoff between the two terms. This gives the following objective (Schulman et al., 2017):\n\\[\n\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{{\\theta'}} \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[ \\sum_{h=0}^{H-1} \\mathop{\\mathbb{E}}_{a'_h\\sim \\pi_{\\theta}(s_h)} A^{\\pi_{\\theta^i}}(s_h, a'_h) \\right] - \\lambda \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta'}}\\right)\n\\end{aligned}\n\\tag{7.71}\\]\nThis optimization problem is also known as the Lagrangian formulation of eq. 7.56.\nHow do we solve this optimization? Let us begin by simplifying the \\(\\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta}}\\right)\\) term. The state transitions cancel as in eq. 7.54, which gives us\n\\[\n\\begin{aligned}\n    \\mathrm{KL}\\left(\\rho^{\\pi_{\\theta^i}}\\parallel\\rho^{\\pi_{\\theta}}\\right) & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\\log \\frac{\\rho^{\\pi_{\\theta^i}}(\\tau)}{\\rho^{\\pi_{\\theta}}(\\tau)}\\right]                                                       \\\\\n                                           & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{\\pi_{\\theta^i}(a_h\\mid s_h)}{\\pi_{\\theta}(a_h\\mid s_h)}\\right] \\\\\n                                           & = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_h\\mid s_h)}\\right] + c\n\\end{aligned}\n\\]\nwhere \\(c\\) doesn’t depend on \\(\\theta\\) and can be ignored. This gives the objective\n\\[\nL^k(\\theta)\n=\n\\mathop{\\mathbb{E}}_{s_0, \\dots, s_{H-1} \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1} \\mathop{\\mathbb{E}}_{a_h\\sim \\pi_{\\theta}(s_h)} A^{\\pi_{\\theta^i}}(s_h, a_h)\n\\right] - \\lambda \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}}\n\\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_h\\mid s_h)}\\right]\n\\]\nOnce again, this takes an expectation over trajectories. But here we cannot directly sample trajectories from \\(\\pi_{\\theta^i}\\), since in the first term, the actions actually come from \\(\\pi_\\theta\\). To make this term line up with the other expectation, we would need the actions to also come from \\(\\pi_{\\theta^i}\\). This should sound familiar: we want to estimate an expectation over one distribution by sampling from another. We can use importance sampling (Theorem 7.3) to rewrite the inner expectation:\n\\[\n\\mathop{\\mathbb{E}}_{a_h\\sim \\pi_{\\theta'}(s_h)} A^{\\pi_{\\theta^i}}(s_h, a_h)\n=\n\\mathop{\\mathbb{E}}_{a_h\\sim \\pi_{\\theta^i}(s_h)} \\left[\n    \\frac{\\pi_{\\theta'}(a_h\\mid s_h)}{\\pi_{\\theta^i}(a_h\\mid s_h)} A^{\\pi_{\\theta^i}}(s_h, a_h)\n\\right]\n\\tag{7.72}\\]\n\nRemark 7.14 (Interpretation of likelihood ratio). Suppose \\(a^+\\) is a “good” action for \\(\\pi_{\\theta^i}\\) in state \\(s_h\\), i.e. \\(A^\\pi_{\\theta^i}(s_h, a^+) &gt; 0\\). Then maximizing eq. 7.72 encourages \\({\\theta'}\\) to increase the probability ratio \\(\\pi_{\\theta'}(a_h\\mid s_h) / \\pi_{\\theta^i}(a_h\\mid s_h)\\).\nOtherwise, if \\(a^-\\) is a “bad” action for \\(\\pi_{\\theta^i}\\) in state \\(s_h\\) (i.e. \\(A^\\pi_{\\theta^i}(s_h, a^-) &lt; 0\\)), then maximizing eq. 7.72 encourages \\({\\theta'}\\) to decrease the probability ratio \\(\\pi_{\\theta'}(a_h\\mid s_h) / \\pi_{\\theta^i}(a_h\\mid s_h)\\).\n\nNow we can combine the expectations together to get the objective\n\nDefinition 7.9 (Penalty objective) \\[\nL^k(\\theta) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1} \\left(\n    \\frac{\\pi_\\theta(a_h\\mid s_h)}{\\pi_{\\theta^i}(a_h\\mid s_h)} A^{\\pi_{\\theta^i}}(s_h, a_h)\n    - \\lambda \\log \\frac{1}{\\pi_\\theta(a_h\\mid s_h)}\n    \\right)\n\\right]\n\\]\n\nNow we can estimate this function by a sample mean over trajectories from \\(\\pi_{\\theta^i}\\). Remember that to complete a single iteration of PPO, we execute\n\\[\n\\theta^{k+1} \\gets \\arg\\max_{\\theta} L^k(\\theta).\n\\]\nIf \\(L^k\\) is differentiable, we can optimize it by gradient descent, completing a single iteration of PPO.\n\n\nCode\nfrom typing import TypeVar\n\nState = TypeVar(\"State\")\nAction = TypeVar(\"Action\")\n\ndef ppo_proximal(\n    env,\n    pi: Callable[[Float[Array, \" D\"]], Callable[[State, Action], float]],\n    λ: float,\n    theta_init: Float[Array, \" D\"],\n    n_iters: int,\n    n_fit_trajectories: int,\n    n_sample_trajectories: int,\n):\n    theta = theta_init\n    for k in range(n_iters):\n        fit_trajectories = sample_trajectories(env, pi(theta), n_fit_trajectories)\n        A_hat = fit(fit_trajectories)\n\n        sample_trajectories = sample_trajectories(env, pi(theta), n_sample_trajectories)\n        \n        def objective(theta_opt):\n            total_objective = 0\n            for tau in sample_trajectories:\n                for s, a, _r in tau:\n                    total_objective += pi(theta_opt)(s, a) / pi(theta)(s, a) * A_hat(s, a) + λ * jnp.log(pi(theta_opt)(s, a))\n            return total_objective / n_sample_trajectories\n        \n        theta = optimize(objective, theta)\n\n    return theta\n\nlatex(ppo_proximal)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{ppo\\_proximal}(\\mathrm{env}, \\pi: (\\mathbb{R}^{D}) \\rightarrow (\\mathrm{State} \\times \\mathrm{Action}) \\rightarrow \\mathbb{R}, λ: \\mathbb{R}, \\theta_{\\mathrm{init}}: \\mathbb{R}^{D}, n_{\\mathrm{iters}}: \\mathbb{Z}, \\mathrm{n\\_fit\\_trajectories}: \\mathbb{Z}, \\mathrm{n\\_sample\\_trajectories}: \\mathbb{Z}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( n_{\\mathrm{iters}} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{fit\\_trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), \\mathrm{n\\_fit\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{A} \\gets \\mathrm{fit} \\mathopen{}\\left( \\mathrm{fit\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{sample\\_trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), \\mathrm{n\\_sample\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{objective}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} \\mathrm{total\\_objective} \\gets 0 \\\\ \\hspace{3em} \\mathbf{for} \\ \\tau \\in \\mathrm{sample\\_trajectories} \\ \\mathbf{do} \\\\ \\hspace{4em} \\mathbf{for} \\ \\mathopen{}\\left( s, a, \\mathrm{\\_r} \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{5em} \\mathrm{total\\_objective} \\gets \\mathrm{total\\_objective} + \\frac{\\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right)}{\\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right)} \\widehat{A} \\mathopen{}\\left( s, a \\mathclose{}\\right) + λ \\cdot \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{4em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{return} \\ \\frac{\\mathrm{total\\_objective}}{\\mathrm{n\\_sample\\_trajectories}} \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\theta \\gets \\mathrm{optimize} \\mathopen{}\\left( \\mathrm{objective}, \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-ppo-clip",
    "href": "pg.html#sec-ppo-clip",
    "title": "7  Policy Gradient Methods",
    "section": "7.9 Advantage clipping",
    "text": "7.9 Advantage clipping\nRecall that the main point of proximal policy optimization methods (TRPO, NPG, PPO) is to encourage the updated policy (after taking a gradient step) to remain similar to the current one. These methods used the KL divergence to measure the distance between policies. Schulman et al. (2017) proposed an alternative way to constrain the step size based on clipping a certain objective function. This method, known as “PPO-clip” or the “clipped surrogate” objective function, is the most widely used proximal policy optimization algorithm in practice.\nAbove, in eq. 7.72, we constructed an objective function by applying importance sampling to the performance difference lemma (Theorem 7.8). Without the KL divergence penalty, this becomes\n\\[\nL^k({\\theta'}) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1}\n    \\frac{\\pi_{\\theta'}(a_h\\mid s_h)}{\\pi_{\\theta^i}(a_h\\mid s_h)} A^{\\pi_{\\theta^i}}(s_h, a_h)\n\\right]\n\\approx V(\\pi_{\\theta'}) - V(\\pi_{\\theta^i}).\n\\tag{7.73}\\]\nIn the following part, define the policy ratio at time \\(h\\in [H]\\) as\n\\[\n\\Lambda_h({\\theta'}, {\\theta^i}) = \\frac{\\pi_{\\theta'}(a_h\\mid s_h)}{\\pi_{\\theta^i}(a_h\\mid s_h)}.\n\\tag{7.74}\\]\nThe clipped surrogate objective function modifies eq. 7.73 to remove incentives for \\(\\pi_{\\theta'}\\) to differ greatly from \\(\\pi_{\\theta^i}\\). Specifically, we choose some small \\(\\epsilon &gt; 0\\), and constrain \\(\\Lambda_h({\\theta'}, {\\theta^i}) \\in (1-\\epsilon, 1+\\epsilon)\\). Formally,\n\\[\n\\Lambda_h^\\text{clipped}({\\theta'}, {\\theta^i})\n=\n\\text{clip}(\\Lambda_h({\\theta'}, {\\theta^i}), 1-\\epsilon, 1+\\epsilon),\n\\tag{7.75}\\]\nwhere\n\\[\n\\text{clip}(x, a, b) := \\max \\{ a, \\min \\{ x, b \\} \\}.\n\\tag{7.76}\\]\n\nRemark 7.15 (Interpretation). As in Remark 7.14, suppose \\(a_h^+\\) and \\(a_h^-\\) are a “good” and “bad” action in \\(s_h\\), i.e. \\(A^\\pi_{\\theta^i}(s_h, a_h^+) &gt; 0\\) and \\(A^\\pi_{\\theta^i}(s_h, a_h^-) &lt; 0\\). By clipping \\(\\Lambda_h({\\theta'}, {\\theta^i})\\), no additional benefit is gained by increasing \\(\\pi_{\\theta'}(a_h^+ \\mid s_h)\\) above \\((1+\\epsilon) \\pi_{\\theta^i}(a_h^+ \\mid s_h)\\), or by decreasing \\(\\pi_{\\theta^i}(a_h^- \\mid s_h)\\) under \\((1-\\epsilon) \\pi_{\\theta^i}(a_h^- \\mid s_h)\\).\n\nAs a final step, we only use this clipped objective if it is smaller than the original objective. For example, if \\(A^\\pi_{\\theta^i}(s_h, a_h) &gt; 0\\) and \\(\\Lambda_h({\\theta'}, {\\theta^i}) \\ll 1 - \\epsilon\\), then the clipped objective would disproportionately incentivize taking action \\(a_h\\). The same thing happens if \\(A^\\pi_{\\theta^i}(s_h, a_h) &lt; 0\\) and \\(\\Lambda_h({\\theta'}, {\\theta^i}) &gt; 1+\\epsilon\\).\nPutting these together, this results in only clipping the policy ratio on the side that it would normally move towards:\n\nCode\nx = jnp.linspace(0, 2, 400)\ny = jnp.minimum(x, jnp.clip(x, 0.8, 1.2))\n\nplt.plot(x, y)\nplt.axvline(1.0, color='gray', linestyle='--', label=r\"$\\Lambda = 1$\")\nplt.axvline(1.2, color='red', linestyle='--', label=r\"$1+\\epsilon$\")\nplt.xlabel(r\"$\\Lambda$\")\nplt.ylabel(r\"$\\min(\\Lambda, \\text{clip}(\\Lambda, 1-\\epsilon, 1+\\epsilon))$\")\nplt.legend()\nplt.xlim(0, 2)\nplt.ylim(0, 1.5)\nplt.title(\"Advantage Positive\")\n\nplt.show()\n\ny_neg = jnp.minimum(-x, jnp.clip(-x, -1.2, -0.8))\n\nplt.plot(x, y_neg)\nplt.axvline(1.0, color='gray', linestyle='--', label=r\"$\\Lambda = 1$\")\nplt.axvline(0.8, color='red', linestyle='--', label=r\"$1-\\epsilon$\")\nplt.xlabel(r\"$\\Lambda$\")\nplt.ylabel(r\"$\\min(-\\Lambda, \\text{clip}(-\\Lambda, -(1+\\epsilon), -(1-\\epsilon)))$\")\nplt.legend()\nplt.xlim(0, 2)\nplt.ylim(-1.5, 0)\nplt.title(\"Advantage Negative\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) For a “good” action \\(a_h^+\\) with positive advantage.\n\n\n\n\n\n\n\n\n\n\n\n(b) For a “poor” action \\(a_h^-\\) with negative advantage.\n\n\n\n\n\n\n\nFigure 7.8: Clipped surrogate objective function\n\n\n\n\nDefinition 7.10 (Clipped surrogate objective) The PPO-Clip objective is\n\\[\nL^i(\\theta) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\theta^i}}} \\left[\n    \\sum_{h=0}^{H-1}\n    \\min \\left\\{\n        \\Lambda_h(\\theta, {\\theta^i}) A^{\\pi_{\\theta^i}}(s_h, a_h),\n        \\text{clip}(\n            \\Lambda_h(\\theta, {\\theta^i}),\n            1-\\epsilon,\n            1+\\epsilon\n        )\n        A^\\pi_{\\theta^i}(s_h, a_h)\n    \\right\\}\n\\right]\n\\tag{7.77}\\]\n\n\nRemark 7.16 (Convergence of the clipped surrogate objective). From a traditional optimization perspective, this objective function seems challenging to analyze: the clip operation (eq. 7.76) is not differentiable",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#key-takeaways",
    "href": "pg.html#key-takeaways",
    "title": "7  Policy Gradient Methods",
    "section": "7.10 Key takeaways",
    "text": "7.10 Key takeaways\nPolicy gradient methods are a powerful family of algorithms that optimize the expected total reward by iteratively updating the policy parameters. Precisely, we estimate the gradient of the expected total reward (with respect to the parameters), and update the parameters in that direction. But estimating the gradient is a tricky task! We saw many ways to reduce the variance of the gradient estimator, culminating in the advantage-based expression eq. 7.47.\nBut updating the parameters doesn’t entirely solve the problem: Sometimes, a small step in the parameters might lead to a big step in the policy. To avoid changing the policy too much at each step, we must account for the curvature in the parameter space. We first did this explicitly with Section 7.6, and then saw ways to relax the constraint via the natural policy gradient update (Section 7.7), the PPO-KL update (Section 7.8), and the PPO-Clip update (Section 7.9).\nThese are still popular methods to this day, especially because they efficiently integrate with deep neural networks for representing complex functions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-pg-bib",
    "href": "pg.html#sec-pg-bib",
    "title": "7  Policy Gradient Methods",
    "section": "7.11 Bibliographic notes and further reading",
    "text": "7.11 Bibliographic notes and further reading\nSutton et al. (1999) popularized the term “policy gradient methods”. Policy gradient methods, including REINFORCE (Williams, 1992), were some of the earliest reinforcement learning methods (Barto et al., 1983; Sutton, 1984; Witten, 1977).\nS. Kakade & Langford (2002) introduced the performance difference lemma, which was used to prove theoretical guarantees in several later works.\nThe expression of the policy gradient in terms of the Q-function was simultaneously observed by Marbach & Tsitsiklis (2001) and Sutton et al. (1999).\nThe natural gradient was suggested by Amari (1998) for general optimization problems and then applied to the policy gradient by S. M. Kakade (2001). The Natural Actor-Critic algorithm combines actor-critic learning with the natural policy gradient estimator (Bhatnagar et al., 2009; Peters et al., 2005; Peters & Schaal, 2008).\nNatural gradient descent is effective because it accounts for the geometry induced by the parameterization and thereby optimizes over the distributions themselves. Another optimization algorithm that accounts for parameter-space geometry is mirror descent (Beck & Teboulle, 2003; Nemirovskij et al., 1983). At each optimization step, this maps the parameter vector to a dual space defined to capture the desired geometry. It then takes a gradient descent step on the dual parameters and maps the result back to an updated “primal” parameter vector. One can also apply mirror descent to policy gradient algorithms (Mahadevan et al., 2013; Mahadevan & Liu, 2012).\nA key reason for the popularity of policy gradient methods is that they are easily parallelizable across multiple computing threads. Each thread collects rollouts and computes its own gradient estimates, and every once in a while, these are accumulated across threads and used to perform an update of the parameters, which are then synchronized across the different threads. Mnih et al. (2016) present multi-threaded variants of policy gradient methods as well as several algorithms from Chapter 6. Schulman et al. (2015) proposed TRPO (def. 7.6). Schulman et al. (2016) introduced generalized advantage estimation (GAE), which accepts some bias in the gradient estimate in exchange for lower variance. This tradeoff is controlled by the parameter \\(\\lambda\\), the same parameter as in \\(\\text{TD}(\\lambda)\\). Schulman et al. (2017) introduced the PPO-KL and PPO-Clip algorithms. Wu et al. (2017) introduced the ACKTR algorithm, which seeks to scale NPG up to deep learning models where naively approximating the Fisher information matrix is impractical. It does so using the Kronecker-factored approximation proposed in Martens & Grosse (2015). Shao et al. (2024) introduce a variant of PPO known as Group Relative Policy Optimization (GRPO).\nPolicy gradient methods, being on-policy algorithms, inherently have worse sample efficiency than off-policy algorithms, since naively each sample can only be used to update at the value of the parameter vector that it was sampled at. Wang et al. (2017) introduce the actor-critic with experience replay (ACER) algorithm, which builds on the Retrace Q function estimator (Munos et al., 2016) to estimate policy gradients using previously collected experience (Lin, 1992).\nThe soft Q-learning and soft actor-critic algorithms introduced in Haarnoja et al. (2017) and Haarnoja et al. (2018) respectively add the entropy of the policy to the objective function. This practice is known as maximum entropy reinforcement learning since we seek to maximize the policy entropy alongside the expected total reward. This encourages policies that “act more randomly”, which aids exploration.\n\n\n\n\nAmari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Comput., 10(2), 251–276. https://doi.org/10.1162/089976698300017746\n\n\nAnsel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., … Chintala, S. (2024, April). PyTorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24). https://doi.org/10.1145/3620665.3640366\n\n\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13(5), 834–846. IEEE Transactions on Systems, Man, and Cybernetics. https://doi.org/10.1109/TSMC.1983.6313077\n\n\nBaydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018, February 5). Automatic differentiation in machine learning: A survey. https://doi.org/10.48550/arXiv.1502.05767\n\n\nBeck, A., & Teboulle, M. (2003). Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3), 167–175. https://doi.org/10.1016/S0167-6377(02)00231-6\n\n\nBhatnagar, S., Sutton, R. S., Ghavamzadeh, M., & Lee, M. (2009). Natural actor–critic algorithms. Automatica, 45(11), 2471–2482. https://doi.org/10.1016/j.automatica.2009.07.008\n\n\nBoyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/\n\n\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., & Zhang, Q. (2018). JAX: Composable transformations of python+NumPy programs (Version 0.3.13) [Computer software]. http://github.com/google/jax\n\n\nHaarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017). Reinforcement learning with deep energy-based policies. Proceedings of the 34th International Conference on Machine Learning - Volume 70, 1352–1361.\n\n\nHaarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Proceedings of the 35th International Conference on Machine Learning, 1861–1870. https://proceedings.mlr.press/v80/haarnoja18b.html\n\n\nKakade, S. M. (2001). A natural policy gradient. Advances in Neural Information Processing Systems, 14. https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html\n\n\nKakade, S., & Langford, J. (2002). Approximately optimal approximate reinforcement learning. Proceedings of the Nineteenth International Conference on Machine Learning, 267–274.\n\n\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8(3), 293–321. https://doi.org/10.1007/BF00992699\n\n\nMahadevan, S., Giguere, S., & Jacek, N. (2013). Basis adaptation for sparse nonlinear reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 27(1, 1), 654–660. https://doi.org/10.1609/aaai.v27i1.8665\n\n\nMahadevan, S., & Liu, B. (2012). Sparse q-learning with mirror descent. Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, 564–573.\n\n\nMarbach, P., & Tsitsiklis, J. N. (2001). Simulation-based optimization of markov reward processes. IEEE Transactions on Automatic Control, 46(2), 191–209. IEEE Transactions on Automatic Control. https://doi.org/10.1109/9.905687\n\n\nMartens, J., & Grosse, R. (2015). Optimizing neural networks with kronecker-factored approximate curvature. Proceedings of the 32nd International Conference on Machine Learning, 2408–2417. https://proceedings.mlr.press/v37/martens15.html\n\n\nMeurer, A., Smith, C. P., Paprocki, M., Čertík, O., Kirpichev, S. B., Rocklin, M., Kumar, A., Ivanov, S., Moore, J. K., Singh, S., Rathnayake, T., Vig, S., Granger, B. E., Muller, R. P., Bonazzi, F., Gupta, H., Vats, S., Johansson, F., Pedregosa, F., … Scopatz, A. (2017). SymPy: Symbolic computing in python. PeerJ Computer Science, 3, e103. https://doi.org/10.7717/peerj-cs.103\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. Proceedings of The 33rd International Conference on Machine Learning, 1928–1937. https://proceedings.mlr.press/v48/mniha16.html\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., & Bellemare, M. G. (2016). Safe and efficient off-policy reinforcement learning. Proceedings of the 30th International Conference on Neural Information Processing Systems, 1054–1062.\n\n\nNemirovskij, A. S., Judin, D. B., Dawson, E. R., & Nemirovskij, A. S. (1983). Problem complexity and method efficiency in optimization. Wiley.\n\n\nNocedal, J., & Wright, S. J. (2006). Numerical optimization (2nd ed). Springer.\n\n\nPeters, J., & Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71(7), 1180–1190. https://doi.org/10.1016/j.neucom.2007.11.026\n\n\nPeters, J., Vijayakumar, S., & Schaal, S. (2005). Natural actor-critic. In J. Gama, R. Camacho, P. B. Brazdil, A. M. Jorge, & L. Torgo (Eds.), Machine Learning: ECML 2005 (pp. 280–291). Springer. https://doi.org/10.1007/11564096_29\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P. (2015). Trust region policy optimization. Proceedings of the 32nd International Conference on Machine Learning, 1889–1897. https://proceedings.mlr.press/v37/schulman15.html\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M. I., & Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. In Y. Bengio & Y. LeCun (Eds.), 4th international conference on learning representations. http://arxiv.org/abs/1506.02438\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017, August 28). Proximal policy optimization algorithms. https://doi.org/10.48550/arXiv.1707.06347\n\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., & Guo, D. (2024, April 27). DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. https://doi.org/10.48550/arXiv.2402.03300\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014). Deterministic policy gradient algorithms. Proceedings of the 31st International Conference on Machine Learning, 387–395. https://proceedings.mlr.press/v32/silver14.html\n\n\nSutton, R. S. (1984). Temporal credit assignment in reinforcement learning [PhD thesis]. University of Massachusetts Amherst.\n\n\nSutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. Proceedings of the 13th International Conference on Neural Information Processing Systems, 1057–1063.\n\n\nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science. Cambridge University Press. https://books.google.com?id=NDdqDwAAQBAJ\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., & Freitas, N. de. (2017, February 6). Sample efficient actor-critic with experience replay. 5th International Conference on Learning Representations. https://openreview.net/forum?id=HyM25Mqel\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3), 229–256. https://doi.org/10.1007/BF00992696\n\n\nWitten, I. H. (1977). An adaptive optimal controller for discrete-time markov environments. Information and Control, 34(4), 286–295. https://doi.org/10.1016/S0019-9958(77)90354-0\n\n\nWu, Y., Mansimov, E., Grosse, R. B., Liao, S., & Ba, J. (2017). Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Advances in Neural Information Processing Systems, 30. https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html",
    "href": "imitation_learning.html",
    "title": "8  Imitation Learning",
    "section": "",
    "text": "8.1 Introduction\nImagine you are tasked with learning how to drive. How do, or did, you go about it? At first, this task might seem insurmountable: there are a vast array of controls, and the cost of making a single mistake could be extremely high, making it hard to explore by trial and error. Luckily, there are already people in the world who know how to drive who can get you started. In almost every challenge we face, we “stand on the shoulders of giants” and learn skills from experts who have already mastered them.\nIn machine learning, we often try to teach machines to accomplish tasks that humans are already proficient at. In such cases, the machine learning algorithm is the one learning the new skill, and humans are the “experts” that can demonstrate how to perform the task. Imitation learning is an approach to sequential decision-making where we aim to learn a policy that performs at least as well as the expert. It is often used as a first step for complex tasks where it is too challenging to learn from scratch or difficult to specify a reward function that captures the desired behaviour.\nWe’ll see that the most naive form of imitation learning, called behaviour cloning, is really an application of supervised learning to interactive tasks. We’ll then explore dataset aggregation (DAgger) as a way to query an expert and learn even more effectively.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#introduction",
    "href": "imitation_learning.html#introduction",
    "title": "8  Imitation Learning",
    "section": "",
    "text": "A robot imitating the pose of a young child. Image from Danilyuk (2021).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#behaviour-cloning",
    "href": "imitation_learning.html#behaviour-cloning",
    "title": "8  Imitation Learning",
    "section": "8.2 Behaviour cloning",
    "text": "8.2 Behaviour cloning\nThis notion of “learning from human-provided data” may remind you of the basic premise of Chapter 5. In supervised learning, there is some mapping from inputs to outputs, such as the task of assigning the correct label to an image, that humans can implicitly compute. To teach a machine to calculate this mapping, we first collect a large training dataset by getting people to label a lot of inputs, and then use some optimization algorithm to produce a predictor that maps from the inputs to the outputs as closely as possible.\nHow does this relate to interactive tasks? Here, the input is the observation seen by the agent and the output is the action it selects, so the mapping is the agent’s policy. What’s stopping us from applying supervised learning techniques to mimic the expert’s policy? In principle, nothing! This is called behaviour cloning.\n\nDefinition 8.1 (Behaviour cloning)  \n\nCollect a training dataset of trajectories \\(\\mathcal{D} = (s^n, a^n)_{n=1}^{N}\\) generated by an expert policy \\(\\pi_\\text{expert}\\). (For example, if the dataset contains \\(M\\) trajectories, each with a finite horizon \\(H\\), then \\(N = M \\times H\\).)\nUse a supervised learning algorithm \\(\\texttt{fit} : \\mathcal{D} \\mapsto \\widetilde{\\pi}\\) to extract a policy \\(\\widetilde{\\pi}\\) that approximates the expert policy.\n\n\nTypically, this second task can be framed as empirical risk minimization (which we previously saw in Section 5.3.2):\n\\[\n\\widetilde{\\pi} = \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=0}^{N-1} \\text{loss}(\\pi(s^n), a^n)\n\\tag{8.1}\\]\nwhere \\(\\Pi\\) is some class of possible policies, \\(\\text{loss}\\) is the loss function to measure how different the policy’s prediction is from the true observed action, and the supervised learning algorithm itself, also known as the fitting method, tells us how to compute this \\(\\arg\\min\\).\nHow should we choose the loss function? In supervised learning, we saw that the mean squared error is a good choice for continuous outputs. However, how should we measure the difference between two actions in a discrete action space? In this setting, the policy acts more like a classifier that picks the best action in a given state. Rather than considering a deterministic policy that just outputs a single action, we’ll consider a stochastic policy \\(\\pi\\) that outputs a distribution over actions. This allows us to assign a likelihood to observing the entire dataset \\(\\mathcal{D}\\) under the policy \\(\\pi\\), as if the state-action pairs are independent:\n\\[\n\\mathbb{P}_\\pi(\\mathcal{D}) = \\prod_{n=1}^{N} \\pi(a_n \\mid s_n)\n\\tag{8.2}\\]\nNote that the states and actions are not, however, actually independent! A key property of interactive tasks is that the agent’s output – the action that it takes – may influence its next observation. We want to find a policy under which the training dataset \\(\\mathcal{D}\\) is the most likely. This is called the maximum likelihood estimate of the policy that generated the dataset:\n\\[\n\\widetilde{\\pi} = \\arg\\max_{\\pi \\in \\Pi} \\mathbb{P}_{\\pi}(\\mathcal{D})\n\\tag{8.3}\\]\nThis is also equivalent to doing empirical risk minimization with the negative log likelihood as the loss function:\n\\[\n\\begin{aligned}\n\\widetilde{\\pi} &= \\arg\\min_{\\pi \\in \\Pi} - \\log \\mathbb{P}_\\pi(\\mathcal{D}) \\\\\n&= \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=1}^N - \\log \\pi(a_n \\mid s_n)\n\\end{aligned}\n\\tag{8.4}\\]\nCan we quantify how well this algorithm works? For simplicity, let’s consider the case where the action space is finite and both the expert policy and learned policy are deterministic.\n\nTheorem 8.1 (Performance of behaviour cloning) Suppose the learned policy obtains \\(\\varepsilon\\) classification error. That is, for trajectories drawn from the expert policy, the learned policy chooses a different action at most \\(\\varepsilon\\) of the time:\n\\[\n\\mathbb{E}_{\\tau \\sim \\rho^{\\pi_{\\text{expert}}}} \\left[ \\frac 1 H\\sum_{h=0}^{H-1} \\mathbf{1}\\left\\{ \\widetilde{\\pi}(s_h) \\ne \\pi_{\\text{expert}} (s_h) \\right\\} \\right] \\le \\varepsilon\n\\tag{8.5}\\]\nThen, their value functions differ by\n\\[\n| V^{\\pi_{\\text{expert}}} - V^{\\widetilde{\\pi}} | \\le H^2 \\varepsilon\n\\]\nwhere \\(H\\) is the horizon of the problem.\n\n\nProof. Recall the Performance Difference Lemma (Theorem 7.8). The Performance Difference Lemma allows us to express the difference between \\(\\pi-{\\text{expert}}\\) and \\(\\widetilde{\\pi}\\) as\n\\[\nV_0^{\\pi_{\\text{expert}}}(s) - V_0^{\\widetilde{\\pi}} (s) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\pi_{\\text{expert}}} \\mid s_0 = s} \\left[ \\sum_{h=0}^{H-1} A_h^{\\widetilde{\\pi}} (s_h, a_h) \\right].\n\\tag{8.6}\\]\nNow since the expert policy is deterministic, we can substitute \\(a_h= \\pi_{\\text{expert}}(s_h)\\). This allows us to make a further simplification: since \\(\\pi_{\\text{expert}}\\) is deterministic, the advantage of the chosen action is exactly zero:\n\\[\nA^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) = Q^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) - V^{\\pi_{\\text{expert}}}(s) = 0.\n\\tag{8.7}\\]\nBut the right-hand-side of eq. 8.6 uses \\(A^{\\widetilde{\\pi}}\\), not \\(A^{\\pi-{\\text{expert}}}\\). To bridge this gap, we now use the assumption that \\(\\widetilde{\\pi}\\) obtains \\(\\varepsilon\\) classification error. Note that \\(A_h^{\\widetilde{\\pi}}(s_h, \\pi_{\\text{expert}}(s_h)) = 0\\) when \\(\\pi_{\\text{expert}}(s_h) = \\widetilde{\\pi}(s_h)\\). In the case where the two policies differ on \\(s_h\\), which occurs with probability \\(\\varepsilon\\), the advantage is naively upper bounded by \\(H\\) (assuming rewards are bounded between \\(0\\) and \\(1\\)). Taking the final sum gives the desired bound.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#distribution-shift",
    "href": "imitation_learning.html#distribution-shift",
    "title": "8  Imitation Learning",
    "section": "8.3 Distribution shift",
    "text": "8.3 Distribution shift\nLet us return to the driving analogy. Suppose you have taken some driving lessons and now feel comfortable in your neighbourhood. But today you have to travel to an area you haven’t visited before, such as a highway, where it would be dangerous to try and apply the techniques you’ve already learned. This is the issue of distribution shift: a policy learned under a certain distribution of states may perform poorly if the distribution of states changes.\nThis is already a common issue in supervised learning, where the training dataset for a model might not resemble the environment where it gets deployed. In interactive environments, this issue is further exacerbated by the dependency between the observations and the agent’s behaviour; if you take a wrong turn early on, it may be difficult or impossible to recover in that trajectory.\nHow could you learn a strategy for these new settings? In the driving example, you might decide to install a dashcam to record the car’s surroundings. That way, once you make it back to safety, you can show the recording to an expert, who can provide feedback at each step of the way. Then the next time you go for a drive, you can remember the expert’s advice, and take a safer route. You could then repeat this training as many times as desired, thereby collecting the expert’s feedback over a diverse range of locations. This is the key idea behind dataset aggregation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#dataset-aggregation-dagger",
    "href": "imitation_learning.html#dataset-aggregation-dagger",
    "title": "8  Imitation Learning",
    "section": "8.4 Dataset aggregation (DAgger)",
    "text": "8.4 Dataset aggregation (DAgger)\nThe DAgger algorithm assumes that we have query access to the expert policy. That is, for a given state \\(s\\), we can ask for the expert’s action \\(\\pi_{\\text{expert}}(s)\\) in that state. We also need access to the environment for rolling out policies. This makes DAgger an online algorithm, as opposed to pure behaviour cloning, which is offline since we don’t need to act in the environment at all.\nYou can think of DAgger as a specific way of collecting the dataset \\(\\mathcal{D}\\).\n\nDefinition 8.2 (DAgger algorithm) Inputs: \\(\\pi_{\\text{expert}}\\), an initial policy \\(\\pi_{\\text{init}}\\), the number of iterations \\(T\\), and the number of trajectories \\(N\\) to collect per iteration.\n\nInitialize \\(\\mathcal{D} = \\{\\}\\) (the empty set) and \\(\\pi = \\pi_{\\text{init}}\\).\nFor \\(i= 1, \\dots, I\\):\n\nCollect \\(T\\) trajectories \\(\\tau_0, \\dots, \\tau_{T-1}\\) using the current policy \\(\\pi\\).\nFor each trajectory \\(\\tau_n\\):\n\nReplace each action \\(a_h\\) in \\(\\tau_n\\) with the expert action \\(\\pi_{\\text{expert}}(s_h)\\).\nCall the resulting trajectory \\(\\tau^{\\text{expert}}_n\\).\n\n\\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{ \\tau^{\\text{expert}}_1, \\dots, \\tau^{\\text{expert}}_n \\}\\).\nLet \\(\\pi \\gets \\texttt{fit}(\\mathcal{D})\\), where \\(\\texttt{fit}\\) is a behaviour cloning algorithm.\n\nReturn \\(\\pi\\).\n\n\nWe leave the implementation as an exercise. How well does DAgger perform?\n\nTheorem 8.2 (Performance of DAgger) Let \\(\\pi_\\text{expert}\\) be the expert policy and \\(\\pi_\\text{DAgger}\\) be the policy resulting from DAgger. In \\(I= \\widetilde O(H^2)\\) iterations, with high probability,\n\\[\n|V^{\\pi_{\\text{expert}}} - V^{\\pi_{\\text{DAgger}}}| \\le H\\varepsilon,\n\\tag{8.8}\\]\nwhere \\(\\varepsilon\\) is the “classification error” guaranteed by the supervised learning algorithm.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#key-takeaways",
    "href": "imitation_learning.html#key-takeaways",
    "title": "8  Imitation Learning",
    "section": "8.5 Key takeaways",
    "text": "8.5 Key takeaways\nGiven a task where learning from scratch is too challenging, if we have access to expert data, we can use supervised learning to find a policy that imitates the expert demonstrations.\nThe simplest way to do this is to apply a supervised learning algorithm to an already-collected dataset of expert state-action pairs. This is called behaviour cloning. However, given query access to the expert policy, we can do better by integrating its feedback in an online loop. The DAgger algorithm is one way of doing this, where we use the expert policy to augment trajectories and then learn from this augmented dataset using behaviour cloning.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#sec-il-bib",
    "href": "imitation_learning.html#sec-il-bib",
    "title": "8  Imitation Learning",
    "section": "8.6 Bibliographic notes and further reading",
    "text": "8.6 Bibliographic notes and further reading\nEarlier interest in imitation learning arose in the context of autonomous driving (Pomerleau, 1991). This task is suitable for imitation learning since expert (or near-expert) driving data is readily available. It is also challenging to express a reward function that captures exactly what we mean by “good driving”. Imitation learning methods sidestep this issue by directly training the algorithm to imitate expert demonstrations. The DAgger algorithm (def. 8.2) is due to Ross et al. (2010). The performance guarantee is stated as Ross et al. (2010, thm. 3.4).\nAnother approach is to infer the reward function from the expert trajectories. This is known as the inverse reinforcement learning (IRL) problem (Abbeel & Ng, 2004; Ng & Russell, 2000; Russell, 1998). The typical RL problem is going from a reward function to an optimal policy, so the inverse problem is going from an optimal policy to the underlying reward function. One can then use typical RL techniques to optimize for the inferred reward function. This tends to generalize better than direct behaviour cloning. The challenge is that this problem is not well-defined, since any policy is optimal for infinitely many possible reward functions. This means that the researcher must come up with a useful “regularization” assumption to select which of the possible reward functions is the most plausible. Three common modelling assumptions are maximum-margin IRL (Ng & Russell, 2000; Ratliff et al., 2006), Bayesian IRL (Ramachandran & Amir, 2007), and maximum-entropy IRL (Ziebart et al., 2008, 2010).\nAnother framework for learning behaviour from expert demonstrations is generative adversarial imitation learning (Ho & Ermon, 2016; Orsini et al., 2021), inspired by generative adversarial networks (GANs) (Goodfellow et al., 2020). Rather than first learning a reward function from expert data and then optimizing the policy in a separate phase, generative adversarial imitation learning simultaneously learns the reward function and the optimal policy, leading to improved performance.\nWe can infer a reward function from other expert data besides demonstrations. Expert demonstrations provide a lot of signal but can be prohibitively expensive to collect for some tasks. It is often easier to recognize a good solution than to generate one. This leads to the related approach of reinforcement learning from human feedback (RLHF). Instead of inferring a reward function from expert demonstrations, we instead infer a reward function from a dataset of expert rankings of trajectories. This is the dominant approach used in RL finetuning of large language models (Ouyang et al., 2022; Ziegler et al., 2020). We recommend Lambert (2024) for a comprehensive treatment of RLHF.\nPiot et al. (2017) unifies imitation learning and IRL in the set-policy framework. Gleave et al. (2022) is a library of modular implementations of imitation learning algorithms in PyTorch. We recommend Zare et al. (2024) for a more comprehensive survey of imitation learning.\n\n\n\n\nAbbeel, P., & Ng, A. Y. (2004, July 4). Apprenticeship learning via inverse reinforcement learning. Proceedings of the Twenty-First International Conference on Machine Learning. International Conference on Machine Learning. https://doi.org/10.1145/1015330.1015430\n\n\nDanilyuk, P. (2021). A robot imitating a girl’s movement [Graphic]. https://www.pexels.com/photo/a-robot-imitating-a-girl-s-movement-8294811/\n\n\nGleave, A., Taufeeque, M., Rocamonde, J., Jenner, E., Wang, S. H., Toyer, S., Ernestus, M., Belrose, N., Emmons, S., & Russell, S. (2022, November 22). imitation: Clean imitation learning implementations. https://doi.org/10.48550/arXiv.2211.11972\n\n\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2020). Generative adversarial networks. Commun. ACM, 63(11), 139–144. https://doi.org/10.1145/3422622\n\n\nHo, J., & Ermon, S. (2016). Generative adversarial imitation learning. Proceedings of the 30th International Conference on Neural Information Processing Systems, 4572–4580.\n\n\nLambert, N. (2024). Reinforcement learning from human feedback. Online. https://rlhfbook.com\n\n\nNg, A. Y., & Russell, S. J. (2000). Algorithms for inverse reinforcement learning. Proceedings of the Seventeenth International Conference on Machine Learning, 663–670. https://ai.stanford.edu/~ang/papers/icml00-irl.pdf\n\n\nOrsini, M., Raichuk, A., Hussenot, L., Vincent, D., Dadashi, R., Girgin, S., Geist, M., Bachem, O., Pietquin, O., & Andrychowicz, M. (2021). What matters for adversarial imitation learning? Proceedings of the 35th International Conference on Neural Information Processing Systems, 14656–14668.\n\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. Proceedings of the 36th International Conference on Neural Information Processing Systems, 27730–27744.\n\n\nPiot, B., Geist, M., & Pietquin, O. (2017). Bridging the gap between imitation learning and inverse reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 28(8), 1814–1826. IEEE Transactions on Neural Networks and Learning Systems. https://doi.org/10.1109/TNNLS.2016.2543000\n\n\nPomerleau, D. A. (1991). Efficient training of artificial neural networks for autonomous navigation. Neural Computation, 3(1), 88–97. Neural Computation. https://doi.org/10.1162/neco.1991.3.1.88\n\n\nRamachandran, D., & Amir, E. (2007). Bayesian inverse reinforcement learning. Proceedings of the 20th International Joint Conference on Artifical Intelligence, 2586–2591.\n\n\nRatliff, N. D., Bagnell, J. A., & Zinkevich, M. A. (2006). Maximum margin planning. Proceedings of the 23rd International Conference on Machine Learning, 729–736. https://doi.org/10.1145/1143844.1143936\n\n\nRoss, S., Gordon, G. J., & Bagnell, J. (2010, November 2). A reduction of imitation learning and structured prediction to no-regret online learning. International Conference on Artificial Intelligence and Statistics. https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e\n\n\nRussell, S. (1998). Learning agents for uncertain environments (extended abstract). Proceedings of the Eleventh Annual Conference on Computational Learning Theory, 101–103. https://doi.org/10.1145/279943.279964\n\n\nZare, M., Kebria, P. M., Khosravi, A., & Nahavandi, S. (2024). A survey of imitation learning: Algorithms, recent developments, and challenges. IEEE Transactions on Cybernetics, 54(12), 7173–7186. IEEE Transactions on Cybernetics. https://doi.org/10.1109/TCYB.2024.3395626\n\n\nZiebart, B. D., Bagnell, J. A., & Dey, A. K. (2010). Modeling interaction via the principle of maximum causal entropy. Proceedings of the 27th International Conference on International Conference on Machine Learning, 1255–1262.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., & Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3, 1433–1438.\n\n\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., & Irving, G. (2020, January 8). Fine-Tuning Language Models from Human Preferences. https://doi.org/10.48550/arXiv.1909.08593",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "planning.html",
    "href": "planning.html",
    "title": "9  Tree Search Methods",
    "section": "",
    "text": "9.1 Introduction\nHave you ever lost a strategy game against a skilled opponent? It probably seemed like they were ahead of you at every turn. They might have been planning ahead and anticipating your actions, then formulating their strategy to counter yours. If this opponent was a computer, they might have been using one of the strategies that we are about to explore.\nCode\n%load_ext autoreload\n%autoreload 2\nCode\nfrom utils import Int, Array, latex, jnp, NamedTuple\nfrom enum import IntEnum",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-two-game",
    "href": "planning.html#sec-two-game",
    "title": "9  Tree Search Methods",
    "section": "9.2 Deterministic, zero sum, fully observable two-player games",
    "text": "9.2 Deterministic, zero sum, fully observable two-player games\nIn this chapter, we will focus on games that are:\n\ndeterministic,\nzero sum (one player wins and the other loses),\nfully observable, that is, the state of the game is perfectly known by both players,\nfor two players that alternate turns,\n\nWe can represent such a game as a complete game tree that describes every possible match. Each possible state is a node in the tree, and since we only consider deterministic games, we can represent actions as edges leading from the current state to the next. Each path through the tree, from root to leaf, represents a single game.\n\n\n\n\n\n\nFigure 9.1: The first two layers of the complete game tree of tic-tac-toe.\n\n\n\n(In games where one can return to a previous board state, to avoid introducing cycles, we might modify the state by also including the number of moves that have been made. This ensures that the complete game tree indeed has no cycles.)\nIf you could store the complete game tree on a computer, you would be able to win every potentially winnable game by searching all paths from your current state and taking a winning move. We will see an explicit algorithm for this in Section 9.3. However, as games become more complex, it becomes computationally impossible to search every possible path.\nFor instance, a chess player has roughly 30 actions to choose from at each turn, and each game takes roughly 40 moves per player, so trying to solve chess exactly using minimax would take somewhere on the order of \\(30^{80} \\approx 10^{118}\\) operations. That’s 10 billion billion billion billion billion billion billion billion billion billion billion billion billion operations. As of the time of writing, the fastest processor can achieve almost 10 GHz (10 billion operations per second), so to fully solve chess using minimax is many, many orders of magnitude out of reach.\nIt is thus intractable, in any realistic setting, to solve the complete game tree exactly. Luckily, only a small fraction of those games ever occur in reality. Later in this chapter, we will explore ways to prune away parts of the tree that we know we can safely ignore. We can also approximate the value of a state without fully evaluating it. Using these approximations, we can no longer guarantee winning the game, but we can come up with strategies that will do well against most opponents.\n\n9.2.1 Notation\nLet us now describe these games formally. We’ll call the first player Max and the second player Min. Max seeks to maximize the final game score, while Min seeks to minimize the final game score.\n\nWe’ll use \\(\\mathcal{S}\\) to denote the set of all possible game states.\nThe game begins in some initial state \\(s_0 \\in \\mathcal{S}\\).\nMax moves on even turn numbers \\(h = 2n\\), and Min moves on odd turn numbers \\(h = 2n+1\\), where \\(n\\) is a natural number.\nThe space of possible actions, \\(\\mathcal{A}_h(s)\\), depends on the state itself, as well as whose turn it is. (For example, in tic-tac-toe, Max can only play Xs while Min can only play Os.)\nThe game ends after \\(H\\) total moves (which might be even or odd). We call the final state a terminal state.\n\\(P\\) denotes the state transitions, that is, \\(P(s, a)\\) denotes the resulting state when taking action \\(a \\in \\mathcal{A}(s)\\) in state \\(s\\). We’ll assume that this function is time-homogeneous (a.k.a. stationary) and doesn’t change across timesteps.\n\\(r(s)\\) denotes the game score of the terminal state \\(s\\). Note that this is some positive or negative value seen by both players: A positive value indicates Max winning, a negative value indicates Min winning, and a value of \\(0\\) indicates a tie.\n\nWe also call the sequence of states and actions a trajectory.\n\nExercise 9.1 (Variable length games) Above, we suppose that the game ends after \\(H\\) total moves. But most real games have a variable length. How would you describe this?\n\n\nExample 9.1 (Tic-tac-toe) Let us frame tic-tac-toe in this setting.\n\nEach of the \\(9\\) squares is either empty, marked X, or marked O. So there are \\(|\\mathcal{S}| = 3^9\\) potential states. Not all of these may be reachable!\nThe initial state \\(s_0\\) is the empty board.\nThe set of possible actions for Max in state \\(s\\), \\(\\mathcal{A}_{2n}(s)\\), is the set of tuples \\((\\text{``X''}, i)\\) where \\(i\\) refers to an empty square in \\(s\\). Similarly, \\(\\mathcal{A}_{2n+1}(s)\\) is the set of tuples \\((\\text{``O''}, i)\\) where \\(i\\) refers to an empty square in \\(s\\).\nWe can take \\(H = 9\\) as the longest possible game length.\n\\(P(s, a)\\) for a nonterminal state \\(s\\) is simply the board with the symbol and square specified by \\(a\\) marked into \\(s\\). Otherwise, if \\(s\\) is a terminal state, i.e. it already has three symbols in a row, the state no longer changes.\n\\(r(s)\\) at a terminal state is \\(+1\\) if there are three Xs in a row, \\(-1\\) if there are three Os in a row, and \\(0\\) otherwise.\n\n\nOur notation may remind you of Chapter 2. Given that these games also involve a sequence of states and actions, can we formulate them as finite-horizon MDPs? The two settings are not exactly analogous, since in MDPs we only consider a single policy, while these games involve two distinct players with opposite objectives. Since we want to analyze the behaviour of both players at the same time, describing such a game as an MDP is more trouble than it’s worth.\n\n\nCode\nclass Player(IntEnum):\n    EMPTY = 0\n    X = 1\n    O = 2\n\n\nif False:\n    class TicTacToeEnv(gym.Env):\n        metadata = {\"render.modes\": [\"human\"]}\n\n        def __init__(self):\n            super().__init__()\n            self.action_space = spaces.Discrete(9)\n            self.observation_space = spaces.Box(\n                low=0, high=2, shape=(3, 3), dtype=jnp.int32\n            )\n            self.board = None\n            self.current_player = None\n            self.done = None\n\n        def reset(self, seed=None, options=None):\n            super().reset(seed=seed)\n            self.board = jnp.zeros((3, 3), dtype=jnp.int32)\n            self.current_player = Player.X\n            self.done = False\n            return self.board, {}\n\n        def step(self, action: jnp.int32) -&gt; Int[Array, \"3 3\"]:\n            \"\"\"Take the action a in state s.\"\"\"\n            if self.done:\n                raise ValueError(\"The game is already over. Call `env.reset()` to reset the environment.\")\n            \n            row, col = divmod(action, 3)\n            if self.board[row, col] != Player.EMPTY:\n                return self.board, -10\n            return s.at[row, col].set(player)\n\n        @staticmethod\n        def is_terminal(s: Int[Array, \"3 3\"]):\n            \"\"\"Check if the game is over.\"\"\"\n            return is_winner(s, Player.X) or is_winner(s, Player.O) or jnp.all(s == Player.EMPTY)\n\n        @staticmethod\n        def is_winner(board: Int[Array, \"3 3\"], player: Player):\n            \"\"\"Check if the given player has won.\"\"\"\n            return any(\n                jnp.all(board[i, :] == player) or\n                jnp.all(board[:, i] == player)\n                for i in range(3)\n            ) or jnp.all(jnp.diag(board) == player) or jnp.all(jnp.diag(jnp.fliplr(board)) == player)\n\n        @staticmethod\n        def show(s: Int[Array, \"3 3\"]):\n            \"\"\"Print the board.\"\"\"\n            for row in range(3):\n                print(\" | \".join(\" XO\"[s[row, col]] for col in range(3)))\n                if row &lt; 2:\n                    print(\"-\" * 5)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-min-max-search",
    "href": "planning.html#sec-min-max-search",
    "title": "9  Tree Search Methods",
    "section": "9.3 Min-max search",
    "text": "9.3 Min-max search\nIn the introduction, we claimed that we could win any potentially winnable game by looking ahead and predicting the opponent’s actions. This would mean that each nonterminal state already has some predetermined game score. That is, in each state, it is already possible to determine which player is going to win.\nLet \\(V_h^\\star(s)\\) denote the game score under optimal play from both players starting in state \\(s\\) at time \\(h\\).\n\nDefinition 9.1 (Min-max search algorithm) The best move for Max is the one that leads to the maximum value. Correspondingly, the best move for Min is the one that leads to the minimum value. This naturally gives rise to a recursive definition of the value of each state under optimal play:\n\\[\nV_h^{\\star}(s) = \\begin{cases}\nr(s) & h= H\\\\\n\\max_{a \\in \\mathcal{A}_h(s)} V_{h+1}^{\\star}(P(s, a)) & h\\text{ is even and } h&lt; H \\\\\n\\min_{a \\in \\mathcal{A}_h(s)} V_{h+1}^{\\star}(P(s, a)) & h\\text{ is odd and } h&lt; H\n\\end{cases}.\n\\]\nRecall that \\(P(s, a)\\) denotes the next state after taking action \\(a\\) in state \\(s\\).\n\nWe can compute this by dynamic programming. We start at the terminal states, where the game’s outcome is known, and work backwards. This might remind you of policy evaluation in finite-horizon MDPs (Section 2.3.1).\nThis translates directly into a recursive depth-first search algorithm for searching the complete game tree.\n\n\nCode\ndef minimax_search(s, player) -&gt; tuple[\"Action\", \"Value\"]:\n    # Return the value of the state (for Max) and the best action for Max to take.\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in env.action_space(s):\n            _, v = minimax_search(env.step(s, a), min)\n            if v &gt; v_max:\n                a_max, v_max = a, v\n        return a_max, v_max\n    else:\n        a_min, v_min = None, None\n        for a in env.action_space(s):\n            _, v = minimax_search(env.step(s, a), max)\n            if v &lt; v_min:\n                a_min, v_min = a, v\n        return a_min, v_min\n\nlatex(minimax_search, id_to_latex={\"env.step\": \"P\", \"env.action_space\": r\"\\mathcal{A}\"})\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{minimax\\_search}(s, \\mathrm{player}) \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{env}.\\mathrm{is\\_terminal} \\mathopen{}\\left( s \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( \\mathrm{None}, \\mathrm{env}.\\mathrm{winner} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{player} \\equiv \\mathrm{max} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathcal{A} \\mathopen{}\\left( s \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( P \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{min} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &gt; v_{\\mathrm{max}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{else} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathcal{A} \\mathopen{}\\left( s \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( P \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{max} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &lt; v_{\\mathrm{min}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\n\nExample 9.2 (Min-max search for a simple game) Consider a simple game with just two steps: Max chooses one of three possible actions (A, B, C), and then Min chooses one of three possible actions (D, E, F). The combination leads to a certain integer outcome, shown in the table below:\n\n\n\n\nD\nE\nF\n\n\n\n\nA\n4\n-2\n5\n\n\nB\n-3\n3\n1\n\n\nC\n0\n3\n-1\n\n\n\nWe can visualize this as the following complete game tree, where each box contains the value \\(V_h^\\star(s)\\) of that node. The min-max values of the terminal states are already known:\n\nWe begin min-max search at the root, exploring each of Max’s actions. Suppose Max chooses action A. Then Min will choose action E to minimize the game score, making the value of this game node \\(\\min(4, -2, 5) = -2\\).\n\nSimilarly, if Max chooses action B, then Min will choose action D, and if Max chooses action C, then Min will choose action F. We can fill in the values of these nodes accordingly:\n\nThus, Max’s best move is to take action C, resulting in a game score of \\(\\max(-2, -3, -1) = -1\\).\n\n\n\n9.3.1 Complexity of min-max search\nAt each of the \\(H\\) timesteps, this algorithm iterates through the entire action space at that state, and therefore has a time complexity of \\(H^{n_A}\\) (where \\(n_A\\) is the largest number of actions possibly available at once). This makes the min-max algorithm impractical for even moderately sized games.\nBut do we need to compute the exact value of every possible state? Instead, is there some way we could “ignore” certain actions and their subtrees if we already know of better options? The alpha-beta search makes use of this intuition.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-alpha-beta-search",
    "href": "planning.html#sec-alpha-beta-search",
    "title": "9  Tree Search Methods",
    "section": "9.4 Alpha-beta pruning",
    "text": "9.4 Alpha-beta pruning\nFor a given deterministic, zero-sum, fully observable two-player game (Section 9.2), we have seen that it is possible to “solve” the game, that is, determine the best move in every situation, using min-max search (Section 9.3). However, the time complexity of min-max search makes it infeasible for most scenarios. Alpha-beta pruning improves min-max search by pruning down the search tree.\nSuppose Max is in state \\(s\\) is deciding between action \\(a\\) and \\(a'\\). If at any point Max finds out that action \\(a'\\) is no better than action \\(a\\), she doesn’t need to evaluate action \\(a'\\) any further.\nConcretely, we run min-max search as above, except now we keep track of two additional parameters \\(\\alpha(s)\\) and \\(\\beta(s)\\) while evaluating each state:\n\nStarting in state \\(s\\), Max can achieve a game score of at least \\(\\alpha(s)\\) assuming Min plays optimally. That is, \\(V^\\star_h(s) \\ge \\alpha(s)\\) at all points.\nAnalogously, starting in state \\(s\\), Min can ensure a game score of at most \\(\\beta(s)\\) assuming Max plays optimally. That is, \\(V^\\star_h(s) \\le \\beta(s)\\) at all points.\n\nSuppose we are evaluating \\(V^\\star_h(s)\\), where it is Max’s turn (\\(h\\) is even). We update \\(\\alpha(s)\\) to be the highest minimax value achievable from \\(s\\) so far. That is, the value of \\(s\\) is at least \\(\\alpha(s)\\). Suppose Max chooses action \\(a\\), which leads to state \\(s'\\), in which it is Min’s turn. If any of Min’s actions in \\(s'\\) achieve a value \\(V^\\star_{h+1}(s') \\le \\alpha(s)\\), we know that Max would not choose action \\(a\\), since they know that it is worse than whichever action gave the value \\(\\alpha(s)\\). Similarly, to evaluate a state on Min’s turn, we update \\(\\beta(s)\\) to be the lowest value achievable from \\(s\\) so far. That is, the value of \\(s\\) is at most \\(\\beta(s)\\). Suppose Min chooses action \\(a\\), which leads to state \\(s'\\) for Max. If Max has any actions that do better than \\(\\beta(s)\\), they would take it, making action \\(a\\) a suboptimal choice for Min.\n\nExample 9.3 (Alpha-beta search for a simple game) Let us use the same simple game from ex. 9.2. We list the values of \\(\\alpha(s), \\beta(s)\\) in each node throughout the algorithm. These values are initialized to \\(-\\infty, +\\infty\\) respectively. We shade any squares that have not been visited by the algorithm, and we assume that actions are evaluated from left to right.\n\nSuppose Max takes action A. Let \\(s'\\) be the resulting game state. The values of \\(\\alpha(s')\\) and \\(\\beta(s')\\) are initialized at the same values as the root state, since we want to prune a subtree if there exists a better action at any step higher in the tree.\n\nThen we iterate through Min’s possible actions, updating the value of \\(\\beta(s')\\) as we go.\n \nOnce the value of state \\(s'\\) is fully evaluated, we know that Max can achieve a value of at least \\(-2\\) starting from the root, and so we update \\(\\alpha(s)\\), where \\(s\\) is the root state:\n\nThen Max imagines taking action B. Again, let \\(s'\\) denote the resulting game state. We initialize \\(\\alpha(s')\\) and \\(\\beta(s')\\) from the root:\n\nNow suppose Min takes action D, resulting in a value of \\(-3\\). We see that \\(V^\\star_h(s') = \\min(-3, x, y)\\), where \\(x\\) and \\(y\\) are the values of the remaining two actions. But since \\(\\min(-3, x, y) \\le -3\\), we know that the value of \\(s'\\) is at most \\(-3\\). But Max can achieve a better value of \\(\\alpha(s') = -2\\) by taking action A, and so Max will never take action B, and we can prune the search here. We will use dotted lines to indicate states that have been ruled out from the search:\n\nFinally, suppose Max takes action C. For Min’s actions D and E, there is still a chance that action C might outperform action A, so we continue expanding:\n \nFinally, we see that Min taking action F achieves the minimum value at this state. This shows that optimal play is for Max to take action C, and Min to take action F.\n\n\n\n\nCode\ndef alpha_beta_search(s, player, alpha, beta) -&gt; tuple[\"Action\", \"Value\"]:\n    # Return the value of the state (for Max) and the best action for Max to take.\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), min, alpha, beta)\n            if v &gt; v_max:\n                a_max, v_max = a, v\n                alpha = max(alpha, v)\n            if v_max &gt;= beta:\n                # we know Min will not choose the action that leads to this state\n                return a_max, v_max\n        return a_max, v_max\n\n    else:\n        a_min, v_min = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), max)\n            if v &lt; v_min:\n                a_min, v_min = a, v\n                beta = min(beta, v)\n            if v_min &lt;= alpha:\n                # we know Max will not choose the action that leads to this state\n                return a_min, v_min\n        return a_min, v_min\n\n\nlatex(alpha_beta_search)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{alpha\\_beta\\_search}(s, \\mathrm{player}, \\alpha, \\beta) \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{env}.\\mathrm{is\\_terminal} \\mathopen{}\\left( s \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( \\mathrm{None}, \\mathrm{env}.\\mathrm{winner} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{player} \\equiv \\mathrm{max} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathrm{actions} \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( \\mathrm{env}.\\mathrm{step} \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{min}, \\alpha, \\beta \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &gt; v_{\\mathrm{max}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{4em} \\alpha \\gets \\mathrm{max} \\mathopen{}\\left( \\alpha, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{3em} \\mathbf{if} \\ v_{\\mathrm{max}} \\ge \\beta \\\\ \\hspace{4em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{else} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathrm{actions} \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( \\mathrm{env}.\\mathrm{step} \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{max} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &lt; v_{\\mathrm{min}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{4em} \\beta \\gets \\mathrm{min} \\mathopen{}\\left( \\beta, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{3em} \\mathbf{if} \\ v_{\\mathrm{min}} \\le \\alpha \\\\ \\hspace{4em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nHow do we choose what order to explore the branches? As you can tell, this significantly affects the efficiency of the pruning algorithm. If Max explores the possible actions in order from worst to best, they will not be able to prune any branches at all! Additionally, to verify that an action is suboptimal, we must run the search recursively from that action, which ultimately requires traversing the tree all the way to a leaf node. The longer the game might possibly last, the more computation we have to run.\nIn practice, we can often use background information about the game to develop a heuristic for evaluating possible actions. If a technique is based on background information or intuition, especially if it isn’t rigorously justified, we call it a heuristic.\nCan we develop heuristic methods for tree exploration that works for all sorts of games?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-mcts",
    "href": "planning.html#sec-mcts",
    "title": "9  Tree Search Methods",
    "section": "9.5 Monte Carlo Tree Search",
    "text": "9.5 Monte Carlo Tree Search\nThe task of evaluating actions in a complex environment might seem familiar. We’ve encountered this problem before in both multi-armed bandits (Chapter 4) and Markov decision processes (Chapter 2). Now we’ll see how to combine concepts from these to form a more general and efficient tree search heuristic called Monte Carlo Tree Search (MCTS).\nWhen a problem is intractable to solve exactly, we often turn to approximate algorithms that sacrifice some accuracy in exchange for computational efficiency. MCTS also improves on alpha-beta search in this sense. As the name suggests, MCTS uses Monte Carlo simulation, that is, collecting random samples and computing the sample statistics, in order to approximate the value of each action.\nAs before, we imagine a game tree in which each path represents an entire game. MCTS assigns values to only the game states that are relevant to the current game. That is, we maintain a search tree that we gradually expand at each move. For comparison, in alpha-beta search, the entire tree only needs to be solved once, and from then on, choosing an action is as simple as taking a maximum over the previously computed values.\nThe crux of MCTS is approximating the win probability of a state by a sample probability. In practice, MCTS is used for games with binary outcomes where \\(r(s) \\in \\{ +1, -1 \\}\\), and so this is equivalent to approximating the final game score. To approximate the win probability from state \\(s\\), MCTS samples random games starting in \\(s\\) and computes the sample proportion of those that the player wins.\nNote that, for a given state \\(s\\), choosing the best action \\(a\\) can be framed as a multi-armed bandit problem, where each action corresponds to an arm, and the reward distribution of arm \\(k\\) is the distribution of the game score over random games after choosing that arm. The most commonly used bandit algorithm in practice for MCTS is the upper confidence bound algorithm (Section 4.7).\n\nRemark 9.1 (Summary of UCB). Let us quickly review the UCB algorithm for a multi-armed bandit problem (Section 4.7) For each arm \\(k\\), we track the sample mean \\[\n\\hat \\mu^k_t = \\frac{1}{N_t^k} \\sum_{\\tau=0}^{t-1} \\mathbf{1}\\left\\{a_\\tau = k\\right\\} r_\\tau\n\\] of all rewards from that arm up to time \\(t\\). Then we construct a confidence interval \\[\nC_t^k = [\\hat \\mu^k_t - B_t^k, \\hat \\mu^k_t + B_t^k],\n\\] where \\(B_t^k = \\sqrt{\\frac{\\ln(2 t / \\delta)}{2 N_t^k}}\\) is given by Hoeffding’s inequality, so that with probability \\(\\delta\\) (some fixed parameter we choose), the true mean \\(\\mu^k\\) lies within \\(C_t^k\\). Note that \\(B_t^k\\) scales like \\(\\sqrt{1/N^k_t}\\), i.e. the more we have visited that arm, the more confident we get about it, and the narrower the confidence interval.\nTo select an arm, we pick the arm with the highest upper confidence bound.\n\nThis means that, for each edge in the game tree, which corresponds to a state-action pair \\((s, a)\\), we keep track of the statistics required to compute its UCB:\n\nHow many times it has been “visited” (\\(N_t^{s, a}\\))\nHow many of those visits resulted in victory (\\(\\sum_{\\tau=0}^{t-1} \\mathbf{1}\\left\\{(s_\\tau, a_\\tau) = (s, a)\\right\\} r_\\tau\\)). Let us call this latter value \\(W^{s, a}_t\\) (for number of “wins”).\n\nWhat does \\(t\\) refer to in the above expressions? Recall \\(t\\) refers to the number of time steps elapsed in the bandit environment. As mentioned above, each state \\(s\\) corresponds to its own bandit environment, and so \\(t\\) refers to \\(N^s\\), that is, how many actions have been taken from state \\(s\\). This term, \\(N^s\\), gets incremented as the algorithm runs; for simplicity, we won’t introduce another index to track how it changes.\n\nDefinition 9.2 (Monte Carlo tree search algorithm) Here we describe how to perform a Monte Carlo tree search for choosing a single action in state \\(s_{\\text{start}}\\).\nInputs:\n\n\\(T\\), the number of iterations per move\n\\(\\pi_{\\text{rollout}}\\), the rollout policy for randomly sampling games\n\\(c\\), a positive value that encourages exploration\n\nTo choose a single move starting at state \\(s_{\\text{start}}\\), MCTS first tries to estimate the UCB values for each of the possible actions \\(\\mathcal{A}(s_\\text{start})\\), and then chooses the best one. To estimate the UCB values, it repeats the following four steps \\(T\\) times:\n\nSelection: We start at \\(s = s_{\\text{start}}\\). Let \\(\\tau\\) be an empty list that we will use to track states and actions.\n\nChoose \\(a \\gets \\arg\\max_k \\text{UCB}^{s, k}\\), where \\[\n\\text{UCB}^{s, a}\n=\n\\frac{W^{s, a}}{N^{s, a}} + c \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\\tag{9.1}\\]\nAppend \\((s, a)\\) to \\(\\tau\\)\nIf \\(s\\) has at least one action that hasn’t been taken, move onto the next step. Otherwise, move to the next state \\(s \\gets P(s, a)\\) and repeat.\n\nExpansion: Let \\(s_\\text{new}\\) denote the final state in \\(\\tau\\) (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from \\(s_\\text{new}\\). Call it \\(a_{\\text{new}}\\). Add it to \\(\\tau\\).\nSimulation: Simulate a complete game episode by starting with the action \\(a_{\\text{new}}\\) and then playing according to \\(\\pi_\\text{rollout}\\). This results in the outcome \\(r \\in \\{ +1, -1 \\}\\).\nBackup: For each \\((s, a) \\in \\tau\\):\n\nSet \\(N^{s, a} \\gets N^{s, a} + 1\\)\n\\(W^{s, a} \\gets W^{s, a} + r\\)\nSet \\(N^s \\gets N^s + 1\\)\n\n\nAfter \\(T\\) repeats of the above, we return the action with the highest UCB value eq. 9.1. Then play continues.\nBetween turns, we can keep the subtree whose statistics we have visited so far. However, the rest of the tree for the actions we did not end up taking gets discarded.\n\nThe application which brought the MCTS algorithm to fame was DeepMind’s AlphaGo Silver et al. (2016). Since then, it has been used in numerous applications ranging from games to automated theorem proving.\nHow accurate is this Monte Carlo estimation? It depends heavily on the rollout policy \\(\\pi_\\text{rollout}\\). If the distribution \\(\\pi_\\text{rollout}\\) induces over games is very different from the distribution seen during real gameplay, we might end up with a poor value approximation.\n\n9.5.1 Incorporating value functions and policies\nTo remedy this, we might make use of a value function \\(v : \\mathcal{S} \\to \\mathbb{R}\\) that more efficiently approximates the value of a state. Then, we can replace the simulation step of def. 9.2 with evaluating \\(r = v(s-\\text{next})\\), where \\(s_\\text{next} = P(s_\\text{new}, a_\\text{new})\\).\nWe might also make use of a “guiding” policy \\(\\pi_\\text{guide} : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\) that provides “intuition” as to which actions are more valuable in a given state. We can scale the exploration term of eq. 9.1 according to the policy’s outputs.\nPutting these together, we can describe an updated version of MCTS that makes use of these value functions and policy:\n\nDefinition 9.3 (Monte Carlo tree search with policy and value functions) Inputs: - \\(T\\), the number of iterations per move - \\(v\\), a value function that evaluates how good a state is - \\(\\pi_\\text{guide}\\), a guiding policy that encourages certain actions - \\(c\\), a positive value that encourages exploration\nTo select a move in state \\(s_\\text{start}\\), we repeat the following four steps \\(T\\) times:\n\nSelection: We start at \\(s = s_{\\text{start}}\\). Let \\(\\tau\\) be an empty list that we will use to track states and actions.\n\nUntil \\(s\\) has at least one action that hasn’t been taken:\n\nChoose \\(a \\gets \\arg\\max_k \\text{UCB}^{s, k}\\), where \\[\n\\text{UCB}^{s, a} = \\frac{W^{s, a}}{N^s} + c \\cdot \\pi_\\text{guide}(a \\mid s) \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\\tag{9.2}\\]\nAppend \\((s, a)\\) to \\(\\tau\\)\nSet \\(s \\gets P(s, a)\\)\n\n\nExpansion: Let \\(s_\\text{new}\\) denote the final state in \\(\\tau\\) (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from \\(s_\\text{new}\\). Call it \\(a_{\\text{new}}\\). Add it to \\(\\tau\\).\nSimulation: Let \\(s_\\text{next} = P(s_\\text{new}, a_\\text{new})\\). Evaluate \\(r = v(s_\\text{next})\\). This approximates the value of the game after taking the action \\(a_\\text{new}\\).\nBackup: For each \\((s, a) \\in \\tau\\):\n\n\\(N^{s, a} \\gets N^{s, a} + 1\\)\n\\(W^{s, a} \\gets W^{s, a} + r\\)\n\\(N^s \\gets N^s + 1\\)\n\n\nWe finally return the action with the highest UCB value eq. 9.2. Then play continues. As before, we can reuse the tree across timesteps.\n\n\n\nCode\nclass EdgeStatistics(NamedTuple):\n    wins: int = 0\n    visits: int = 0\n\nclass MCTSTree:\n    \"\"\"A representation of the search tree.\n\n    Maps each state-action pair to its number of wins and the number of visits.\n    \"\"\"\n\n    edges: dict[tuple[\"State\", \"Action\"], EdgeStatistics]\n\ndef mcts_iter(tree, s_init):\n    s = s_init\n    # while all((s, a) in tree for a in env.action_state(s)):\n\n\nHow do we actually compute a useful \\(\\pi_\\text{guide}\\) and \\(v\\)? If we have some existing dataset of trajectories, we could use Chapter 8 (that is, imitation learning) to generate a policy \\(\\pi_\\text{guide}\\) via behaviour cloning and learn \\(v\\) by regressing the game outcomes onto states. Then, plugging these into def. 9.3 results in a stronger policy by using tree search to “think ahead”.\nBut we don’t have to stop at just one improvement step; we could iterate this process via self-play.\n\n\n9.5.2 Self-play\nRecall the policy iteration algorithm for solving an MDP (Section 2.4.4.2). Policy iteration alternates between policy evaluation (taking \\(\\pi\\) and computing \\(V^\\pi\\)) and policy improvement (setting \\(\\pi\\) to be greedy with respect to \\(V^\\pi\\)). We can think of MCTS as a “policy improvement” operation: for a given policy \\(\\pi^0\\), we can use it to guide MCTS. This results in an algorithm that is itself a policy that maps from states to actions. This improved policy (using MCTS) is usually called the search policy. Denote it by \\(\\pi^0_\\text{MCTS}\\). Now, we can use imitation learning techniques (Chapter 8) to obtain a new policy \\(\\pi^1\\) that imitates \\(\\pi^0_\\text{MCTS}\\). We can now use \\(\\pi^1\\) to guide MCTS, and repeat.\n\nDefinition 9.4 (MCTS with self-play) Input:\n\nA parameterized policy class \\(\\pi_\\theta : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\)\nA parameterized value function class \\(v_\\lambda : \\mathcal{S} \\to \\mathbb{R}\\)\nA number of trajectories \\(M\\) to generate\nThe initial parameters \\(\\theta^0, \\lambda^0\\)\n\nFor \\(t = 0, \\dots, T-1\\):\n\nPolicy improvement: Let \\(\\pi^t_\\text{MCTS}\\) denote the policy obtained by def. 9.3 with \\(\\pi_{\\theta^t}\\) and \\(v-{\\lambda^t}\\). We use \\(\\pi^t-\\text{MCTS}\\) to play against itself \\(M\\) times. This generates \\(M\\) trajectories \\(\\tau-0, \\dots, \\tau-{M-1}\\).\nPolicy evaluation: Use behaviour cloning to find a set of policy parameters \\(\\theta^{t+1}\\) that mimic the behaviour of \\(\\pi^t_\\text{MCTS}\\) and a set of value function parameters \\(\\lambda^{t+1}\\) that approximate its value function. That is, \\[\n\\begin{aligned}\n\\theta^{t+1} &\\gets \\arg\\min_\\theta \\sum_{m=0}^{M-1} \\sum_{h=0}^{H-1} - \\log \\pi_\\theta(a^m_h\\mid s^m_h) \\\\\n\\lambda^{t+1} &\\gets \\arg\\min_\\lambda \\sum_{m=0}^{M-1} \\sum_{h=0}^{H-1} (v_\\lambda(s^m_h) - R(\\tau_m))^2\n\\end{aligned}\n\\]\n\nNote that in implementation, the policy and value are typically both returned by a single deep neural network, that is, with a single set of parameters, and the two loss functions are added together.\n\nThis algorithm was brought to fame by AlphaGo Zero (Silver et al., 2017).\n\n9.5.2.1 Extending to continuous rewards (*)\nIn the search algorithm above, we used \\(W^{s, a}\\) to track the number of times the policy wins after taking action \\(a\\) in state \\(s\\). This binary outcome can easily be generalized to a continuous reward at each state-action pair. This is the reward function we assumed when discussing MDPs (Chapter 2).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#key-takeaways",
    "href": "planning.html#key-takeaways",
    "title": "9  Tree Search Methods",
    "section": "9.6 Key takeaways",
    "text": "9.6 Key takeaways\nIn this chapter, we explored tree search-based algorithms for deterministic, zero sum, fully observable two-player games. We began with min-max search (Section 9.3), an algorithm for exactly solving the game value of every possible state. However, this is impossible to execute in practice, and so we must resort to various ways to reduce the number of states and actions that we must explore. Alpha-beta search (Section 9.4) does this by pruning away states that we already know to be suboptimal, and MCTS (Section 9.5) approximates the value of states instead of evaluating them exactly.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-planning-bib",
    "href": "planning.html#sec-planning-bib",
    "title": "9  Tree Search Methods",
    "section": "9.7 Bibliographic notes and further reading",
    "text": "9.7 Bibliographic notes and further reading\nRussell & Norvig (2021, ch. 5) provides an excellent overview of search methods in games. The original AlphaGo paper Silver et al. (2016) was a groundbreaking application of these technologies. Silver et al. (2017) removed the imitation learning phase, learning the optimal policy from scratch using self-play. AlphaZero (Silver et al., 2018) then extended to other games beyond Go, namely shogi and chess, also learning from scratch. In MuZero (Schrittwieser et al., 2020), this was further extended by learning a model of the game dynamics. EfficientZero (Ye et al., 2021) presented a more sample-efficient algorithm based on MuZero. Gumbel MuZero (Danihelka et al., 2021) greatly improved the computational efficiency of MuZero by reducing the number of rollouts required. Stochastic MuZero (Antonoglou et al., 2021) extends MuZero to stochastic environments.\nWhile search methods are extremely powerful, they are also computationally intensive, and have therefore historically been written in lower-level languages such as C or C++ rather than Python. The development of the JAX framework addresses this issue by providing a readable high-level Python library that compiles to code optimized for specific hardware. In particular, the Mctx library (Babuschkin et al., 2020) provides usable implementations of MuZero and its variants above.\n\n\n\n\nAntonoglou, I., Schrittwieser, J., Ozair, S., Hubert, T. K., & Silver, D. (2021, October 6). Planning in stochastic environments with a learned model. The Tenth International Conference on Learning Representations. International Conference on Learning Representations. https://openreview.net/forum?id=X6D9bAHhBQ1\n\n\nBabuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S., … Viola, F. (2020). The DeepMind JAX ecosystem [Computer software]. http://github.com/deepmind\n\n\nDanihelka, I., Guez, A., Schrittwieser, J., & Silver, D. (2021, October 6). Policy improvement by planning with gumbel. The Tenth International Conference on Learning Representations. https://openreview.net/forum?id=bERaNdoegnO\n\n\nRussell, S. J., & Norvig, P. (2021). Artificial intelligence: A modern approach (Fourth edition). Pearson.\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., & Silver, D. (2020). Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839, 7839), 604–609. https://doi.org/10.1038/s41586-020-03051-4\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). Mastering the game of go with deep neural networks and tree search. Nature, 529(7587, 7587), 484–489. https://doi.org/10.1038/nature16961\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., & Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404\n\n\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., & Hassabis, D. (2017). Mastering the game of go without human knowledge. Nature, 550(7676, 7676), 354–359. https://doi.org/10.1038/nature24270\n\n\nYe, W., Liu, S., Kurutach, T., Abbeel, P., & Gao, Y. (2021). Mastering atari games with limited data. NeurIPS 2021, 25476–25488. https://doi.org/10.48550/arXiv.2111.00210",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "10  Exploration in MDPs",
    "section": "",
    "text": "10.1 Introduction\nOne of the key challenges of reinforcement learning is the exploration-exploitation tradeoff. Should we exploit actions we know will give high reward, or should we explore different actions to discover potentially better strategies? An algorithm that doesn’t explore effectively might easily overfit to certain areas of the state space, and fail to generalize once they enter a region they haven’t yet seen.\nIn the multi-armed bandit setting (Chapter 4), we studied the upper confidence bound (UCB) algorithm (Section 4.7) that incentivizes the learner to explore arms that it is uncertain about. In particular, UCB relies on optimism in the face of uncertainty: it chooses arms based on an overestimate of the arm’s true mean reward. In this chapter, we will see how to generalize this idea to the MDP setting.\nCode\nfrom utils import latex",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#introduction",
    "href": "exploration.html#introduction",
    "title": "10  Exploration in MDPs",
    "section": "",
    "text": "10.1.1 Sparse reward\nExploration is crucial in sparse reward problems where \\(r(s, a) = 0\\) for most (or nearly all) states and actions. Often, the agent must take a specific sequence of actions before any reward is observed.\n\nExample 10.1 (Chain MDP) Here’s a simple example of an MDP with sparse rewards:\n\n\n\nAn illustration of the chain MDP environment.\n\n\nThere are \\(|\\mathcal{S}|\\) cells arranged in a chain. The agent starts in the leftmost cell. The rightmost state is a terminal state. In every state, there are three possible actions, two of which move the agent left and one which moves the agent right. (The two “left” actions do nothing in the leftmost cell.) The reward function gives a reward of \\(1\\) for taking the action that enters the rightmost cell, and zero otherwise.\n\nThe problem of sparse rewards is especially prevalent in RL compared to supervised learning. In most supervised learning tasks, every labelled sample provides some useful signal. However, in RL, algorithms that don’t systematically explore new states may fail to learn anything meaningful within a reasonable amount of time.\nConsider the algorithms we’ve covered so far for unknown environments: policy gradient methods (Chapter 7) and fitted DP methods (Chapter 6). How would these do on this problem?\n\nRemark 10.1 (Policy gradient methods fail on sparse reward). Policy gradient algorithms require the gradient to be nonzero in order to learn. If we never observe any reward, the gradient will always be zero, and the policy will never change or improve. If we think of the expected total reward as a function \\(J(\\theta)\\) of the policy parameters, we can visualize the graph of \\(J\\) as being mostly flat, making it impossible to “climb the hill” from almost every random initialization.\n\n\nRemark 10.2 (Fitted DP methods fail on sparse reward). Fitted DP algorithms run into a similar issue: as we randomly interact with the environment, we never observe any reward, and so the reward model simply gives zero for every state-action pair. In expectation, it would take a computationally infeasible number of rollouts to observe the reward by chance.\n\nThis is quite disheartening! The sophisticated methods we’ve developed, which can exceed human-level performance on a wide variety of tasks, fail on this problem that seems almost trivial.\nOf course, a simple way to solve the “chain MDP” in ex. 10.1 is to actively visit unseen states. For a policy that visits a new state in each rollout, the final cell can be reached in \\(O(|\\mathcal{S}|)\\) rollouts (i.e. \\(O(|\\mathcal{S}|^2)\\) time). The rest of this chapter will consider ways to explicitly explore unknown states.\n\n\n10.1.2 Reward shaping\nOne workaround to sparse reward problems in practice is to shape the reward function using domain knowledge. For example, in ex. 10.1, we (that is, the practitioners) know that travelling to the right is the correct action, so we could design a reward function that provides a reward of \\(0.1\\) for the action that moves to the right. A similar strategy is used in practice for many chess or board game algorithms where capturing the opponent’s pieces earns some positive reward.\nThough this might seem obvious, designing a useful reward function can be challenging in practice. The agent may learn to exploit the intermediate rewards rather than solve the original goal. A famous example is the agent trained to play the CoastRunners game, in which players race boats around a racetrack. However, the algorithm found that it could achieve higher reward (i.e. in-game score) by refusing to go around the racetrack and instead collecting bonus points in a loop!\n\n\n\nAn RL agent collects bonus points instead of participating in the race. Image from Clark & Amodei (2024).\n\n\nThis phenomenon is known as reward hacking or Goodhart’s law. Reward hacking is essentially a special case of “finding loopholes” around the written guidelines (or in this case, the reward signal used for training); think of folk stories such as King Midas or the Monkey’s Paw. When RL algorithms are deployed in high-stakes scenarios, it is crucial to verify the learned policy’s behaviour and ensure that it is aligned to the designer’s intentions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#sec-explore-det-mdp",
    "href": "exploration.html#sec-explore-det-mdp",
    "title": "10  Exploration in MDPs",
    "section": "10.2 Exploration in deterministic MDPs",
    "text": "10.2 Exploration in deterministic MDPs\nLet us address the exploration problem in a deterministic MDP, that is, where taking action \\(a\\) in state \\(s\\) always leads to the state \\(P(s, a) \\in \\mathcal{S}\\). How can we methodically visit every single state-action pair?\nIn the multi-armed bandit setting (Chapter 4), there are no states, so it’s trivial to visit every “state-action pair”: just pull each arm once. But in the MDP setting, in order to achieve a particular state-action pair \\((s, a)\\), one must plan out a path from the initial state.\nWe can do this by constructing an MDP where only unseen state-action pairs are rewarded, and using value iteration/dynamic programming (Section 2.3.2) to reach the unknown states in \\(M_{\\mathcal{D}}\\). Concretely, we keep a set \\(\\mathcal{D}\\) of all the \\((s, a, r, s')\\) tuples we’ve observed. Each episode, we use \\(\\mathcal{D}\\) to construct a fully known MDP, \\(M_{\\mathcal{D}}\\), in which only unseen state-action pairs are rewarded.\n\nDefinition 10.1 (Explore-then-exploit algorithm) Suppose that every state can be reached from the initial state within a single episode.\n\n\\(\\mathcal{D} \\gets \\emptyset\\)\nFor \\(T = 0, 1, 2, \\dots\\) (until the entire MDP has been explored):\n\nConstruct \\(M_{\\mathcal{D}}\\) using \\(\\mathcal{D}\\). That is, the state transitions are set to those observed in \\(\\mathcal{D}\\), and the reward is set to \\(0\\) for all state-action pairs in \\(\\mathcal{D}\\), and \\(1\\) otherwise.\nExecute DP (Section 2.3.2) on the known MDP \\(M_{\\mathcal{D}}\\) to compute the optimal policy \\(\\pi^\\star_{\\mathcal{D}}\\).\nExecute \\(\\pi^\\star_{\\mathcal{D}}\\) in \\(M_{\\mathcal{D}}\\). This will visit some \\((s, a)\\) not yet in \\(\\mathcal{D}\\), and observe the reward \\(r(s, a)\\) and next state \\(P(s, a)\\).\n\\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{ (s, a, r, s') \\}\\), where \\(s' = P(s, a), r = r(s, a)\\) are the observed state transition and reward.\n\n\n\n\nRemark 10.3 (Path planning is graph traversal). Review the dynamic programming algorithm for a finite-horizon MDP (Section 2.3.2). Note that in the constructed MDP \\(M_{\\mathcal{D}}\\), this is identical to a breadth-first search beginning from the desired state at the final timestep: each state-timestep pair is a node in the graph, and the state transitions determine the (directed) edges. Each state-timestep pair from which it is possible to reach the desired state is assigned a value of \\(1\\). The policy serves to backtrack through these state-timestep pairs, returning to the root node of the search: the desired state.\n\nWe can easily measure the per-episode regret of this algorithm.\n\nDefinition 10.2 (Per-episode regret) We aim to evaluate some iterative policy optimization algorithm. Let \\(\\pi^t\\) be the policy returned by the algorithm after \\(t\\) iterations. The per-episode regret across \\(T\\) iterations is given by\n\\[\n\\text{Regret}_T= \\mathop{\\mathbb{E}}_{s_0 \\sim P_0}\\left[ \\sum_{t= 0}^{T- 1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right]\n\\tag{10.1}\\]\nwhere the randomness is in the initial state distribution.\n\n\nRemark 10.4 (MDP policies as MAB arms). What does this have to do with the definition of regret in the MAB setting (def. 4.3)? Here, policies are arms, and the “mean reward” is the expected total reward of a trajectory. We’ll make this connection more explicit in Section 10.3.\n\n\nTheorem 10.1 (Performance of explore-then-exploit) The regret of the explore-then-exploit algorithm (def. 10.1) can be upper-bounded by\n\\[\n\\sum_{t= 0}^{T- 1} V^\\star_0 - V_0^{\\pi^t} \\le |\\mathcal{S}||\\mathcal{A}| H.\n\\tag{10.2}\\]\n(This MDP and algorithm are deterministic, assuming there is a single starting state, so the regret is not random.)\n\n\nProof. As long as every state can be reached from \\(s_0\\) within a single episode, i.e. \\(|\\mathcal{S}| \\le H\\), def. 10.2 will eventually be able to explore all \\(|\\mathcal{S}| |\\mathcal{A}|\\) state-action pairs, adding one new transition per episode.\nLet \\(M\\) denote the original MDP that we aim to solve. We know it will take at most \\(|\\mathcal{S}| |\\mathcal{A}|\\) iterations to explore the entire MDP, after which \\(M_{\\mathcal{D}} = M\\) and \\(\\pi^\\star_{\\mathcal{D}}\\) is the optimal policy in \\(M\\), incurring no additional regret. For each “shortest-path” policy \\(\\pi^\\star_{\\mathcal{D}}\\) up until then, its value will differ from that of \\(\\pi^\\star\\) by at most \\(H\\), since the policies will differ by at most \\(1\\) reward at each timestep.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#sec-mdp-mab",
    "href": "exploration.html#sec-mdp-mab",
    "title": "10  Exploration in MDPs",
    "section": "10.3 Treating an unknown MDP as a MAB",
    "text": "10.3 Treating an unknown MDP as a MAB\nWe explored the exploration-exploitation tradeoff in the multi-armed bandits setting (Chapter 4). Can we apply the MAB algorithms we discovered to MDPs as well? Let us formally describe an unknown MDP as an MAB problem.\nIn a MAB problem, we want to find the arm with the highest mean reward. In an MDP, we want to find the policy that achieves the highest expected total reward. So if we want to apply MAB techniques to solving an MDP, it makes sense to draw an equivalence between arms and policies. We can summarize this equivalence in the following table:\n\n\n\nTable 10.1: Treating an MDP with finite states and actions as a MAB.\n\n\n\n\n\n\n\n\n\nMAB\nMDP\n\n\n\n\n\\(K\\) arms\n\\((\\vert\\mathcal{A}\\vert^{\\vert\\mathcal{S}\\vert})^H\\) deterministic policies\n\n\nunknown reward distributions \\(\\nu^k\\)\nunknown trajectory distributions \\(\\rho^\\pi\\)\n\n\n\\(k^\\star = \\arg\\max_{k \\in [K]} \\mathop{\\mathbb{E}}_{r \\sim \\nu^k} [r]\\)\n\\(\\pi^\\star = \\arg\\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^\\pi} \\left[ \\sum_{h=0}^{H-1} r(s_h, a_h) \\right]\\)\n\n\npull arm \\(k\\) and observe reward\nroll out with \\(\\pi\\) and observe total reward\n\n\n\n\n\n\n(For the sake of this example, assume that the MDP’s reward function is stochastic, so that the MAB reward distributions are nondegenerate.)\nRecall that UCB incurs regret \\(\\widetilde{O}(\\sqrt{TK})\\), where \\(T\\) is the number of pulls and \\(K\\) is the number of arms. So in the MDP-as-MAB problem, using UCB for \\(T\\) episodes would achieve regret\n\\[\n\\widetilde{O}\\left(\n    \\sqrt{|\\mathcal{A}|^{|\\mathcal{S}|H} T}\n\\right)\n\\tag{10.3}\\]\nThis scales exponentially in \\(|\\mathcal{S}|\\) and \\(H\\), which quickly becomes intractable. Notably, this method treats each policy as entirely independent from the others, but the performance of different policies are typically correlated. We can illustrate this with the following example:\n\nExample 10.2 (Treating an MDP as a MAB) Consider a “coin MDP” with two states “heads” and “tails”, two actions “Y” and “N”, and a time horizon of \\(H=2\\). The state transition flips the coin, and doesn’t depend on the action. The reward only depends on the action: Taking action Y gives reward \\(1\\), and taking action N gives reward \\(0\\).\nSuppose we collect data from the two constant policies \\(\\pi_{\\text{Y}}(s) = \\text{Y}\\) and \\(\\pi_{\\text{N}}(s) = \\text{N}\\). Now we want to learn about the policy \\(\\widetilde{\\pi}\\) that takes action Y and then N. Do we need to collect data from \\(\\widetilde{\\pi}\\) to evaluate it? No: Since the reward only depends on the action, we can infer its value from our data on the policies \\(\\pi_{\\text{Y}}\\) and \\(\\pi_{\\text{N}}\\). However, if we treat the MDP as a bandit in which \\(\\widetilde{\\pi}\\) is a new, unknown arm, we ignore the known correlation between the action and the reward.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#sec-ucbvi",
    "href": "exploration.html#sec-ucbvi",
    "title": "10  Exploration in MDPs",
    "section": "10.4 Upper confidence bound value iteration",
    "text": "10.4 Upper confidence bound value iteration\nWe shouldn’t need to consider all \\(|\\mathcal{A}|^{|\\mathcal{S}| H}\\) deterministic policies to achieve low regret. Rather, all we need to describe the optimal policy is \\(Q^\\star\\), which has \\(H |\\mathcal{S}||\\mathcal{A}|\\) entries to be learned. In this section, we’ll study the upper confidence bound value iteration (UCBVI) algorithm (Azar et al., 2017), which indeed achieves polynomial regret in \\(|\\mathcal{S}|\\), \\(|\\mathcal{A}|\\), and \\(H\\).\nAs its name suggests, UCBVI combines the upper confidence bound (UCB) algorithm from the multi-armed bandits setting (Section 4.7) with value iteration (VI) from the MDP setting (Section 2.3.2):\n\nUCB strikes a good exploration-exploitation tradeoff in an (unknown) MAB;\nVI (what we simply call DP in the finite-horizon setting) computes the optimal value function in a known MDP (with finite states and actions).\n\nLet us briefly review these two algorithms:\n\nRemark 10.5 (Review of UCB). At each iteration \\(t\\), for each arm \\(k\\), we construct a confidence interval for the mean of arm \\(k\\)’s reward distribution. We then choose the arm with the highest upper confidence bound:\n\\[\n\\begin{aligned}\nk_{t+1} &\\gets \\arg\\max_{k \\in [K]} \\text{ucb}^k_t \\\\\n\\text{where } \\text{ucb}^k_t &= \\frac{R^{k}_t}{N^{k}_t} + \\sqrt{\\frac{\\ln(2t/\\delta)}{2 N^{k}_t}}\n\\end{aligned}\n\\tag{10.4}\\]\nwhere \\(N_t^k\\) indicates the number of times arm \\(k\\) has been pulled up until time \\(t\\), \\(R_t^k\\) indicates the total reward obtained by pulling arm \\(k\\) up until time \\(t\\), and \\(\\delta &gt; 0\\) controls the width of the confidence interval.\n\nWe can treat the upper confidence bound as a “proxy reward” that is the estimated mean reward plus a bonus exploration term. Since the size of the bonus term is proportional to our uncertainty (i.e. predicted variance) about that arm’s mean, this is called an optimistic bonus. In UCBVI, we will extend this idea to the case of an unknown MDP \\(\\mathcal{M}\\) by adding an exploration term to the reward function. Then, we will use DP to solve for the optimal policy in \\(\\widetilde{\\mathcal{M}}\\).\n\nRemark 10.6 (Review of VI/DP). Value iteration (VI) is a dynamic programming (DP) algorithm for computing the optimal policy and value function in an MDP where the state transitions and reward function are known. We begin at the final timestep, where \\(V_H^\\star(s) = 0\\), and work backwards using Bellman’s optimality equations (Theorem 2.5):\nFor \\(h= H-1, \\dots, 0\\):\n\\[\n\\begin{aligned}\nQ_h^\\star(s, a) &= r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(\\cdot \\mid s, a)}[V_{h+1}^\\star(s')] \\\\\n\\pi_h^\\star(s) &= \\arg\\max_{a \\in \\mathcal{A}} Q_h^\\star(s, a) \\\\\nV_h^\\star(s) &= Q_h^\\star(s, \\pi_h^\\star(s)).\n\\end{aligned}\n\\tag{10.5}\\]\n\nAssumptions: We will consider the general case of a time-varying MDP where the transition and reward functions may change over time. Recall our convention that \\(P_h\\) is the distribution of \\(s_{h+1} \\mid s_{h}, a_{h}\\) and \\(r_h\\) is applied to \\(s_h, a_h\\).\n\nDefinition 10.3 (UCBVI) At a high level, the UCBVI algorithm can be described as follows: For \\(i = 0, \\dots, I-1\\):\n\nModeling: Use previously collected data to model the state transitions \\(\\widehat{P}_0, \\dots, \\widehat{P}_{H-1}\\) and reward functions \\(\\widehat{r}_0, \\dots, \\widehat{r}_{H-1}\\).\nReward bonus: Design a reward bonus \\(b_h(s, a) \\in \\mathbb{R}\\) to encourage exploration, analogous to the UCB term.\nOptimistic planning: Use VI (i.e. DP) to compute the optimal policy \\(\\widehat \\pi\\) in the modelled MDP\n\n\\[\n\\widetilde{\\mathcal{M}} = (\\mathcal{S}, \\mathcal{A}, \\{ \\widehat{P}_h\\}_{h \\in [H]}, \\{ \\widehat{r}_h+ b_h\\}_{h \\in [H]}, H).\n\\]\n\nExecution: Use \\(\\widehat \\pi\\) to collect a new trajectory.\n\nWe detail each of these steps below.\n\n\n10.4.1 Modeling the transitions\nRecall that we don’t know the state transitions or reward function of the MDP we aim to solve. We seek to approximate\n\\[\nP_h(s_{h+ 1} \\mid s_h, a_h) = \\frac{\\mathbb{P}(s_h, a_h, s_{h+ 1})}{\\mathbb{P}(s_h, a_h)},\n\\tag{10.6}\\]\nwhere \\(\\mathbb{P}\\) denotes the true joint probabilities. We can estimate these using their sample probabilities across a set of collected transitions. That is, define\n\\[\n\\begin{aligned}\n    N_h^i(s, a, s') & := \\sum_{i'=0}^{i-1} \\mathbf{1}\\left\\{ (s_h^{i'}, a_h^{i'}, s_{h+1}^{i'}) = (s, a, s') \\right\\} \\\\\n    N_h^i(s, a)     & := \\sum_{i'=0}^{i-1} \\mathbf{1}\\left\\{ (s_h^{i'}, a_h^{i'}) = (s, a) \\right\\}                \\\\\n\\end{aligned}\n\\tag{10.7}\\]\nto be the number of times the tuple \\(s, a, s'\\) appears in the collected data, and similar for the state-action pair \\(s, a\\). Then we can model\n\\[\n\\widehat{P}_h^t(s' \\mid s, a) = \\frac{N_h^t(s, a, s')}{N_h^t(s, a)}.\n\\tag{10.8}\\]\nSimilarly, we can model the rewards by the sample mean in each state-action pair:\n\\[\n\\widehat{r}_h^t(s, a) = \\frac{N_h^t(s, a)} \\sum_{t'=0}^{t-1} \\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\} r_h^i.\n\\tag{10.9}\\]\nThis is a fairly naive, nonparametric estimator that doesn’t assume any underlying structure of the MDP. We’ll see how to incorporate assumptions about the MDP in the following section.\n\n\n10.4.2 Reward bonus\nTo motivate the reward bonus term, recall how we designed the reward bonus term for UCB (Section 4.7):\n\nWe used Hoeffding’s inequality to bound, with high probability, how far the sample mean \\(\\widehat \\mu_t^k\\) deviated from the true mean \\(\\mu^k\\).\nBy inverting this inequality, we obtained a \\((1-\\delta)\\)-confidence interval for the true mean, centered at our estimate.\nTo make this bound uniform across all timesteps \\(t \\in [T]\\), we applied the union bound and multiplied \\(\\delta\\) by a factor of \\(T\\).\n\nWe’d like to do the same for UCBVI, and construct the bonus term such that \\(V^\\star_h(s) \\le \\widehat{V}_h^t(s)\\) with high probability. However, our construction will be more complex than the MAB case, since \\(\\widehat{V}_h^t(s)\\) depends on the bonus \\(b_h^t(s, a)\\) implicitly via DP. We claim that the bonus term that gives the proper bound is\n\\[\nb_h^i(s, a) = 2 H\\sqrt{\\frac{\\log( |\\mathcal{S}||\\mathcal{A}|HI/\\delta )}{N_h^t(s, a)}}.\n\\tag{10.10}\\]\nWe provide a heuristic sketch of the proof in Section B.2; see Agarwal et al. (2022), Section 7.3 for a full proof.\n\n\n10.4.3 Performance of UCBVI\nHow exactly does UCBVI strike a good balance between exploration and exploitation? In UCB for MABs, the bonus exploration term is simple to interpret: It encourages the learner to take actions with a high exploration term. Here, the policy depends on the bonus term indirectly: The policy is obtained by planning in an MDP where the bonus term is added to the reward function. Note that the bonuses propagate backwards in DP, effectively enabling the learner to plan to explore unknown states. This effect takes some further interpretation.\nRecall we constructed \\(b^t_h\\) so that, with high probability, \\(V^\\star_h(s) \\le \\widehat{V}_h^t(s)\\) and so\n\\[\nV^\\star_h(s) - V^{\\pi^t}_h(s) \\le \\widehat{V}_h^t(s) - V^{\\pi^t}_h(s).\n\\]\nThat is, the l.h.s. measures how suboptimal policy \\(\\pi^t\\) is in the true environment, while the r.h.s. is the difference in the policy’s value when acting in the modelled MDP \\(\\widetilde{\\mathcal{M}}^t\\) instead of the true one \\(\\mathcal{M}\\).\nIf the r.h.s. is small, this implies that the l.h.s. difference is also small, i.e. that \\(\\pi^t\\) is exploiting actions that are giving high reward.\nIf the r.h.s. is large, then we have overestimated the value: \\(\\pi^t\\), the optimal policy of \\(\\widetilde{\\mathcal{M}}^t\\), does not perform well in the true environment \\(\\mathcal{M}\\). This indicates that one of the \\(b_h^t(s, a)\\) terms must be large, or some \\(\\widehat P^t_h(\\cdot \\mid s, a)\\) must be inaccurate, indicating a state-action pair with a low visit count \\(N^t_h(s, a)\\) that the learner was encouraged to explore.\nIt turns out that UCBVI achieves a regret of\n\nTheorem 10.2 (UCBVI regret) The expected regret of UCBVI satisfies\n\\[\n\\mathop{\\mathbb{E}}\\left[\n    \\sum_{t= 0}^{T- 1} \\left(V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right)\n\\right] =\n\\widetilde{O}(H^2 \\sqrt{|\\mathcal{S}| |\\mathcal{A}| I})\n\\]\n\nComparing this to the UCB regret bound \\(\\widetilde{O}(\\sqrt{T K})\\), where \\(K\\) is the number of arms of the MAB, we see that we’ve reduced the number of effective arms from \\(|\\mathcal{A}|^{|\\mathcal{S}|H}\\) (in eq. 10.3) to \\(H^4 |\\mathcal{S}||\\mathcal{A}|\\), which is indeed polynomial in \\(|\\mathcal{S}|\\), \\(|\\mathcal{A}|\\), and \\(H\\), as desired. This is also roughly the number of episodes it takes to achieve constant-order average regret:\n\\[\n\\frac{1}{T} \\mathop{\\mathbb{E}}[\\text{Regret}_T] = \\widetilde{O}\\left(\\sqrt{\\frac{H^4 |\\mathcal{S}||\\mathcal{A}|}{T}}\\right)\n\\]\nNote that the time-dependent transition matrix has \\(H |\\mathcal{S}|^2 |\\mathcal{A}|\\) entries. Assuming \\(H \\ll |\\mathcal{S}|\\), this shows that it’s possible to achieve low regret, and achieve a near-optimal policy, while only understanding a \\(1/|\\mathcal{S}|\\) fraction of the world’s dynamics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#linear-mdps",
    "href": "exploration.html#linear-mdps",
    "title": "10  Exploration in MDPs",
    "section": "10.5 Linear MDPs",
    "text": "10.5 Linear MDPs\nA polynomial dependency on \\(|\\mathcal{S}|\\) and \\(|\\mathcal{A}|\\) is manageable when the state and action spaces are small. But for large or continuous state and action spaces, even this polynomial factor will become intractable. Can we find algorithms that don’t depend on \\(|\\mathcal{S}|\\) or \\(|\\mathcal{A}|\\) at all, effectively reducing the dimensionality of the MDP? In this section, we’ll explore linear MDPs: an example of a parameterized MDP where the rewards and state transitions depend only on some parameter space of dimension \\(d\\) that is independent from \\(|\\mathcal{S}|\\) or \\(|\\mathcal{A}|\\).\n\nDefinition 10.4 (Linear MDP) We assume that the transition probabilities and rewards are linear in some feature vector\n\\(\\phi(s, a) \\in \\mathbb{R}^d\\):\n\\[\n\\begin{aligned}\n        P_h(s' \\mid s, a) & = \\phi(s, a)^\\top \\mu^\\star_h(s') \\\\\n        r_h(s, a)         & = \\phi(s, a)^\\top \\theta_h^\\star\n\\end{aligned}\n\\]\nNote that we can also think of \\(P_h(\\cdot \\mid s, a) = \\mu_h^\\star\\) as an \\(|\\mathcal{S}| \\times d\\) matrix, and think of \\(\\mu^\\star_h(s')\\) as indexing into the \\(s'\\)-th row of this matrix (treating it as a column vector). Thinking of \\(V^\\star_{h+1}\\) as an \\(|\\mathcal{S}|\\)-dimensional vector, this allows us to write\n\\[\n\\mathop{\\mathbb{E}}_{s' \\sim P_h(\\cdot \\mid s, a)}[V^\\star_{h+1}(s)] = (\\mu^\\star_h\\phi(s, a))^\\top V^\\star_{h+1}.\n\\]\nThe \\(\\phi\\) feature mapping can be designed to capture interactions between the state \\(s\\) and action \\(a\\). In this book, we’ll assume that the feature map \\(\\phi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}^d\\) and the reward function (described by \\(\\theta_h^\\star\\)) are known to the learner.\n\n\n10.5.1 Planning in a linear MDP\nIt turns out that \\(Q^\\star_h\\) is also linear with respect to this feature mapping. We can prove this by simply computing it using DP. We initialize the value function at the end of the time horizon by setting \\(V_{H}^\\star(s) = 0\\) for all states \\(s\\). Then we iterate:\n\\[\n\\begin{aligned}\n    Q^\\star_h(s, a)  & = r_h(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P_h(\\cdot \\mid s, a)} [V^\\star_{h+1}(s')]                          \\\\\n                     & = \\phi(s, a)^\\top \\theta_h^\\star + (\\mu_h^\\star \\phi(s, a))^\\top V^\\star_{h+1}               \\\\\n                     & = \\phi(s, a)^\\top \\underbrace{( \\theta_h^\\star + (\\mu_h^\\star)^\\top  V^\\star_{h+1})}_{w_h} \\\\\n    V^\\star_h(s)     & = \\max_a Q^\\star_h(s, a)                                                                       \\\\\n    \\pi^\\star_h(s) & = \\arg\\max_a Q^\\star_h(s, a)\n\\end{aligned}\n\\]\n\nExercise 10.1 (Action-value function is linear in features) Show that \\(Q^\\pi_h\\) is also linear with respect to \\(\\phi(s, a)\\) for any policy \\(\\pi\\).\n\n\n\n10.5.2 UCBVI in a linear MDP\n\n10.5.2.1 Modeling the transitions\nThis linear assumption on the MDP will also allow us to model the unknown dynamics \\(P^?_h(s' \\mid s, a)\\) with techniques from supervised learning (SL). Recall that SL is useful for estimating conditional expectations by minimizing mean squared error. We can rephrase the estimation of \\(P^?_h(s' \\mid s, a)\\) as a least-squares problem as follows: Write \\(\\delta_s\\) to denote a one-hot vector in \\(\\mathbb{R}^{|\\mathcal{S}|}\\), with a \\(1\\) in the \\(s\\)-th entry and \\(0\\) everywhere else. Note that\n\\[\n\\mathop{\\mathbb{E}}_{s' \\sim P_h(\\cdot \\mid s, a)} [\\delta_{s'}] = P_h(\\cdot \\mid s, a) = \\mu_h^\\star \\phi(s, a).\n\\]\nFurthermore, since the expectation here is linear with respect to \\(\\phi(s, a)\\), we can directly apply least-squares multi-target linear regression to construct the estimate\n\\[\n\\widehat \\mu = \\arg\\min_{\\mu \\in \\mathbb{R}^{|\\mathcal{S}| \\times d}} \\sum_{t= 0}^{T- 1} \\|\\mu \\phi(s_h^i, a_h^i) - \\delta_{s_{h+1}^i} \\|_2^2.\n\\]\nThis has a well-known closed-form solution:\n\\[\n\\begin{aligned}\n    \\widehat \\mu^\\top            & = (A_h^t)^{-1} \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\delta_{s_{h+1}^i}^\\top \\\\\n    \\text{where} \\quad A_h^t & = \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\phi(s_h^i, a_h^i)^\\top + \\lambda I\n\\end{aligned}\n\\]\nwhere we include a \\(\\lambda I\\) term to ensure that the matrix \\(A^t_h\\) is invertible. (This can also be derived by adding a \\(\\lambda \\|\\mu\\|_{\\text{F}}^2\\) regularization term to the objective.) We can directly plug in this estimate into \\(\\widehat{P}^t_h(\\cdot \\mid s, a) = \\widehat \\mu^t_h \\phi(s, a)\\).\n\n\n10.5.2.2 Reward bonus\nNow, to design the reward bonus, we can’t apply Hoeffding’s inequality anymore, since the terms no longer involve sample means of bounded random variables; Instead, we’re incorporating information across different states and actions. Rather, we can construct an upper bound using Chebyshev’s inequality in the same way we did for the LinUCB algorithm in the MAB setting Section 4.9.1:\n\\[\nb^t_h(s, a) = \\beta \\sqrt{\\phi(s, a)^\\top (A^t_h)^{-1} \\phi(s, a)}, \\quad \\beta = \\widetilde O(d H).\n\\]\nNote that this isn’t explicitly inversely proportional to \\(N_h^t(s, a)\\) as in the original UCBVI bonus term eq. 10.10. Rather, it is inversely proportional to the amount that the direction \\(\\phi(s, a)\\) has been explored in the history. That is, if \\(A-h^t\\) has a large component in the direction \\(\\phi(s, a)\\), implying that this direction is well explored, then the bonus term will be small, and vice versa.\nWe can now plug in these transition estimates and reward bonuses into the UCBVI algorithm def. 10.3.\n\nTheorem 10.3 (LinUCBVI regret) The LinUCBVI algorithm achieves expected regret\n\\[\n\\mathop{\\mathbb{E}}[\\text{Regret}_T] = \\mathop{\\mathbb{E}}\\left[\\sum_{t= 0}^{T- 1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right] \\le \\widetilde O(H^2 d^{1.5} \\sqrt{T})\n\\]\n\nComparing this to our bound for UCBVI in an environment without this linear assumption, we see that we go from a sample complexity of \\(\\widetilde \\Omega(H^4 |\\mathcal{S}||\\mathcal{A}|)\\) to \\(\\widetilde \\Omega(H^4 d^{3})\\). This new sample complexity only depends on the feature dimension and not on the state or action space of the MDP!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#key-takeaways",
    "href": "exploration.html#key-takeaways",
    "title": "10  Exploration in MDPs",
    "section": "10.6 Key takeaways",
    "text": "10.6 Key takeaways\nWe first discussed the explore-then-exploit algorithm (def. 10.1), a simple way to explore a deterministic MDP by visiting all state-action pairs. This is essentially a graph traversal algorithm, where each state represents an edge of the graph. We then discussed how to treat an unknown MDP as a MAB (Section 10.3), and how this approach is inefficient since it doesn’t make use of correlations between different policies. We then introduced the UCBVI algorithm (def. 10.3), the key algorithm of this chapter, which models the unknown MDP by a proxy MDP with a reward bonus term that encourages exploration. Finally, assuming that the transitions and rewards are linear with respect to a feature transformation of the state and action, we introduced the LinUCBVI algorithm (Section 10.5.2), which has a sample complexity independent of the size of the state and action spaces. This makes it possible to scale up UCBVI to large problems that have a simple underlying structure.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#sec-exploration-bib",
    "href": "exploration.html#sec-exploration-bib",
    "title": "10  Exploration in MDPs",
    "section": "10.7 Bibliographic notes and further reading",
    "text": "10.7 Bibliographic notes and further reading\nSparse reward problems are frequent throughout reinforcement learning. The chain MDP example is from Thrun (1992). One of the most famous sparse reward problems is Montezuma’s Revenge, one of the tasks in the popular arcade learning environment (ALE) benchmark of Atari 2600 games (Bellemare et al., 2013; Machado et al., 2018). These were first solved by algorithms that explicitly encourage exploration (Bellemare et al., 2016; Burda et al., 2018).\nThe issue of reward hacking is one of many possible concerns relating to AI safety. We refer the reader to Amodei et al. (2016) for an overview of such risks. Reward hacking has been empirically demonstrated in large language model training (Gao et al., 2023).\nThe UCBVI algorithm was first presented in Azar et al. (2017). UCBVI extends the UCB algorithm from multi-armed bandits to the MDP by estimating a model of the environment. Later work by Drago et al. (2025) improved the regret bound on UCBVI. Other model-based methods for strategic exploration have been studied at least since Schmidhuber (1991) and Meyer & Wilson (1991). UCBVI computes the reward bonus using the count of the number of times that state-action pair has been visited. Tang et al. (2017) surveys other such count-based exploration algorithms.\nIt is also possible to encourage model-free algorithms to strategically explore. Badia et al. (2020) designed a Q-learning algorithm with exploration incentives that surpassed the human baseline on the challenging Atari tasks.\nIntrinsic motivation is another family of approaches to strategic exploration. In some sense, intrinsic motivation approaches are to RL as self-supervised approaches are to unsupervised learning: typically, we add some intrinsic reward to the objective function that encourages the policy to explore. See Schmidhuber (2010) and Aubret et al. (2019) for a recent survey on this family of methods.\nWe refer the reader to the survey article Ladosz et al. (2022) for further reading on exploration in RL.\n\n\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022). Reinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). Concrete problems in AI safety. http://arxiv.org/abs/1606.06565\n\n\nAubret, A., Matignon, L., & Hassas, S. (2019, November 19). A survey on intrinsic motivation in reinforcement learning. https://doi.org/10.48550/arXiv.1908.06976\n\n\nAzar, M. G., Osband, I., & Munos, R. (2017). Minimax regret bounds for reinforcement learning. Proceedings of the 34th International Conference on Machine Learning, 263–272. https://proceedings.mlr.press/v70/azar17a.html\n\n\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo, D., & Blundell, C. (2020). Agent57: Outperforming the atari human benchmark. ICML 2020, 507–517. https://doi.org/10.48550/arXiv.2003.13350\n\n\nBellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47, 253–279. https://doi.org/10.1613/jair.3912\n\n\nBellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. Proceedings of the 30th International Conference on Neural Information Processing Systems, 1479–1487.\n\n\nBurda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018, September 27). Exploration by random network distillation. The Seventh International Conference on Learning Representations. International Conference on Learning Representations. https://openreview.net/forum?id=H1lJJnR5Ym\n\n\nClark, J., & Amodei, D. (2024, February 14). Faulty reward functions in the wild. OpenAI. https://openai.com/index/faulty-reward-functions/\n\n\nDrago, S., Mussi, M., & Metelli, A. M. (2025, February 24). A refined analysis of UCBVI. https://doi.org/10.48550/arXiv.2502.17370\n\n\nGao, L., Schulman, J., & Hilton, J. (2023). Scaling laws for reward model overoptimization. Proceedings of the 40th International Conference on Machine Learning, 202, 10835–10866.\n\n\nLadosz, P., Weng, L., Kim, M., & Oh, H. (2022). Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85(C), 1–22. https://doi.org/10.1016/j.inffus.2022.03.003\n\n\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., & Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61, 523–562. https://doi.org/10.1613/jair.5699\n\n\nMeyer, J.-A., & Wilson, S. W. (1991). A possibility for implementing curiosity and boredom in model-building neural controllers. In From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior (pp. 222–227). From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior. MIT Press. https://ieeexplore.ieee.org/document/6294131\n\n\nSchmidhuber, J. (1991). Curious model-building control systems. [Proceedings] 1991 IEEE International Joint Conference on Neural Networks, 1458–1463 vol.2. https://doi.org/10.1109/IJCNN.1991.170605\n\n\nSchmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation (1990–2010). IEEE Transactions on Autonomous Mental Development, 2(3), 230–247. IEEE Transactions on Autonomous Mental Development. https://doi.org/10.1109/TAMD.2010.2056368\n\n\nTang, H., Houthooft, R., Foote, D., Stooke, A., Xi Chen, O., Duan, Y., Schulman, J., DeTurck, F., & Abbeel, P. (2017). #Exploration: A study of count-based exploration for deep reinforcement learning. Advances in Neural Information Processing Systems, 30. https://proceedings.neurips.cc/paper_files/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html\n\n\nThrun, S. B. (1992). Efficient exploration in reinforcement learning [Technical Report]. Carnegie Mellon University.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbeel, P., & Ng, A. Y. (2004, July 4). Apprenticeship learning via\ninverse reinforcement learning. Proceedings of the\nTwenty-First International Conference on Machine\nLearning. International Conference on\nMachine Learning. https://doi.org/10.1145/1015330.1015430\n\n\nAgarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2022).\nReinforcement learning: Theory and algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf\n\n\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., &\nBellemare, M. (2021). Deep reinforcement learning at the edge of the\nstatistical precipice. Advances in Neural Information\nProcessing Systems, 34, 29304–29320. https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html\n\n\nAlbrecht, S. V., Christianos, F., & Schäfer, L. (2023).\nMulti-agent reinforcement learning: Foundations and modern\napproaches. MIT Press. https://www.marl-book.com\n\n\nAllaire, J. J., Teague, C., Scheidegger, C., Xie, Y., & Dervieux, C.\n(2024). Quarto (Version 1.4) [Computer software]. https://doi.org/10.5281/zenodo.5960048\n\n\nAmari, S.-I. (1998). Natural gradient works efficiently in learning.\nNeural Comput., 10(2), 251–276. https://doi.org/10.1162/089976698300017746\n\n\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J.,\n& Mané, D. (2016). Concrete problems in AI\nsafety. http://arxiv.org/abs/1606.06565\n\n\nAnsel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M.,\nBao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A.,\nConstable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong,\nJ., Gschwind, M., … Chintala, S. (2024, April). PyTorch 2:\nFaster machine learning through dynamic python bytecode transformation\nand graph compilation. 29th ACM International\nConference on Architectural Support for Programming Languages and\nOperating Systems, Volume 2 (ASPLOS ’24). https://doi.org/10.1145/3620665.3640366\n\n\nAntonoglou, I., Schrittwieser, J., Ozair, S., Hubert, T. K., &\nSilver, D. (2021, October 6). Planning in stochastic environments with a\nlearned model. The Tenth International Conference on Learning\nRepresentations. International Conference on\nLearning Representations. https://openreview.net/forum?id=X6D9bAHhBQ1\n\n\nAthans, M., & Falb, P. L. (1966). Optimal control:\nAn introduction to the theory and its applications.\nMcGraw-Hill. https://books.google.com?id=pfJHAQAAIAAJ\n\n\nAubret, A., Matignon, L., & Hassas, S. (2019, November 19). A\nsurvey on intrinsic motivation in reinforcement learning. https://doi.org/10.48550/arXiv.1908.06976\n\n\nAuer, P. (2002). Using confidence bounds for exploitation-exploration\ntrade-offs. Journal of Machine Learning Research, 3,\n397–422. https://www.jmlr.org/papers/v3/auer02a.html\n\n\nAzar, M. G., Osband, I., & Munos, R. (2017). Minimax regret bounds\nfor reinforcement learning. Proceedings of the 34th\nInternational Conference on Machine\nLearning, 263–272. https://proceedings.mlr.press/v70/azar17a.html\n\n\nBabuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J.,\nBuchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu,\nA., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T.,\nHessel, M., Hou, S., Kapturowski, S., … Viola, F. (2020). The\nDeepMind JAX ecosystem [Computer software]. http://github.com/deepmind\n\n\nBadia, A. P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A.,\nGuo, D., & Blundell, C. (2020). Agent57: Outperforming\nthe atari human benchmark. ICML 2020, 507–517. https://doi.org/10.48550/arXiv.2003.13350\n\n\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike\nadaptive elements that can solve difficult learning control problems.\nIEEE Transactions on Systems, Man, and Cybernetics,\nSMC-13(5), 834–846. IEEE Transactions on\nSystems, Man, and Cybernetics. https://doi.org/10.1109/TSMC.1983.6313077\n\n\nBaydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M.\n(2018, February 5). Automatic differentiation in machine learning: A\nsurvey. https://doi.org/10.48550/arXiv.1502.05767\n\n\nBeck, A., & Teboulle, M. (2003). Mirror descent and nonlinear\nprojected subgradient methods for convex optimization. Operations\nResearch Letters, 31(3), 167–175. https://doi.org/10.1016/S0167-6377(02)00231-6\n\n\nBellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The\narcade learning environment: An evaluation platform for\ngeneral agents. Journal of Artificial Intelligence Research,\n47, 253–279. https://doi.org/10.1613/jair.3912\n\n\nBellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D.,\n& Munos, R. (2016). Unifying count-based exploration and intrinsic\nmotivation. Proceedings of the 30th International\nConference on Neural Information Processing\nSystems, 1479–1487.\n\n\nBellman, R. (1957). Dynamic programming. Princeton University\nPress. https://books.google.com?id=rZW4ugAACAAJ\n\n\nBellman, R. (1961). Adaptive control processes: A guided tour.\nPrinceton University Press. https://books.google.com?id=POAmAAAAMAAJ\n\n\nBerkovitz, L. D. (1974). Optimal control theory. Springer\nScience+Business Media LLC.\n\n\nBerry, D. A., & Fristedt, B. (1985). Bandit problems.\nSpringer Netherlands. https://doi.org/10.1007/978-94-015-3711-7\n\n\nBertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic\nprogramming. Athena scientific.\n\n\nBhatnagar, S., Sutton, R. S., Ghavamzadeh, M., & Lee, M. (2009).\nNatural actor–critic algorithms. Automatica, 45(11),\n2471–2482. https://doi.org/10.1016/j.automatica.2009.07.008\n\n\nBishop, C. M. (2006). Pattern recognition and machine learning.\nSpringer.\n\n\nBoyd, S., & Vandenberghe, L. (2004). Convex optimization.\nCambridge University Press. https://web.stanford.edu/~boyd/cvxbook/\n\n\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C.,\nMaclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne,\nS., & Zhang, Q. (2018). JAX:\nComposable transformations of python+NumPy\nprograms (Version 0.3.13) [Computer software]. http://github.com/google/jax\n\n\nBurda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018, September\n27). Exploration by random network distillation. The Seventh\nInternational Conference on Learning Representations. International\nConference on Learning Representations. https://openreview.net/forum?id=H1lJJnR5Ym\n\n\nClark, J., & Amodei, D. (2024, February 14). Faulty reward\nfunctions in the wild. OpenAI. https://openai.com/index/faulty-reward-functions/\n\n\nDanihelka, I., Guez, A., Schrittwieser, J., & Silver, D. (2021,\nOctober 6). Policy improvement by planning with gumbel. The Tenth\nInternational Conference on Learning Representations. https://openreview.net/forum?id=bERaNdoegnO\n\n\nDanilyuk, P. (2021). A robot imitating a girl’s movement\n[Graphic]. https://www.pexels.com/photo/a-robot-imitating-a-girl-s-movement-8294811/\n\n\nDeng, L. (2012). The MNIST database of handwritten digit\nimages for machine learning research. IEEE Signal Processing\nMagazine, 29(6), 141–142. IEEE Signal Processing\nMagazine. https://doi.org/10.1109/MSP.2012.2211477\n\n\nDrago, S., Mussi, M., & Metelli, A. M. (2025, February 24). A\nrefined analysis of UCBVI. https://doi.org/10.48550/arXiv.2502.17370\n\n\nFisher, R. A. (1925). Statistical methods for research workers, 11th\ned. rev. Edinburgh.\n\n\nFrans Berkelaar. (2009). Container ship MSC davos -\nwesterschelde - zeeland [Graphic]. https://www.flickr.com/photos/28169156@N03/52957948820/\n\n\nGao, L., Schulman, J., & Hilton, J. (2023). Scaling laws for reward\nmodel overoptimization. Proceedings of the 40th International\nConference on Machine Learning, 202,\n10835–10866.\n\n\nGittins, J. C. (2011). Multi-armed bandit allocation indices\n(2nd ed). Wiley.\n\n\nGleave, A., Taufeeque, M., Rocamonde, J., Jenner, E., Wang, S. H.,\nToyer, S., Ernestus, M., Belrose, N., Emmons, S., & Russell, S.\n(2022, November 22). imitation: Clean\nimitation learning implementations. https://doi.org/10.48550/arXiv.2211.11972\n\n\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,\nOzair, S., Courville, A., & Bengio, Y. (2020). Generative\nadversarial networks. Commun. ACM, 63(11), 139–144. https://doi.org/10.1145/3422622\n\n\nGPA Photo Archive. (2017). Robotic arm [Graphic]. https://www.flickr.com/photos/iip-photo-archive/36123310136/\n\n\nGuy, R. (2006). Chess [Graphic]. https://www.flickr.com/photos/romainguy/230416692/\n\n\nHaarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017).\nReinforcement learning with deep energy-based policies. Proceedings\nof the 34th International Conference on Machine\nLearning - Volume 70, 1352–1361.\n\n\nHaarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018). Soft\nactor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. Proceedings of\nthe 35th International Conference on Machine\nLearning, 1861–1870. https://proceedings.mlr.press/v80/haarnoja18b.html\n\n\nHasselt, H. (2010). Double q-learning. Advances in Neural\nInformation Processing Systems, 23. https://proceedings.neurips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html\n\n\nHasselt, H. van, Guez, A., & Silver, D. (2016). Deep reinforcement\nlearning with double q-learning. Proceedings of the Thirtieth\nAAAI Conference on Artificial Intelligence, 2094–2100.\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2013). The elements\nof statistical learning: Data mining, inference, and\nprediction. Springer Science & Business Media. https://books.google.com?id=yPfZBwAAQBAJ\n\n\nHausknecht, M., & Stone, P. (2017, January 11). Deep recurrent\nq-learning for partially observable mdps. https://doi.org/10.48550/arXiv.1507.06527\n\n\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G.,\nDabney, W., Horgan, D., Piot, B., Azar, M., & Silver, D. (2018).\nRainbow: Combining improvements in deep reinforcement learning.\nProceedings of the Thirty-Second AAAI Conference on\nArtificial Intelligence and Thirtieth Innovative\nApplications of Artificial Intelligence Conference\nand Eighth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, 3215–3222.\n\n\nHo, J., & Ermon, S. (2016). Generative adversarial imitation\nlearning. Proceedings of the 30th International\nConference on Neural Information Processing\nSystems, 4572–4580.\n\n\nIvanov, S., & D’yakonov, A. (2019, July 6). Modern deep\nreinforcement learning algorithms. https://doi.org/10.48550/arXiv.1906.10025\n\n\nJames, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J.\n(2023). An introduction to statistical learning: With applications\nin python. Springer International Publishing. https://doi.org/10.1007/978-3-031-38747-0\n\n\nKakade, S. M. (2001). A natural policy gradient. Advances in\nNeural Information Processing Systems, 14. https://proceedings.neurips.cc/paper_files/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html\n\n\nKakade, S., & Langford, J. (2002). Approximately optimal approximate\nreinforcement learning. Proceedings of the Nineteenth\nInternational Conference on Machine Learning,\n267–274.\n\n\nKeviczky, L., Bars, R., Hetthéssy, J., & Bányász, C. (2019).\nControl engineering. Springer. https://doi.org/10.1007/978-981-10-8297-9\n\n\nKochenderfer, M. J., Wheeler, T. A., & Wray, K. H. (2022).\nAlgorithms for decision making. https://mitpress.mit.edu/9780262047012/algorithms-for-decision-making/\n\n\nLadosz, P., Weng, L., Kim, M., & Oh, H. (2022). Exploration in deep\nreinforcement learning: A survey. Inf. Fusion, 85(C),\n1–22. https://doi.org/10.1016/j.inffus.2022.03.003\n\n\nLai, T. L., & Robbins, H. (1985). Asymptotically efficient adaptive\nallocation rules. Advances in Applied Mathematics,\n6(1), 4–22. https://doi.org/10.1016/0196-8858(85)90002-8\n\n\nLambert, N. (2024). Reinforcement learning from human feedback.\nOnline. https://rlhfbook.com\n\n\nLewis, F. L., Vrabie, D. L., & Syrmos, V. L. (2012). Optimal\ncontrol (3rd ed). John Wiley & Sons. https://doi.org/10.1002/9781118122631\n\n\nLi, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A\ncontextual-bandit approach to personalized news article recommendation.\nProceedings of the 19th International Conference on\nWorld Wide Web, 661–670. https://doi.org/10.1145/1772690.1772758\n\n\nLi, S. E. (2023). Reinforcement learning for sequential decision and\noptimal control. Springer Nature. https://doi.org/10.1007/978-981-19-7784-8\n\n\nLi, Y. (2018). Deep reinforcement learning. https://doi.org/10.48550/arXiv.1810.06339\n\n\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement\nlearning, planning and teaching. Machine Learning,\n8(3), 293–321. https://doi.org/10.1007/BF00992699\n\n\nLjapunov, A. M., & Fuller, A. T. (1992). The general problem of\nthe stability of motion. Taylor & Francis.\n\n\nLyapunov, A. M. (1892). The general problem of the stability of\nmotion. University of Kharkov.\n\n\nMacFarlane, A. (1979). The development of frequency-response methods in\nautomatic control [perspectives]. IEEE Transactions on Automatic\nControl, 24(2), 250–265. IEEE Transactions on\nAutomatic Control. https://doi.org/10.1109/TAC.1979.1101978\n\n\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht,\nM., & Bowling, M. (2018). Revisiting the arcade learning\nenvironment: Evaluation protocols and open problems for\ngeneral agents. Journal of Artificial Intelligence Research,\n61, 523–562. https://doi.org/10.1613/jair.5699\n\n\nMaei, H., Szepesvári, C., Bhatnagar, S., Precup, D., Silver, D., &\nSutton, R. S. (2009). Convergent temporal-difference learning with\narbitrary smooth function approximation. Advances in Neural\nInformation Processing Systems, 22. https://papers.nips.cc/paper_files/paper/2009/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html\n\n\nMahadevan, S., Giguere, S., & Jacek, N. (2013). Basis adaptation for\nsparse nonlinear reinforcement learning. Proceedings of the AAAI\nConference on Artificial Intelligence, 27(1, 1), 654–660.\nhttps://doi.org/10.1609/aaai.v27i1.8665\n\n\nMahadevan, S., & Liu, B. (2012). Sparse q-learning with mirror\ndescent. Proceedings of the Twenty-Eighth Conference on\nUncertainty in Artificial Intelligence,\n564–573.\n\n\nMannor, S., Mansour, Y., & Tamar, A. (2024). Reinforcement\nlearning: foundations. https://sites.google.com/view/rlfoundations/home\n\n\nMarbach, P., & Tsitsiklis, J. N. (2001). Simulation-based\noptimization of markov reward processes. IEEE Transactions on\nAutomatic Control, 46(2), 191–209. IEEE\nTransactions on Automatic Control. https://doi.org/10.1109/9.905687\n\n\nMartens, J., & Grosse, R. (2015). Optimizing neural networks with\nkronecker-factored approximate curvature. Proceedings of the 32nd\nInternational Conference on Machine\nLearning, 2408–2417. https://proceedings.mlr.press/v37/martens15.html\n\n\nMaxwell, J. C. (1867). On governors. Proceedings of the Royal\nSociety of London, 16, 270–283. https://www.jstor.org/stable/112510\n\n\nMayr, E. (1970). Populations, species and evolution: An\nabridgment of animal species and evolution. Belknap Press of\nHarvard University Press.\n\n\nMeurer, A., Smith, C. P., Paprocki, M., Čertík, O., Kirpichev, S. B.,\nRocklin, M., Kumar, A., Ivanov, S., Moore, J. K., Singh, S., Rathnayake,\nT., Vig, S., Granger, B. E., Muller, R. P., Bonazzi, F., Gupta, H.,\nVats, S., Johansson, F., Pedregosa, F., … Scopatz, A. (2017).\nSymPy: Symbolic computing in python. PeerJ Computer\nScience, 3, e103. https://doi.org/10.7717/peerj-cs.103\n\n\nMeyer, J.-A., & Wilson, S. W. (1991). A possibility for implementing\ncuriosity and boredom in model-building neural controllers. In From\nAnimals to Animats: Proceedings\nof the First International Conference on\nSimulation of Adaptive Behavior (pp.\n222–227). From Animals to Animats:\nProceedings of the First International\nConference on Simulation of Adaptive\nBehavior. MIT Press. https://ieeexplore.ieee.org/document/6294131\n\n\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley,\nT., Silver, D., & Kavukcuoglu, K. (2016). Asynchronous methods for\ndeep reinforcement learning. Proceedings of The 33rd\nInternational Conference on Machine\nLearning, 1928–1937. https://proceedings.mlr.press/v48/mniha16.html\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I.,\nWierstra, D., & Riedmiller, M. A. (2013). Playing atari with deep\nreinforcement learning. CoRR, abs/1312.5602. http://arxiv.org/abs/1312.5602\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., & Bellemare, M. G.\n(2016). Safe and efficient off-policy reinforcement learning.\nProceedings of the 30th International Conference on\nNeural Information Processing Systems, 1054–1062.\n\n\nMurphy, K. (2025, March 24). Reinforcement learning: A comprehensive\noverview. https://doi.org/10.48550/arXiv.2412.05265\n\n\nNegative Space. (2015). Photo of commercial district during\ndawn [Graphic]. https://www.pexels.com/photo/photo-of-commercial-district-during-dawn-34639/\n\n\nNemirovskij, A. S., Judin, D. B., Dawson, E. R., & Nemirovskij, A.\nS. (1983). Problem complexity and method efficiency in\noptimization. Wiley.\n\n\nNg, A. Y., & Russell, S. J. (2000). Algorithms for inverse\nreinforcement learning. Proceedings of the Seventeenth\nInternational Conference on Machine Learning,\n663–670. https://ai.stanford.edu/~ang/papers/icml00-irl.pdf\n\n\nNielsen, M. A. (2015). Neural networks and deep learning.\nDetermination Press. http://neuralnetworksanddeeplearning.com/\n\n\nNocedal, J., & Wright, S. J. (2006). Numerical optimization\n(2nd ed). Springer.\n\n\nOpenAI. (2022, November 30). Introducing ChatGPT.\nOpenAI News. https://openai.com/index/chatgpt/\n\n\nOrsini, M., Raichuk, A., Hussenot, L., Vincent, D., Dadashi, R., Girgin,\nS., Geist, M., Bachem, O., Pietquin, O., & Andrychowicz, M. (2021).\nWhat matters for adversarial imitation learning? Proceedings of the\n35th International Conference on Neural Information\nProcessing Systems, 14656–14668.\n\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin,\nP., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,\nJ., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P.,\nChristiano, P., Leike, J., & Lowe, R. (2022). Training language\nmodels to follow instructions with human feedback. Proceedings of\nthe 36th International Conference on Neural\nInformation Processing Systems, 27730–27744.\n\n\nPeters, J., & Schaal, S. (2008). Natural actor-critic.\nNeurocomputing, 71(7), 1180–1190. https://doi.org/10.1016/j.neucom.2007.11.026\n\n\nPeters, J., Vijayakumar, S., & Schaal, S. (2005). Natural\nactor-critic. In J. Gama, R. Camacho, P. B. Brazdil, A. M. Jorge, &\nL. Torgo (Eds.), Machine Learning: ECML\n2005 (pp. 280–291). Springer. https://doi.org/10.1007/11564096_29\n\n\nPiot, B., Geist, M., & Pietquin, O. (2017). Bridging the gap between\nimitation learning and inverse reinforcement learning. IEEE\nTransactions on Neural Networks and Learning Systems,\n28(8), 1814–1826. IEEE Transactions on\nNeural Networks and Learning Systems. https://doi.org/10.1109/TNNLS.2016.2543000\n\n\nPixabay. (2016a). 20 mg label blister pack [Graphic]. https://www.pexels.com/photo/20-mg-label-blister-pack-208512/\n\n\nPixabay. (2016b). Coins on brown wood [Graphic]. https://www.pexels.com/photo/coins-on-brown-wood-210600/\n\n\nPlaat, A. (2022). Deep reinforcement learning. Springer Nature.\nhttps://doi.org/10.1007/978-981-19-0638-1\n\n\nPomerleau, D. A. (1991). Efficient training of artificial neural\nnetworks for autonomous navigation. Neural Computation,\n3(1), 88–97. Neural Computation. https://doi.org/10.1162/neco.1991.3.1.88\n\n\nPowell, W. B. (2022). Reinforcement learning and stochastic\noptimization: A unified framework for sequential decisions. Wiley.\n\n\nPuterman, M. L. (1994). Markov decision processes: Discrete\nstochastic dynamic programming. Wiley.\n\n\nRamachandran, D., & Amir, E. (2007). Bayesian inverse reinforcement\nlearning. Proceedings of the 20th International Joint Conference on\nArtifical Intelligence, 2586–2591.\n\n\nRao, A., & Jelvis, T. (2022). Foundations of reinforcement\nlearning with applications in finance. Chapman and\nHall/CRC. https://doi.org/10.1201/9781003229193\n\n\nRatliff, N. D., Bagnell, J. A., & Zinkevich, M. A. (2006). Maximum\nmargin planning. Proceedings of the 23rd International Conference on\nMachine Learning, 729–736. https://doi.org/10.1145/1143844.1143936\n\n\nRobbins, H. (1952). Some aspects of the sequential design of\nexperiments. Bulletin of the American Mathematical Society,\n58(5), 527–535. https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society/volume-58/issue-5/Some-aspects-of-the-sequential-design-of-experiments/bams/1183517370.full\n\n\nRoss, S., Gordon, G. J., & Bagnell, J. (2010, November 2). A\nreduction of imitation learning and structured prediction to no-regret\nonline learning. International Conference on\nArtificial Intelligence and Statistics. https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e\n\n\nRussell, S. (1998). Learning agents for uncertain environments (extended\nabstract). Proceedings of the Eleventh Annual Conference on\nComputational Learning Theory, 101–103. https://doi.org/10.1145/279943.279964\n\n\nRussell, S. J., & Norvig, P. (2021). Artificial intelligence: A\nmodern approach (Fourth edition). Pearson.\n\n\nSchmidhuber, J. (1991). Curious model-building control systems.\n[Proceedings] 1991 IEEE International Joint\nConference on Neural Networks, 1458–1463 vol.2.\nhttps://doi.org/10.1109/IJCNN.1991.170605\n\n\nSchmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic\nmotivation (1990–2010). IEEE Transactions on Autonomous Mental\nDevelopment, 2(3), 230–247. IEEE Transactions\non Autonomous Mental Development. https://doi.org/10.1109/TAMD.2010.2056368\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,\nSchmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T.,\nLillicrap, T., & Silver, D. (2020). Mastering atari, go, chess and\nshogi by planning with a learned model. Nature,\n588(7839, 7839), 604–609. https://doi.org/10.1038/s41586-020-03051-4\n\n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P.\n(2015). Trust region policy optimization. Proceedings of the 32nd\nInternational Conference on Machine Learning, 1889–1897. https://proceedings.mlr.press/v37/schulman15.html\n\n\nSchulman, J., Moritz, P., Levine, S., Jordan, M. I., & Abbeel, P.\n(2016). High-dimensional continuous control using generalized advantage\nestimation. In Y. Bengio & Y. LeCun (Eds.), 4th international\nconference on learning representations. http://arxiv.org/abs/1506.02438\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O.\n(2017, August 28). Proximal policy optimization algorithms. https://doi.org/10.48550/arXiv.1707.06347\n\n\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A neural substrate\nof prediction and reward. Science, 275(5306),\n1593–1599. https://doi.org/10.1126/science.275.5306.1593\n\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang,\nM., Li, Y. K., Wu, Y., & Guo, D. (2024, April 27).\nDeepSeekMath: Pushing the limits of\nmathematical reasoning in open language models. https://doi.org/10.48550/arXiv.2402.03300\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,\nLanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N.,\nSutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\n& Hassabis, D. (2016). Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529(7587, 7587),\n484–489. https://doi.org/10.1038/nature16961\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M.,\nGuez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT., Simonyan, K., & Hassabis, D. (2018). A general reinforcement\nlearning algorithm that masters chess, shogi, and go through self-play.\nScience, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404\n\n\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., &\nRiedmiller, M. (2014). Deterministic policy gradient algorithms.\nProceedings of the 31st International Conference on\nMachine Learning, 387–395. https://proceedings.mlr.press/v32/silver14.html\n\n\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A.,\nGuez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y.,\nLillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T.,\n& Hassabis, D. (2017). Mastering the game of go without human\nknowledge. Nature, 550(7676, 7676), 354–359. https://doi.org/10.1038/nature24270\n\n\nSilver, D., Singh, S., Precup, D., & Sutton, R. S. (2021). Reward is\nenough. Artificial Intelligence, 299, 103535. https://doi.org/10.1016/j.artint.2021.103535\n\n\nStigler, S. M. (2003). The history of statistics: The measurement of\nuncertainty before 1900 (9. print). Belknap Pr. of Harvard Univ.\nPr.\n\n\nSussman, G. J., Wisdom, J., & Farr, W. (2013). Functional\ndifferential geometry. The MIT Press.\n\n\nSutton, R. S. (1984). Temporal credit assignment in reinforcement\nlearning [PhD thesis]. University of Massachusetts Amherst.\n\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal\ndifferences. Machine Learning, 3(1), 9–44. https://doi.org/10.1007/BF00115009\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An\nintroduction (Second edition). The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf\n\n\nSutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999).\nPolicy gradient methods for reinforcement learning with function\napproximation. Proceedings of the 13th International\nConference on Neural Information Processing\nSystems, 1057–1063.\n\n\nSzepesvári, C. (2010). Algorithms for reinforcement learning.\nSpringer International Publishing. https://doi.org/10.1007/978-3-031-01551-9\n\n\nTang, H., Houthooft, R., Foote, D., Stooke, A., Xi Chen, O., Duan, Y.,\nSchulman, J., DeTurck, F., & Abbeel, P. (2017).\n#Exploration: A study of count-based exploration for deep\nreinforcement learning. Advances in Neural Information\nProcessing Systems, 30. https://proceedings.neurips.cc/paper_files/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html\n\n\nThompson, W. R. (1933). On the likelihood that one unknown probability\nexceeds another in view of the evidence of two samples.\nBiometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286\n\n\nThompson, W. R. (1935). On the theory of apportionment. American\nJournal of Mathematics, 57(2), 450–456. https://doi.org/10.2307/2371219\n\n\nThorndike, E. L. (1911). Animal intelligence:\nExperimental studies (pp. viii, 297). Macmillan Press.\nhttps://doi.org/10.5962/bhl.title.55072\n\n\nThrun, S. B. (1992). Efficient exploration in reinforcement\nlearning [Technical Report]. Carnegie Mellon University.\n\n\nTuring, A. (1948). Intelligent machinery. National Physical\nLaboratory. https://weightagnostic.github.io/papers/turing1948.pdf\n\n\nvan Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., &\nModayil, J. (2018, December 6). Deep reinforcement learning and the\ndeadly triad. https://doi.org/10.48550/arXiv.1812.02648\n\n\nVapnik, V. N. (2000). The nature of statistical learning\ntheory. Springer. https://doi.org/10.1007/978-1-4757-3264-1\n\n\nVershynin, R. (2018). High-dimensional probability: An\nintroduction with applications in data science. Cambridge\nUniversity Press. https://books.google.com?id=NDdqDwAAQBAJ\n\n\nWald, A. (1949). Statistical decision functions. The Annals of\nMathematical Statistics, 20(2), 165–205. https://doi.org/10.1214/aoms/1177730030\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K.,\n& Freitas, N. de. (2017, February 6). Sample efficient actor-critic\nwith experience replay. 5th International Conference on Learning\nRepresentations. https://openreview.net/forum?id=HyM25Mqel\n\n\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., &\nFreitas, N. (2016). Dueling network architectures for deep reinforcement\nlearning. Proceedings of The 33rd International\nConference on Machine Learning, 1995–2003. https://proceedings.mlr.press/v48/wangf16.html\n\n\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms\nfor connectionist reinforcement learning. Machine Learning,\n8(3), 229–256. https://doi.org/10.1007/BF00992696\n\n\nWitten, I. H. (1977). An adaptive optimal controller for discrete-time\nmarkov environments. Information and Control, 34(4),\n286–295. https://doi.org/10.1016/S0019-9958(77)90354-0\n\n\nWu, Y., Mansimov, E., Grosse, R. B., Liao, S., & Ba, J. (2017).\nScalable trust-region method for deep reinforcement learning using\nkronecker-factored approximation. Advances in Neural Information\nProcessing Systems, 30. https://proceedings.neurips.cc/paper/2017/hash/361440528766bbaaaa1901845cf4152b-Abstract.html\n\n\nYe, W., Liu, S., Kurutach, T., Abbeel, P., & Gao, Y. (2021).\nMastering atari games with limited data. NeurIPS\n2021, 25476–25488. https://doi.org/10.48550/arXiv.2111.00210\n\n\nZare, M., Kebria, P. M., Khosravi, A., & Nahavandi, S. (2024). A\nsurvey of imitation learning: Algorithms, recent developments, and\nchallenges. IEEE Transactions on Cybernetics, 54(12),\n7173–7186. IEEE Transactions on Cybernetics.\nhttps://doi.org/10.1109/TCYB.2024.3395626\n\n\nZiebart, B. D., Bagnell, J. A., & Dey, A. K. (2010). Modeling\ninteraction via the principle of maximum causal entropy. Proceedings\nof the 27th International Conference on International\nConference on Machine Learning, 1255–1262.\n\n\nZiebart, B. D., Maas, A., Bagnell, J. A., & Dey, A. K. (2008).\nMaximum entropy inverse reinforcement learning. Proceedings of the\n23rd National Conference on Artificial Intelligence -\nVolume 3, 1433–1438.\n\n\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei,\nD., Christiano, P., & Irving, G. (2020, January 8).\nFine-Tuning Language Models from Human\nPreferences. https://doi.org/10.48550/arXiv.1909.08593",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "App. A: Background",
    "section": "",
    "text": "A.1 O notation\nThroughout this chapter and the rest of the book, we will describe the asymptotic behaviour of a function using \\(O\\) notation.\nFor two functions \\(f(t)\\) and \\(g(t)\\), we say that \\(f(t) \\le O(g(t))\\) if \\(f\\) is asymptotically upper bounded by \\(g\\). Formally, this means that there exists some constant \\(C &gt; 0\\) such that \\(f(t) \\le C \\cdot g(t)\\) for all \\(t\\) past some point \\(t_0\\).\nWe say \\(f(t) &lt; o(g(t))\\) if asymptotically \\(f\\) grows strictly slower than \\(g\\). Formally, this means that for any scalar \\(C &gt; 0\\), there exists some \\(t_0\\) such that \\(f(t) \\le C \\cdot g(t)\\) for all \\(t &gt; t_0\\). Equivalently, we say \\(f(t) &lt; o(g(t))\\) if \\(\\lim_{t \\to \\infty} f(t)/g(t) = 0\\).\n\\(f(t) = \\Theta(g(t))\\) means that \\(f\\) and \\(g\\) grow at the same rate asymptotically. That is, \\(f(t) \\le O(g(t))\\) and \\(g(t) \\le O(f(t))\\).\nFinally, we use \\(f(t) \\ge \\Omega(g(t))\\) to mean that \\(g(t) \\le O(f(t))\\), and \\(f(t) &gt; \\omega(g(t))\\) to mean that \\(g(t) &lt; o(f(t))\\).\nWe also use the notation \\(\\widetilde O(g(t))\\) to hide logarithmic factors. That is, \\(f(t) = \\widetilde O(g(t))\\) if there exists some constant \\(C\\) such that \\(f(t) \\le C \\cdot g(t) \\cdot \\log^k(t)\\) for some \\(k\\) and all \\(t\\).\nOccasionally, we will also use \\(O(f(t))\\) (or one of the other symbols) as shorthand to manipulate function classes. For example, we might write \\(O(f(t)) + O(g(t)) = O(f(t) + g(t))\\) to mean that the sum of two functions in \\(O(f(t))\\) and \\(O(g(t))\\) is in \\(O(f(t) + g(t))\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-bg-union-bound",
    "href": "background.html#sec-bg-union-bound",
    "title": "App. A: Background",
    "section": "A.2 Union bound",
    "text": "A.2 Union bound\n\nTheorem A.1 (Union bound) Consider a set of events \\(A_0, \\dots, A_{N-1}\\). Then\n\\[\n\\mathbb{P}\\left(\n    \\bigcup_{n=0}^{N-1} A_n\n\\right) \\le \\sum_{n=0}^{N-1} \\mathbb{P}(A_n).\n\\tag{A.1}\\]\nIn particular, if \\(\\mathbb{P}(A_n) \\ge 1 - \\delta\\) for each \\(n \\in [N]\\), we have\n\\[\n\\mathbb{P}\\left(\n    \\bigcap_{n=0}^{N-1} A_n\n\\right) \\ge 1 - N \\delta.\n\\tag{A.2}\\]\n\nIn other words, if each event \\(A_n\\) has a small probability \\(\\delta\\) of “failure”, then to get the probability that there are any failures out of all \\(N\\) events, we multiply the failure probability by \\(N\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "proofs.html",
    "href": "proofs.html",
    "title": "App. B: Proofs",
    "section": "",
    "text": "B.1 LQR proof\nBase case:\nAt the final timestep, there are no possible actions to take, and so \\(V^\\star_H(x) = c(x) = x^\\top Q x\\). Thus \\(V_H^\\star(x) = x^\\top P_Hx+ p_H\\) where \\(P_H= Q\\) and \\(p_H= 0\\).\nInductive hypothesis:\nWe seek to show that the inductive step holds for both theorems: If \\(V^\\star_{h+1}(x)\\) is an upward-curved quadratic, then \\(V^\\star_h(x)\\) must also be an upward-curved quadratic, and \\(\\pi^\\star_h(x)\\) must be linear. We’ll break this down into the following steps:\nWe first assume the inductive hypothesis that our theorems are true at time \\(h+1\\). That is,\n\\[\nV^\\star_{h+1}(x) = x^\\top P_{h+1} x+ p_{h+1} \\quad \\forall x\\in \\mathcal{S}.\n\\]\nNow we’ve shown that \\(V^\\star_h(x) = x^\\top P_hx+ p_h\\), where \\(P_h\\) is sym. p.d., proving the inductive hypothesis and completing the proof of Theorem 3.3 and Theorem 3.2.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "proofs.html#sec-proof-lqr",
    "href": "proofs.html#sec-proof-lqr",
    "title": "App. B: Proofs",
    "section": "",
    "text": "We’ll compute \\(V_H^\\star\\) (at the end of the horizon) as our base case.\nThen we’ll work step-by-step backwards in time, using \\(V_{h+1}^\\star\\) to compute \\(Q_h^\\star\\), \\(\\pi_{h}^\\star\\), and \\(V_h^\\star\\).\n\n\n\n\n\n\nShow that \\(Q^\\star_h(x, u)\\) is an upward-curved quadratic (in both \\(x\\) and \\(u\\)).\nDerive the optimal policy \\(\\pi^\\star_h(x) = \\arg \\min_uQ^\\star_h(x, u)\\) and show that it’s linear.\nShow that \\(V^\\star_h(x)\\) is an upward-curved quadratic.\n\n\n\n\nLemma B.1 (\\(Q^\\star_h(x, u)\\) is an upward-curved quadratic) Let us decompose \\(Q^\\star_h: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) into the immediate reward plus the expected cost-to-go:\n\\[\nQ^\\star_h(x, u) = c(x, u) + \\mathop{\\mathbb{E}}_{x' \\sim f(x, u, w_{h+1})} [V^\\star_{h+1}(x')].\n\\]\nRecall \\(c(x, u) := x^\\top Q x+ u^\\top R u\\). Let’s consider the expectation over the next timestep. The only randomness in the dynamics comes from the noise \\(w_{h+1} \\sim \\mathcal{N}(0, \\sigma^2 I)\\), so we can expand the expectation as:\n\\[\n\\begin{aligned}\n            & \\mathop{\\mathbb{E}}_{x'} [V^\\star_{h+1}(x')]                                                                                                         \\\\\n    {} = {} & \\mathop{\\mathbb{E}}_{w_{h+1}} [V^\\star_{h+1}(A x+ B u+ w_{h+1})]                                             &  & \\text{definition of } f     \\\\\n    {} = {} & \\mathop{\\mathbb{E}}_{w_{h+1}} [ (A x+ B u+ w_{h+1})^\\top P_{h+1} (A x+ B u+ w_{h+1}) + p_{h+1} ]. &  & \\text{inductive hypothesis}\n\\end{aligned}\n\\]\nSumming and combining like terms, we get\n\\[\n\\begin{aligned}\n    Q^\\star_h(x, u) & = x^\\top Q x+ u^\\top R u+ \\mathop{\\mathbb{E}}_{w_{h+1}} [(A x+ B u+ w_{h+1})^\\top P_{h+1} (A x+ B u+ w_{h+1}) + p_{h+1}] \\\\\n                           & = x^\\top (Q + A^\\top P_{h+1} A)x+ u^\\top (R + B^\\top P_{h+1} B) u+ 2 x^\\top A^\\top P_{h+1} B u\\\\\n                           & \\qquad + \\mathop{\\mathbb{E}}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] + p_{h+1}.\n\\end{aligned}\n\\]\nNote that the terms that are linear in \\(w_h\\) have mean zero and vanish. Now consider the remaining expectation over the noise. By expanding out the product and using linearity of expectation, we can write this out as\n\\[\n\\begin{aligned}\n    \\mathop{\\mathbb{E}}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] & = \\sum_{i=1}^d \\sum_{j=1}^d (P_{h+1})_{ij} \\mathop{\\mathbb{E}}_{w_{h+1}} [(w_{h+1})_i (w_{h+1})_j] \\\\\n    & = \\sigma^2 \\mathrm{Tr}(P_{h+ 1})\n\\end{aligned}\n\\]\n\nB.1.0.1 Quadratic forms\nWhen solving quadratic forms, i.e. expressions of the form \\(x^\\top A x\\), it’s often helpful to consider the terms on the diagonal (\\(i = j\\)) separately from those off the diagonal.\nIn this case, the expectation of each diagonal term becomes\n\\[\n(P_{h+1})_{ii} \\mathop{\\mathbb{E}}(w_{h+1})_i^2 = \\sigma^2 (P_{h+1})_{ii}.\n\\tag{B.1}\\]\nOff the diagonal, since the elements of \\(w_{h+1}\\) are independent, the expectation factors, and since each element has mean zero, the term vanishes:\n\\[\n(P_{h+1})_{ij} \\mathop{\\mathbb{E}}[(w_{h+1})_i] \\mathop{\\mathbb{E}}[(w_{h+1})_j] = 0.\n\\tag{B.2}\\]\nThus, the only terms left are the ones on the diagonal, so the sum of these can be expressed as the trace of \\(\\sigma^2 P_{h+1}\\):\n\\[\n\\mathop{\\mathbb{E}}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] = \\sigma^2 \\mathrm{Tr}(P_{h+1}).\n\\tag{B.3}\\]\n\nSubstituting this back into the expression for \\(Q^\\star_h\\), we have:\n\\[\n\\begin{aligned}\n    Q^\\star_h(x, u) & = x^\\top (Q + A^\\top P_{h+1} A) x+ u^\\top (R + B^\\top P_{h+1} B) u\n    + 2x^\\top A^\\top P_{h+1} B u\\\\\n                            & \\qquad + \\sigma^2 \\mathrm{Tr}(P_{h+1}) + p_{h+1}.\n\\end{aligned}\n\\tag{B.4}\\]\nAs we hoped, this expression is quadratic in \\(x\\) and \\(u\\). Furthermore, we’d like to show that it also curves upwards with respect to \\(u\\) so that its minimum with respect to \\(u\\) is well-defined. We can do this by noting that the Hessian matrix of second derivatives is positive definite:\n\\[\n\\nabla_{uu} Q_h^\\star(x, u) = R + B^\\top P_{h+1} B\n\\]\nSince \\(R\\) is sym. p.d. (def. 3.5), and \\(P_{h+1}\\) is sym. p.d. (by the inductive hypothesis), this sum must also be sym. p.d., and so \\(Q^\\star_h\\) is indeed an upward-curved quadratic with respect to \\(u\\). (If this isn’t clear, try proving it as an exercise.) The proof of its upward curvature with respect to \\(x\\) is equivalent.\n\n\nLemma B.2 (\\(\\pi^\\star_h\\) is linear) Since \\(Q^\\star_h\\) is an upward-curved quadratic, finding its minimum over \\(u\\) is easy: we simply set the gradient with respect to \\(u\\) equal to zero and solve for \\(u\\). First, we calculate the gradient:\n\\[\n\\begin{aligned}\n    \\nabla_uQ^\\star_h(x, u) & = \\nabla_u[ u^\\top (R + B^\\top P_{h+1} B) u+ 2 x^\\top A^\\top P_{h+1} B u] \\\\\n                                       & = 2 (R + B^\\top P_{h+1} B) u+ 2 (x^\\top A^\\top P_{h+1} B)^\\top\n\\end{aligned}\n\\]\nSetting this to zero, we get\n\\[\n\\begin{aligned}\n    0                  & = (R + B^\\top P_{h+1} B) \\pi^\\star_h(x) + B^\\top P_{h+1} A x\\nonumber \\\\\n    \\pi^\\star_h(x) & = (R + B^\\top P_{h+1} B)^{-1} (-B^\\top P_{h+1} A x) \\nonumber              \\\\\n                       & = - K_hx,\n\\end{aligned}\n\\]\nwhere\n\\[\nK_h= (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A.\n\\tag{B.5}\\]\nNote that this optimal policy doesn’t depend on the starting distribution \\(\\mu_0\\). It’s also fully deterministic and isn’t affected by the noise terms \\(w_0, \\dots, w_{H-1}\\).\n\n\nLemma B.4 (The value function is an upward-curved quadratic) Using the identity \\(V^\\star_h(x) = Q^\\star_h(x, \\pi^\\star(x))\\), we have:\n\\[\n\\begin{aligned}\n    V^\\star_h(x) & = Q^\\star_h(x, \\pi^\\star(x))                                                                \\\\\n                     & = x^\\top (Q + A^\\top P_{h+1} A) x+ (-K_hx)^\\top (R + B^\\top P_{h+1} B) (-K_hx)\n    + 2x^\\top A^\\top P_{h+1} B (-K_hx)                                                                          \\\\\n                     & \\qquad + \\mathrm{Tr}(\\sigma^2 P_{h+1}) + p_{h+1}\n\\end{aligned}\n\\]\nNote that with respect to \\(x\\), this is the sum of a quadratic term and a constant, which is exactly what we were aiming for! The scalar term is clearly\n\\[\np_h= \\mathrm{Tr}(\\sigma^2 P_{h+1}) + p_{h+1}.\n\\tag{B.6}\\]\nWe can simplify the quadratic term by substituting in \\(K_h\\) from eq. B.5. Notice that when we do this, the \\((R+B^\\top P_{h+1} B)\\) term in the expression is cancelled out by its inverse, and the remaining terms combine to give the Riccati equation:\n\\[\nP_h= Q + A^\\top P_{h+1} A - A^\\top P_{h+1} B (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A.\n\\tag{B.7}\\]\nIt remains to prove that \\(V^\\star_h\\) curves upwards, that is, that \\(P_h\\) is sym. p.d. We will use the following fact about Schur complements:\n\nLemma B.3 (Positive definiteness of Schur complements) Let\n\\[\nD = \\begin{pmatrix}\nA & B \\\\\nB^\\top & C\n\\end{pmatrix}\n\\tag{B.8}\\]\nbe a symmetric \\((m+n) \\times (m+n)\\) block matrix, where \\(A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{n \\times n}\\). The Schur complement of \\(A\\) is denoted\n\\[\nD/A = C - B^\\top A^{-1} B.\n\\tag{B.9}\\]\nSchur complements have various uses in linear algebra and numerical computation.\nA useful fact for us is that if \\(A\\) is positive definite, then \\(D\\) is positive semidefinite if and only if \\(D/A\\) is positive semidefinite.\n\nLet \\(P\\) denote \\(P_{h+ 1}\\) for brevity. We already know \\(Q\\) is sym. p.d., so it suffices to show that\n\\[\nS = P - P B (R + B^\\top P B)^{-1} B^\\top P\n\\tag{B.10}\\]\nis p.s.d. (positive semidefinite), since left- and right- multiplying by \\(A^\\top\\) and \\(A\\) respectively preserves p.s.d. We note that \\(S\\) is the Schur complement \\(D/(R + B^\\top P B)\\), where\n\\[\nD = \\begin{pmatrix}\nR + B^\\top P B & B^\\top P \\\\\nP B & P\n\\end{pmatrix}.\n\\]\nThus we must show that \\(D\\) is p.s.d.. This can be seen by computing\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\ny^\\top & z^\\top\n\\end{pmatrix}\nD\n\\begin{pmatrix}\ny \\\\ z\n\\end{pmatrix}\n&= y^\\top R y + y^\\top B^\\top P B y + 2 y^\\top B^\\top P z + z^\\top P z \\\\\n&= y^\\top R y + (By + z)^\\top P (By + z) \\\\\n&&gt; 0.\n\\end{aligned}\n\\]\nSince \\(R + B^\\top P B\\) is sym. p.d. and \\(D\\) is p.s.d., then \\(S = D / (R + B^\\top P B)\\) must be p.s.d., and \\(P_h= Q + A S A^\\top\\) must be sym. p.d.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Proofs</span>"
    ]
  },
  {
    "objectID": "proofs.html#sec-ucbvi-proof",
    "href": "proofs.html#sec-ucbvi-proof",
    "title": "App. B: Proofs",
    "section": "B.2 UCBVI reward bonus proof",
    "text": "B.2 UCBVI reward bonus proof\nWe aim to show that, with high probability,\n\\[\nV_h^\\star(s) \\le \\widehat{V}_h^t(s) \\quad \\forall t \\in [T], h \\in [H], s \\in \\mathcal{S}.\n\\]\nWe’ll do this by bounding the error incurred at each step of DP. Recall that DP solves for \\(\\widehat{V}_h^t(s)\\) recursively as follows:\n\\[\n\\widehat{V}_h^t(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\widetilde r^t_h(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ \\widehat{V}_{h+1}^t(s') \\right] \\right]\n\\]\nwhere \\(\\widetilde r^t_h(s, a) = r_h(s, a) + b_h^t(s, a)\\) is the reward function of our modelled MDP \\(\\widetilde{\\mathcal{M}}^t\\). On the other hand, we know that \\(V^\\star\\) must satisfy\n\\[\nV^\\star_h(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\widetilde r^t_h(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P^?_h(\\cdot \\mid s, a)} [V^\\star_{h+1}(s')] \\right]\n\\]\nso it suffices to bound the difference between the two inner expectations. There are two sources of error:\n\nThe value functions \\(\\widehat{V}^t_{h+1}\\) v.s. \\(V^\\star_{h+1}\\)\nThe transition probabilities \\(\\widehat{P}_h^t\\) v.s. \\(P^?_h\\).\n\nWe can bound these individually, and then combine them by the triangle inequality. For the former, we can simply bound the difference by \\(H\\), assuming that the rewards are within \\([0, 1]\\). Now, all that is left is to bound the error from the transition probabilities:\n\\[\n\\text{error} = \\left| \\mathop{\\mathbb{E}}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] - \\mathop{\\mathbb{E}}_{s' \\sim P^?_h(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]. \\right|\n\\tag{B.11}\\]\nLet us bound this term for a fixed \\(s, a, h, t\\). (Later we can make this uniform across \\(s, a, h, t\\) using the union bound.) Note that expanding out the definition of \\(\\widehat{P}_h^t\\) gives\n\\[\n\\begin{aligned}\n        \\mathop{\\mathbb{E}}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] & = \\sum_{s' \\in \\mathcal{S}} \\frac{N^t_h(s, a, s')}{N^t_h(s, a)} V^\\star_{h+1}(s')                                                     \\\\\n    & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\sum_{s' \\in \\mathcal{S}} \\mathbf{1}\\left\\{ (s_h^i, a_h^i, s_{h+1}^i) = (s, a, s') \\right\\} V^\\star_{h+1}(s') \\\\\n    & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\underbrace{\\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\} V^\\star_{h+1}(s_{h+1}^i)}_{X^i}\n\\end{aligned}\n\\]\nsince the terms where \\(s' \\neq s_{h+1}^i\\) vanish.\nNow, in order to apply Hoeffding’s inequality, we would like to express the second term in eq. B.11 as a sum over \\(t\\) random variables as well. We will do this by redundantly averaging over all desired trajectories (i.e. where we visit state \\(s\\) and action \\(a\\) at time \\(h\\)):\n\\[\n\\begin{aligned}\n        \\mathop{\\mathbb{E}}_{s' \\sim P^?_h(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]\n         & = \\sum_{s' \\in \\mathcal{S}} P^?_h(s' \\mid s, a) V^\\star_{h+1}(s')                                                                              \\\\\n         & = \\sum_{s' \\in \\mathcal{S}} \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\} P^?_h(s' \\mid s, a) V^\\star_{h+1}(s') \\\\\n         & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\mathop{\\mathbb{E}}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i.\n\\end{aligned}\n\\]\nNow we can apply Hoeffding’s inequality to \\(X^i - \\mathop{\\mathbb{E}}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i\\), which is bounded by \\(H\\), to obtain that, with probability at least \\(1-\\delta\\),\n\\[\n\\text{error} = \\left| \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\left(X^i - \\mathop{\\mathbb{E}}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i \\right) \\right| \\le 2 H \\sqrt{\\frac{\\ln(1/\\delta)}{N_h^t(s, a)}}.\n\\]\nApplying a union bound over all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}, t \\in [T], h \\in [H]\\) gives the \\(b_h^t(s, a)\\) term above.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Proofs</span>"
    ]
  }
]